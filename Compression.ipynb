{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "f0a886df",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "f94be421",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mps\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "5fa4979a",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "train_dataset = datasets.MNIST(root='./data', train=True, download=True, transform=transform)\n",
    "test_dataset = datasets.MNIST(root='./data', train=False, download=True, transform=transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "afcd05cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = train_dataset.data.to(device).float() / 255.0\n",
    "train_targets = train_dataset.targets.to(device)\n",
    "\n",
    "test_data = test_dataset.data.to(device).float() / 255.0\n",
    "test_targets = test_dataset.targets.to(device)\n",
    "\n",
    "train_data = train_data.unsqueeze(1)\n",
    "test_data = test_data.unsqueeze(1)\n",
    "\n",
    "def get_batches(data, targets, batch_size):\n",
    "    for i in range(0, len(data), batch_size):\n",
    "        yield data[i:i + batch_size], targets[i:i + batch_size]\n",
    "\n",
    "batch_size = 500\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "b2a1fe5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleConcatRNNCell(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size):\n",
    "        super().__init__()\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.fc = nn.Linear(input_size + hidden_size, hidden_size + 18 + 1)\n",
    "\n",
    "    def forward(self, x_t, h_prev):\n",
    "        combined = torch.cat([x_t, h_prev], dim=1)\n",
    "        h_t = F.tanh(self.fc(combined))\n",
    "        return h_t[:, :5], h_t[:, 5:5+18], h_t[:, 5+18]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1211edf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ExperimentalModel(nn.Module):\n",
    "    def __init__(self, input_size=784, hidden_size=5):\n",
    "        super().__init__()\n",
    "        self.rnn_cell = SimpleConcatRNNCell(input_size, hidden_size)\n",
    "\n",
    "        self.x0 = nn.Parameter(torch.zeros(1, 28, 28))\n",
    "        self.h0 = nn.Parameter(torch.randn(1, hidden_size))\n",
    "\n",
    "        self.fc = nn.Linear(28 * 28, 10)\n",
    "\n",
    "    def forward(self, x): # (B, 1, 28, 28)\n",
    "        B, H, W = x.size(0), x.size(2), x.size(3)\n",
    "\n",
    "        x_t = self.x0.expand(B, 1, H, W)\n",
    "        h_t = self.h0.expand(B, -1)\n",
    "\n",
    "        for _ in range(4):\n",
    "            x_t_flat = x_t.view(B, -1)\n",
    "            h_t, weight, bias = self.rnn_cell(x_t_flat, torch.zeros_like(h_t))\n",
    "\n",
    "            x_cat = torch.cat([x_t, x], dim=1)\n",
    "            weight = weight.view(B, 1, 2, 3, 3)\n",
    "\n",
    "            x_cat = x_cat.view(1, B * 2, H, W)\n",
    "            weight = weight.view(B, 2, 3, 3)\n",
    "\n",
    "            x_t = F.conv2d(x_cat, weight, bias=bias, padding=1, stride=1, groups=B)\n",
    "            x_t = x_t.view(B, 1, H, W)\n",
    "\n",
    "        return self.fc(x_t.view(B, -1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d812474e",
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.001 * 1\n",
    "epochs = 10000\n",
    "\n",
    "device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "model = ExperimentalModel().to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7ebaf5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x0: 784 params, requires_grad=True\n",
      "h0: 5 params, requires_grad=True\n",
      "rnn_cell.fc.weight: 18936 params, requires_grad=True\n",
      "rnn_cell.fc.bias: 24 params, requires_grad=True\n",
      "fc.weight: 7840 params, requires_grad=True\n",
      "fc.bias: 10 params, requires_grad=True\n",
      "\n",
      "27599\n"
     ]
    }
   ],
   "source": [
    "for name, param in model.named_parameters():\n",
    "    print(f\"{name}: {param.numel()} params, requires_grad={param.requires_grad}\")\n",
    "\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "print()\n",
    "print(total_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "22925496",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/1000], Training Loss: 0.6865\n",
      "Epoch [1/1000], Validation Loss: 0.2616, Validation Accuracy: 92.33%\n",
      "Output Summary: Max=30.9226, Min=-30.8429, Median=-0.2518, Mean=-0.1578\n",
      "\n",
      "Epoch [2/1000], Training Loss: 0.2617\n",
      "Epoch [2/1000], Validation Loss: 0.2114, Validation Accuracy: 94.03%\n",
      "Output Summary: Max=32.3505, Min=-28.7160, Median=-0.2892, Mean=-0.3914\n",
      "\n",
      "Epoch [3/1000], Training Loss: 0.2146\n",
      "Epoch [3/1000], Validation Loss: 0.2080, Validation Accuracy: 94.21%\n",
      "Output Summary: Max=39.9824, Min=-38.4827, Median=-0.2550, Mean=-0.3853\n",
      "\n",
      "Epoch [4/1000], Training Loss: 0.1920\n",
      "Epoch [4/1000], Validation Loss: 0.1862, Validation Accuracy: 94.66%\n",
      "Output Summary: Max=37.9198, Min=-43.8055, Median=-0.2580, Mean=-0.4008\n",
      "\n",
      "Epoch [5/1000], Training Loss: 0.1784\n",
      "Epoch [5/1000], Validation Loss: 0.1769, Validation Accuracy: 94.84%\n",
      "Output Summary: Max=40.2884, Min=-43.3893, Median=-0.3838, Mean=-0.5492\n",
      "\n",
      "Epoch [6/1000], Training Loss: 0.1670\n",
      "Epoch [6/1000], Validation Loss: 0.1741, Validation Accuracy: 94.95%\n",
      "Output Summary: Max=48.3778, Min=-46.0507, Median=-0.3891, Mean=-0.5344\n",
      "\n",
      "Epoch [7/1000], Training Loss: 0.1601\n",
      "Epoch [7/1000], Validation Loss: 0.1761, Validation Accuracy: 95.05%\n",
      "Output Summary: Max=47.7407, Min=-49.5944, Median=-0.4521, Mean=-0.6501\n",
      "\n",
      "Epoch [8/1000], Training Loss: 0.1583\n",
      "Epoch [8/1000], Validation Loss: 0.1778, Validation Accuracy: 94.90%\n",
      "Output Summary: Max=50.1898, Min=-51.7327, Median=-0.7378, Mean=-1.0442\n",
      "\n",
      "Epoch [9/1000], Training Loss: 0.1495\n",
      "Epoch [9/1000], Validation Loss: 0.1864, Validation Accuracy: 94.81%\n",
      "Output Summary: Max=44.1290, Min=-46.7939, Median=-0.7503, Mean=-1.1139\n",
      "\n",
      "Epoch [10/1000], Training Loss: 0.1486\n",
      "Epoch [10/1000], Validation Loss: 0.1835, Validation Accuracy: 95.01%\n",
      "Output Summary: Max=51.1683, Min=-63.4550, Median=-0.7629, Mean=-1.2582\n",
      "\n",
      "Epoch [11/1000], Training Loss: 0.1449\n",
      "Epoch [11/1000], Validation Loss: 0.1831, Validation Accuracy: 95.21%\n",
      "Output Summary: Max=56.6836, Min=-63.6820, Median=-0.7180, Mean=-1.0630\n",
      "\n",
      "Epoch [12/1000], Training Loss: 0.1422\n",
      "Epoch [12/1000], Validation Loss: 0.1787, Validation Accuracy: 95.19%\n",
      "Output Summary: Max=43.9043, Min=-49.2970, Median=-0.7270, Mean=-1.0809\n",
      "\n",
      "Epoch [13/1000], Training Loss: 0.1363\n",
      "Epoch [13/1000], Validation Loss: 0.1797, Validation Accuracy: 95.22%\n",
      "Output Summary: Max=60.4956, Min=-67.8213, Median=-1.0569, Mean=-1.6157\n",
      "\n",
      "Epoch [14/1000], Training Loss: 0.1383\n",
      "Epoch [14/1000], Validation Loss: 0.1765, Validation Accuracy: 95.13%\n",
      "Output Summary: Max=48.0745, Min=-50.7553, Median=-0.8623, Mean=-1.2208\n",
      "\n",
      "Epoch [15/1000], Training Loss: 0.1316\n",
      "Epoch [15/1000], Validation Loss: 0.1808, Validation Accuracy: 95.08%\n",
      "Output Summary: Max=49.1009, Min=-46.8402, Median=-0.5376, Mean=-0.8412\n",
      "\n",
      "Epoch [16/1000], Training Loss: 0.1270\n",
      "Epoch [16/1000], Validation Loss: 0.1786, Validation Accuracy: 95.12%\n",
      "Output Summary: Max=47.4150, Min=-57.6064, Median=-0.6607, Mean=-1.0596\n",
      "\n",
      "Epoch [17/1000], Training Loss: 0.1257\n",
      "Epoch [17/1000], Validation Loss: 0.2100, Validation Accuracy: 94.54%\n",
      "Output Summary: Max=53.2190, Min=-60.5147, Median=-0.5491, Mean=-0.9828\n",
      "\n",
      "Epoch [18/1000], Training Loss: 0.1313\n",
      "Epoch [18/1000], Validation Loss: 0.1735, Validation Accuracy: 95.25%\n",
      "Output Summary: Max=41.8741, Min=-47.8448, Median=-0.6938, Mean=-1.0882\n",
      "\n",
      "Epoch [19/1000], Training Loss: 0.1337\n",
      "Epoch [19/1000], Validation Loss: 0.1833, Validation Accuracy: 95.21%\n",
      "Output Summary: Max=56.1618, Min=-56.5529, Median=-0.6994, Mean=-1.1546\n",
      "\n",
      "Epoch [20/1000], Training Loss: 0.1305\n",
      "Epoch [20/1000], Validation Loss: 0.1633, Validation Accuracy: 95.55%\n",
      "Output Summary: Max=46.1304, Min=-54.3434, Median=-1.1454, Mean=-1.6410\n",
      "\n",
      "Epoch [21/1000], Training Loss: 0.1227\n",
      "Epoch [21/1000], Validation Loss: 0.1666, Validation Accuracy: 95.25%\n",
      "Output Summary: Max=46.7506, Min=-47.6252, Median=-1.1786, Mean=-1.6047\n",
      "\n",
      "Epoch [22/1000], Training Loss: 0.1163\n",
      "Epoch [22/1000], Validation Loss: 0.1748, Validation Accuracy: 95.31%\n",
      "Output Summary: Max=45.9911, Min=-54.4757, Median=-1.2885, Mean=-1.8246\n",
      "\n",
      "Epoch [23/1000], Training Loss: 0.1136\n",
      "Epoch [23/1000], Validation Loss: 0.1760, Validation Accuracy: 95.21%\n",
      "Output Summary: Max=52.4283, Min=-53.9264, Median=-1.0541, Mean=-1.5958\n",
      "\n",
      "Epoch [24/1000], Training Loss: 0.1106\n",
      "Epoch [24/1000], Validation Loss: 0.1788, Validation Accuracy: 95.28%\n",
      "Output Summary: Max=51.6311, Min=-61.5424, Median=-1.3711, Mean=-2.0691\n",
      "\n",
      "Epoch [25/1000], Training Loss: 0.1016\n",
      "Epoch [25/1000], Validation Loss: 0.1756, Validation Accuracy: 95.48%\n",
      "Output Summary: Max=54.9784, Min=-66.4751, Median=-1.6319, Mean=-2.3052\n",
      "\n",
      "Epoch [26/1000], Training Loss: 0.0998\n",
      "Epoch [26/1000], Validation Loss: 0.1676, Validation Accuracy: 95.66%\n",
      "Output Summary: Max=57.6376, Min=-66.2876, Median=-1.2646, Mean=-1.9622\n",
      "\n",
      "Epoch [27/1000], Training Loss: 0.0942\n",
      "Epoch [27/1000], Validation Loss: 0.1717, Validation Accuracy: 95.46%\n",
      "Output Summary: Max=58.4647, Min=-58.1586, Median=-1.6469, Mean=-2.1678\n",
      "\n",
      "Epoch [28/1000], Training Loss: 0.0901\n",
      "Epoch [28/1000], Validation Loss: 0.1676, Validation Accuracy: 95.52%\n",
      "Output Summary: Max=56.7987, Min=-58.2460, Median=-1.9056, Mean=-2.5703\n",
      "\n",
      "Epoch [29/1000], Training Loss: 0.0903\n",
      "Epoch [29/1000], Validation Loss: 0.1728, Validation Accuracy: 95.54%\n",
      "Output Summary: Max=55.4426, Min=-64.3584, Median=-2.1015, Mean=-2.8366\n",
      "\n",
      "Epoch [30/1000], Training Loss: 0.0933\n",
      "Epoch [30/1000], Validation Loss: 0.1814, Validation Accuracy: 95.37%\n",
      "Output Summary: Max=61.4753, Min=-60.5336, Median=-1.8913, Mean=-2.5553\n",
      "\n",
      "Epoch [31/1000], Training Loss: 0.0917\n",
      "Epoch [31/1000], Validation Loss: 0.1802, Validation Accuracy: 95.31%\n",
      "Output Summary: Max=61.5975, Min=-58.3190, Median=-1.6505, Mean=-2.1553\n",
      "\n",
      "Epoch [32/1000], Training Loss: 0.0870\n",
      "Epoch [32/1000], Validation Loss: 0.1903, Validation Accuracy: 95.18%\n",
      "Output Summary: Max=67.8390, Min=-69.8353, Median=-2.0350, Mean=-2.6653\n",
      "\n",
      "Epoch [33/1000], Training Loss: 0.0903\n",
      "Epoch [33/1000], Validation Loss: 0.2117, Validation Accuracy: 95.08%\n",
      "Output Summary: Max=71.9686, Min=-77.5945, Median=-2.2578, Mean=-2.9382\n",
      "\n",
      "Epoch [34/1000], Training Loss: 0.0845\n",
      "Epoch [34/1000], Validation Loss: 0.1958, Validation Accuracy: 95.34%\n",
      "Output Summary: Max=68.1639, Min=-83.1786, Median=-2.5239, Mean=-3.4003\n",
      "\n",
      "Epoch [35/1000], Training Loss: 0.0772\n",
      "Epoch [35/1000], Validation Loss: 0.2254, Validation Accuracy: 94.91%\n",
      "Output Summary: Max=81.0828, Min=-83.1568, Median=-2.7872, Mean=-3.7467\n",
      "\n",
      "Epoch [36/1000], Training Loss: 0.0813\n",
      "Epoch [36/1000], Validation Loss: 0.2001, Validation Accuracy: 95.19%\n",
      "Output Summary: Max=71.4949, Min=-81.2815, Median=-1.7084, Mean=-2.5447\n",
      "\n",
      "Epoch [37/1000], Training Loss: 0.0764\n",
      "Epoch [37/1000], Validation Loss: 0.2363, Validation Accuracy: 94.73%\n",
      "Output Summary: Max=78.7775, Min=-85.2470, Median=-1.9837, Mean=-2.7973\n",
      "\n",
      "Epoch [38/1000], Training Loss: 0.0780\n",
      "Epoch [38/1000], Validation Loss: 0.2300, Validation Accuracy: 95.07%\n",
      "Output Summary: Max=86.1466, Min=-94.4495, Median=-2.9732, Mean=-4.0349\n",
      "\n",
      "Epoch [39/1000], Training Loss: 0.0740\n",
      "Epoch [39/1000], Validation Loss: 0.2314, Validation Accuracy: 95.34%\n",
      "Output Summary: Max=87.6762, Min=-102.9021, Median=-2.7060, Mean=-3.8433\n",
      "\n",
      "Epoch [40/1000], Training Loss: 0.0740\n",
      "Epoch [40/1000], Validation Loss: 0.2197, Validation Accuracy: 95.28%\n",
      "Output Summary: Max=78.8002, Min=-91.6390, Median=-2.6023, Mean=-3.5136\n",
      "\n",
      "Epoch [41/1000], Training Loss: 0.0697\n",
      "Epoch [41/1000], Validation Loss: 0.2214, Validation Accuracy: 95.15%\n",
      "Output Summary: Max=90.6270, Min=-92.5172, Median=-2.5450, Mean=-3.4764\n",
      "\n",
      "Epoch [42/1000], Training Loss: 0.0747\n",
      "Epoch [42/1000], Validation Loss: 0.2363, Validation Accuracy: 95.25%\n",
      "Output Summary: Max=97.0838, Min=-100.7452, Median=-2.8169, Mean=-4.0111\n",
      "\n",
      "Epoch [43/1000], Training Loss: 0.0693\n",
      "Epoch [43/1000], Validation Loss: 0.2112, Validation Accuracy: 95.29%\n",
      "Output Summary: Max=72.7322, Min=-77.1527, Median=-2.2141, Mean=-3.0072\n",
      "\n",
      "Epoch [44/1000], Training Loss: 0.0775\n",
      "Epoch [44/1000], Validation Loss: 0.2083, Validation Accuracy: 95.46%\n",
      "Output Summary: Max=68.8694, Min=-91.2176, Median=-2.7753, Mean=-3.8370\n",
      "\n",
      "Epoch [45/1000], Training Loss: 0.0697\n",
      "Epoch [45/1000], Validation Loss: 0.1996, Validation Accuracy: 95.20%\n",
      "Output Summary: Max=71.3435, Min=-76.2135, Median=-1.9892, Mean=-2.8015\n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[133]\u001b[39m\u001b[32m, line 19\u001b[39m\n\u001b[32m     16\u001b[39m     loss.backward()\n\u001b[32m     18\u001b[39m     optimizer.step()\n\u001b[32m---> \u001b[39m\u001b[32m19\u001b[39m     running_loss += \u001b[43mloss\u001b[49m\u001b[43m.\u001b[49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     20\u001b[39m     num_batches += \u001b[32m1\u001b[39m\n\u001b[32m     22\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mEpoch [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;250m \u001b[39m+\u001b[38;5;250m \u001b[39m\u001b[32m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepochs\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m], Training Loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrunning_loss\u001b[38;5;250m \u001b[39m/\u001b[38;5;250m \u001b[39mnum_batches\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "patience = 40\n",
    "best_val_loss = float('inf')\n",
    "no_improvement_epochs = 0\n",
    "\n",
    "all_outputs = []\n",
    "\n",
    "for epoch in range(10000):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    num_batches = 0\n",
    "    \n",
    "    for data, target in get_batches(train_data, train_targets, batch_size):\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = criterion(output, target)\n",
    "        loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "        num_batches += 1\n",
    "\n",
    "    print(f\"Epoch [{epoch + 1}/{epochs}], Training Loss: {running_loss / num_batches:.4f}\")\n",
    "\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    num_batches = 0\n",
    "    epoch_outputs = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for data, target in get_batches(test_data, test_targets, batch_size):\n",
    "            outputs = model(data)\n",
    "            loss = criterion(outputs, target)\n",
    "            val_loss += loss.item()\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            total += target.size(0)\n",
    "            num_batches += 1\n",
    "            correct += (predicted == target).sum().item()\n",
    "\n",
    "            epoch_outputs.append(outputs)\n",
    "\n",
    "    all_outputs_tensor = torch.cat(epoch_outputs, dim=0)\n",
    "    all_outputs.append(all_outputs_tensor)\n",
    "\n",
    "    max_val = torch.max(all_outputs_tensor).item()\n",
    "    min_val = torch.min(all_outputs_tensor).item()\n",
    "    median_val = torch.median(all_outputs_tensor).item()\n",
    "    mean_val = torch.mean(all_outputs_tensor).item()\n",
    "\n",
    "    accuracy = 100 * correct / total\n",
    "    val_loss /= num_batches\n",
    "    print(f\"Epoch [{epoch + 1}/{epochs}], Validation Loss: {val_loss:.4f}, Validation Accuracy: {accuracy:.2f}%\")\n",
    "    print(f\"Output Summary: Max={max_val:.4f}, Min={min_val:.4f}, Median={median_val:.4f}, Mean={mean_val:.4f}\")\n",
    "    print()\n",
    "\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        no_improvement_epochs = 0\n",
    "    else:\n",
    "        no_improvement_epochs += 1\n",
    "\n",
    "    if no_improvement_epochs >= patience:\n",
    "        print(f\"Early stopping triggered after {epoch + 1} epochs.\")\n",
    "        print(best_val_loss)\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "04bece12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x0 Parameter containing:\n",
      "tensor([[[-4.8991e-03,  2.5201e-02, -1.1596e-02,  9.7011e-04,  8.4435e-03,\n",
      "           5.4457e-03,  1.2147e-02,  2.1471e-02,  7.7073e-03, -3.0992e-03,\n",
      "          -1.3756e-02,  2.0155e-02,  1.1545e-02, -9.1131e-02, -2.3543e-02,\n",
      "          -2.2076e-02,  7.2532e-03,  1.0363e-02, -1.7747e-02, -1.6975e-02,\n",
      "          -7.4943e-03, -1.7222e-02, -6.3775e-03,  6.7688e-02,  4.6023e-04,\n",
      "           3.6882e-03,  8.9947e-03,  1.8488e-02],\n",
      "         [-5.2839e-03,  8.6376e-04,  3.4281e-02,  2.9808e-03,  3.6648e-03,\n",
      "           4.5234e-02,  2.3797e-02,  2.6553e-03,  1.0506e-02, -4.4785e-03,\n",
      "           6.2421e-03,  2.2334e-02, -3.2451e-02,  2.3809e-02, -2.4318e-02,\n",
      "          -1.3675e-01,  5.2540e-03, -9.0208e-04, -9.9969e-03,  2.3349e-02,\n",
      "           1.3842e-02, -1.1336e-03,  4.6699e-03,  2.9924e-02,  1.9460e-02,\n",
      "           3.9729e-03,  4.3285e-02,  4.0165e-02],\n",
      "         [ 9.5744e-03,  3.5709e-02, -1.2582e-03,  1.4985e-03, -5.3291e-03,\n",
      "           1.3135e-02,  2.2672e-02,  3.1037e-02, -1.8326e-02,  2.6862e-02,\n",
      "          -3.5729e-02, -3.2729e-02,  2.9682e-02, -6.3957e-03, -4.5634e-02,\n",
      "          -2.5230e-04, -3.1417e-02, -1.6841e-02,  1.0984e-02,  8.0537e-03,\n",
      "           2.2585e-02,  6.1132e-02,  1.6658e-02,  3.7283e-02,  3.3012e-02,\n",
      "           1.4852e-02,  3.1384e-02, -9.6145e-03],\n",
      "         [-5.4239e-03,  3.9087e-03,  1.8618e-03,  9.6999e-03,  4.2435e-02,\n",
      "           1.5893e-02,  5.0930e-02,  4.7556e-03, -1.2318e-02,  1.4244e-02,\n",
      "           6.4413e-04,  3.9264e-02,  4.4818e-02,  2.8725e-02,  1.1627e-02,\n",
      "          -1.2918e-01, -1.4531e-01,  7.3921e-02,  3.9457e-02,  7.4995e-03,\n",
      "           1.7739e-02,  6.5564e-02, -2.3232e-02,  2.4610e-02,  1.2647e-01,\n",
      "           1.0907e-02,  4.8028e-04,  3.5226e-02],\n",
      "         [ 3.6320e-02,  6.3288e-02,  2.2889e-03, -2.7736e-02, -3.8148e-03,\n",
      "           1.2955e-01,  6.6828e-02,  5.8562e-03, -1.7771e-02, -1.3860e-02,\n",
      "           1.1073e-02,  2.5141e-04,  8.2382e-02,  1.6391e-02,  3.4971e-03,\n",
      "          -6.6577e-03, -8.9701e-03,  2.9529e-02, -1.1816e-02,  2.8387e-02,\n",
      "           3.8715e-02,  1.4846e-02,  2.3710e-02,  6.9195e-02,  4.6279e-02,\n",
      "           7.9980e-03,  6.0943e-03,  1.2739e-02],\n",
      "         [ 1.6627e-03,  2.1782e-02, -2.7874e-03, -2.5065e-03,  9.7418e-02,\n",
      "           3.0673e-03,  1.6598e-03,  1.9710e-02, -2.1749e-02,  1.3712e-02,\n",
      "           3.1590e-02,  1.2532e-02,  5.7280e-03,  6.3113e-02, -1.7581e-02,\n",
      "          -4.7214e-02, -6.7060e-02,  8.5770e-03,  5.4295e-02, -5.5434e-02,\n",
      "          -5.6287e-03,  7.6538e-03, -1.4942e-02, -4.8120e-03,  1.9996e-02,\n",
      "           7.0815e-02,  8.9174e-02, -9.6729e-02],\n",
      "         [-4.6316e-03,  8.1492e-02, -2.8650e-03, -2.0049e-02,  9.2378e-04,\n",
      "           5.7280e-03, -6.7487e-03,  3.1499e-02, -1.4591e-02, -1.7140e-02,\n",
      "           8.2873e-04, -1.0978e-01, -6.2930e-02, -3.3878e-02, -7.1980e-02,\n",
      "          -7.9069e-02, -1.0469e-01, -5.7380e-02, -4.8415e-02,  1.3888e-02,\n",
      "           4.7626e-02,  3.6083e-03,  2.8908e-02, -4.2657e-03,  3.0039e-02,\n",
      "           5.0113e-02, -1.7540e-02, -2.8373e-02],\n",
      "         [ 1.0971e-02,  7.3578e-02,  5.1539e-02, -4.1191e-03,  6.1856e-03,\n",
      "           2.1872e-02, -3.7163e-03, -6.6990e-03, -5.6189e-02, -7.0818e-02,\n",
      "          -7.3033e-02, -7.2616e-02, -1.4232e-01, -5.2520e-02, -8.3509e-02,\n",
      "          -1.1758e-01, -1.3060e-01, -7.6487e-02, -2.7715e-02, -2.3400e-02,\n",
      "           2.6058e-02, -1.7620e-03,  3.1166e-03,  3.4874e-02, -5.2479e-03,\n",
      "          -1.9155e-03,  8.8183e-03, -1.7676e-02],\n",
      "         [-2.2499e-03,  2.9696e-02,  6.2048e-02, -1.8253e-04,  3.6441e-02,\n",
      "          -2.6000e-02, -3.0490e-02, -2.4638e-02, -3.6091e-02, -8.5181e-02,\n",
      "          -4.9405e-02, -9.3946e-02, -1.4024e-01, -5.5614e-02, -9.9792e-02,\n",
      "          -8.4434e-02, -7.7498e-02, -8.3829e-02, -5.0897e-02, -1.7656e-02,\n",
      "          -6.8107e-03,  5.9298e-04, -2.0157e-03,  4.4601e-02,  2.9137e-02,\n",
      "           7.3381e-03,  3.5391e-02, -4.4254e-02],\n",
      "         [ 7.6982e-02, -6.9241e-04,  2.0801e-02,  1.0492e-02, -7.0586e-04,\n",
      "           7.9576e-03, -5.4334e-02, -5.8987e-02,  4.8741e-03, -3.2848e-02,\n",
      "          -8.4705e-02, -8.4545e-02, -5.2596e-02, -7.9821e-02, -5.4118e-02,\n",
      "          -6.2916e-02, -8.9748e-02, -3.8813e-02, -5.3912e-02, -6.0027e-02,\n",
      "           2.2891e-02,  3.7191e-02, -2.4654e-02,  2.6163e-02, -1.4151e-02,\n",
      "           3.5009e-06, -4.9108e-03, -2.7459e-02],\n",
      "         [-1.6637e-02, -4.6753e-03,  1.0022e-01, -3.2901e-02, -7.9092e-03,\n",
      "           1.7766e-02,  3.8349e-03, -2.7029e-02, -3.5573e-02, -4.7071e-04,\n",
      "          -9.4243e-02, -8.0122e-02, -6.4087e-02, -9.2217e-02, -1.1588e-01,\n",
      "          -8.0112e-02, -8.7383e-02, -1.0609e-01, -4.4884e-02, -5.7069e-02,\n",
      "          -5.7964e-02,  4.0098e-03,  2.4207e-02, -2.8785e-02, -1.9888e-02,\n",
      "           1.8571e-02,  1.4086e-02, -2.2316e-02],\n",
      "         [-1.8704e-02,  2.0728e-02,  1.7759e-03, -3.4699e-02, -1.1029e-02,\n",
      "           1.8601e-02,  1.0962e-03, -6.0825e-02, -5.8103e-02, -8.2727e-02,\n",
      "          -9.4717e-02, -7.7544e-02, -8.6220e-02, -1.5202e-01, -1.0378e-01,\n",
      "          -8.6438e-02, -9.4741e-02, -8.0546e-02, -3.7532e-02, -4.2873e-03,\n",
      "          -1.2820e-02,  2.0450e-02, -6.0272e-03,  5.4400e-02,  4.5177e-02,\n",
      "          -1.7116e-02, -4.9602e-03, -5.0620e-02],\n",
      "         [-4.0014e-02, -2.2193e-03,  1.6393e-02,  1.0484e-02, -2.3081e-03,\n",
      "           1.7566e-02,  1.7610e-02, -2.9137e-02, -8.2721e-02, -4.3737e-02,\n",
      "          -5.0126e-02, -7.6470e-02, -7.5027e-02, -6.3216e-02, -1.3272e-01,\n",
      "          -2.7240e-02, -8.0819e-02, -1.0107e-01, -6.5558e-02, -2.0691e-02,\n",
      "          -1.0201e-02,  2.2666e-02,  5.5559e-02,  3.8898e-02,  3.3287e-02,\n",
      "           1.1071e-01, -8.0921e-03, -8.9666e-03],\n",
      "         [ 1.2133e-02, -1.8892e-02,  2.7016e-02,  3.3504e-03,  1.7098e-02,\n",
      "           1.4439e-02,  1.9382e-04, -1.8588e-05, -6.6815e-02, -7.9016e-02,\n",
      "          -6.8661e-02, -1.1361e-02, -4.1888e-02, -1.1676e-01, -8.6807e-02,\n",
      "          -6.4335e-02, -8.7603e-02, -6.9902e-02, -9.4003e-02,  1.9215e-02,\n",
      "           3.2799e-02, -1.2627e-02,  2.5821e-02,  7.1968e-02,  3.6865e-02,\n",
      "           3.5675e-02,  2.9704e-02,  2.5027e-02],\n",
      "         [ 1.1098e-02,  1.1699e-02,  2.2046e-02,  1.0804e-02, -5.0973e-03,\n",
      "           5.8792e-02,  5.3035e-02, -4.4992e-02, -1.6312e-02, -3.5735e-02,\n",
      "          -4.6777e-02, -3.1354e-02, -1.4214e-01, -3.6966e-02, -9.9119e-02,\n",
      "          -1.2171e-01, -3.1323e-02, -1.3695e-01, -7.3833e-02, -2.9140e-02,\n",
      "           1.2734e-02,  1.9498e-02, -9.8137e-03,  8.5616e-02,  4.9671e-02,\n",
      "           3.9608e-02,  2.7073e-02, -3.4744e-02],\n",
      "         [ 5.7667e-02,  2.5875e-02,  1.0597e-02,  3.4307e-02, -4.5070e-03,\n",
      "           4.2305e-02,  8.7086e-03, -3.5599e-02, -4.4521e-02, -4.8829e-03,\n",
      "          -6.4371e-02, -2.2193e-02, -4.4957e-02, -5.5195e-02, -8.3574e-02,\n",
      "          -8.1705e-02, -8.4130e-02, -5.4297e-02, -7.9833e-02,  2.1801e-02,\n",
      "           2.2131e-02,  3.6708e-04, -7.9690e-03, -2.4983e-02, -1.5357e-02,\n",
      "          -1.9804e-02, -1.7661e-02, -3.0056e-03],\n",
      "         [ 5.9447e-02,  5.6386e-02,  2.8671e-02,  2.6423e-02, -5.2756e-04,\n",
      "          -1.1268e-02, -1.7003e-02, -5.2800e-02, -4.6885e-03, -1.8746e-02,\n",
      "          -3.3909e-02,  3.6074e-02, -9.3160e-03, -1.5786e-02, -8.3506e-02,\n",
      "          -1.0302e-01, -6.6318e-02, -1.1417e-01,  1.8782e-02, -1.6914e-02,\n",
      "           2.0579e-03,  7.0053e-03, -3.1419e-02,  1.5744e-02,  2.9352e-02,\n",
      "          -3.2034e-02, -2.2262e-02, -2.6694e-02],\n",
      "         [ 5.0425e-02,  4.8353e-02, -6.0122e-03,  2.3905e-02,  2.5635e-03,\n",
      "           5.2928e-03, -8.7173e-03, -5.8921e-02, -1.9588e-03, -1.2030e-02,\n",
      "          -7.2601e-02, -6.6562e-03, -3.4291e-02, -9.7270e-02, -1.4338e-02,\n",
      "          -1.4360e-01, -7.4974e-02,  3.2866e-03, -1.1983e-01, -1.2798e-02,\n",
      "           2.6590e-02, -2.8702e-02,  5.1903e-04,  8.9549e-03, -1.3884e-02,\n",
      "           5.3023e-03,  1.8544e-03, -2.6401e-02],\n",
      "         [ 4.7002e-02,  3.3573e-02,  2.2626e-04,  7.3531e-03,  6.1006e-02,\n",
      "          -8.3514e-03, -5.5578e-03,  8.8643e-03, -2.2214e-02, -2.1929e-02,\n",
      "          -4.4530e-02, -3.4239e-02, -9.6571e-02, -1.5329e-02, -1.1178e-01,\n",
      "          -1.2420e-01, -6.1934e-02, -9.9844e-02, -3.4244e-02, -5.3355e-02,\n",
      "          -6.0154e-02, -2.1427e-02,  1.5539e-02, -1.8358e-02,  3.8079e-02,\n",
      "           9.0966e-02, -5.3162e-03, -6.6021e-02],\n",
      "         [ 7.3953e-02,  6.0123e-02,  4.2645e-02,  5.2920e-03,  8.3574e-03,\n",
      "           2.9948e-02, -1.1337e-02, -3.6337e-02,  9.5128e-03, -6.9444e-02,\n",
      "          -5.7451e-02, -3.6051e-02, -8.7064e-02, -3.6171e-02, -7.7204e-02,\n",
      "          -5.9452e-02, -7.3726e-02, -1.0958e-01, -9.9445e-02,  1.2356e-02,\n",
      "          -1.6224e-02,  6.8872e-02, -2.3884e-02, -7.3106e-02,  4.5187e-02,\n",
      "           4.1065e-02,  2.3754e-02, -6.6456e-02],\n",
      "         [ 1.1336e-02,  2.1433e-02,  4.4323e-02,  2.1771e-02,  5.9791e-02,\n",
      "          -1.8473e-02, -3.3990e-02, -1.3252e-02, -1.3065e-01, -6.5737e-02,\n",
      "          -7.0830e-02, -4.5233e-02, -1.0874e-01, -8.9117e-02, -7.9345e-02,\n",
      "          -7.7488e-02, -5.3470e-02, -5.5534e-02, -3.7647e-02, -3.1205e-02,\n",
      "           1.5088e-02,  1.2480e-02,  5.5237e-02,  1.0835e-01,  7.8286e-03,\n",
      "           4.4178e-02, -1.0591e-02, -3.8207e-02],\n",
      "         [ 5.1185e-02,  2.0832e-02,  1.3532e-02,  2.6456e-02, -4.7704e-02,\n",
      "          -1.1052e-02,  4.0090e-03, -1.2061e-01, -1.0336e-01, -1.1048e-01,\n",
      "          -3.5027e-02, -1.3253e-01, -1.7188e-01, -7.8054e-02, -6.1875e-02,\n",
      "          -7.0853e-02, -6.8911e-02, -1.7776e-02, -7.4508e-02,  3.0164e-02,\n",
      "           8.6584e-02, -3.6257e-02, -2.7599e-02,  2.7160e-02,  3.2314e-02,\n",
      "           5.1600e-02,  4.0066e-02, -1.6903e-02],\n",
      "         [-9.2092e-03,  4.2811e-02,  6.7703e-02, -3.0697e-03,  2.5967e-02,\n",
      "          -2.6045e-02, -1.8099e-02,  3.8091e-04, -3.0447e-02, -2.0193e-02,\n",
      "          -4.3553e-02, -5.1601e-02, -6.2331e-02, -1.3834e-01, -5.0690e-02,\n",
      "          -7.5540e-02, -1.3431e-03,  1.3255e-02, -7.0083e-02,  1.6603e-02,\n",
      "           7.6114e-02,  4.5418e-02,  3.6399e-03, -1.5384e-03,  2.0069e-02,\n",
      "           6.1720e-03,  2.1289e-02,  1.1345e-02],\n",
      "         [-4.0758e-03, -8.0782e-02, -3.0459e-02,  4.0066e-02,  3.7819e-02,\n",
      "           4.1899e-02,  7.3373e-02,  1.3364e-02, -2.2375e-03, -6.3030e-02,\n",
      "          -5.4808e-02, -6.3866e-02, -7.5980e-02, -4.8398e-02,  6.6328e-03,\n",
      "          -1.6493e-02,  1.4673e-03,  2.3299e-02, -1.7826e-02, -3.3946e-02,\n",
      "           3.4786e-02,  9.4379e-03, -3.6009e-03,  9.5994e-03,  1.5345e-02,\n",
      "           6.8541e-02,  3.0303e-02, -1.6682e-02],\n",
      "         [ 1.0318e-02, -3.6923e-03,  2.5656e-02,  5.1485e-03, -1.3659e-03,\n",
      "           4.2978e-03,  3.7224e-03,  1.5549e-02,  7.7733e-03, -1.3251e-03,\n",
      "          -5.7734e-02, -3.0749e-02, -4.4005e-03, -1.9029e-02, -8.2671e-02,\n",
      "          -7.7908e-02, -7.4415e-03,  4.1698e-02,  3.3274e-02, -1.6408e-02,\n",
      "           1.5510e-02, -1.1691e-02,  3.2934e-03, -2.9681e-03, -5.4237e-04,\n",
      "          -2.9457e-02, -3.3177e-02,  2.1791e-02],\n",
      "         [-4.5803e-02, -1.3920e-02,  2.0016e-02, -1.3576e-01,  2.0797e-02,\n",
      "           8.2022e-03, -3.8838e-03,  2.9653e-02,  3.8909e-03, -1.2032e-02,\n",
      "           3.0130e-02, -6.1297e-02, -8.4639e-03,  3.3898e-02, -9.1776e-04,\n",
      "          -2.2042e-03, -1.7178e-02,  3.8200e-02, -2.4018e-02, -8.2282e-03,\n",
      "           1.8740e-02,  9.7346e-03, -3.4255e-02,  1.0776e-03, -9.4489e-03,\n",
      "           2.3094e-02,  5.6986e-02, -1.7982e-02],\n",
      "         [ 1.7497e-02,  5.5542e-03, -3.5165e-03,  8.7860e-02, -3.9079e-02,\n",
      "          -1.7818e-03,  2.3941e-03, -1.0634e-02,  1.4605e-02,  4.4737e-03,\n",
      "          -1.9008e-02,  5.3846e-03,  2.0619e-02, -1.0634e-02,  1.9981e-02,\n",
      "          -1.9444e-02, -4.7205e-02, -1.4167e-03,  4.5998e-02,  9.1099e-03,\n",
      "          -6.2317e-03,  5.9080e-02,  3.2565e-02,  1.8715e-02,  3.7181e-02,\n",
      "           1.3792e-01,  8.8865e-03, -2.4143e-02],\n",
      "         [-2.0314e-02,  3.6041e-02,  1.9668e-02, -3.8043e-03, -2.7491e-03,\n",
      "           4.1059e-02,  8.3690e-03,  1.7857e-02,  1.4065e-02,  1.2598e-02,\n",
      "           1.0947e-04, -2.1187e-02, -2.0713e-02,  2.0740e-02,  1.7648e-02,\n",
      "          -3.5375e-02, -4.0763e-02, -1.3890e-02,  1.2897e-02, -1.3471e-03,\n",
      "           1.7165e-02,  4.2966e-02,  8.0453e-03,  3.2107e-02, -5.8894e-03,\n",
      "           2.5344e-02,  6.6178e-03,  1.2700e-02]]], device='mps:0',\n",
      "       requires_grad=True)\n",
      "h0 Parameter containing:\n",
      "tensor([[-0.0348, -0.0087,  0.8572, -0.5675, -0.6130,  0.1411,  0.1402,  0.6550,\n",
      "         -0.5596,  0.4733, -1.0735,  0.0504, -1.4166,  0.0584, -2.2291,  0.4211,\n",
      "         -0.0337, -0.0734,  0.4203,  2.2424, -0.0814,  0.0336, -0.1621, -0.0035,\n",
      "         -0.0820,  1.1010,  0.5685,  0.3328, -0.3807, -0.3136, -0.0872,  1.3586]],\n",
      "       device='mps:0', requires_grad=True)\n",
      "rnn_cell.fc.weight Parameter containing:\n",
      "tensor([[ 0.1137,  0.0268, -0.0195,  ...,  0.0983,  0.1568, -0.1247],\n",
      "        [-0.0182,  0.0482, -0.0109,  ...,  0.0087, -0.0595,  0.0259],\n",
      "        [-0.0884,  0.0207, -0.0041,  ..., -0.0795,  0.0663,  0.0804],\n",
      "        ...,\n",
      "        [-0.0681,  0.0095,  0.0216,  ...,  0.2414,  0.3599, -0.0673],\n",
      "        [-0.2373, -0.0486, -0.0137,  ...,  0.1850,  0.5794,  0.2005],\n",
      "        [-0.0686,  0.0222, -0.0694,  ..., -0.1187,  0.0790,  0.0353]],\n",
      "       device='mps:0', requires_grad=True)\n",
      "rnn_cell.fc.bias Parameter containing:\n",
      "tensor([ 0.0244,  0.0136, -0.0315,  0.1249,  0.1441,  0.0928,  0.0581, -0.0092,\n",
      "         0.0938,  0.0085,  0.0953, -0.0014,  0.0041, -0.1127, -0.1956,  0.0234,\n",
      "        -0.0186,  0.0133,  0.0190,  0.0673,  0.2536,  0.0268, -0.1003, -0.0093,\n",
      "         0.0551, -0.0543, -0.0325, -0.0344,  0.2245,  0.0258,  0.2371,  0.0989],\n",
      "       device='mps:0', requires_grad=True)\n",
      "fc.weight Parameter containing:\n",
      "tensor([[ 0.3347, -0.3230,  0.6589, -0.3244, -0.0237, -0.9474, -0.7827, -0.5439,\n",
      "          0.4499,  0.1188,  0.8663, -0.5646, -0.8603,  0.5528, -0.8129, -0.6426,\n",
      "         -0.8759, -0.2614,  0.7314,  0.6455,  0.3861,  0.6081,  0.1303,  0.1388,\n",
      "         -0.4118, -0.4864,  0.5126,  0.1495,  0.4803,  0.3523,  0.5284, -0.5544],\n",
      "        [-0.8103,  0.1098,  0.1362, -0.9803, -0.3084,  0.2427,  0.1156,  0.8367,\n",
      "         -0.6814, -0.5539, -0.5920,  0.4243, -0.9868, -0.2507, -0.2359, -0.1079,\n",
      "         -0.7660,  0.8138, -0.1117, -0.8026, -0.4408,  0.6885, -0.5143,  1.2138,\n",
      "         -0.4044,  0.7861, -0.4343,  0.4082,  0.7295, -0.7521, -0.8482,  0.5203],\n",
      "        [-0.5222, -0.8823,  0.7067,  0.6097,  0.2868,  0.9079,  0.0308, -0.3689,\n",
      "         -0.3499, -0.7726, -0.0330, -0.4582, -0.4627, -0.5181,  0.6209,  0.9813,\n",
      "          1.0936,  0.1810,  0.1878,  0.8271,  0.0261,  0.5162,  0.7132, -0.0377,\n",
      "          0.4763, -0.4483,  0.6218,  0.4631,  0.4614,  0.4885, -0.2085, -0.6250],\n",
      "        [ 0.4425,  0.5204, -0.4259,  0.4902,  0.9182, -0.5434,  0.5288, -0.2050,\n",
      "         -0.0493,  0.4776, -0.2696,  0.0211,  0.9141, -0.0456,  0.7308,  0.6846,\n",
      "          0.3857,  0.8409, -0.3609, -0.8211, -0.4828,  0.8019, -0.9775, -0.9199,\n",
      "         -0.6578, -1.1867,  0.9084, -0.5835,  0.5929, -0.1838,  0.7517, -0.1547],\n",
      "        [ 0.7179, -0.3615, -0.5220, -0.4844,  0.3801,  0.1739,  0.2748,  0.3893,\n",
      "          0.1443,  0.8624, -0.1538, -0.8984,  0.1507,  0.8959, -0.6666, -0.3764,\n",
      "          0.7853,  0.2307, -0.4356, -0.5388,  0.0087, -0.8775,  0.2965, -0.5245,\n",
      "          1.0866,  0.1719, -0.2175, -0.8475, -0.1879,  1.2268, -0.3044, -0.4998],\n",
      "        [-0.1502,  0.4005, -0.6348,  0.2094, -0.7490,  0.8460,  0.6799, -0.5017,\n",
      "          0.3314,  0.2129,  0.6551,  0.2067,  0.6129, -0.1855,  0.6373, -0.3864,\n",
      "         -1.1669, -1.1338, -0.7162, -1.1828,  0.9763, -0.8706, -0.9919,  1.0894,\n",
      "         -0.5682, -0.6966, -0.7869,  0.2354,  0.3282,  0.7596,  1.1600, -0.1131],\n",
      "        [-0.9886, -0.0813,  0.4757,  0.4669, -1.0260,  0.2769,  0.0098,  0.8002,\n",
      "         -0.7317, -0.0357, -0.5853, -0.3428, -0.6740,  0.5759,  0.1691, -0.7860,\n",
      "         -0.3889, -0.1086,  0.1605,  0.7588,  0.9354, -0.8933,  0.7090, -0.6400,\n",
      "         -0.3081,  0.3421,  0.4987,  1.0228, -0.7628,  0.3470,  0.8323,  1.0497],\n",
      "        [ 0.0849, -0.3747, -0.0980,  0.6126,  0.4890, -0.1624, -0.2535, -0.4186,\n",
      "          0.1140, -0.6323,  0.8275,  0.6973,  0.8792,  0.1674, -0.5896,  0.7392,\n",
      "         -0.3554, -0.4782, -0.6025,  0.8758, -0.1194,  0.3732,  0.5889,  0.4909,\n",
      "          1.1967,  0.8869,  0.5323, -1.3297,  0.7740, -0.4281, -0.5187,  0.7451],\n",
      "        [ 0.1430,  1.0072, -0.0167,  0.3457, -0.6679, -0.4483,  0.2972, -0.7405,\n",
      "          0.4812,  0.4976, -0.2109, -0.2745,  0.4347, -0.4050,  0.6051, -0.2969,\n",
      "          0.3394,  0.3640, -0.0385,  1.0086, -0.8666,  0.3304,  1.2499,  0.1607,\n",
      "         -1.1423, -0.2826, -0.6260,  1.0324, -0.6630,  0.3934, -0.4943, -0.2879],\n",
      "        [ 0.8122, -0.3891, -0.3821, -0.3979,  0.3679, -0.0920, -0.5259,  0.2203,\n",
      "          0.2799, -0.2166, -0.8199,  0.5309, -0.4130, -1.0593, -0.4658, -0.6841,\n",
      "          0.6928, -0.5452,  0.8093, -1.2292,  0.4696,  0.5850, -0.7118,  0.0961,\n",
      "          0.6970,  0.0364, -0.3890, -0.4798, -0.9160, -1.5495, -0.8729, -0.7249]],\n",
      "       device='mps:0', requires_grad=True)\n",
      "fc.bias Parameter containing:\n",
      "tensor([-0.6710,  0.0627,  0.3802,  0.2248, -0.1059,  0.9339, -0.4748, -0.2499,\n",
      "        -0.3663,  0.1558], device='mps:0', requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "for name, param in model.named_parameters():\n",
    "    print(name, param)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed2a7b21",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
