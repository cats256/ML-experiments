{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "f0a886df",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "f94be421",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mps\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "5fa4979a",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "train_dataset = datasets.MNIST(root='./data', train=True, download=True, transform=transform)\n",
    "test_dataset = datasets.MNIST(root='./data', train=False, download=True, transform=transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "afcd05cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = train_dataset.data.to(device).float() / 255.0\n",
    "train_targets = train_dataset.targets.to(device)\n",
    "\n",
    "test_data = test_dataset.data.to(device).float() / 255.0\n",
    "test_targets = test_dataset.targets.to(device)\n",
    "\n",
    "train_data = train_data.unsqueeze(1)\n",
    "test_data = test_data.unsqueeze(1)\n",
    "\n",
    "def get_batches(data, targets, batch_size):\n",
    "    for i in range(0, len(data), batch_size):\n",
    "        yield data[i:i + batch_size], targets[i:i + batch_size]\n",
    "\n",
    "batch_size = 500\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "id": "b2a1fe5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleConcatRNNCell(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size):\n",
    "        super().__init__()\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.fc = nn.Linear(input_size + hidden_size, hidden_size)\n",
    "\n",
    "    def forward(self, x_t, h_prev):\n",
    "        combined = torch.cat([x_t, h_prev], dim=1)\n",
    "        h_t = F.tanh(self.fc(combined))\n",
    "        return h_t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1211edf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ExperimentalModel(nn.Module):\n",
    "    def __init__(self, hidden_size=20):\n",
    "        super().__init__()\n",
    "        self.rnn_cell = SimpleConcatRNNCell(1, hidden_size)\n",
    "\n",
    "        self.x0 = nn.Parameter(torch.zeros(1, 28, 28))\n",
    "        self.h0 = nn.Parameter(torch.randn(1, hidden_size))\n",
    "\n",
    "        self.fc = nn.Linear(28 * 28, 10)\n",
    "\n",
    "    def forward(self, x): # (B, 1, 28, 28)\n",
    "        B, H, W = x.size(0), x.size(2), x.size(3)\n",
    "\n",
    "        x_t = self.x0.expand(B, 1, H, W)\n",
    "        h_t = self.h0.expand(B, -1)\n",
    "\n",
    "        for _ in range(5):\n",
    "            x_t_flat = x_t.view(B, -1)\n",
    "            h_t = self.rnn_cell(x_t_flat.mean(dim=1).unsqueeze(1), h_t)\n",
    "\n",
    "            x_cat = torch.cat([x_t, x], dim=1)\n",
    "            weight = h_t[:, :18].view(B, 1, 2, 3, 3)\n",
    "            bias = h_t[:, 18]\n",
    "\n",
    "            x_cat = x_cat.view(1, B * 2, H, W)\n",
    "            weight = weight.view(B, 2, 3, 3)\n",
    "\n",
    "            x_t = F.conv2d(x_cat, weight, bias=bias, padding=1, stride=1, groups=B)\n",
    "            x_t = F.relu(x_t.view(B, 1, H, W))\n",
    "\n",
    "        return self.fc(x_t.view(B, -1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "id": "d812474e",
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.001 * 1\n",
    "epochs = 10000\n",
    "\n",
    "device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "model = ExperimentalModel().to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "id": "d7ebaf5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x0: 784 params, requires_grad=True\n",
      "h0: 19 params, requires_grad=True\n",
      "rnn_cell.fc.weight: 380 params, requires_grad=True\n",
      "rnn_cell.fc.bias: 19 params, requires_grad=True\n",
      "fc.weight: 7840 params, requires_grad=True\n",
      "fc.bias: 10 params, requires_grad=True\n",
      "\n",
      "9052\n"
     ]
    }
   ],
   "source": [
    "for name, param in model.named_parameters():\n",
    "    print(f\"{name}: {param.numel()} params, requires_grad={param.requires_grad}\")\n",
    "\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "print()\n",
    "print(total_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "id": "22925496",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10000], Training Loss: 0.8581\n",
      "Epoch [1/10000], Validation Loss: 0.3279, Validation Accuracy: 90.52%\n",
      "Output Summary: Max=13.4369, Min=-24.3840, Median=-3.7755, Mean=-3.7363\n",
      "\n",
      "Epoch [2/10000], Training Loss: 0.3192\n",
      "Epoch [2/10000], Validation Loss: 0.2778, Validation Accuracy: 91.90%\n",
      "Output Summary: Max=14.5049, Min=-27.8686, Median=-3.8174, Mean=-3.8794\n",
      "\n",
      "Epoch [3/10000], Training Loss: 0.2859\n",
      "Epoch [3/10000], Validation Loss: 0.2585, Validation Accuracy: 92.50%\n",
      "Output Summary: Max=15.5644, Min=-29.8379, Median=-3.7387, Mean=-3.8870\n",
      "\n",
      "Epoch [4/10000], Training Loss: 0.2684\n",
      "Epoch [4/10000], Validation Loss: 0.2459, Validation Accuracy: 92.81%\n",
      "Output Summary: Max=16.3785, Min=-31.5484, Median=-3.7306, Mean=-3.9521\n",
      "\n",
      "Epoch [5/10000], Training Loss: 0.2561\n",
      "Epoch [5/10000], Validation Loss: 0.2370, Validation Accuracy: 93.28%\n",
      "Output Summary: Max=17.0178, Min=-32.5589, Median=-3.7082, Mean=-3.9872\n",
      "\n",
      "Epoch [6/10000], Training Loss: 0.2452\n",
      "Epoch [6/10000], Validation Loss: 0.2283, Validation Accuracy: 93.43%\n",
      "Output Summary: Max=17.4453, Min=-33.2808, Median=-3.7404, Mean=-4.0570\n",
      "\n",
      "Epoch [7/10000], Training Loss: 0.2337\n",
      "Epoch [7/10000], Validation Loss: 0.2182, Validation Accuracy: 93.67%\n",
      "Output Summary: Max=18.0955, Min=-33.9729, Median=-3.8721, Mean=-4.2033\n",
      "\n",
      "Epoch [8/10000], Training Loss: 0.2215\n",
      "Epoch [8/10000], Validation Loss: 0.2065, Validation Accuracy: 93.90%\n",
      "Output Summary: Max=18.4343, Min=-35.2316, Median=-4.0980, Mean=-4.4445\n",
      "\n",
      "Epoch [9/10000], Training Loss: 0.2091\n",
      "Epoch [9/10000], Validation Loss: 0.1947, Validation Accuracy: 94.37%\n",
      "Output Summary: Max=18.3508, Min=-37.0167, Median=-4.3194, Mean=-4.6873\n",
      "\n",
      "Epoch [10/10000], Training Loss: 0.1973\n",
      "Epoch [10/10000], Validation Loss: 0.1845, Validation Accuracy: 94.65%\n",
      "Output Summary: Max=18.1855, Min=-38.7701, Median=-4.4852, Mean=-4.8770\n",
      "\n",
      "Epoch [11/10000], Training Loss: 0.1859\n",
      "Epoch [11/10000], Validation Loss: 0.1749, Validation Accuracy: 94.95%\n",
      "Output Summary: Max=18.0155, Min=-40.3202, Median=-4.6597, Mean=-5.0797\n",
      "\n",
      "Epoch [12/10000], Training Loss: 0.1755\n",
      "Epoch [12/10000], Validation Loss: 0.1663, Validation Accuracy: 95.08%\n",
      "Output Summary: Max=18.2519, Min=-42.2338, Median=-4.9044, Mean=-5.3582\n",
      "\n",
      "Epoch [13/10000], Training Loss: 0.1667\n",
      "Epoch [13/10000], Validation Loss: 0.1596, Validation Accuracy: 95.31%\n",
      "Output Summary: Max=18.6073, Min=-43.1511, Median=-5.0257, Mean=-5.4949\n",
      "\n",
      "Epoch [14/10000], Training Loss: 0.1587\n",
      "Epoch [14/10000], Validation Loss: 0.1541, Validation Accuracy: 95.40%\n",
      "Output Summary: Max=19.0625, Min=-44.2948, Median=-5.1856, Mean=-5.6672\n",
      "\n",
      "Epoch [15/10000], Training Loss: 0.1517\n",
      "Epoch [15/10000], Validation Loss: 0.1485, Validation Accuracy: 95.58%\n",
      "Output Summary: Max=19.3299, Min=-44.9891, Median=-5.3654, Mean=-5.8548\n",
      "\n",
      "Epoch [16/10000], Training Loss: 0.1458\n",
      "Epoch [16/10000], Validation Loss: 0.1433, Validation Accuracy: 95.74%\n",
      "Output Summary: Max=19.3856, Min=-44.9092, Median=-5.4642, Mean=-5.9599\n",
      "\n",
      "Epoch [17/10000], Training Loss: 0.1402\n",
      "Epoch [17/10000], Validation Loss: 0.1389, Validation Accuracy: 95.78%\n",
      "Output Summary: Max=19.4415, Min=-45.2766, Median=-5.5405, Mean=-6.0383\n",
      "\n",
      "Epoch [18/10000], Training Loss: 0.1350\n",
      "Epoch [18/10000], Validation Loss: 0.1347, Validation Accuracy: 95.86%\n",
      "Output Summary: Max=19.5020, Min=-45.5451, Median=-5.6437, Mean=-6.1442\n",
      "\n",
      "Epoch [19/10000], Training Loss: 0.1302\n",
      "Epoch [19/10000], Validation Loss: 0.1314, Validation Accuracy: 95.96%\n",
      "Output Summary: Max=19.6831, Min=-45.8468, Median=-5.7703, Mean=-6.2696\n",
      "\n",
      "Epoch [20/10000], Training Loss: 0.1260\n",
      "Epoch [20/10000], Validation Loss: 0.1283, Validation Accuracy: 96.08%\n",
      "Output Summary: Max=20.0014, Min=-46.1850, Median=-5.8602, Mean=-6.3540\n",
      "\n",
      "Epoch [21/10000], Training Loss: 0.1223\n",
      "Epoch [21/10000], Validation Loss: 0.1253, Validation Accuracy: 96.13%\n",
      "Output Summary: Max=20.3939, Min=-46.4548, Median=-5.9830, Mean=-6.4534\n",
      "\n",
      "Epoch [22/10000], Training Loss: 0.1189\n",
      "Epoch [22/10000], Validation Loss: 0.1232, Validation Accuracy: 96.19%\n",
      "Output Summary: Max=20.8985, Min=-46.9120, Median=-6.0593, Mean=-6.5148\n",
      "\n",
      "Epoch [23/10000], Training Loss: 0.1159\n",
      "Epoch [23/10000], Validation Loss: 0.1216, Validation Accuracy: 96.27%\n",
      "Output Summary: Max=21.1408, Min=-46.9578, Median=-6.1124, Mean=-6.5456\n",
      "\n",
      "Epoch [24/10000], Training Loss: 0.1130\n",
      "Epoch [24/10000], Validation Loss: 0.1195, Validation Accuracy: 96.30%\n",
      "Output Summary: Max=21.2984, Min=-46.7895, Median=-6.1822, Mean=-6.6002\n",
      "\n",
      "Epoch [25/10000], Training Loss: 0.1099\n",
      "Epoch [25/10000], Validation Loss: 0.1187, Validation Accuracy: 96.29%\n",
      "Output Summary: Max=21.5317, Min=-46.5247, Median=-6.1890, Mean=-6.5871\n",
      "\n",
      "Epoch [26/10000], Training Loss: 0.1078\n",
      "Epoch [26/10000], Validation Loss: 0.1182, Validation Accuracy: 96.33%\n",
      "Output Summary: Max=21.7272, Min=-46.4929, Median=-6.2322, Mean=-6.6164\n",
      "\n",
      "Epoch [27/10000], Training Loss: 0.1053\n",
      "Epoch [27/10000], Validation Loss: 0.1177, Validation Accuracy: 96.35%\n",
      "Output Summary: Max=21.7769, Min=-46.3884, Median=-6.1526, Mean=-6.5364\n",
      "\n",
      "Epoch [28/10000], Training Loss: 0.1030\n",
      "Epoch [28/10000], Validation Loss: 0.1175, Validation Accuracy: 96.38%\n",
      "Output Summary: Max=21.8297, Min=-46.3754, Median=-6.2075, Mean=-6.5882\n",
      "\n",
      "Epoch [29/10000], Training Loss: 0.1010\n",
      "Epoch [29/10000], Validation Loss: 0.1169, Validation Accuracy: 96.39%\n",
      "Output Summary: Max=21.9871, Min=-46.6046, Median=-6.2849, Mean=-6.6645\n",
      "\n",
      "Epoch [30/10000], Training Loss: 0.0991\n",
      "Epoch [30/10000], Validation Loss: 0.1170, Validation Accuracy: 96.39%\n",
      "Output Summary: Max=21.9077, Min=-46.5204, Median=-6.2987, Mean=-6.6806\n",
      "\n",
      "Epoch [31/10000], Training Loss: 0.0972\n",
      "Epoch [31/10000], Validation Loss: 0.1164, Validation Accuracy: 96.49%\n",
      "Output Summary: Max=22.1272, Min=-46.7775, Median=-6.3319, Mean=-6.7240\n",
      "\n",
      "Epoch [32/10000], Training Loss: 0.0953\n",
      "Epoch [32/10000], Validation Loss: 0.1155, Validation Accuracy: 96.55%\n",
      "Output Summary: Max=22.2052, Min=-46.7783, Median=-6.4139, Mean=-6.8043\n",
      "\n",
      "Epoch [33/10000], Training Loss: 0.0938\n",
      "Epoch [33/10000], Validation Loss: 0.1161, Validation Accuracy: 96.54%\n",
      "Output Summary: Max=22.2783, Min=-46.7228, Median=-6.4653, Mean=-6.8515\n",
      "\n",
      "Epoch [34/10000], Training Loss: 0.0922\n",
      "Epoch [34/10000], Validation Loss: 0.1150, Validation Accuracy: 96.65%\n",
      "Output Summary: Max=22.3377, Min=-47.1889, Median=-6.5481, Mean=-6.9378\n",
      "\n",
      "Epoch [35/10000], Training Loss: 0.0908\n",
      "Epoch [35/10000], Validation Loss: 0.1141, Validation Accuracy: 96.58%\n",
      "Output Summary: Max=22.3752, Min=-47.4014, Median=-6.6202, Mean=-6.9992\n",
      "\n",
      "Epoch [36/10000], Training Loss: 0.0895\n",
      "Epoch [36/10000], Validation Loss: 0.1139, Validation Accuracy: 96.59%\n",
      "Output Summary: Max=22.6051, Min=-47.8675, Median=-6.7790, Mean=-7.1584\n",
      "\n",
      "Epoch [37/10000], Training Loss: 0.0882\n",
      "Epoch [37/10000], Validation Loss: 0.1136, Validation Accuracy: 96.57%\n",
      "Output Summary: Max=22.7402, Min=-48.1390, Median=-6.8545, Mean=-7.2304\n",
      "\n",
      "Epoch [38/10000], Training Loss: 0.0870\n",
      "Epoch [38/10000], Validation Loss: 0.1128, Validation Accuracy: 96.53%\n",
      "Output Summary: Max=22.8689, Min=-48.3091, Median=-6.9259, Mean=-7.2957\n",
      "\n",
      "Epoch [39/10000], Training Loss: 0.0858\n",
      "Epoch [39/10000], Validation Loss: 0.1128, Validation Accuracy: 96.60%\n",
      "Output Summary: Max=23.0540, Min=-48.5942, Median=-7.0204, Mean=-7.3897\n",
      "\n",
      "Epoch [40/10000], Training Loss: 0.0845\n",
      "Epoch [40/10000], Validation Loss: 0.1123, Validation Accuracy: 96.61%\n",
      "Output Summary: Max=23.2220, Min=-48.9736, Median=-7.1273, Mean=-7.5009\n",
      "\n",
      "Epoch [41/10000], Training Loss: 0.0834\n",
      "Epoch [41/10000], Validation Loss: 0.1126, Validation Accuracy: 96.61%\n",
      "Output Summary: Max=23.2575, Min=-49.2712, Median=-7.1860, Mean=-7.5598\n",
      "\n",
      "Epoch [42/10000], Training Loss: 0.0823\n",
      "Epoch [42/10000], Validation Loss: 0.1120, Validation Accuracy: 96.63%\n",
      "Output Summary: Max=23.3529, Min=-49.4929, Median=-7.3278, Mean=-7.6918\n",
      "\n",
      "Epoch [43/10000], Training Loss: 0.0814\n",
      "Epoch [43/10000], Validation Loss: 0.1123, Validation Accuracy: 96.60%\n",
      "Output Summary: Max=23.5124, Min=-49.4019, Median=-7.3369, Mean=-7.7042\n",
      "\n",
      "Epoch [44/10000], Training Loss: 0.0804\n",
      "Epoch [44/10000], Validation Loss: 0.1121, Validation Accuracy: 96.63%\n",
      "Output Summary: Max=23.6059, Min=-49.7201, Median=-7.4493, Mean=-7.8182\n",
      "\n",
      "Epoch [45/10000], Training Loss: 0.0793\n",
      "Epoch [45/10000], Validation Loss: 0.1116, Validation Accuracy: 96.65%\n",
      "Output Summary: Max=23.6854, Min=-49.8486, Median=-7.5124, Mean=-7.8657\n",
      "\n",
      "Epoch [46/10000], Training Loss: 0.0784\n",
      "Epoch [46/10000], Validation Loss: 0.1116, Validation Accuracy: 96.66%\n",
      "Output Summary: Max=23.9447, Min=-50.8163, Median=-7.6359, Mean=-8.0089\n",
      "\n",
      "Epoch [47/10000], Training Loss: 0.0776\n",
      "Epoch [47/10000], Validation Loss: 0.1115, Validation Accuracy: 96.67%\n",
      "Output Summary: Max=23.9880, Min=-50.5703, Median=-7.6330, Mean=-7.9886\n",
      "\n",
      "Epoch [48/10000], Training Loss: 0.0769\n",
      "Epoch [48/10000], Validation Loss: 0.1114, Validation Accuracy: 96.66%\n",
      "Output Summary: Max=24.0796, Min=-50.9350, Median=-7.7434, Mean=-8.1003\n",
      "\n",
      "Epoch [49/10000], Training Loss: 0.0759\n",
      "Epoch [49/10000], Validation Loss: 0.1112, Validation Accuracy: 96.69%\n",
      "Output Summary: Max=24.1462, Min=-51.3680, Median=-7.7954, Mean=-8.1674\n",
      "\n",
      "Epoch [50/10000], Training Loss: 0.0754\n",
      "Epoch [50/10000], Validation Loss: 0.1116, Validation Accuracy: 96.67%\n",
      "Output Summary: Max=24.3484, Min=-51.7300, Median=-7.8919, Mean=-8.2472\n",
      "\n",
      "Epoch [51/10000], Training Loss: 0.0748\n",
      "Epoch [51/10000], Validation Loss: 0.1117, Validation Accuracy: 96.68%\n",
      "Output Summary: Max=24.3754, Min=-51.7263, Median=-7.8356, Mean=-8.2013\n",
      "\n",
      "Epoch [52/10000], Training Loss: 0.0742\n",
      "Epoch [52/10000], Validation Loss: 0.1122, Validation Accuracy: 96.71%\n",
      "Output Summary: Max=24.4572, Min=-51.9532, Median=-7.9141, Mean=-8.2926\n",
      "\n",
      "Epoch [53/10000], Training Loss: 0.0738\n",
      "Epoch [53/10000], Validation Loss: 0.1126, Validation Accuracy: 96.72%\n",
      "Output Summary: Max=24.7281, Min=-52.3588, Median=-7.9514, Mean=-8.3166\n",
      "\n",
      "Epoch [54/10000], Training Loss: 0.0734\n",
      "Epoch [54/10000], Validation Loss: 0.1121, Validation Accuracy: 96.71%\n",
      "Output Summary: Max=24.7221, Min=-52.2804, Median=-7.9377, Mean=-8.3066\n",
      "\n",
      "Epoch [55/10000], Training Loss: 0.0728\n",
      "Epoch [55/10000], Validation Loss: 0.1131, Validation Accuracy: 96.69%\n",
      "Output Summary: Max=24.8552, Min=-52.1792, Median=-7.9339, Mean=-8.2834\n",
      "\n",
      "Epoch [56/10000], Training Loss: 0.0725\n",
      "Epoch [56/10000], Validation Loss: 0.1136, Validation Accuracy: 96.68%\n",
      "Output Summary: Max=25.0198, Min=-52.8435, Median=-7.9959, Mean=-8.3529\n",
      "\n",
      "Epoch [57/10000], Training Loss: 0.0724\n",
      "Epoch [57/10000], Validation Loss: 0.1137, Validation Accuracy: 96.69%\n",
      "Output Summary: Max=24.9116, Min=-51.9610, Median=-7.7898, Mean=-8.1580\n",
      "\n",
      "Epoch [58/10000], Training Loss: 0.0719\n",
      "Epoch [58/10000], Validation Loss: 0.1127, Validation Accuracy: 96.72%\n",
      "Output Summary: Max=24.9917, Min=-52.7020, Median=-7.8155, Mean=-8.1924\n",
      "\n",
      "Epoch [59/10000], Training Loss: 0.0715\n",
      "Epoch [59/10000], Validation Loss: 0.1142, Validation Accuracy: 96.70%\n",
      "Output Summary: Max=25.1743, Min=-53.5841, Median=-7.9257, Mean=-8.3106\n",
      "\n",
      "Epoch [60/10000], Training Loss: 0.0710\n",
      "Epoch [60/10000], Validation Loss: 0.1147, Validation Accuracy: 96.68%\n",
      "Output Summary: Max=25.1709, Min=-54.3109, Median=-7.9826, Mean=-8.3841\n",
      "\n",
      "Epoch [61/10000], Training Loss: 0.0701\n",
      "Epoch [61/10000], Validation Loss: 0.1152, Validation Accuracy: 96.68%\n",
      "Output Summary: Max=25.3040, Min=-55.5310, Median=-8.1355, Mean=-8.5581\n",
      "\n",
      "Epoch [62/10000], Training Loss: 0.0689\n",
      "Epoch [62/10000], Validation Loss: 0.1167, Validation Accuracy: 96.62%\n",
      "Output Summary: Max=25.8004, Min=-56.4653, Median=-8.2953, Mean=-8.7284\n",
      "\n",
      "Epoch [63/10000], Training Loss: 0.0681\n",
      "Epoch [63/10000], Validation Loss: 0.1173, Validation Accuracy: 96.58%\n",
      "Output Summary: Max=25.8957, Min=-56.8784, Median=-8.4108, Mean=-8.8527\n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[192]\u001b[39m\u001b[32m, line 16\u001b[39m\n\u001b[32m     14\u001b[39m output = model(data)\n\u001b[32m     15\u001b[39m loss = criterion(output, target)\n\u001b[32m---> \u001b[39m\u001b[32m16\u001b[39m \u001b[43mloss\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     18\u001b[39m optimizer.step()\n\u001b[32m     19\u001b[39m running_loss += loss.item()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Downloads/All_Mac/Code/ML_experiments/.venv/lib/python3.11/site-packages/torch/_tensor.py:626\u001b[39m, in \u001b[36mTensor.backward\u001b[39m\u001b[34m(self, gradient, retain_graph, create_graph, inputs)\u001b[39m\n\u001b[32m    616\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    617\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[32m    618\u001b[39m         Tensor.backward,\n\u001b[32m    619\u001b[39m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[32m   (...)\u001b[39m\u001b[32m    624\u001b[39m         inputs=inputs,\n\u001b[32m    625\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m626\u001b[39m \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mautograd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    627\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m=\u001b[49m\u001b[43minputs\u001b[49m\n\u001b[32m    628\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Downloads/All_Mac/Code/ML_experiments/.venv/lib/python3.11/site-packages/torch/autograd/__init__.py:347\u001b[39m, in \u001b[36mbackward\u001b[39m\u001b[34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[39m\n\u001b[32m    342\u001b[39m     retain_graph = create_graph\n\u001b[32m    344\u001b[39m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[32m    345\u001b[39m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[32m    346\u001b[39m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m347\u001b[39m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    348\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    349\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    350\u001b[39m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    351\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    352\u001b[39m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    353\u001b[39m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    354\u001b[39m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    355\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Downloads/All_Mac/Code/ML_experiments/.venv/lib/python3.11/site-packages/torch/autograd/graph.py:823\u001b[39m, in \u001b[36m_engine_run_backward\u001b[39m\u001b[34m(t_outputs, *args, **kwargs)\u001b[39m\n\u001b[32m    821\u001b[39m     unregister_hooks = _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[32m    822\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m823\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_execution_engine\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[32m    824\u001b[39m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m    825\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[32m    826\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    827\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "patience = 40\n",
    "best_val_loss = float('inf')\n",
    "no_improvement_epochs = 0\n",
    "\n",
    "all_outputs = []\n",
    "\n",
    "for epoch in range(10000):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    num_batches = 0\n",
    "    \n",
    "    for data, target in get_batches(train_data, train_targets, batch_size):\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = criterion(output, target)\n",
    "        loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "        num_batches += 1\n",
    "\n",
    "    print(f\"Epoch [{epoch + 1}/{epochs}], Training Loss: {running_loss / num_batches:.4f}\")\n",
    "\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    num_batches = 0\n",
    "    epoch_outputs = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for data, target in get_batches(test_data, test_targets, batch_size):\n",
    "            outputs = model(data)\n",
    "            loss = criterion(outputs, target)\n",
    "            val_loss += loss.item()\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            total += target.size(0)\n",
    "            num_batches += 1\n",
    "            correct += (predicted == target).sum().item()\n",
    "\n",
    "            epoch_outputs.append(outputs)\n",
    "\n",
    "    all_outputs_tensor = torch.cat(epoch_outputs, dim=0)\n",
    "    all_outputs.append(all_outputs_tensor)\n",
    "\n",
    "    max_val = torch.max(all_outputs_tensor).item()\n",
    "    min_val = torch.min(all_outputs_tensor).item()\n",
    "    median_val = torch.median(all_outputs_tensor).item()\n",
    "    mean_val = torch.mean(all_outputs_tensor).item()\n",
    "\n",
    "    accuracy = 100 * correct / total\n",
    "    val_loss /= num_batches\n",
    "    print(f\"Epoch [{epoch + 1}/{epochs}], Validation Loss: {val_loss:.4f}, Validation Accuracy: {accuracy:.2f}%\")\n",
    "    print(f\"Output Summary: Max={max_val:.4f}, Min={min_val:.4f}, Median={median_val:.4f}, Mean={mean_val:.4f}\")\n",
    "    print()\n",
    "\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        no_improvement_epochs = 0\n",
    "    else:\n",
    "        no_improvement_epochs += 1\n",
    "\n",
    "    if no_improvement_epochs >= patience:\n",
    "        print(f\"Early stopping triggered after {epoch + 1} epochs.\")\n",
    "        print(best_val_loss)\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "04bece12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x0 Parameter containing:\n",
      "tensor([[[-4.8991e-03,  2.5201e-02, -1.1596e-02,  9.7011e-04,  8.4435e-03,\n",
      "           5.4457e-03,  1.2147e-02,  2.1471e-02,  7.7073e-03, -3.0992e-03,\n",
      "          -1.3756e-02,  2.0155e-02,  1.1545e-02, -9.1131e-02, -2.3543e-02,\n",
      "          -2.2076e-02,  7.2532e-03,  1.0363e-02, -1.7747e-02, -1.6975e-02,\n",
      "          -7.4943e-03, -1.7222e-02, -6.3775e-03,  6.7688e-02,  4.6023e-04,\n",
      "           3.6882e-03,  8.9947e-03,  1.8488e-02],\n",
      "         [-5.2839e-03,  8.6376e-04,  3.4281e-02,  2.9808e-03,  3.6648e-03,\n",
      "           4.5234e-02,  2.3797e-02,  2.6553e-03,  1.0506e-02, -4.4785e-03,\n",
      "           6.2421e-03,  2.2334e-02, -3.2451e-02,  2.3809e-02, -2.4318e-02,\n",
      "          -1.3675e-01,  5.2540e-03, -9.0208e-04, -9.9969e-03,  2.3349e-02,\n",
      "           1.3842e-02, -1.1336e-03,  4.6699e-03,  2.9924e-02,  1.9460e-02,\n",
      "           3.9729e-03,  4.3285e-02,  4.0165e-02],\n",
      "         [ 9.5744e-03,  3.5709e-02, -1.2582e-03,  1.4985e-03, -5.3291e-03,\n",
      "           1.3135e-02,  2.2672e-02,  3.1037e-02, -1.8326e-02,  2.6862e-02,\n",
      "          -3.5729e-02, -3.2729e-02,  2.9682e-02, -6.3957e-03, -4.5634e-02,\n",
      "          -2.5230e-04, -3.1417e-02, -1.6841e-02,  1.0984e-02,  8.0537e-03,\n",
      "           2.2585e-02,  6.1132e-02,  1.6658e-02,  3.7283e-02,  3.3012e-02,\n",
      "           1.4852e-02,  3.1384e-02, -9.6145e-03],\n",
      "         [-5.4239e-03,  3.9087e-03,  1.8618e-03,  9.6999e-03,  4.2435e-02,\n",
      "           1.5893e-02,  5.0930e-02,  4.7556e-03, -1.2318e-02,  1.4244e-02,\n",
      "           6.4413e-04,  3.9264e-02,  4.4818e-02,  2.8725e-02,  1.1627e-02,\n",
      "          -1.2918e-01, -1.4531e-01,  7.3921e-02,  3.9457e-02,  7.4995e-03,\n",
      "           1.7739e-02,  6.5564e-02, -2.3232e-02,  2.4610e-02,  1.2647e-01,\n",
      "           1.0907e-02,  4.8028e-04,  3.5226e-02],\n",
      "         [ 3.6320e-02,  6.3288e-02,  2.2889e-03, -2.7736e-02, -3.8148e-03,\n",
      "           1.2955e-01,  6.6828e-02,  5.8562e-03, -1.7771e-02, -1.3860e-02,\n",
      "           1.1073e-02,  2.5141e-04,  8.2382e-02,  1.6391e-02,  3.4971e-03,\n",
      "          -6.6577e-03, -8.9701e-03,  2.9529e-02, -1.1816e-02,  2.8387e-02,\n",
      "           3.8715e-02,  1.4846e-02,  2.3710e-02,  6.9195e-02,  4.6279e-02,\n",
      "           7.9980e-03,  6.0943e-03,  1.2739e-02],\n",
      "         [ 1.6627e-03,  2.1782e-02, -2.7874e-03, -2.5065e-03,  9.7418e-02,\n",
      "           3.0673e-03,  1.6598e-03,  1.9710e-02, -2.1749e-02,  1.3712e-02,\n",
      "           3.1590e-02,  1.2532e-02,  5.7280e-03,  6.3113e-02, -1.7581e-02,\n",
      "          -4.7214e-02, -6.7060e-02,  8.5770e-03,  5.4295e-02, -5.5434e-02,\n",
      "          -5.6287e-03,  7.6538e-03, -1.4942e-02, -4.8120e-03,  1.9996e-02,\n",
      "           7.0815e-02,  8.9174e-02, -9.6729e-02],\n",
      "         [-4.6316e-03,  8.1492e-02, -2.8650e-03, -2.0049e-02,  9.2378e-04,\n",
      "           5.7280e-03, -6.7487e-03,  3.1499e-02, -1.4591e-02, -1.7140e-02,\n",
      "           8.2873e-04, -1.0978e-01, -6.2930e-02, -3.3878e-02, -7.1980e-02,\n",
      "          -7.9069e-02, -1.0469e-01, -5.7380e-02, -4.8415e-02,  1.3888e-02,\n",
      "           4.7626e-02,  3.6083e-03,  2.8908e-02, -4.2657e-03,  3.0039e-02,\n",
      "           5.0113e-02, -1.7540e-02, -2.8373e-02],\n",
      "         [ 1.0971e-02,  7.3578e-02,  5.1539e-02, -4.1191e-03,  6.1856e-03,\n",
      "           2.1872e-02, -3.7163e-03, -6.6990e-03, -5.6189e-02, -7.0818e-02,\n",
      "          -7.3033e-02, -7.2616e-02, -1.4232e-01, -5.2520e-02, -8.3509e-02,\n",
      "          -1.1758e-01, -1.3060e-01, -7.6487e-02, -2.7715e-02, -2.3400e-02,\n",
      "           2.6058e-02, -1.7620e-03,  3.1166e-03,  3.4874e-02, -5.2479e-03,\n",
      "          -1.9155e-03,  8.8183e-03, -1.7676e-02],\n",
      "         [-2.2499e-03,  2.9696e-02,  6.2048e-02, -1.8253e-04,  3.6441e-02,\n",
      "          -2.6000e-02, -3.0490e-02, -2.4638e-02, -3.6091e-02, -8.5181e-02,\n",
      "          -4.9405e-02, -9.3946e-02, -1.4024e-01, -5.5614e-02, -9.9792e-02,\n",
      "          -8.4434e-02, -7.7498e-02, -8.3829e-02, -5.0897e-02, -1.7656e-02,\n",
      "          -6.8107e-03,  5.9298e-04, -2.0157e-03,  4.4601e-02,  2.9137e-02,\n",
      "           7.3381e-03,  3.5391e-02, -4.4254e-02],\n",
      "         [ 7.6982e-02, -6.9241e-04,  2.0801e-02,  1.0492e-02, -7.0586e-04,\n",
      "           7.9576e-03, -5.4334e-02, -5.8987e-02,  4.8741e-03, -3.2848e-02,\n",
      "          -8.4705e-02, -8.4545e-02, -5.2596e-02, -7.9821e-02, -5.4118e-02,\n",
      "          -6.2916e-02, -8.9748e-02, -3.8813e-02, -5.3912e-02, -6.0027e-02,\n",
      "           2.2891e-02,  3.7191e-02, -2.4654e-02,  2.6163e-02, -1.4151e-02,\n",
      "           3.5009e-06, -4.9108e-03, -2.7459e-02],\n",
      "         [-1.6637e-02, -4.6753e-03,  1.0022e-01, -3.2901e-02, -7.9092e-03,\n",
      "           1.7766e-02,  3.8349e-03, -2.7029e-02, -3.5573e-02, -4.7071e-04,\n",
      "          -9.4243e-02, -8.0122e-02, -6.4087e-02, -9.2217e-02, -1.1588e-01,\n",
      "          -8.0112e-02, -8.7383e-02, -1.0609e-01, -4.4884e-02, -5.7069e-02,\n",
      "          -5.7964e-02,  4.0098e-03,  2.4207e-02, -2.8785e-02, -1.9888e-02,\n",
      "           1.8571e-02,  1.4086e-02, -2.2316e-02],\n",
      "         [-1.8704e-02,  2.0728e-02,  1.7759e-03, -3.4699e-02, -1.1029e-02,\n",
      "           1.8601e-02,  1.0962e-03, -6.0825e-02, -5.8103e-02, -8.2727e-02,\n",
      "          -9.4717e-02, -7.7544e-02, -8.6220e-02, -1.5202e-01, -1.0378e-01,\n",
      "          -8.6438e-02, -9.4741e-02, -8.0546e-02, -3.7532e-02, -4.2873e-03,\n",
      "          -1.2820e-02,  2.0450e-02, -6.0272e-03,  5.4400e-02,  4.5177e-02,\n",
      "          -1.7116e-02, -4.9602e-03, -5.0620e-02],\n",
      "         [-4.0014e-02, -2.2193e-03,  1.6393e-02,  1.0484e-02, -2.3081e-03,\n",
      "           1.7566e-02,  1.7610e-02, -2.9137e-02, -8.2721e-02, -4.3737e-02,\n",
      "          -5.0126e-02, -7.6470e-02, -7.5027e-02, -6.3216e-02, -1.3272e-01,\n",
      "          -2.7240e-02, -8.0819e-02, -1.0107e-01, -6.5558e-02, -2.0691e-02,\n",
      "          -1.0201e-02,  2.2666e-02,  5.5559e-02,  3.8898e-02,  3.3287e-02,\n",
      "           1.1071e-01, -8.0921e-03, -8.9666e-03],\n",
      "         [ 1.2133e-02, -1.8892e-02,  2.7016e-02,  3.3504e-03,  1.7098e-02,\n",
      "           1.4439e-02,  1.9382e-04, -1.8588e-05, -6.6815e-02, -7.9016e-02,\n",
      "          -6.8661e-02, -1.1361e-02, -4.1888e-02, -1.1676e-01, -8.6807e-02,\n",
      "          -6.4335e-02, -8.7603e-02, -6.9902e-02, -9.4003e-02,  1.9215e-02,\n",
      "           3.2799e-02, -1.2627e-02,  2.5821e-02,  7.1968e-02,  3.6865e-02,\n",
      "           3.5675e-02,  2.9704e-02,  2.5027e-02],\n",
      "         [ 1.1098e-02,  1.1699e-02,  2.2046e-02,  1.0804e-02, -5.0973e-03,\n",
      "           5.8792e-02,  5.3035e-02, -4.4992e-02, -1.6312e-02, -3.5735e-02,\n",
      "          -4.6777e-02, -3.1354e-02, -1.4214e-01, -3.6966e-02, -9.9119e-02,\n",
      "          -1.2171e-01, -3.1323e-02, -1.3695e-01, -7.3833e-02, -2.9140e-02,\n",
      "           1.2734e-02,  1.9498e-02, -9.8137e-03,  8.5616e-02,  4.9671e-02,\n",
      "           3.9608e-02,  2.7073e-02, -3.4744e-02],\n",
      "         [ 5.7667e-02,  2.5875e-02,  1.0597e-02,  3.4307e-02, -4.5070e-03,\n",
      "           4.2305e-02,  8.7086e-03, -3.5599e-02, -4.4521e-02, -4.8829e-03,\n",
      "          -6.4371e-02, -2.2193e-02, -4.4957e-02, -5.5195e-02, -8.3574e-02,\n",
      "          -8.1705e-02, -8.4130e-02, -5.4297e-02, -7.9833e-02,  2.1801e-02,\n",
      "           2.2131e-02,  3.6708e-04, -7.9690e-03, -2.4983e-02, -1.5357e-02,\n",
      "          -1.9804e-02, -1.7661e-02, -3.0056e-03],\n",
      "         [ 5.9447e-02,  5.6386e-02,  2.8671e-02,  2.6423e-02, -5.2756e-04,\n",
      "          -1.1268e-02, -1.7003e-02, -5.2800e-02, -4.6885e-03, -1.8746e-02,\n",
      "          -3.3909e-02,  3.6074e-02, -9.3160e-03, -1.5786e-02, -8.3506e-02,\n",
      "          -1.0302e-01, -6.6318e-02, -1.1417e-01,  1.8782e-02, -1.6914e-02,\n",
      "           2.0579e-03,  7.0053e-03, -3.1419e-02,  1.5744e-02,  2.9352e-02,\n",
      "          -3.2034e-02, -2.2262e-02, -2.6694e-02],\n",
      "         [ 5.0425e-02,  4.8353e-02, -6.0122e-03,  2.3905e-02,  2.5635e-03,\n",
      "           5.2928e-03, -8.7173e-03, -5.8921e-02, -1.9588e-03, -1.2030e-02,\n",
      "          -7.2601e-02, -6.6562e-03, -3.4291e-02, -9.7270e-02, -1.4338e-02,\n",
      "          -1.4360e-01, -7.4974e-02,  3.2866e-03, -1.1983e-01, -1.2798e-02,\n",
      "           2.6590e-02, -2.8702e-02,  5.1903e-04,  8.9549e-03, -1.3884e-02,\n",
      "           5.3023e-03,  1.8544e-03, -2.6401e-02],\n",
      "         [ 4.7002e-02,  3.3573e-02,  2.2626e-04,  7.3531e-03,  6.1006e-02,\n",
      "          -8.3514e-03, -5.5578e-03,  8.8643e-03, -2.2214e-02, -2.1929e-02,\n",
      "          -4.4530e-02, -3.4239e-02, -9.6571e-02, -1.5329e-02, -1.1178e-01,\n",
      "          -1.2420e-01, -6.1934e-02, -9.9844e-02, -3.4244e-02, -5.3355e-02,\n",
      "          -6.0154e-02, -2.1427e-02,  1.5539e-02, -1.8358e-02,  3.8079e-02,\n",
      "           9.0966e-02, -5.3162e-03, -6.6021e-02],\n",
      "         [ 7.3953e-02,  6.0123e-02,  4.2645e-02,  5.2920e-03,  8.3574e-03,\n",
      "           2.9948e-02, -1.1337e-02, -3.6337e-02,  9.5128e-03, -6.9444e-02,\n",
      "          -5.7451e-02, -3.6051e-02, -8.7064e-02, -3.6171e-02, -7.7204e-02,\n",
      "          -5.9452e-02, -7.3726e-02, -1.0958e-01, -9.9445e-02,  1.2356e-02,\n",
      "          -1.6224e-02,  6.8872e-02, -2.3884e-02, -7.3106e-02,  4.5187e-02,\n",
      "           4.1065e-02,  2.3754e-02, -6.6456e-02],\n",
      "         [ 1.1336e-02,  2.1433e-02,  4.4323e-02,  2.1771e-02,  5.9791e-02,\n",
      "          -1.8473e-02, -3.3990e-02, -1.3252e-02, -1.3065e-01, -6.5737e-02,\n",
      "          -7.0830e-02, -4.5233e-02, -1.0874e-01, -8.9117e-02, -7.9345e-02,\n",
      "          -7.7488e-02, -5.3470e-02, -5.5534e-02, -3.7647e-02, -3.1205e-02,\n",
      "           1.5088e-02,  1.2480e-02,  5.5237e-02,  1.0835e-01,  7.8286e-03,\n",
      "           4.4178e-02, -1.0591e-02, -3.8207e-02],\n",
      "         [ 5.1185e-02,  2.0832e-02,  1.3532e-02,  2.6456e-02, -4.7704e-02,\n",
      "          -1.1052e-02,  4.0090e-03, -1.2061e-01, -1.0336e-01, -1.1048e-01,\n",
      "          -3.5027e-02, -1.3253e-01, -1.7188e-01, -7.8054e-02, -6.1875e-02,\n",
      "          -7.0853e-02, -6.8911e-02, -1.7776e-02, -7.4508e-02,  3.0164e-02,\n",
      "           8.6584e-02, -3.6257e-02, -2.7599e-02,  2.7160e-02,  3.2314e-02,\n",
      "           5.1600e-02,  4.0066e-02, -1.6903e-02],\n",
      "         [-9.2092e-03,  4.2811e-02,  6.7703e-02, -3.0697e-03,  2.5967e-02,\n",
      "          -2.6045e-02, -1.8099e-02,  3.8091e-04, -3.0447e-02, -2.0193e-02,\n",
      "          -4.3553e-02, -5.1601e-02, -6.2331e-02, -1.3834e-01, -5.0690e-02,\n",
      "          -7.5540e-02, -1.3431e-03,  1.3255e-02, -7.0083e-02,  1.6603e-02,\n",
      "           7.6114e-02,  4.5418e-02,  3.6399e-03, -1.5384e-03,  2.0069e-02,\n",
      "           6.1720e-03,  2.1289e-02,  1.1345e-02],\n",
      "         [-4.0758e-03, -8.0782e-02, -3.0459e-02,  4.0066e-02,  3.7819e-02,\n",
      "           4.1899e-02,  7.3373e-02,  1.3364e-02, -2.2375e-03, -6.3030e-02,\n",
      "          -5.4808e-02, -6.3866e-02, -7.5980e-02, -4.8398e-02,  6.6328e-03,\n",
      "          -1.6493e-02,  1.4673e-03,  2.3299e-02, -1.7826e-02, -3.3946e-02,\n",
      "           3.4786e-02,  9.4379e-03, -3.6009e-03,  9.5994e-03,  1.5345e-02,\n",
      "           6.8541e-02,  3.0303e-02, -1.6682e-02],\n",
      "         [ 1.0318e-02, -3.6923e-03,  2.5656e-02,  5.1485e-03, -1.3659e-03,\n",
      "           4.2978e-03,  3.7224e-03,  1.5549e-02,  7.7733e-03, -1.3251e-03,\n",
      "          -5.7734e-02, -3.0749e-02, -4.4005e-03, -1.9029e-02, -8.2671e-02,\n",
      "          -7.7908e-02, -7.4415e-03,  4.1698e-02,  3.3274e-02, -1.6408e-02,\n",
      "           1.5510e-02, -1.1691e-02,  3.2934e-03, -2.9681e-03, -5.4237e-04,\n",
      "          -2.9457e-02, -3.3177e-02,  2.1791e-02],\n",
      "         [-4.5803e-02, -1.3920e-02,  2.0016e-02, -1.3576e-01,  2.0797e-02,\n",
      "           8.2022e-03, -3.8838e-03,  2.9653e-02,  3.8909e-03, -1.2032e-02,\n",
      "           3.0130e-02, -6.1297e-02, -8.4639e-03,  3.3898e-02, -9.1776e-04,\n",
      "          -2.2042e-03, -1.7178e-02,  3.8200e-02, -2.4018e-02, -8.2282e-03,\n",
      "           1.8740e-02,  9.7346e-03, -3.4255e-02,  1.0776e-03, -9.4489e-03,\n",
      "           2.3094e-02,  5.6986e-02, -1.7982e-02],\n",
      "         [ 1.7497e-02,  5.5542e-03, -3.5165e-03,  8.7860e-02, -3.9079e-02,\n",
      "          -1.7818e-03,  2.3941e-03, -1.0634e-02,  1.4605e-02,  4.4737e-03,\n",
      "          -1.9008e-02,  5.3846e-03,  2.0619e-02, -1.0634e-02,  1.9981e-02,\n",
      "          -1.9444e-02, -4.7205e-02, -1.4167e-03,  4.5998e-02,  9.1099e-03,\n",
      "          -6.2317e-03,  5.9080e-02,  3.2565e-02,  1.8715e-02,  3.7181e-02,\n",
      "           1.3792e-01,  8.8865e-03, -2.4143e-02],\n",
      "         [-2.0314e-02,  3.6041e-02,  1.9668e-02, -3.8043e-03, -2.7491e-03,\n",
      "           4.1059e-02,  8.3690e-03,  1.7857e-02,  1.4065e-02,  1.2598e-02,\n",
      "           1.0947e-04, -2.1187e-02, -2.0713e-02,  2.0740e-02,  1.7648e-02,\n",
      "          -3.5375e-02, -4.0763e-02, -1.3890e-02,  1.2897e-02, -1.3471e-03,\n",
      "           1.7165e-02,  4.2966e-02,  8.0453e-03,  3.2107e-02, -5.8894e-03,\n",
      "           2.5344e-02,  6.6178e-03,  1.2700e-02]]], device='mps:0',\n",
      "       requires_grad=True)\n",
      "h0 Parameter containing:\n",
      "tensor([[-0.0348, -0.0087,  0.8572, -0.5675, -0.6130,  0.1411,  0.1402,  0.6550,\n",
      "         -0.5596,  0.4733, -1.0735,  0.0504, -1.4166,  0.0584, -2.2291,  0.4211,\n",
      "         -0.0337, -0.0734,  0.4203,  2.2424, -0.0814,  0.0336, -0.1621, -0.0035,\n",
      "         -0.0820,  1.1010,  0.5685,  0.3328, -0.3807, -0.3136, -0.0872,  1.3586]],\n",
      "       device='mps:0', requires_grad=True)\n",
      "rnn_cell.fc.weight Parameter containing:\n",
      "tensor([[ 0.1137,  0.0268, -0.0195,  ...,  0.0983,  0.1568, -0.1247],\n",
      "        [-0.0182,  0.0482, -0.0109,  ...,  0.0087, -0.0595,  0.0259],\n",
      "        [-0.0884,  0.0207, -0.0041,  ..., -0.0795,  0.0663,  0.0804],\n",
      "        ...,\n",
      "        [-0.0681,  0.0095,  0.0216,  ...,  0.2414,  0.3599, -0.0673],\n",
      "        [-0.2373, -0.0486, -0.0137,  ...,  0.1850,  0.5794,  0.2005],\n",
      "        [-0.0686,  0.0222, -0.0694,  ..., -0.1187,  0.0790,  0.0353]],\n",
      "       device='mps:0', requires_grad=True)\n",
      "rnn_cell.fc.bias Parameter containing:\n",
      "tensor([ 0.0244,  0.0136, -0.0315,  0.1249,  0.1441,  0.0928,  0.0581, -0.0092,\n",
      "         0.0938,  0.0085,  0.0953, -0.0014,  0.0041, -0.1127, -0.1956,  0.0234,\n",
      "        -0.0186,  0.0133,  0.0190,  0.0673,  0.2536,  0.0268, -0.1003, -0.0093,\n",
      "         0.0551, -0.0543, -0.0325, -0.0344,  0.2245,  0.0258,  0.2371,  0.0989],\n",
      "       device='mps:0', requires_grad=True)\n",
      "fc.weight Parameter containing:\n",
      "tensor([[ 0.3347, -0.3230,  0.6589, -0.3244, -0.0237, -0.9474, -0.7827, -0.5439,\n",
      "          0.4499,  0.1188,  0.8663, -0.5646, -0.8603,  0.5528, -0.8129, -0.6426,\n",
      "         -0.8759, -0.2614,  0.7314,  0.6455,  0.3861,  0.6081,  0.1303,  0.1388,\n",
      "         -0.4118, -0.4864,  0.5126,  0.1495,  0.4803,  0.3523,  0.5284, -0.5544],\n",
      "        [-0.8103,  0.1098,  0.1362, -0.9803, -0.3084,  0.2427,  0.1156,  0.8367,\n",
      "         -0.6814, -0.5539, -0.5920,  0.4243, -0.9868, -0.2507, -0.2359, -0.1079,\n",
      "         -0.7660,  0.8138, -0.1117, -0.8026, -0.4408,  0.6885, -0.5143,  1.2138,\n",
      "         -0.4044,  0.7861, -0.4343,  0.4082,  0.7295, -0.7521, -0.8482,  0.5203],\n",
      "        [-0.5222, -0.8823,  0.7067,  0.6097,  0.2868,  0.9079,  0.0308, -0.3689,\n",
      "         -0.3499, -0.7726, -0.0330, -0.4582, -0.4627, -0.5181,  0.6209,  0.9813,\n",
      "          1.0936,  0.1810,  0.1878,  0.8271,  0.0261,  0.5162,  0.7132, -0.0377,\n",
      "          0.4763, -0.4483,  0.6218,  0.4631,  0.4614,  0.4885, -0.2085, -0.6250],\n",
      "        [ 0.4425,  0.5204, -0.4259,  0.4902,  0.9182, -0.5434,  0.5288, -0.2050,\n",
      "         -0.0493,  0.4776, -0.2696,  0.0211,  0.9141, -0.0456,  0.7308,  0.6846,\n",
      "          0.3857,  0.8409, -0.3609, -0.8211, -0.4828,  0.8019, -0.9775, -0.9199,\n",
      "         -0.6578, -1.1867,  0.9084, -0.5835,  0.5929, -0.1838,  0.7517, -0.1547],\n",
      "        [ 0.7179, -0.3615, -0.5220, -0.4844,  0.3801,  0.1739,  0.2748,  0.3893,\n",
      "          0.1443,  0.8624, -0.1538, -0.8984,  0.1507,  0.8959, -0.6666, -0.3764,\n",
      "          0.7853,  0.2307, -0.4356, -0.5388,  0.0087, -0.8775,  0.2965, -0.5245,\n",
      "          1.0866,  0.1719, -0.2175, -0.8475, -0.1879,  1.2268, -0.3044, -0.4998],\n",
      "        [-0.1502,  0.4005, -0.6348,  0.2094, -0.7490,  0.8460,  0.6799, -0.5017,\n",
      "          0.3314,  0.2129,  0.6551,  0.2067,  0.6129, -0.1855,  0.6373, -0.3864,\n",
      "         -1.1669, -1.1338, -0.7162, -1.1828,  0.9763, -0.8706, -0.9919,  1.0894,\n",
      "         -0.5682, -0.6966, -0.7869,  0.2354,  0.3282,  0.7596,  1.1600, -0.1131],\n",
      "        [-0.9886, -0.0813,  0.4757,  0.4669, -1.0260,  0.2769,  0.0098,  0.8002,\n",
      "         -0.7317, -0.0357, -0.5853, -0.3428, -0.6740,  0.5759,  0.1691, -0.7860,\n",
      "         -0.3889, -0.1086,  0.1605,  0.7588,  0.9354, -0.8933,  0.7090, -0.6400,\n",
      "         -0.3081,  0.3421,  0.4987,  1.0228, -0.7628,  0.3470,  0.8323,  1.0497],\n",
      "        [ 0.0849, -0.3747, -0.0980,  0.6126,  0.4890, -0.1624, -0.2535, -0.4186,\n",
      "          0.1140, -0.6323,  0.8275,  0.6973,  0.8792,  0.1674, -0.5896,  0.7392,\n",
      "         -0.3554, -0.4782, -0.6025,  0.8758, -0.1194,  0.3732,  0.5889,  0.4909,\n",
      "          1.1967,  0.8869,  0.5323, -1.3297,  0.7740, -0.4281, -0.5187,  0.7451],\n",
      "        [ 0.1430,  1.0072, -0.0167,  0.3457, -0.6679, -0.4483,  0.2972, -0.7405,\n",
      "          0.4812,  0.4976, -0.2109, -0.2745,  0.4347, -0.4050,  0.6051, -0.2969,\n",
      "          0.3394,  0.3640, -0.0385,  1.0086, -0.8666,  0.3304,  1.2499,  0.1607,\n",
      "         -1.1423, -0.2826, -0.6260,  1.0324, -0.6630,  0.3934, -0.4943, -0.2879],\n",
      "        [ 0.8122, -0.3891, -0.3821, -0.3979,  0.3679, -0.0920, -0.5259,  0.2203,\n",
      "          0.2799, -0.2166, -0.8199,  0.5309, -0.4130, -1.0593, -0.4658, -0.6841,\n",
      "          0.6928, -0.5452,  0.8093, -1.2292,  0.4696,  0.5850, -0.7118,  0.0961,\n",
      "          0.6970,  0.0364, -0.3890, -0.4798, -0.9160, -1.5495, -0.8729, -0.7249]],\n",
      "       device='mps:0', requires_grad=True)\n",
      "fc.bias Parameter containing:\n",
      "tensor([-0.6710,  0.0627,  0.3802,  0.2248, -0.1059,  0.9339, -0.4748, -0.2499,\n",
      "        -0.3663,  0.1558], device='mps:0', requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "for name, param in model.named_parameters():\n",
    "    print(name, param)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed2a7b21",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
