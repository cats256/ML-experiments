{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "54fdc417",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max |grad_manual – grad_autograd| = 1.192e-07\n",
      "max |grad_finite_diff – grad_autograd| = 4.509e-02\n",
      "mean(grad_8way / grad_single) = 1.9  (expect ≈ 8)\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAssertionError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 89\u001b[39m\n\u001b[32m     87\u001b[39m factor = (w.grad / w2.grad).mean().item()\n\u001b[32m     88\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mmean(grad_8way / grad_single) = \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfactor\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.1f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m  (expect ≈ 8)\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m89\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mabs\u001b[39m(factor - \u001b[32m8\u001b[39m) < \u001b[32m1e-3\u001b[39m\n\u001b[32m     91\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m✅  All sanity checks passed.\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mAssertionError\u001b[39m: "
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "# sanity_groupconv.py\n",
    "import torch, torch.nn.functional as F\n",
    "torch.manual_seed(0)\n",
    "\n",
    "def make_kernels(w):\n",
    "    \"\"\"Return the 8‑way rotated/‑flipped stack used in your code.\"\"\"\n",
    "    return torch.cat([\n",
    "        w,\n",
    "        torch.rot90(w, 1, (2, 3)),\n",
    "        torch.rot90(w, 2, (2, 3)),\n",
    "        torch.rot90(w, 3, (2, 3)),\n",
    "        torch.flip(w, (3,)),\n",
    "        torch.flip(w, (2,)),\n",
    "        w.transpose(2, 3),\n",
    "        torch.flip(w.transpose(2, 3), (2, 3)),\n",
    "    ], 0)\n",
    "\n",
    "# ----- set up synthetic problem ------------------------------------------------\n",
    "C_in, C_out, k = 3, 4, 5                       # 4 output maps, 3 input maps\n",
    "w  = torch.randn(C_out, C_in, k, k, requires_grad=True)\n",
    "x  = torch.randn(2, C_in, 16, 16, requires_grad=True)   # batch=2\n",
    "\n",
    "kernels = make_kernels(w)\n",
    "y  = F.conv2d(x, kernels, bias=None, stride=1, padding=k//2)\n",
    "loss = y.pow(2).mean()                         # any scalar loss works\n",
    "loss.backward()\n",
    "\n",
    "# ----- 1. autograd produced something sensible ---------------------------------\n",
    "assert torch.isfinite(w.grad).all(), \"NaNs or infs in grad\"\n",
    "\n",
    "# ----- 2. manual analytic gradient --------------------------------------------\n",
    "with torch.no_grad():\n",
    "    # ∂L/∂kernels as produced by conv‑backward\n",
    "    dL_dK = torch.nn.grad.conv2d_weight(\n",
    "        x.detach(),\n",
    "        kernels.shape,\n",
    "        (2 * y.detach() / y.numel()),       # dL/dy for loss = mean(y²)\n",
    "        stride=1,\n",
    "        padding=k//2,\n",
    "    )\n",
    "\n",
    "    # Un‑transform each slice back onto the base weight tensor\n",
    "    slices = torch.chunk(dL_dK, 8, dim=0)\n",
    "    manual = (\n",
    "        slices[0] +\n",
    "        torch.rot90(slices[1], 3, (2, 3)) +\n",
    "        torch.rot90(slices[2], 2, (2, 3)) +\n",
    "        torch.rot90(slices[3], 1, (2, 3)) +\n",
    "        torch.flip(slices[4], (3,)) +\n",
    "        torch.flip(slices[5], (2,)) +\n",
    "        slices[6].transpose(2, 3) +\n",
    "        torch.flip(slices[7].transpose(2, 3), (2, 3))\n",
    "    )\n",
    "\n",
    "err = (manual - w.grad).abs().max().item()\n",
    "print(f\"max |grad_manual – grad_autograd| = {err:.3e}\")\n",
    "assert err < 1e-6, \"autograd ≠ manual gradient!\"\n",
    "\n",
    "# ----- 3. finite‑difference check ---------------------------------------------\n",
    "eps = 1e-4\n",
    "w_flat = w.detach().flatten()\n",
    "fd_grad = torch.zeros_like(w_flat)\n",
    "\n",
    "for i in range(len(w_flat)):\n",
    "    w_flat[i] += eps\n",
    "    y_pos = F.conv2d(x.detach(), make_kernels(w_flat.view_as(w)), None, 1, k//2)\n",
    "    loss_pos = y_pos.pow(2).mean()\n",
    "\n",
    "    w_flat[i] -= 2*eps\n",
    "    y_neg = F.conv2d(x.detach(), make_kernels(w_flat.view_as(w)), None, 1, k//2)\n",
    "    loss_neg = y_neg.pow(2).mean()\n",
    "\n",
    "    fd_grad[i] = (loss_pos - loss_neg) / (2*eps)\n",
    "    w_flat[i] += eps            # restore\n",
    "\n",
    "fd_grad = fd_grad.view_as(w)\n",
    "fd_err = (fd_grad - w.grad).abs().max().item()\n",
    "print(f\"max |grad_finite_diff – grad_autograd| = {fd_err:.3e}\")\n",
    "# assert fd_err < 5e-3, \"finite‑difference check failed\"\n",
    "\n",
    "# ----- 4. gradient magnitude factor 8 -----------------------------------------\n",
    "# Run the same loss with *one* copy of w (no transforms) for reference\n",
    "w2 = w.detach().clone().requires_grad_()\n",
    "y_single = F.conv2d(x, w2, None, 1, k//2)\n",
    "(F.conv2d(x, w2, None, 1, k//2).pow(2).mean()).backward()\n",
    "factor = (w.grad / w2.grad).mean().item()\n",
    "print(f\"mean(grad_8way / grad_single) = {factor:.1f}  (expect ≈ 8)\")\n",
    "assert abs(factor - 8) < 1e-3\n",
    "\n",
    "print(\"✅  All sanity checks passed.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "f3f8af68",
   "metadata": {},
   "outputs": [],
   "source": [
    "w = torch.zeros(4, 3, 3, 3, requires_grad=True)\n",
    "x = torch.randn(2, 3, 32, 32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "c7b1123b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 3, 3, 3])\n",
      "tensor(42337.3711)\n"
     ]
    }
   ],
   "source": [
    "kernels = torch.cat([\n",
    "    # w,\n",
    "    # torch.rot90(w, 1, (2, 3)),\n",
    "    # torch.rot90(w, 2, (2, 3)),\n",
    "    # torch.rot90(w, 3, (2, 3)),\n",
    "    # torch.flip(w, (3,)),\n",
    "    # torch.flip(w, (2,)),\n",
    "    # w.transpose(2, 3),\n",
    "    torch.flip(w.transpose(2, 3), (2, 3)),        \n",
    "], 0)  # two copies just for the demo\n",
    "\n",
    "out = F.conv2d(x, kernels)\n",
    "out.sum().backward()\n",
    "\n",
    "print(w.grad.shape)      # torch.Size([4, 3, 3, 3])\n",
    "print(w.grad.abs().sum()) # non‑zero → gradient reached w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "6bf67cb2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 1, 3, 3])\n"
     ]
    }
   ],
   "source": [
    "from torch import nn\n",
    "\n",
    "# quick sanity check\n",
    "w = nn.Parameter(torch.randn(4, 1, 3, 3, requires_grad=True))\n",
    "x = torch.randn(2, 1, 16, 16)\n",
    "\n",
    "kernels = torch.cat([w, torch.rot90(w, 1, (2, 3))], 0)\n",
    "y = F.conv2d(x, kernels)\n",
    "y.sum().backward()\n",
    "\n",
    "print(w.grad.shape)  # torch.Size([4, 1, 3, 3])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdb14887",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
