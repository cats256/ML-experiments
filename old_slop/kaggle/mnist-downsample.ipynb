{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[],"dockerImageVersionId":30787,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torchvision import datasets, transforms\nfrom torch.utils.data import DataLoader\nfrom torchinfo import summary","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2024-11-16T17:46:36.839681Z","iopub.execute_input":"2024-11-16T17:46:36.840564Z","iopub.status.idle":"2024-11-16T17:46:41.386372Z","shell.execute_reply.started":"2024-11-16T17:46:36.840523Z","shell.execute_reply":"2024-11-16T17:46:41.385558Z"}},"outputs":[],"execution_count":1},{"cell_type":"code","source":"train_dataset = datasets.MNIST(root='./data', train=True, download=True, transform=transform)\ntest_dataset = datasets.MNIST(root='./data', train=False, download=True, transform=transform)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-16T07:43:54.270487Z","iopub.execute_input":"2024-11-16T07:43:54.271163Z","iopub.status.idle":"2024-11-16T07:43:54.359100Z","shell.execute_reply.started":"2024-11-16T07:43:54.271122Z","shell.execute_reply":"2024-11-16T07:43:54.358056Z"}},"outputs":[],"execution_count":360},{"cell_type":"code","source":"device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n\ntrain_data = train_dataset.data.to(device).float() / 255.0\ntrain_targets = train_dataset.targets.to(device)\n\ntest_data = test_dataset.data.to(device).float() / 255.0\ntest_targets = test_dataset.targets.to(device)\n\ntrain_data = train_data.unsqueeze(1)\ntest_data = test_data.unsqueeze(1)\n\ndef get_batches(data, targets, batch_size):\n    for i in range(0, len(data), batch_size):\n        yield data[i:i + batch_size], targets[i:i + batch_size]\n\ntransform = transforms.Compose([\n    transforms.ToTensor(),\n])\n\ntrain_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\ntest_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-16T07:43:57.324053Z","iopub.execute_input":"2024-11-16T07:43:57.324917Z","iopub.status.idle":"2024-11-16T07:43:57.355656Z","shell.execute_reply.started":"2024-11-16T07:43:57.324862Z","shell.execute_reply":"2024-11-16T07:43:57.354697Z"}},"outputs":[],"execution_count":361},{"cell_type":"code","source":"class SimpleCNN(nn.Module):\n    def __init__(self):\n        super(SimpleCNN, self).__init__()\n        self.conv1 = nn.Conv2d(1, 16, kernel_size=3, stride=1, padding=2)\n        self.conv2 = nn.Conv2d(16, 32, kernel_size=4, stride=1, padding=3)\n        self.conv3 = nn.Conv2d(32, 64, kernel_size=4, stride=1, padding=3)\n        self.fc1 = nn.Linear(64 * 6 * 6, 10)\n\n        self.aug_pool = nn.AvgPool2d(kernel_size=2, stride=1, padding=1)\n        self.enable_aug = True\n        \n        self.pool = nn.MaxPool2d(2, 2)\n        self.relu = nn.ReLU()\n\n        nn.init.constant_(self.fc1.weight, 0)\n        nn.init.constant_(self.fc1.bias, 0)\n\n    def forward(self, x, i):\n        # if self.enable_aug:\n        #     x = self.aug_pool(x)\n        \n        if i < 300:\n            x = x + (1 - i / 300) * torch.randn_like(x)\n        x = self.relu(self.conv1(x))\n        x = self.pool(x)\n        x = self.relu(self.conv2(x))\n        x = self.pool(x)\n        x = self.relu(self.conv3(x))\n        x = self.pool(x)\n        x = x.view(-1, 64 * 6 * 6)\n        x = self.fc1(x)\n        return x\n\nbatch_size = 6000\nlearning_rate = 0.001 * 0.5\nepochs = 1000\n\ntransform = transforms.Compose([\n    transforms.ToTensor(),\n])\n\ntrain_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\ntest_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nmodel = SimpleCNN().to(device)\ncriterion = nn.CrossEntropyLoss()\noptimizer = optim.Adam(model.parameters(), lr=learning_rate)\n\nprint(len(train_loader.dataset))\nprint(len(test_loader.dataset))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-16T09:01:25.508494Z","iopub.execute_input":"2024-11-16T09:01:25.509206Z","iopub.status.idle":"2024-11-16T09:01:25.527608Z","shell.execute_reply.started":"2024-11-16T09:01:25.509162Z","shell.execute_reply":"2024-11-16T09:01:25.526545Z"}},"outputs":[{"name":"stdout","text":"60000\n10000\n","output_type":"stream"}],"execution_count":420},{"cell_type":"code","source":"model.enable_aug = False\nprint(model.enable_aug)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-16T09:01:26.871354Z","iopub.execute_input":"2024-11-16T09:01:26.872083Z","iopub.status.idle":"2024-11-16T09:01:26.876966Z","shell.execute_reply.started":"2024-11-16T09:01:26.872042Z","shell.execute_reply":"2024-11-16T09:01:26.875811Z"}},"outputs":[{"name":"stdout","text":"False\n","output_type":"stream"}],"execution_count":421},{"cell_type":"code","source":"patience = 100\nbest_val_loss = float('inf')\nno_improvement_epochs = 0\n\nfor epoch in range(epochs):\n    model.train()\n    running_loss = 0.0\n    num_batches = 0\n    \n    for data, target in get_batches(train_data, train_targets, batch_size):\n        optimizer.zero_grad()\n        output = model(data, epoch)\n        loss = criterion(output, target)\n        loss.backward()\n        optimizer.step()\n        running_loss += loss.item()\n        num_batches += 1\n\n    print(f\"Epoch [{epoch + 1}/{epochs}], Training Loss: {running_loss / num_batches:.4f}\")\n\n    model.eval()\n    val_loss = 0.0\n    correct = 0\n    total = 0\n    num_batches = 0\n\n    with torch.no_grad():\n        for data, target in get_batches(test_data, test_targets, batch_size):\n            outputs = model(data, 300)\n            loss = criterion(outputs, target)\n            val_loss += loss.item()\n            _, predicted = torch.max(outputs, 1)\n            total += target.size(0)\n            num_batches += 1\n            correct += (predicted == target).sum().item()\n\n    accuracy = 100 * correct / total\n    val_loss /= num_batches\n    print(f\"Epoch [{epoch + 1}/{epochs}], Validation Loss: {val_loss:.4f}, Validation Accuracy: {accuracy:.2f}%\")\n    print()\n\n    if val_loss < best_val_loss:\n        best_val_loss = val_loss\n        no_improvement_epochs = 0\n    else:\n        no_improvement_epochs += 1\n\n    if no_improvement_epochs >= patience:\n        print(f\"Early stopping triggered after {epoch + 1} epochs.\")\n        break","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-16T09:01:28.135451Z","iopub.execute_input":"2024-11-16T09:01:28.136345Z","iopub.status.idle":"2024-11-16T09:08:35.872803Z","shell.execute_reply.started":"2024-11-16T09:01:28.136302Z","shell.execute_reply":"2024-11-16T09:08:35.871393Z"}},"outputs":[{"name":"stdout","text":"Epoch [1/1000], Training Loss: 2.2914\nEpoch [1/1000], Validation Loss: 2.2739, Validation Accuracy: 47.48%\n\nEpoch [2/1000], Training Loss: 2.1885\nEpoch [2/1000], Validation Loss: 2.1160, Validation Accuracy: 42.61%\n\nEpoch [3/1000], Training Loss: 1.8772\nEpoch [3/1000], Validation Loss: 1.8183, Validation Accuracy: 45.92%\n\nEpoch [4/1000], Training Loss: 1.5527\nEpoch [4/1000], Validation Loss: 1.5923, Validation Accuracy: 49.16%\n\nEpoch [5/1000], Training Loss: 1.3982\nEpoch [5/1000], Validation Loss: 1.4350, Validation Accuracy: 52.83%\n\nEpoch [6/1000], Training Loss: 1.3074\nEpoch [6/1000], Validation Loss: 1.2742, Validation Accuracy: 58.31%\n\nEpoch [7/1000], Training Loss: 1.2410\nEpoch [7/1000], Validation Loss: 1.1368, Validation Accuracy: 62.62%\n\nEpoch [8/1000], Training Loss: 1.1827\nEpoch [8/1000], Validation Loss: 1.0369, Validation Accuracy: 65.46%\n\nEpoch [9/1000], Training Loss: 1.1295\nEpoch [9/1000], Validation Loss: 0.9315, Validation Accuracy: 72.28%\n\nEpoch [10/1000], Training Loss: 1.0811\nEpoch [10/1000], Validation Loss: 0.8505, Validation Accuracy: 77.19%\n\nEpoch [11/1000], Training Loss: 1.0350\nEpoch [11/1000], Validation Loss: 0.7728, Validation Accuracy: 80.59%\n\nEpoch [12/1000], Training Loss: 0.9978\nEpoch [12/1000], Validation Loss: 0.7131, Validation Accuracy: 82.83%\n\nEpoch [13/1000], Training Loss: 0.9679\nEpoch [13/1000], Validation Loss: 0.6757, Validation Accuracy: 83.11%\n\nEpoch [14/1000], Training Loss: 0.9395\nEpoch [14/1000], Validation Loss: 0.6184, Validation Accuracy: 85.79%\n\nEpoch [15/1000], Training Loss: 0.9025\nEpoch [15/1000], Validation Loss: 0.5808, Validation Accuracy: 87.08%\n\nEpoch [16/1000], Training Loss: 0.8772\nEpoch [16/1000], Validation Loss: 0.5486, Validation Accuracy: 87.87%\n\nEpoch [17/1000], Training Loss: 0.8514\nEpoch [17/1000], Validation Loss: 0.5123, Validation Accuracy: 88.66%\n\nEpoch [18/1000], Training Loss: 0.8267\nEpoch [18/1000], Validation Loss: 0.4787, Validation Accuracy: 89.62%\n\nEpoch [19/1000], Training Loss: 0.8019\nEpoch [19/1000], Validation Loss: 0.4528, Validation Accuracy: 90.19%\n\nEpoch [20/1000], Training Loss: 0.7882\nEpoch [20/1000], Validation Loss: 0.4286, Validation Accuracy: 90.64%\n\nEpoch [21/1000], Training Loss: 0.7592\nEpoch [21/1000], Validation Loss: 0.4052, Validation Accuracy: 91.18%\n\nEpoch [22/1000], Training Loss: 0.7418\nEpoch [22/1000], Validation Loss: 0.3839, Validation Accuracy: 91.58%\n\nEpoch [23/1000], Training Loss: 0.7251\nEpoch [23/1000], Validation Loss: 0.3645, Validation Accuracy: 91.90%\n\nEpoch [24/1000], Training Loss: 0.7045\nEpoch [24/1000], Validation Loss: 0.3455, Validation Accuracy: 92.26%\n\nEpoch [25/1000], Training Loss: 0.6933\nEpoch [25/1000], Validation Loss: 0.3319, Validation Accuracy: 92.60%\n\nEpoch [26/1000], Training Loss: 0.6743\nEpoch [26/1000], Validation Loss: 0.3136, Validation Accuracy: 92.86%\n\nEpoch [27/1000], Training Loss: 0.6667\nEpoch [27/1000], Validation Loss: 0.3005, Validation Accuracy: 93.18%\n\nEpoch [28/1000], Training Loss: 0.6431\nEpoch [28/1000], Validation Loss: 0.2889, Validation Accuracy: 93.32%\n\nEpoch [29/1000], Training Loss: 0.6271\nEpoch [29/1000], Validation Loss: 0.2785, Validation Accuracy: 93.42%\n\nEpoch [30/1000], Training Loss: 0.6137\nEpoch [30/1000], Validation Loss: 0.2670, Validation Accuracy: 93.51%\n\nEpoch [31/1000], Training Loss: 0.5965\nEpoch [31/1000], Validation Loss: 0.2564, Validation Accuracy: 93.68%\n\nEpoch [32/1000], Training Loss: 0.5807\nEpoch [32/1000], Validation Loss: 0.2445, Validation Accuracy: 94.07%\n\nEpoch [33/1000], Training Loss: 0.5744\nEpoch [33/1000], Validation Loss: 0.2363, Validation Accuracy: 94.25%\n\nEpoch [34/1000], Training Loss: 0.5703\nEpoch [34/1000], Validation Loss: 0.2272, Validation Accuracy: 94.32%\n\nEpoch [35/1000], Training Loss: 0.5563\nEpoch [35/1000], Validation Loss: 0.2210, Validation Accuracy: 94.38%\n\nEpoch [36/1000], Training Loss: 0.5411\nEpoch [36/1000], Validation Loss: 0.2146, Validation Accuracy: 94.54%\n\nEpoch [37/1000], Training Loss: 0.5314\nEpoch [37/1000], Validation Loss: 0.2068, Validation Accuracy: 94.65%\n\nEpoch [38/1000], Training Loss: 0.5207\nEpoch [38/1000], Validation Loss: 0.2000, Validation Accuracy: 94.82%\n\nEpoch [39/1000], Training Loss: 0.5124\nEpoch [39/1000], Validation Loss: 0.1943, Validation Accuracy: 95.03%\n\nEpoch [40/1000], Training Loss: 0.5020\nEpoch [40/1000], Validation Loss: 0.1893, Validation Accuracy: 95.03%\n\nEpoch [41/1000], Training Loss: 0.4889\nEpoch [41/1000], Validation Loss: 0.1860, Validation Accuracy: 95.10%\n\nEpoch [42/1000], Training Loss: 0.4860\nEpoch [42/1000], Validation Loss: 0.1772, Validation Accuracy: 95.35%\n\nEpoch [43/1000], Training Loss: 0.4729\nEpoch [43/1000], Validation Loss: 0.1772, Validation Accuracy: 95.30%\n\nEpoch [44/1000], Training Loss: 0.4664\nEpoch [44/1000], Validation Loss: 0.1714, Validation Accuracy: 95.41%\n\nEpoch [45/1000], Training Loss: 0.4635\nEpoch [45/1000], Validation Loss: 0.1628, Validation Accuracy: 95.55%\n\nEpoch [46/1000], Training Loss: 0.4542\nEpoch [46/1000], Validation Loss: 0.1638, Validation Accuracy: 95.46%\n\nEpoch [47/1000], Training Loss: 0.4427\nEpoch [47/1000], Validation Loss: 0.1618, Validation Accuracy: 95.60%\n\nEpoch [48/1000], Training Loss: 0.4361\nEpoch [48/1000], Validation Loss: 0.1550, Validation Accuracy: 95.73%\n\nEpoch [49/1000], Training Loss: 0.4323\nEpoch [49/1000], Validation Loss: 0.1517, Validation Accuracy: 95.85%\n\nEpoch [50/1000], Training Loss: 0.4240\nEpoch [50/1000], Validation Loss: 0.1487, Validation Accuracy: 95.86%\n\nEpoch [51/1000], Training Loss: 0.4187\nEpoch [51/1000], Validation Loss: 0.1456, Validation Accuracy: 95.89%\n\nEpoch [52/1000], Training Loss: 0.4115\nEpoch [52/1000], Validation Loss: 0.1431, Validation Accuracy: 95.91%\n\nEpoch [53/1000], Training Loss: 0.4048\nEpoch [53/1000], Validation Loss: 0.1401, Validation Accuracy: 96.00%\n\nEpoch [54/1000], Training Loss: 0.4041\nEpoch [54/1000], Validation Loss: 0.1352, Validation Accuracy: 96.14%\n\nEpoch [55/1000], Training Loss: 0.3956\nEpoch [55/1000], Validation Loss: 0.1323, Validation Accuracy: 96.17%\n\nEpoch [56/1000], Training Loss: 0.3916\nEpoch [56/1000], Validation Loss: 0.1333, Validation Accuracy: 96.11%\n\nEpoch [57/1000], Training Loss: 0.3878\nEpoch [57/1000], Validation Loss: 0.1312, Validation Accuracy: 96.22%\n\nEpoch [58/1000], Training Loss: 0.3831\nEpoch [58/1000], Validation Loss: 0.1314, Validation Accuracy: 96.18%\n\nEpoch [59/1000], Training Loss: 0.3724\nEpoch [59/1000], Validation Loss: 0.1278, Validation Accuracy: 96.37%\n\nEpoch [60/1000], Training Loss: 0.3666\nEpoch [60/1000], Validation Loss: 0.1255, Validation Accuracy: 96.38%\n\nEpoch [61/1000], Training Loss: 0.3572\nEpoch [61/1000], Validation Loss: 0.1225, Validation Accuracy: 96.40%\n\nEpoch [62/1000], Training Loss: 0.3598\nEpoch [62/1000], Validation Loss: 0.1248, Validation Accuracy: 96.44%\n\nEpoch [63/1000], Training Loss: 0.3572\nEpoch [63/1000], Validation Loss: 0.1164, Validation Accuracy: 96.60%\n\nEpoch [64/1000], Training Loss: 0.3479\nEpoch [64/1000], Validation Loss: 0.1196, Validation Accuracy: 96.48%\n\nEpoch [65/1000], Training Loss: 0.3432\nEpoch [65/1000], Validation Loss: 0.1153, Validation Accuracy: 96.55%\n\nEpoch [66/1000], Training Loss: 0.3438\nEpoch [66/1000], Validation Loss: 0.1158, Validation Accuracy: 96.64%\n\nEpoch [67/1000], Training Loss: 0.3297\nEpoch [67/1000], Validation Loss: 0.1121, Validation Accuracy: 96.65%\n\nEpoch [68/1000], Training Loss: 0.3321\nEpoch [68/1000], Validation Loss: 0.1137, Validation Accuracy: 96.63%\n\nEpoch [69/1000], Training Loss: 0.3220\nEpoch [69/1000], Validation Loss: 0.1080, Validation Accuracy: 96.74%\n\nEpoch [70/1000], Training Loss: 0.3218\nEpoch [70/1000], Validation Loss: 0.1077, Validation Accuracy: 96.74%\n\nEpoch [71/1000], Training Loss: 0.3168\nEpoch [71/1000], Validation Loss: 0.1072, Validation Accuracy: 96.77%\n\nEpoch [72/1000], Training Loss: 0.3103\nEpoch [72/1000], Validation Loss: 0.1065, Validation Accuracy: 96.80%\n\nEpoch [73/1000], Training Loss: 0.3109\nEpoch [73/1000], Validation Loss: 0.1037, Validation Accuracy: 96.84%\n\nEpoch [74/1000], Training Loss: 0.3045\nEpoch [74/1000], Validation Loss: 0.1047, Validation Accuracy: 96.84%\n\nEpoch [75/1000], Training Loss: 0.2995\nEpoch [75/1000], Validation Loss: 0.1027, Validation Accuracy: 97.00%\n\nEpoch [76/1000], Training Loss: 0.2994\nEpoch [76/1000], Validation Loss: 0.0995, Validation Accuracy: 97.03%\n\nEpoch [77/1000], Training Loss: 0.2977\nEpoch [77/1000], Validation Loss: 0.1005, Validation Accuracy: 97.01%\n\nEpoch [78/1000], Training Loss: 0.2946\nEpoch [78/1000], Validation Loss: 0.1008, Validation Accuracy: 97.03%\n\nEpoch [79/1000], Training Loss: 0.2860\nEpoch [79/1000], Validation Loss: 0.0971, Validation Accuracy: 97.02%\n\nEpoch [80/1000], Training Loss: 0.2841\nEpoch [80/1000], Validation Loss: 0.0934, Validation Accuracy: 97.17%\n\nEpoch [81/1000], Training Loss: 0.2785\nEpoch [81/1000], Validation Loss: 0.0925, Validation Accuracy: 97.13%\n\nEpoch [82/1000], Training Loss: 0.2766\nEpoch [82/1000], Validation Loss: 0.0887, Validation Accuracy: 97.18%\n\nEpoch [83/1000], Training Loss: 0.2709\nEpoch [83/1000], Validation Loss: 0.0874, Validation Accuracy: 97.32%\n\nEpoch [84/1000], Training Loss: 0.2674\nEpoch [84/1000], Validation Loss: 0.0875, Validation Accuracy: 97.30%\n\nEpoch [85/1000], Training Loss: 0.2647\nEpoch [85/1000], Validation Loss: 0.0864, Validation Accuracy: 97.37%\n\nEpoch [86/1000], Training Loss: 0.2595\nEpoch [86/1000], Validation Loss: 0.0852, Validation Accuracy: 97.36%\n\nEpoch [87/1000], Training Loss: 0.2575\nEpoch [87/1000], Validation Loss: 0.0848, Validation Accuracy: 97.39%\n\nEpoch [88/1000], Training Loss: 0.2511\nEpoch [88/1000], Validation Loss: 0.0842, Validation Accuracy: 97.31%\n\nEpoch [89/1000], Training Loss: 0.2466\nEpoch [89/1000], Validation Loss: 0.0825, Validation Accuracy: 97.47%\n\nEpoch [90/1000], Training Loss: 0.2491\nEpoch [90/1000], Validation Loss: 0.0801, Validation Accuracy: 97.53%\n\nEpoch [91/1000], Training Loss: 0.2403\nEpoch [91/1000], Validation Loss: 0.0798, Validation Accuracy: 97.56%\n\nEpoch [92/1000], Training Loss: 0.2402\nEpoch [92/1000], Validation Loss: 0.0798, Validation Accuracy: 97.47%\n\nEpoch [93/1000], Training Loss: 0.2378\nEpoch [93/1000], Validation Loss: 0.0788, Validation Accuracy: 97.53%\n\nEpoch [94/1000], Training Loss: 0.2321\nEpoch [94/1000], Validation Loss: 0.0797, Validation Accuracy: 97.57%\n\nEpoch [95/1000], Training Loss: 0.2310\nEpoch [95/1000], Validation Loss: 0.0762, Validation Accuracy: 97.64%\n\nEpoch [96/1000], Training Loss: 0.2299\nEpoch [96/1000], Validation Loss: 0.0755, Validation Accuracy: 97.66%\n\nEpoch [97/1000], Training Loss: 0.2252\nEpoch [97/1000], Validation Loss: 0.0766, Validation Accuracy: 97.61%\n\nEpoch [98/1000], Training Loss: 0.2212\nEpoch [98/1000], Validation Loss: 0.0739, Validation Accuracy: 97.61%\n\nEpoch [99/1000], Training Loss: 0.2226\nEpoch [99/1000], Validation Loss: 0.0740, Validation Accuracy: 97.71%\n\nEpoch [100/1000], Training Loss: 0.2150\nEpoch [100/1000], Validation Loss: 0.0720, Validation Accuracy: 97.69%\n\nEpoch [101/1000], Training Loss: 0.2160\nEpoch [101/1000], Validation Loss: 0.0719, Validation Accuracy: 97.78%\n\nEpoch [102/1000], Training Loss: 0.2140\nEpoch [102/1000], Validation Loss: 0.0709, Validation Accuracy: 97.78%\n\nEpoch [103/1000], Training Loss: 0.2054\nEpoch [103/1000], Validation Loss: 0.0727, Validation Accuracy: 97.75%\n\nEpoch [104/1000], Training Loss: 0.2082\nEpoch [104/1000], Validation Loss: 0.0692, Validation Accuracy: 97.82%\n\nEpoch [105/1000], Training Loss: 0.2024\nEpoch [105/1000], Validation Loss: 0.0684, Validation Accuracy: 97.84%\n\nEpoch [106/1000], Training Loss: 0.1994\nEpoch [106/1000], Validation Loss: 0.0686, Validation Accuracy: 97.88%\n\nEpoch [107/1000], Training Loss: 0.1973\nEpoch [107/1000], Validation Loss: 0.0671, Validation Accuracy: 97.90%\n\nEpoch [108/1000], Training Loss: 0.1912\nEpoch [108/1000], Validation Loss: 0.0671, Validation Accuracy: 97.90%\n\nEpoch [109/1000], Training Loss: 0.1953\nEpoch [109/1000], Validation Loss: 0.0663, Validation Accuracy: 97.86%\n\nEpoch [110/1000], Training Loss: 0.1907\nEpoch [110/1000], Validation Loss: 0.0651, Validation Accuracy: 97.97%\n\nEpoch [111/1000], Training Loss: 0.1896\nEpoch [111/1000], Validation Loss: 0.0640, Validation Accuracy: 97.98%\n\nEpoch [112/1000], Training Loss: 0.1814\nEpoch [112/1000], Validation Loss: 0.0642, Validation Accuracy: 97.99%\n\nEpoch [113/1000], Training Loss: 0.1812\nEpoch [113/1000], Validation Loss: 0.0632, Validation Accuracy: 97.96%\n\nEpoch [114/1000], Training Loss: 0.1812\nEpoch [114/1000], Validation Loss: 0.0623, Validation Accuracy: 98.03%\n\nEpoch [115/1000], Training Loss: 0.1791\nEpoch [115/1000], Validation Loss: 0.0618, Validation Accuracy: 98.12%\n\nEpoch [116/1000], Training Loss: 0.1740\nEpoch [116/1000], Validation Loss: 0.0610, Validation Accuracy: 98.07%\n\nEpoch [117/1000], Training Loss: 0.1726\nEpoch [117/1000], Validation Loss: 0.0602, Validation Accuracy: 98.09%\n\nEpoch [118/1000], Training Loss: 0.1721\nEpoch [118/1000], Validation Loss: 0.0598, Validation Accuracy: 98.13%\n\nEpoch [119/1000], Training Loss: 0.1652\nEpoch [119/1000], Validation Loss: 0.0611, Validation Accuracy: 98.10%\n\nEpoch [120/1000], Training Loss: 0.1708\nEpoch [120/1000], Validation Loss: 0.0610, Validation Accuracy: 98.03%\n\nEpoch [121/1000], Training Loss: 0.1632\nEpoch [121/1000], Validation Loss: 0.0594, Validation Accuracy: 98.11%\n\nEpoch [122/1000], Training Loss: 0.1638\nEpoch [122/1000], Validation Loss: 0.0584, Validation Accuracy: 98.22%\n\nEpoch [123/1000], Training Loss: 0.1571\nEpoch [123/1000], Validation Loss: 0.0586, Validation Accuracy: 98.18%\n\nEpoch [124/1000], Training Loss: 0.1554\nEpoch [124/1000], Validation Loss: 0.0589, Validation Accuracy: 98.15%\n\nEpoch [125/1000], Training Loss: 0.1547\nEpoch [125/1000], Validation Loss: 0.0588, Validation Accuracy: 98.16%\n\nEpoch [126/1000], Training Loss: 0.1498\nEpoch [126/1000], Validation Loss: 0.0569, Validation Accuracy: 98.19%\n\nEpoch [127/1000], Training Loss: 0.1526\nEpoch [127/1000], Validation Loss: 0.0570, Validation Accuracy: 98.16%\n\nEpoch [128/1000], Training Loss: 0.1444\nEpoch [128/1000], Validation Loss: 0.0561, Validation Accuracy: 98.19%\n\nEpoch [129/1000], Training Loss: 0.1467\nEpoch [129/1000], Validation Loss: 0.0543, Validation Accuracy: 98.27%\n\nEpoch [130/1000], Training Loss: 0.1431\nEpoch [130/1000], Validation Loss: 0.0550, Validation Accuracy: 98.24%\n\nEpoch [131/1000], Training Loss: 0.1413\nEpoch [131/1000], Validation Loss: 0.0549, Validation Accuracy: 98.25%\n\nEpoch [132/1000], Training Loss: 0.1409\nEpoch [132/1000], Validation Loss: 0.0541, Validation Accuracy: 98.23%\n\nEpoch [133/1000], Training Loss: 0.1377\nEpoch [133/1000], Validation Loss: 0.0517, Validation Accuracy: 98.39%\n\nEpoch [134/1000], Training Loss: 0.1338\nEpoch [134/1000], Validation Loss: 0.0513, Validation Accuracy: 98.37%\n\nEpoch [135/1000], Training Loss: 0.1326\nEpoch [135/1000], Validation Loss: 0.0531, Validation Accuracy: 98.39%\n\nEpoch [136/1000], Training Loss: 0.1302\nEpoch [136/1000], Validation Loss: 0.0514, Validation Accuracy: 98.29%\n\nEpoch [137/1000], Training Loss: 0.1287\nEpoch [137/1000], Validation Loss: 0.0532, Validation Accuracy: 98.28%\n\nEpoch [138/1000], Training Loss: 0.1297\nEpoch [138/1000], Validation Loss: 0.0501, Validation Accuracy: 98.41%\n\nEpoch [139/1000], Training Loss: 0.1260\nEpoch [139/1000], Validation Loss: 0.0521, Validation Accuracy: 98.34%\n\nEpoch [140/1000], Training Loss: 0.1250\nEpoch [140/1000], Validation Loss: 0.0502, Validation Accuracy: 98.40%\n\nEpoch [141/1000], Training Loss: 0.1213\nEpoch [141/1000], Validation Loss: 0.0485, Validation Accuracy: 98.44%\n\nEpoch [142/1000], Training Loss: 0.1176\nEpoch [142/1000], Validation Loss: 0.0513, Validation Accuracy: 98.30%\n\nEpoch [143/1000], Training Loss: 0.1201\nEpoch [143/1000], Validation Loss: 0.0472, Validation Accuracy: 98.50%\n\nEpoch [144/1000], Training Loss: 0.1183\nEpoch [144/1000], Validation Loss: 0.0478, Validation Accuracy: 98.45%\n\nEpoch [145/1000], Training Loss: 0.1146\nEpoch [145/1000], Validation Loss: 0.0487, Validation Accuracy: 98.49%\n\nEpoch [146/1000], Training Loss: 0.1133\nEpoch [146/1000], Validation Loss: 0.0467, Validation Accuracy: 98.48%\n\nEpoch [147/1000], Training Loss: 0.1104\nEpoch [147/1000], Validation Loss: 0.0477, Validation Accuracy: 98.44%\n\nEpoch [148/1000], Training Loss: 0.1100\nEpoch [148/1000], Validation Loss: 0.0451, Validation Accuracy: 98.58%\n\nEpoch [149/1000], Training Loss: 0.1086\nEpoch [149/1000], Validation Loss: 0.0453, Validation Accuracy: 98.50%\n\nEpoch [150/1000], Training Loss: 0.1089\nEpoch [150/1000], Validation Loss: 0.0453, Validation Accuracy: 98.59%\n\nEpoch [151/1000], Training Loss: 0.1053\nEpoch [151/1000], Validation Loss: 0.0455, Validation Accuracy: 98.52%\n\nEpoch [152/1000], Training Loss: 0.1036\nEpoch [152/1000], Validation Loss: 0.0450, Validation Accuracy: 98.55%\n\nEpoch [153/1000], Training Loss: 0.1004\nEpoch [153/1000], Validation Loss: 0.0441, Validation Accuracy: 98.55%\n\nEpoch [154/1000], Training Loss: 0.1007\nEpoch [154/1000], Validation Loss: 0.0433, Validation Accuracy: 98.59%\n\nEpoch [155/1000], Training Loss: 0.1013\nEpoch [155/1000], Validation Loss: 0.0456, Validation Accuracy: 98.50%\n\nEpoch [156/1000], Training Loss: 0.0988\nEpoch [156/1000], Validation Loss: 0.0431, Validation Accuracy: 98.60%\n\nEpoch [157/1000], Training Loss: 0.0983\nEpoch [157/1000], Validation Loss: 0.0434, Validation Accuracy: 98.62%\n\nEpoch [158/1000], Training Loss: 0.0941\nEpoch [158/1000], Validation Loss: 0.0434, Validation Accuracy: 98.60%\n\nEpoch [159/1000], Training Loss: 0.0925\nEpoch [159/1000], Validation Loss: 0.0419, Validation Accuracy: 98.65%\n\nEpoch [160/1000], Training Loss: 0.0941\nEpoch [160/1000], Validation Loss: 0.0432, Validation Accuracy: 98.60%\n\nEpoch [161/1000], Training Loss: 0.0914\nEpoch [161/1000], Validation Loss: 0.0429, Validation Accuracy: 98.61%\n\nEpoch [162/1000], Training Loss: 0.0893\nEpoch [162/1000], Validation Loss: 0.0426, Validation Accuracy: 98.55%\n\nEpoch [163/1000], Training Loss: 0.0904\nEpoch [163/1000], Validation Loss: 0.0423, Validation Accuracy: 98.64%\n\nEpoch [164/1000], Training Loss: 0.0881\nEpoch [164/1000], Validation Loss: 0.0407, Validation Accuracy: 98.68%\n\nEpoch [165/1000], Training Loss: 0.0843\nEpoch [165/1000], Validation Loss: 0.0424, Validation Accuracy: 98.61%\n\nEpoch [166/1000], Training Loss: 0.0847\nEpoch [166/1000], Validation Loss: 0.0396, Validation Accuracy: 98.77%\n\nEpoch [167/1000], Training Loss: 0.0816\nEpoch [167/1000], Validation Loss: 0.0394, Validation Accuracy: 98.73%\n\nEpoch [168/1000], Training Loss: 0.0834\nEpoch [168/1000], Validation Loss: 0.0397, Validation Accuracy: 98.69%\n\nEpoch [169/1000], Training Loss: 0.0791\nEpoch [169/1000], Validation Loss: 0.0395, Validation Accuracy: 98.71%\n\nEpoch [170/1000], Training Loss: 0.0803\nEpoch [170/1000], Validation Loss: 0.0383, Validation Accuracy: 98.80%\n\nEpoch [171/1000], Training Loss: 0.0778\nEpoch [171/1000], Validation Loss: 0.0382, Validation Accuracy: 98.76%\n\nEpoch [172/1000], Training Loss: 0.0779\nEpoch [172/1000], Validation Loss: 0.0379, Validation Accuracy: 98.80%\n\nEpoch [173/1000], Training Loss: 0.0769\nEpoch [173/1000], Validation Loss: 0.0376, Validation Accuracy: 98.80%\n\nEpoch [174/1000], Training Loss: 0.0741\nEpoch [174/1000], Validation Loss: 0.0389, Validation Accuracy: 98.74%\n\nEpoch [175/1000], Training Loss: 0.0730\nEpoch [175/1000], Validation Loss: 0.0380, Validation Accuracy: 98.81%\n\nEpoch [176/1000], Training Loss: 0.0739\nEpoch [176/1000], Validation Loss: 0.0376, Validation Accuracy: 98.82%\n\nEpoch [177/1000], Training Loss: 0.0719\nEpoch [177/1000], Validation Loss: 0.0363, Validation Accuracy: 98.84%\n\nEpoch [178/1000], Training Loss: 0.0700\nEpoch [178/1000], Validation Loss: 0.0376, Validation Accuracy: 98.78%\n\nEpoch [179/1000], Training Loss: 0.0703\nEpoch [179/1000], Validation Loss: 0.0353, Validation Accuracy: 98.85%\n\nEpoch [180/1000], Training Loss: 0.0692\nEpoch [180/1000], Validation Loss: 0.0370, Validation Accuracy: 98.85%\n\nEpoch [181/1000], Training Loss: 0.0686\nEpoch [181/1000], Validation Loss: 0.0359, Validation Accuracy: 98.87%\n\nEpoch [182/1000], Training Loss: 0.0664\nEpoch [182/1000], Validation Loss: 0.0364, Validation Accuracy: 98.79%\n\nEpoch [183/1000], Training Loss: 0.0651\nEpoch [183/1000], Validation Loss: 0.0362, Validation Accuracy: 98.88%\n\nEpoch [184/1000], Training Loss: 0.0637\nEpoch [184/1000], Validation Loss: 0.0350, Validation Accuracy: 98.84%\n\nEpoch [185/1000], Training Loss: 0.0640\nEpoch [185/1000], Validation Loss: 0.0347, Validation Accuracy: 98.87%\n\nEpoch [186/1000], Training Loss: 0.0621\nEpoch [186/1000], Validation Loss: 0.0346, Validation Accuracy: 98.91%\n\nEpoch [187/1000], Training Loss: 0.0615\nEpoch [187/1000], Validation Loss: 0.0343, Validation Accuracy: 98.90%\n\nEpoch [188/1000], Training Loss: 0.0598\nEpoch [188/1000], Validation Loss: 0.0341, Validation Accuracy: 98.94%\n\nEpoch [189/1000], Training Loss: 0.0596\nEpoch [189/1000], Validation Loss: 0.0353, Validation Accuracy: 98.84%\n\nEpoch [190/1000], Training Loss: 0.0592\nEpoch [190/1000], Validation Loss: 0.0325, Validation Accuracy: 98.97%\n\nEpoch [191/1000], Training Loss: 0.0581\nEpoch [191/1000], Validation Loss: 0.0351, Validation Accuracy: 98.84%\n\nEpoch [192/1000], Training Loss: 0.0570\nEpoch [192/1000], Validation Loss: 0.0332, Validation Accuracy: 98.95%\n\nEpoch [193/1000], Training Loss: 0.0549\nEpoch [193/1000], Validation Loss: 0.0323, Validation Accuracy: 98.95%\n\nEpoch [194/1000], Training Loss: 0.0553\nEpoch [194/1000], Validation Loss: 0.0336, Validation Accuracy: 98.91%\n\nEpoch [195/1000], Training Loss: 0.0549\nEpoch [195/1000], Validation Loss: 0.0334, Validation Accuracy: 98.90%\n\nEpoch [196/1000], Training Loss: 0.0542\nEpoch [196/1000], Validation Loss: 0.0326, Validation Accuracy: 98.97%\n\nEpoch [197/1000], Training Loss: 0.0536\nEpoch [197/1000], Validation Loss: 0.0318, Validation Accuracy: 98.97%\n\nEpoch [198/1000], Training Loss: 0.0528\nEpoch [198/1000], Validation Loss: 0.0326, Validation Accuracy: 98.93%\n\nEpoch [199/1000], Training Loss: 0.0514\nEpoch [199/1000], Validation Loss: 0.0341, Validation Accuracy: 98.89%\n\nEpoch [200/1000], Training Loss: 0.0513\nEpoch [200/1000], Validation Loss: 0.0316, Validation Accuracy: 99.02%\n\nEpoch [201/1000], Training Loss: 0.0504\nEpoch [201/1000], Validation Loss: 0.0323, Validation Accuracy: 98.94%\n\nEpoch [202/1000], Training Loss: 0.0497\nEpoch [202/1000], Validation Loss: 0.0317, Validation Accuracy: 98.98%\n\nEpoch [203/1000], Training Loss: 0.0491\nEpoch [203/1000], Validation Loss: 0.0320, Validation Accuracy: 98.96%\n\nEpoch [204/1000], Training Loss: 0.0477\nEpoch [204/1000], Validation Loss: 0.0312, Validation Accuracy: 98.99%\n\nEpoch [205/1000], Training Loss: 0.0481\nEpoch [205/1000], Validation Loss: 0.0331, Validation Accuracy: 98.95%\n\nEpoch [206/1000], Training Loss: 0.0478\nEpoch [206/1000], Validation Loss: 0.0325, Validation Accuracy: 98.91%\n\nEpoch [207/1000], Training Loss: 0.0467\nEpoch [207/1000], Validation Loss: 0.0329, Validation Accuracy: 98.91%\n\nEpoch [208/1000], Training Loss: 0.0462\nEpoch [208/1000], Validation Loss: 0.0316, Validation Accuracy: 98.95%\n\nEpoch [209/1000], Training Loss: 0.0440\nEpoch [209/1000], Validation Loss: 0.0309, Validation Accuracy: 98.94%\n\nEpoch [210/1000], Training Loss: 0.0441\nEpoch [210/1000], Validation Loss: 0.0314, Validation Accuracy: 98.99%\n\nEpoch [211/1000], Training Loss: 0.0424\nEpoch [211/1000], Validation Loss: 0.0321, Validation Accuracy: 98.91%\n\nEpoch [212/1000], Training Loss: 0.0426\nEpoch [212/1000], Validation Loss: 0.0307, Validation Accuracy: 99.03%\n\nEpoch [213/1000], Training Loss: 0.0413\nEpoch [213/1000], Validation Loss: 0.0324, Validation Accuracy: 98.92%\n\nEpoch [214/1000], Training Loss: 0.0418\nEpoch [214/1000], Validation Loss: 0.0314, Validation Accuracy: 98.95%\n\nEpoch [215/1000], Training Loss: 0.0410\nEpoch [215/1000], Validation Loss: 0.0308, Validation Accuracy: 98.95%\n\nEpoch [216/1000], Training Loss: 0.0393\nEpoch [216/1000], Validation Loss: 0.0308, Validation Accuracy: 98.96%\n\nEpoch [217/1000], Training Loss: 0.0392\nEpoch [217/1000], Validation Loss: 0.0312, Validation Accuracy: 98.94%\n\nEpoch [218/1000], Training Loss: 0.0394\nEpoch [218/1000], Validation Loss: 0.0309, Validation Accuracy: 98.98%\n\nEpoch [219/1000], Training Loss: 0.0374\nEpoch [219/1000], Validation Loss: 0.0311, Validation Accuracy: 99.01%\n\nEpoch [220/1000], Training Loss: 0.0381\nEpoch [220/1000], Validation Loss: 0.0314, Validation Accuracy: 98.96%\n\nEpoch [221/1000], Training Loss: 0.0373\nEpoch [221/1000], Validation Loss: 0.0310, Validation Accuracy: 98.97%\n\nEpoch [222/1000], Training Loss: 0.0367\nEpoch [222/1000], Validation Loss: 0.0306, Validation Accuracy: 98.97%\n\nEpoch [223/1000], Training Loss: 0.0358\nEpoch [223/1000], Validation Loss: 0.0306, Validation Accuracy: 98.97%\n\nEpoch [224/1000], Training Loss: 0.0347\nEpoch [224/1000], Validation Loss: 0.0295, Validation Accuracy: 99.03%\n\nEpoch [225/1000], Training Loss: 0.0356\nEpoch [225/1000], Validation Loss: 0.0323, Validation Accuracy: 98.93%\n\nEpoch [226/1000], Training Loss: 0.0350\nEpoch [226/1000], Validation Loss: 0.0303, Validation Accuracy: 98.94%\n\nEpoch [227/1000], Training Loss: 0.0346\nEpoch [227/1000], Validation Loss: 0.0298, Validation Accuracy: 99.01%\n\nEpoch [228/1000], Training Loss: 0.0334\nEpoch [228/1000], Validation Loss: 0.0298, Validation Accuracy: 99.00%\n\nEpoch [229/1000], Training Loss: 0.0327\nEpoch [229/1000], Validation Loss: 0.0303, Validation Accuracy: 98.92%\n\nEpoch [230/1000], Training Loss: 0.0327\nEpoch [230/1000], Validation Loss: 0.0290, Validation Accuracy: 99.05%\n\nEpoch [231/1000], Training Loss: 0.0323\nEpoch [231/1000], Validation Loss: 0.0286, Validation Accuracy: 99.07%\n\nEpoch [232/1000], Training Loss: 0.0306\nEpoch [232/1000], Validation Loss: 0.0290, Validation Accuracy: 99.07%\n\nEpoch [233/1000], Training Loss: 0.0321\nEpoch [233/1000], Validation Loss: 0.0298, Validation Accuracy: 99.03%\n\nEpoch [234/1000], Training Loss: 0.0315\nEpoch [234/1000], Validation Loss: 0.0301, Validation Accuracy: 99.02%\n\nEpoch [235/1000], Training Loss: 0.0313\nEpoch [235/1000], Validation Loss: 0.0313, Validation Accuracy: 98.98%\n\nEpoch [236/1000], Training Loss: 0.0304\nEpoch [236/1000], Validation Loss: 0.0299, Validation Accuracy: 98.98%\n\nEpoch [237/1000], Training Loss: 0.0301\nEpoch [237/1000], Validation Loss: 0.0298, Validation Accuracy: 98.99%\n\nEpoch [238/1000], Training Loss: 0.0302\nEpoch [238/1000], Validation Loss: 0.0306, Validation Accuracy: 98.96%\n\nEpoch [239/1000], Training Loss: 0.0296\nEpoch [239/1000], Validation Loss: 0.0297, Validation Accuracy: 99.02%\n\nEpoch [240/1000], Training Loss: 0.0292\nEpoch [240/1000], Validation Loss: 0.0293, Validation Accuracy: 99.07%\n\nEpoch [241/1000], Training Loss: 0.0291\nEpoch [241/1000], Validation Loss: 0.0294, Validation Accuracy: 99.01%\n\nEpoch [242/1000], Training Loss: 0.0277\nEpoch [242/1000], Validation Loss: 0.0291, Validation Accuracy: 99.00%\n\nEpoch [243/1000], Training Loss: 0.0274\nEpoch [243/1000], Validation Loss: 0.0295, Validation Accuracy: 98.97%\n\nEpoch [244/1000], Training Loss: 0.0268\nEpoch [244/1000], Validation Loss: 0.0290, Validation Accuracy: 99.02%\n\nEpoch [245/1000], Training Loss: 0.0262\nEpoch [245/1000], Validation Loss: 0.0282, Validation Accuracy: 99.11%\n\nEpoch [246/1000], Training Loss: 0.0261\nEpoch [246/1000], Validation Loss: 0.0281, Validation Accuracy: 99.08%\n\nEpoch [247/1000], Training Loss: 0.0258\nEpoch [247/1000], Validation Loss: 0.0289, Validation Accuracy: 99.05%\n\nEpoch [248/1000], Training Loss: 0.0258\nEpoch [248/1000], Validation Loss: 0.0286, Validation Accuracy: 99.10%\n\nEpoch [249/1000], Training Loss: 0.0251\nEpoch [249/1000], Validation Loss: 0.0276, Validation Accuracy: 99.09%\n\nEpoch [250/1000], Training Loss: 0.0253\nEpoch [250/1000], Validation Loss: 0.0283, Validation Accuracy: 99.07%\n\nEpoch [251/1000], Training Loss: 0.0250\nEpoch [251/1000], Validation Loss: 0.0272, Validation Accuracy: 99.12%\n\nEpoch [252/1000], Training Loss: 0.0240\nEpoch [252/1000], Validation Loss: 0.0272, Validation Accuracy: 99.11%\n\nEpoch [253/1000], Training Loss: 0.0239\nEpoch [253/1000], Validation Loss: 0.0277, Validation Accuracy: 99.07%\n\nEpoch [254/1000], Training Loss: 0.0235\nEpoch [254/1000], Validation Loss: 0.0274, Validation Accuracy: 99.08%\n\nEpoch [255/1000], Training Loss: 0.0238\nEpoch [255/1000], Validation Loss: 0.0277, Validation Accuracy: 99.11%\n\nEpoch [256/1000], Training Loss: 0.0230\nEpoch [256/1000], Validation Loss: 0.0263, Validation Accuracy: 99.13%\n\nEpoch [257/1000], Training Loss: 0.0223\nEpoch [257/1000], Validation Loss: 0.0259, Validation Accuracy: 99.19%\n\nEpoch [258/1000], Training Loss: 0.0218\nEpoch [258/1000], Validation Loss: 0.0260, Validation Accuracy: 99.18%\n\nEpoch [259/1000], Training Loss: 0.0220\nEpoch [259/1000], Validation Loss: 0.0258, Validation Accuracy: 99.18%\n\nEpoch [260/1000], Training Loss: 0.0223\nEpoch [260/1000], Validation Loss: 0.0264, Validation Accuracy: 99.13%\n\nEpoch [261/1000], Training Loss: 0.0217\nEpoch [261/1000], Validation Loss: 0.0263, Validation Accuracy: 99.13%\n\nEpoch [262/1000], Training Loss: 0.0212\nEpoch [262/1000], Validation Loss: 0.0259, Validation Accuracy: 99.16%\n\nEpoch [263/1000], Training Loss: 0.0204\nEpoch [263/1000], Validation Loss: 0.0254, Validation Accuracy: 99.17%\n\nEpoch [264/1000], Training Loss: 0.0204\nEpoch [264/1000], Validation Loss: 0.0252, Validation Accuracy: 99.17%\n\nEpoch [265/1000], Training Loss: 0.0204\nEpoch [265/1000], Validation Loss: 0.0254, Validation Accuracy: 99.17%\n\nEpoch [266/1000], Training Loss: 0.0201\nEpoch [266/1000], Validation Loss: 0.0252, Validation Accuracy: 99.14%\n\nEpoch [267/1000], Training Loss: 0.0202\nEpoch [267/1000], Validation Loss: 0.0248, Validation Accuracy: 99.17%\n\nEpoch [268/1000], Training Loss: 0.0200\nEpoch [268/1000], Validation Loss: 0.0249, Validation Accuracy: 99.18%\n\nEpoch [269/1000], Training Loss: 0.0197\nEpoch [269/1000], Validation Loss: 0.0248, Validation Accuracy: 99.18%\n\nEpoch [270/1000], Training Loss: 0.0194\nEpoch [270/1000], Validation Loss: 0.0249, Validation Accuracy: 99.17%\n\nEpoch [271/1000], Training Loss: 0.0192\nEpoch [271/1000], Validation Loss: 0.0249, Validation Accuracy: 99.18%\n\nEpoch [272/1000], Training Loss: 0.0190\nEpoch [272/1000], Validation Loss: 0.0251, Validation Accuracy: 99.15%\n\nEpoch [273/1000], Training Loss: 0.0189\nEpoch [273/1000], Validation Loss: 0.0248, Validation Accuracy: 99.14%\n\nEpoch [274/1000], Training Loss: 0.0192\nEpoch [274/1000], Validation Loss: 0.0251, Validation Accuracy: 99.15%\n\nEpoch [275/1000], Training Loss: 0.0191\nEpoch [275/1000], Validation Loss: 0.0252, Validation Accuracy: 99.10%\n\nEpoch [276/1000], Training Loss: 0.0191\nEpoch [276/1000], Validation Loss: 0.0254, Validation Accuracy: 99.06%\n\nEpoch [277/1000], Training Loss: 0.0188\nEpoch [277/1000], Validation Loss: 0.0259, Validation Accuracy: 99.06%\n\nEpoch [278/1000], Training Loss: 0.0190\nEpoch [278/1000], Validation Loss: 0.0270, Validation Accuracy: 98.98%\n\nEpoch [279/1000], Training Loss: 0.0192\nEpoch [279/1000], Validation Loss: 0.0279, Validation Accuracy: 98.95%\n\nEpoch [280/1000], Training Loss: 0.0194\nEpoch [280/1000], Validation Loss: 0.0290, Validation Accuracy: 98.97%\n\nEpoch [281/1000], Training Loss: 0.0194\nEpoch [281/1000], Validation Loss: 0.0307, Validation Accuracy: 98.96%\n\nEpoch [282/1000], Training Loss: 0.0194\nEpoch [282/1000], Validation Loss: 0.0315, Validation Accuracy: 98.89%\n\nEpoch [283/1000], Training Loss: 0.0193\nEpoch [283/1000], Validation Loss: 0.0334, Validation Accuracy: 98.83%\n\nEpoch [284/1000], Training Loss: 0.0196\nEpoch [284/1000], Validation Loss: 0.0344, Validation Accuracy: 98.77%\n\nEpoch [285/1000], Training Loss: 0.0198\nEpoch [285/1000], Validation Loss: 0.0349, Validation Accuracy: 98.75%\n\nEpoch [286/1000], Training Loss: 0.0198\nEpoch [286/1000], Validation Loss: 0.0331, Validation Accuracy: 98.86%\n\nEpoch [287/1000], Training Loss: 0.0190\nEpoch [287/1000], Validation Loss: 0.0296, Validation Accuracy: 98.97%\n\nEpoch [288/1000], Training Loss: 0.0172\nEpoch [288/1000], Validation Loss: 0.0268, Validation Accuracy: 99.02%\n\nEpoch [289/1000], Training Loss: 0.0158\nEpoch [289/1000], Validation Loss: 0.0252, Validation Accuracy: 99.04%\n\nEpoch [290/1000], Training Loss: 0.0145\nEpoch [290/1000], Validation Loss: 0.0243, Validation Accuracy: 99.11%\n\nEpoch [291/1000], Training Loss: 0.0138\nEpoch [291/1000], Validation Loss: 0.0242, Validation Accuracy: 99.12%\n\nEpoch [292/1000], Training Loss: 0.0135\nEpoch [292/1000], Validation Loss: 0.0245, Validation Accuracy: 99.15%\n\nEpoch [293/1000], Training Loss: 0.0134\nEpoch [293/1000], Validation Loss: 0.0246, Validation Accuracy: 99.14%\n\nEpoch [294/1000], Training Loss: 0.0132\nEpoch [294/1000], Validation Loss: 0.0246, Validation Accuracy: 99.11%\n\nEpoch [295/1000], Training Loss: 0.0130\nEpoch [295/1000], Validation Loss: 0.0247, Validation Accuracy: 99.07%\n\nEpoch [296/1000], Training Loss: 0.0127\nEpoch [296/1000], Validation Loss: 0.0246, Validation Accuracy: 99.11%\n\nEpoch [297/1000], Training Loss: 0.0125\nEpoch [297/1000], Validation Loss: 0.0246, Validation Accuracy: 99.11%\n\nEpoch [298/1000], Training Loss: 0.0123\nEpoch [298/1000], Validation Loss: 0.0247, Validation Accuracy: 99.10%\n\nEpoch [299/1000], Training Loss: 0.0121\nEpoch [299/1000], Validation Loss: 0.0247, Validation Accuracy: 99.10%\n\nEpoch [300/1000], Training Loss: 0.0120\nEpoch [300/1000], Validation Loss: 0.0248, Validation Accuracy: 99.09%\n\nEpoch [301/1000], Training Loss: 0.0118\nEpoch [301/1000], Validation Loss: 0.0248, Validation Accuracy: 99.08%\n\nEpoch [302/1000], Training Loss: 0.0116\nEpoch [302/1000], Validation Loss: 0.0249, Validation Accuracy: 99.07%\n\nEpoch [303/1000], Training Loss: 0.0115\nEpoch [303/1000], Validation Loss: 0.0250, Validation Accuracy: 99.06%\n\nEpoch [304/1000], Training Loss: 0.0113\nEpoch [304/1000], Validation Loss: 0.0250, Validation Accuracy: 99.06%\n\nEpoch [305/1000], Training Loss: 0.0112\nEpoch [305/1000], Validation Loss: 0.0251, Validation Accuracy: 99.06%\n\nEpoch [306/1000], Training Loss: 0.0111\nEpoch [306/1000], Validation Loss: 0.0252, Validation Accuracy: 99.06%\n\nEpoch [307/1000], Training Loss: 0.0109\nEpoch [307/1000], Validation Loss: 0.0252, Validation Accuracy: 99.05%\n\nEpoch [308/1000], Training Loss: 0.0108\nEpoch [308/1000], Validation Loss: 0.0253, Validation Accuracy: 99.05%\n\nEpoch [309/1000], Training Loss: 0.0106\nEpoch [309/1000], Validation Loss: 0.0254, Validation Accuracy: 99.06%\n\nEpoch [310/1000], Training Loss: 0.0105\nEpoch [310/1000], Validation Loss: 0.0254, Validation Accuracy: 99.06%\n\nEpoch [311/1000], Training Loss: 0.0104\nEpoch [311/1000], Validation Loss: 0.0254, Validation Accuracy: 99.07%\n\nEpoch [312/1000], Training Loss: 0.0102\nEpoch [312/1000], Validation Loss: 0.0255, Validation Accuracy: 99.08%\n\nEpoch [313/1000], Training Loss: 0.0101\nEpoch [313/1000], Validation Loss: 0.0255, Validation Accuracy: 99.08%\n\nEpoch [314/1000], Training Loss: 0.0100\nEpoch [314/1000], Validation Loss: 0.0256, Validation Accuracy: 99.08%\n\nEpoch [315/1000], Training Loss: 0.0099\nEpoch [315/1000], Validation Loss: 0.0257, Validation Accuracy: 99.08%\n\nEpoch [316/1000], Training Loss: 0.0097\nEpoch [316/1000], Validation Loss: 0.0257, Validation Accuracy: 99.08%\n\nEpoch [317/1000], Training Loss: 0.0096\nEpoch [317/1000], Validation Loss: 0.0258, Validation Accuracy: 99.11%\n\nEpoch [318/1000], Training Loss: 0.0095\nEpoch [318/1000], Validation Loss: 0.0258, Validation Accuracy: 99.11%\n\nEpoch [319/1000], Training Loss: 0.0094\nEpoch [319/1000], Validation Loss: 0.0259, Validation Accuracy: 99.10%\n\nEpoch [320/1000], Training Loss: 0.0093\nEpoch [320/1000], Validation Loss: 0.0260, Validation Accuracy: 99.09%\n\nEpoch [321/1000], Training Loss: 0.0092\nEpoch [321/1000], Validation Loss: 0.0260, Validation Accuracy: 99.09%\n\nEpoch [322/1000], Training Loss: 0.0091\nEpoch [322/1000], Validation Loss: 0.0261, Validation Accuracy: 99.10%\n\nEpoch [323/1000], Training Loss: 0.0089\nEpoch [323/1000], Validation Loss: 0.0262, Validation Accuracy: 99.11%\n\nEpoch [324/1000], Training Loss: 0.0088\nEpoch [324/1000], Validation Loss: 0.0262, Validation Accuracy: 99.11%\n\nEpoch [325/1000], Training Loss: 0.0087\nEpoch [325/1000], Validation Loss: 0.0263, Validation Accuracy: 99.11%\n\nEpoch [326/1000], Training Loss: 0.0086\nEpoch [326/1000], Validation Loss: 0.0264, Validation Accuracy: 99.11%\n\nEpoch [327/1000], Training Loss: 0.0085\nEpoch [327/1000], Validation Loss: 0.0265, Validation Accuracy: 99.10%\n\nEpoch [328/1000], Training Loss: 0.0084\nEpoch [328/1000], Validation Loss: 0.0265, Validation Accuracy: 99.10%\n\nEpoch [329/1000], Training Loss: 0.0083\nEpoch [329/1000], Validation Loss: 0.0266, Validation Accuracy: 99.10%\n\nEpoch [330/1000], Training Loss: 0.0082\nEpoch [330/1000], Validation Loss: 0.0267, Validation Accuracy: 99.10%\n\nEpoch [331/1000], Training Loss: 0.0081\nEpoch [331/1000], Validation Loss: 0.0268, Validation Accuracy: 99.10%\n\nEpoch [332/1000], Training Loss: 0.0081\nEpoch [332/1000], Validation Loss: 0.0269, Validation Accuracy: 99.10%\n\nEpoch [333/1000], Training Loss: 0.0080\nEpoch [333/1000], Validation Loss: 0.0270, Validation Accuracy: 99.10%\n\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","Cell \u001b[0;32mIn[422], line 16\u001b[0m\n\u001b[1;32m     14\u001b[0m     loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[1;32m     15\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[0;32m---> 16\u001b[0m     running_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     17\u001b[0m     num_batches \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpoch [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;250m \u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepochs\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m], Training Loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrunning_loss\u001b[38;5;250m \u001b[39m\u001b[38;5;241m/\u001b[39m\u001b[38;5;250m \u001b[39mnum_batches\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "],"ename":"KeyboardInterrupt","evalue":"","output_type":"error"}],"execution_count":422},{"cell_type":"code","source":"patience = 10\nbest_val_loss = float('inf')\nno_improvement_epochs = 0\n\nfor epoch in range(epochs):\n    model.train()\n    running_loss = 0.0 \n    num_batches = 0\n    \n    for data, target in get_batches(train_data, train_targets, batch_size):\n        optimizer.zero_grad()\n        output = model(data)\n        loss = criterion(output, target)\n        loss.backward()\n        optimizer.step()\n        running_loss += loss.item()\n        num_batches += 1\n\n    print(f\"Epoch [{epoch + 1}/{epochs}], Training Loss: {running_loss / num_batches:.4f}\")\n\n    model.eval()\n    val_loss = 0.0\n    correct = 0\n    total = 0\n    num_batches = 0\n\n    with torch.no_grad():\n        for data, target in get_batches(test_data, test_targets, batch_size):\n            outputs = model(data)\n            loss = criterion(outputs, target)\n            val_loss += loss.item()\n            _, predicted = torch.max(outputs, 1)\n            total += target.size(0)\n            num_batches += 1\n            correct += (predicted == target).sum().item()\n\n    accuracy = 100 * correct / total\n    val_loss /= num_batches\n    print(f\"Epoch [{epoch + 1}/{epochs}], Validation Loss: {val_loss:.4f}, Validation Accuracy: {accuracy:.2f}%\")\n    print()\n\n    if val_loss < best_val_loss:\n        best_val_loss = val_loss\n        no_improvement_epochs = 0\n    else:\n        no_improvement_epochs += 1\n\n    if no_improvement_epochs >= patience:\n        print(f\"Early stopping triggered after {epoch + 1} epochs.\")\n        break","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-16T08:04:09.693813Z","iopub.execute_input":"2024-11-16T08:04:09.694742Z","iopub.status.idle":"2024-11-16T08:05:55.945891Z","shell.execute_reply.started":"2024-11-16T08:04:09.694697Z","shell.execute_reply":"2024-11-16T08:05:55.944966Z"}},"outputs":[{"name":"stdout","text":"Epoch [1/100], Training Loss: 0.0315\nEpoch [1/100], Validation Loss: 0.0347, Validation Accuracy: 98.76%\n\nEpoch [2/100], Training Loss: 0.0312\nEpoch [2/100], Validation Loss: 0.0346, Validation Accuracy: 98.76%\n\nEpoch [3/100], Training Loss: 0.0309\nEpoch [3/100], Validation Loss: 0.0345, Validation Accuracy: 98.75%\n\nEpoch [4/100], Training Loss: 0.0306\nEpoch [4/100], Validation Loss: 0.0344, Validation Accuracy: 98.76%\n\nEpoch [5/100], Training Loss: 0.0303\nEpoch [5/100], Validation Loss: 0.0343, Validation Accuracy: 98.76%\n\nEpoch [6/100], Training Loss: 0.0300\nEpoch [6/100], Validation Loss: 0.0342, Validation Accuracy: 98.78%\n\nEpoch [7/100], Training Loss: 0.0297\nEpoch [7/100], Validation Loss: 0.0342, Validation Accuracy: 98.78%\n\nEpoch [8/100], Training Loss: 0.0294\nEpoch [8/100], Validation Loss: 0.0341, Validation Accuracy: 98.78%\n\nEpoch [9/100], Training Loss: 0.0291\nEpoch [9/100], Validation Loss: 0.0340, Validation Accuracy: 98.78%\n\nEpoch [10/100], Training Loss: 0.0288\nEpoch [10/100], Validation Loss: 0.0340, Validation Accuracy: 98.78%\n\nEpoch [11/100], Training Loss: 0.0285\nEpoch [11/100], Validation Loss: 0.0339, Validation Accuracy: 98.77%\n\nEpoch [12/100], Training Loss: 0.0282\nEpoch [12/100], Validation Loss: 0.0338, Validation Accuracy: 98.77%\n\nEpoch [13/100], Training Loss: 0.0279\nEpoch [13/100], Validation Loss: 0.0338, Validation Accuracy: 98.77%\n\nEpoch [14/100], Training Loss: 0.0276\nEpoch [14/100], Validation Loss: 0.0337, Validation Accuracy: 98.74%\n\nEpoch [15/100], Training Loss: 0.0274\nEpoch [15/100], Validation Loss: 0.0336, Validation Accuracy: 98.74%\n\nEpoch [16/100], Training Loss: 0.0271\nEpoch [16/100], Validation Loss: 0.0336, Validation Accuracy: 98.73%\n\nEpoch [17/100], Training Loss: 0.0268\nEpoch [17/100], Validation Loss: 0.0335, Validation Accuracy: 98.72%\n\nEpoch [18/100], Training Loss: 0.0266\nEpoch [18/100], Validation Loss: 0.0334, Validation Accuracy: 98.73%\n\nEpoch [19/100], Training Loss: 0.0263\nEpoch [19/100], Validation Loss: 0.0334, Validation Accuracy: 98.74%\n\nEpoch [20/100], Training Loss: 0.0260\nEpoch [20/100], Validation Loss: 0.0333, Validation Accuracy: 98.75%\n\nEpoch [21/100], Training Loss: 0.0258\nEpoch [21/100], Validation Loss: 0.0332, Validation Accuracy: 98.75%\n\nEpoch [22/100], Training Loss: 0.0255\nEpoch [22/100], Validation Loss: 0.0331, Validation Accuracy: 98.75%\n\nEpoch [23/100], Training Loss: 0.0253\nEpoch [23/100], Validation Loss: 0.0331, Validation Accuracy: 98.76%\n\nEpoch [24/100], Training Loss: 0.0250\nEpoch [24/100], Validation Loss: 0.0330, Validation Accuracy: 98.76%\n\nEpoch [25/100], Training Loss: 0.0248\nEpoch [25/100], Validation Loss: 0.0329, Validation Accuracy: 98.76%\n\nEpoch [26/100], Training Loss: 0.0245\nEpoch [26/100], Validation Loss: 0.0329, Validation Accuracy: 98.78%\n\nEpoch [27/100], Training Loss: 0.0243\nEpoch [27/100], Validation Loss: 0.0328, Validation Accuracy: 98.80%\n\nEpoch [28/100], Training Loss: 0.0240\nEpoch [28/100], Validation Loss: 0.0327, Validation Accuracy: 98.80%\n\nEpoch [29/100], Training Loss: 0.0238\nEpoch [29/100], Validation Loss: 0.0326, Validation Accuracy: 98.80%\n\nEpoch [30/100], Training Loss: 0.0236\nEpoch [30/100], Validation Loss: 0.0326, Validation Accuracy: 98.80%\n\nEpoch [31/100], Training Loss: 0.0233\nEpoch [31/100], Validation Loss: 0.0325, Validation Accuracy: 98.80%\n\nEpoch [32/100], Training Loss: 0.0231\nEpoch [32/100], Validation Loss: 0.0325, Validation Accuracy: 98.80%\n\nEpoch [33/100], Training Loss: 0.0229\nEpoch [33/100], Validation Loss: 0.0324, Validation Accuracy: 98.80%\n\nEpoch [34/100], Training Loss: 0.0226\nEpoch [34/100], Validation Loss: 0.0324, Validation Accuracy: 98.80%\n\nEpoch [35/100], Training Loss: 0.0224\nEpoch [35/100], Validation Loss: 0.0323, Validation Accuracy: 98.80%\n\nEpoch [36/100], Training Loss: 0.0222\nEpoch [36/100], Validation Loss: 0.0323, Validation Accuracy: 98.80%\n\nEpoch [37/100], Training Loss: 0.0219\nEpoch [37/100], Validation Loss: 0.0322, Validation Accuracy: 98.79%\n\nEpoch [38/100], Training Loss: 0.0217\nEpoch [38/100], Validation Loss: 0.0322, Validation Accuracy: 98.79%\n\nEpoch [39/100], Training Loss: 0.0215\nEpoch [39/100], Validation Loss: 0.0321, Validation Accuracy: 98.79%\n\nEpoch [40/100], Training Loss: 0.0213\nEpoch [40/100], Validation Loss: 0.0321, Validation Accuracy: 98.82%\n\nEpoch [41/100], Training Loss: 0.0211\nEpoch [41/100], Validation Loss: 0.0320, Validation Accuracy: 98.83%\n\nEpoch [42/100], Training Loss: 0.0208\nEpoch [42/100], Validation Loss: 0.0319, Validation Accuracy: 98.83%\n\nEpoch [43/100], Training Loss: 0.0206\nEpoch [43/100], Validation Loss: 0.0319, Validation Accuracy: 98.85%\n\nEpoch [44/100], Training Loss: 0.0204\nEpoch [44/100], Validation Loss: 0.0318, Validation Accuracy: 98.86%\n\nEpoch [45/100], Training Loss: 0.0202\nEpoch [45/100], Validation Loss: 0.0318, Validation Accuracy: 98.86%\n\nEpoch [46/100], Training Loss: 0.0200\nEpoch [46/100], Validation Loss: 0.0318, Validation Accuracy: 98.84%\n\nEpoch [47/100], Training Loss: 0.0198\nEpoch [47/100], Validation Loss: 0.0317, Validation Accuracy: 98.84%\n\nEpoch [48/100], Training Loss: 0.0196\nEpoch [48/100], Validation Loss: 0.0317, Validation Accuracy: 98.87%\n\nEpoch [49/100], Training Loss: 0.0194\nEpoch [49/100], Validation Loss: 0.0316, Validation Accuracy: 98.88%\n\nEpoch [50/100], Training Loss: 0.0192\nEpoch [50/100], Validation Loss: 0.0316, Validation Accuracy: 98.88%\n\nEpoch [51/100], Training Loss: 0.0190\nEpoch [51/100], Validation Loss: 0.0316, Validation Accuracy: 98.88%\n\nEpoch [52/100], Training Loss: 0.0188\nEpoch [52/100], Validation Loss: 0.0315, Validation Accuracy: 98.88%\n\nEpoch [53/100], Training Loss: 0.0186\nEpoch [53/100], Validation Loss: 0.0315, Validation Accuracy: 98.88%\n\nEpoch [54/100], Training Loss: 0.0184\nEpoch [54/100], Validation Loss: 0.0314, Validation Accuracy: 98.87%\n\nEpoch [55/100], Training Loss: 0.0182\nEpoch [55/100], Validation Loss: 0.0314, Validation Accuracy: 98.88%\n\nEpoch [56/100], Training Loss: 0.0180\nEpoch [56/100], Validation Loss: 0.0314, Validation Accuracy: 98.88%\n\nEpoch [57/100], Training Loss: 0.0178\nEpoch [57/100], Validation Loss: 0.0313, Validation Accuracy: 98.89%\n\nEpoch [58/100], Training Loss: 0.0176\nEpoch [58/100], Validation Loss: 0.0313, Validation Accuracy: 98.89%\n\nEpoch [59/100], Training Loss: 0.0174\nEpoch [59/100], Validation Loss: 0.0313, Validation Accuracy: 98.89%\n\nEpoch [60/100], Training Loss: 0.0172\nEpoch [60/100], Validation Loss: 0.0312, Validation Accuracy: 98.88%\n\nEpoch [61/100], Training Loss: 0.0170\nEpoch [61/100], Validation Loss: 0.0312, Validation Accuracy: 98.90%\n\nEpoch [62/100], Training Loss: 0.0169\nEpoch [62/100], Validation Loss: 0.0312, Validation Accuracy: 98.90%\n\nEpoch [63/100], Training Loss: 0.0167\nEpoch [63/100], Validation Loss: 0.0311, Validation Accuracy: 98.90%\n\nEpoch [64/100], Training Loss: 0.0165\nEpoch [64/100], Validation Loss: 0.0311, Validation Accuracy: 98.91%\n\nEpoch [65/100], Training Loss: 0.0163\nEpoch [65/100], Validation Loss: 0.0311, Validation Accuracy: 98.92%\n\nEpoch [66/100], Training Loss: 0.0161\nEpoch [66/100], Validation Loss: 0.0311, Validation Accuracy: 98.92%\n\nEpoch [67/100], Training Loss: 0.0160\nEpoch [67/100], Validation Loss: 0.0311, Validation Accuracy: 98.92%\n\nEpoch [68/100], Training Loss: 0.0158\nEpoch [68/100], Validation Loss: 0.0311, Validation Accuracy: 98.91%\n\nEpoch [69/100], Training Loss: 0.0156\nEpoch [69/100], Validation Loss: 0.0311, Validation Accuracy: 98.91%\n\nEpoch [70/100], Training Loss: 0.0154\nEpoch [70/100], Validation Loss: 0.0311, Validation Accuracy: 98.91%\n\nEpoch [71/100], Training Loss: 0.0153\nEpoch [71/100], Validation Loss: 0.0311, Validation Accuracy: 98.90%\n\nEpoch [72/100], Training Loss: 0.0151\nEpoch [72/100], Validation Loss: 0.0311, Validation Accuracy: 98.90%\n\nEpoch [73/100], Training Loss: 0.0149\nEpoch [73/100], Validation Loss: 0.0311, Validation Accuracy: 98.90%\n\nEpoch [74/100], Training Loss: 0.0148\nEpoch [74/100], Validation Loss: 0.0311, Validation Accuracy: 98.90%\n\nEpoch [75/100], Training Loss: 0.0146\nEpoch [75/100], Validation Loss: 0.0311, Validation Accuracy: 98.89%\n\nEpoch [76/100], Training Loss: 0.0144\nEpoch [76/100], Validation Loss: 0.0311, Validation Accuracy: 98.90%\n\nEpoch [77/100], Training Loss: 0.0143\nEpoch [77/100], Validation Loss: 0.0311, Validation Accuracy: 98.91%\n\nEpoch [78/100], Training Loss: 0.0141\nEpoch [78/100], Validation Loss: 0.0311, Validation Accuracy: 98.91%\n\nEpoch [79/100], Training Loss: 0.0139\nEpoch [79/100], Validation Loss: 0.0311, Validation Accuracy: 98.91%\n\nEpoch [80/100], Training Loss: 0.0138\nEpoch [80/100], Validation Loss: 0.0311, Validation Accuracy: 98.91%\n\nEpoch [81/100], Training Loss: 0.0136\nEpoch [81/100], Validation Loss: 0.0311, Validation Accuracy: 98.91%\n\nEpoch [82/100], Training Loss: 0.0135\nEpoch [82/100], Validation Loss: 0.0311, Validation Accuracy: 98.91%\n\nEpoch [83/100], Training Loss: 0.0133\nEpoch [83/100], Validation Loss: 0.0311, Validation Accuracy: 98.91%\n\nEarly stopping triggered after 83 epochs.\n","output_type":"stream"}],"execution_count":391},{"cell_type":"code","source":"patience = 10\nbest_val_loss = float('inf')\nno_improvement_epochs = 0\n\nfor epoch in range(epochs):\n    model.train()\n    running_loss = 0.0\n    num_batches = 0\n    \n    for data, target in get_batches(train_data, train_targets, batch_size):\n        optimizer.zero_grad()\n        output = model(data)\n        loss = criterion(output, target)\n        loss.backward()\n        optimizer.step()\n        running_loss += loss.item()\n        num_batches += 1\n\n    print(f\"Epoch [{epoch + 1}/{epochs}], Training Loss: {running_loss / num_batches:.4f}\")\n\n    model.eval()\n    val_loss = 0.0\n    correct = 0\n    total = 0\n    num_batches = 0\n\n    with torch.no_grad():\n        for data, target in get_batches(test_data, test_targets, batch_size):\n            outputs = model(data)\n            loss = criterion(outputs, target)\n            val_loss += loss.item()\n            _, predicted = torch.max(outputs, 1)\n            total += target.size(0)\n            num_batches += 1\n            correct += (predicted == target).sum().item()\n\n    accuracy = 100 * correct / total\n    val_loss /= num_batches\n    print(f\"Epoch [{epoch + 1}/{epochs}], Validation Loss: {val_loss:.4f}, Validation Accuracy: {accuracy:.2f}%\")\n    print()\n\n    if val_loss < best_val_loss:\n        best_val_loss = val_loss\n        no_improvement_epochs = 0\n    else:\n        no_improvement_epochs += 1\n\n    if no_improvement_epochs >= patience:\n        print(f\"Early stopping triggered after {epoch + 1} epochs.\")\n        break","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-16T07:57:06.088315Z","iopub.execute_input":"2024-11-16T07:57:06.088707Z","iopub.status.idle":"2024-11-16T07:59:15.868164Z","shell.execute_reply.started":"2024-11-16T07:57:06.088667Z","shell.execute_reply":"2024-11-16T07:59:15.867239Z"}},"outputs":[{"name":"stdout","text":"Epoch [1/100], Training Loss: 2.2795\nEpoch [1/100], Validation Loss: 2.2230, Validation Accuracy: 39.66%\n\nEpoch [2/100], Training Loss: 2.1031\nEpoch [2/100], Validation Loss: 1.8653, Validation Accuracy: 69.79%\n\nEpoch [3/100], Training Loss: 1.5604\nEpoch [3/100], Validation Loss: 1.1186, Validation Accuracy: 74.33%\n\nEpoch [4/100], Training Loss: 0.9090\nEpoch [4/100], Validation Loss: 0.6861, Validation Accuracy: 79.13%\n\nEpoch [5/100], Training Loss: 0.6496\nEpoch [5/100], Validation Loss: 0.5461, Validation Accuracy: 83.10%\n\nEpoch [6/100], Training Loss: 0.5338\nEpoch [6/100], Validation Loss: 0.4625, Validation Accuracy: 86.27%\n\nEpoch [7/100], Training Loss: 0.4561\nEpoch [7/100], Validation Loss: 0.3981, Validation Accuracy: 88.23%\n\nEpoch [8/100], Training Loss: 0.3978\nEpoch [8/100], Validation Loss: 0.3481, Validation Accuracy: 89.52%\n\nEpoch [9/100], Training Loss: 0.3518\nEpoch [9/100], Validation Loss: 0.3085, Validation Accuracy: 90.79%\n\nEpoch [10/100], Training Loss: 0.3159\nEpoch [10/100], Validation Loss: 0.2769, Validation Accuracy: 91.75%\n\nEpoch [11/100], Training Loss: 0.2853\nEpoch [11/100], Validation Loss: 0.2505, Validation Accuracy: 92.52%\n\nEpoch [12/100], Training Loss: 0.2601\nEpoch [12/100], Validation Loss: 0.2285, Validation Accuracy: 93.09%\n\nEpoch [13/100], Training Loss: 0.2388\nEpoch [13/100], Validation Loss: 0.2102, Validation Accuracy: 93.65%\n\nEpoch [14/100], Training Loss: 0.2207\nEpoch [14/100], Validation Loss: 0.1944, Validation Accuracy: 94.13%\n\nEpoch [15/100], Training Loss: 0.2052\nEpoch [15/100], Validation Loss: 0.1807, Validation Accuracy: 94.44%\n\nEpoch [16/100], Training Loss: 0.1918\nEpoch [16/100], Validation Loss: 0.1686, Validation Accuracy: 94.62%\n\nEpoch [17/100], Training Loss: 0.1801\nEpoch [17/100], Validation Loss: 0.1577, Validation Accuracy: 95.03%\n\nEpoch [18/100], Training Loss: 0.1698\nEpoch [18/100], Validation Loss: 0.1481, Validation Accuracy: 95.24%\n\nEpoch [19/100], Training Loss: 0.1606\nEpoch [19/100], Validation Loss: 0.1397, Validation Accuracy: 95.51%\n\nEpoch [20/100], Training Loss: 0.1522\nEpoch [20/100], Validation Loss: 0.1320, Validation Accuracy: 95.70%\n\nEpoch [21/100], Training Loss: 0.1446\nEpoch [21/100], Validation Loss: 0.1247, Validation Accuracy: 95.85%\n\nEpoch [22/100], Training Loss: 0.1378\nEpoch [22/100], Validation Loss: 0.1180, Validation Accuracy: 96.05%\n\nEpoch [23/100], Training Loss: 0.1319\nEpoch [23/100], Validation Loss: 0.1122, Validation Accuracy: 96.21%\n\nEpoch [24/100], Training Loss: 0.1266\nEpoch [24/100], Validation Loss: 0.1070, Validation Accuracy: 96.35%\n\nEpoch [25/100], Training Loss: 0.1217\nEpoch [25/100], Validation Loss: 0.1023, Validation Accuracy: 96.51%\n\nEpoch [26/100], Training Loss: 0.1171\nEpoch [26/100], Validation Loss: 0.0979, Validation Accuracy: 96.66%\n\nEpoch [27/100], Training Loss: 0.1128\nEpoch [27/100], Validation Loss: 0.0939, Validation Accuracy: 96.87%\n\nEpoch [28/100], Training Loss: 0.1088\nEpoch [28/100], Validation Loss: 0.0903, Validation Accuracy: 97.02%\n\nEpoch [29/100], Training Loss: 0.1052\nEpoch [29/100], Validation Loss: 0.0871, Validation Accuracy: 97.03%\n\nEpoch [30/100], Training Loss: 0.1018\nEpoch [30/100], Validation Loss: 0.0841, Validation Accuracy: 97.14%\n\nEpoch [31/100], Training Loss: 0.0986\nEpoch [31/100], Validation Loss: 0.0814, Validation Accuracy: 97.25%\n\nEpoch [32/100], Training Loss: 0.0958\nEpoch [32/100], Validation Loss: 0.0789, Validation Accuracy: 97.31%\n\nEpoch [33/100], Training Loss: 0.0931\nEpoch [33/100], Validation Loss: 0.0766, Validation Accuracy: 97.39%\n\nEpoch [34/100], Training Loss: 0.0907\nEpoch [34/100], Validation Loss: 0.0744, Validation Accuracy: 97.49%\n\nEpoch [35/100], Training Loss: 0.0883\nEpoch [35/100], Validation Loss: 0.0724, Validation Accuracy: 97.55%\n\nEpoch [36/100], Training Loss: 0.0862\nEpoch [36/100], Validation Loss: 0.0705, Validation Accuracy: 97.60%\n\nEpoch [37/100], Training Loss: 0.0841\nEpoch [37/100], Validation Loss: 0.0687, Validation Accuracy: 97.68%\n\nEpoch [38/100], Training Loss: 0.0822\nEpoch [38/100], Validation Loss: 0.0671, Validation Accuracy: 97.72%\n\nEpoch [39/100], Training Loss: 0.0804\nEpoch [39/100], Validation Loss: 0.0655, Validation Accuracy: 97.76%\n\nEpoch [40/100], Training Loss: 0.0786\nEpoch [40/100], Validation Loss: 0.0641, Validation Accuracy: 97.78%\n\nEpoch [41/100], Training Loss: 0.0770\nEpoch [41/100], Validation Loss: 0.0627, Validation Accuracy: 97.81%\n\nEpoch [42/100], Training Loss: 0.0754\nEpoch [42/100], Validation Loss: 0.0614, Validation Accuracy: 97.83%\n\nEpoch [43/100], Training Loss: 0.0739\nEpoch [43/100], Validation Loss: 0.0602, Validation Accuracy: 97.90%\n\nEpoch [44/100], Training Loss: 0.0724\nEpoch [44/100], Validation Loss: 0.0590, Validation Accuracy: 97.93%\n\nEpoch [45/100], Training Loss: 0.0711\nEpoch [45/100], Validation Loss: 0.0579, Validation Accuracy: 97.95%\n\nEpoch [46/100], Training Loss: 0.0697\nEpoch [46/100], Validation Loss: 0.0568, Validation Accuracy: 97.99%\n\nEpoch [47/100], Training Loss: 0.0685\nEpoch [47/100], Validation Loss: 0.0558, Validation Accuracy: 98.00%\n\nEpoch [48/100], Training Loss: 0.0672\nEpoch [48/100], Validation Loss: 0.0549, Validation Accuracy: 98.02%\n\nEpoch [49/100], Training Loss: 0.0661\nEpoch [49/100], Validation Loss: 0.0540, Validation Accuracy: 98.04%\n\nEpoch [50/100], Training Loss: 0.0649\nEpoch [50/100], Validation Loss: 0.0531, Validation Accuracy: 98.08%\n\nEpoch [51/100], Training Loss: 0.0638\nEpoch [51/100], Validation Loss: 0.0523, Validation Accuracy: 98.11%\n\nEpoch [52/100], Training Loss: 0.0628\nEpoch [52/100], Validation Loss: 0.0515, Validation Accuracy: 98.12%\n\nEpoch [53/100], Training Loss: 0.0618\nEpoch [53/100], Validation Loss: 0.0508, Validation Accuracy: 98.17%\n\nEpoch [54/100], Training Loss: 0.0608\nEpoch [54/100], Validation Loss: 0.0501, Validation Accuracy: 98.20%\n\nEpoch [55/100], Training Loss: 0.0598\nEpoch [55/100], Validation Loss: 0.0494, Validation Accuracy: 98.21%\n\nEpoch [56/100], Training Loss: 0.0589\nEpoch [56/100], Validation Loss: 0.0487, Validation Accuracy: 98.27%\n\nEpoch [57/100], Training Loss: 0.0580\nEpoch [57/100], Validation Loss: 0.0481, Validation Accuracy: 98.31%\n\nEpoch [58/100], Training Loss: 0.0571\nEpoch [58/100], Validation Loss: 0.0475, Validation Accuracy: 98.33%\n\nEpoch [59/100], Training Loss: 0.0563\nEpoch [59/100], Validation Loss: 0.0470, Validation Accuracy: 98.34%\n\nEpoch [60/100], Training Loss: 0.0555\nEpoch [60/100], Validation Loss: 0.0464, Validation Accuracy: 98.34%\n\nEpoch [61/100], Training Loss: 0.0547\nEpoch [61/100], Validation Loss: 0.0459, Validation Accuracy: 98.35%\n\nEpoch [62/100], Training Loss: 0.0539\nEpoch [62/100], Validation Loss: 0.0455, Validation Accuracy: 98.35%\n\nEpoch [63/100], Training Loss: 0.0532\nEpoch [63/100], Validation Loss: 0.0450, Validation Accuracy: 98.36%\n\nEpoch [64/100], Training Loss: 0.0525\nEpoch [64/100], Validation Loss: 0.0445, Validation Accuracy: 98.38%\n\nEpoch [65/100], Training Loss: 0.0518\nEpoch [65/100], Validation Loss: 0.0441, Validation Accuracy: 98.38%\n\nEpoch [66/100], Training Loss: 0.0511\nEpoch [66/100], Validation Loss: 0.0437, Validation Accuracy: 98.39%\n\nEpoch [67/100], Training Loss: 0.0504\nEpoch [67/100], Validation Loss: 0.0433, Validation Accuracy: 98.43%\n\nEpoch [68/100], Training Loss: 0.0498\nEpoch [68/100], Validation Loss: 0.0430, Validation Accuracy: 98.42%\n\nEpoch [69/100], Training Loss: 0.0492\nEpoch [69/100], Validation Loss: 0.0426, Validation Accuracy: 98.43%\n\nEpoch [70/100], Training Loss: 0.0486\nEpoch [70/100], Validation Loss: 0.0423, Validation Accuracy: 98.45%\n\nEpoch [71/100], Training Loss: 0.0480\nEpoch [71/100], Validation Loss: 0.0419, Validation Accuracy: 98.46%\n\nEpoch [72/100], Training Loss: 0.0474\nEpoch [72/100], Validation Loss: 0.0416, Validation Accuracy: 98.50%\n\nEpoch [73/100], Training Loss: 0.0468\nEpoch [73/100], Validation Loss: 0.0412, Validation Accuracy: 98.50%\n\nEpoch [74/100], Training Loss: 0.0463\nEpoch [74/100], Validation Loss: 0.0409, Validation Accuracy: 98.49%\n\nEpoch [75/100], Training Loss: 0.0457\nEpoch [75/100], Validation Loss: 0.0406, Validation Accuracy: 98.52%\n\nEpoch [76/100], Training Loss: 0.0452\nEpoch [76/100], Validation Loss: 0.0402, Validation Accuracy: 98.52%\n\nEpoch [77/100], Training Loss: 0.0447\nEpoch [77/100], Validation Loss: 0.0399, Validation Accuracy: 98.53%\n\nEpoch [78/100], Training Loss: 0.0442\nEpoch [78/100], Validation Loss: 0.0395, Validation Accuracy: 98.56%\n\nEpoch [79/100], Training Loss: 0.0437\nEpoch [79/100], Validation Loss: 0.0391, Validation Accuracy: 98.57%\n\nEpoch [80/100], Training Loss: 0.0432\nEpoch [80/100], Validation Loss: 0.0387, Validation Accuracy: 98.57%\n\nEpoch [81/100], Training Loss: 0.0427\nEpoch [81/100], Validation Loss: 0.0383, Validation Accuracy: 98.57%\n\nEpoch [82/100], Training Loss: 0.0422\nEpoch [82/100], Validation Loss: 0.0379, Validation Accuracy: 98.58%\n\nEpoch [83/100], Training Loss: 0.0418\nEpoch [83/100], Validation Loss: 0.0374, Validation Accuracy: 98.58%\n\nEpoch [84/100], Training Loss: 0.0413\nEpoch [84/100], Validation Loss: 0.0370, Validation Accuracy: 98.56%\n\nEpoch [85/100], Training Loss: 0.0408\nEpoch [85/100], Validation Loss: 0.0366, Validation Accuracy: 98.59%\n\nEpoch [86/100], Training Loss: 0.0403\nEpoch [86/100], Validation Loss: 0.0361, Validation Accuracy: 98.62%\n\nEpoch [87/100], Training Loss: 0.0398\nEpoch [87/100], Validation Loss: 0.0357, Validation Accuracy: 98.64%\n\nEpoch [88/100], Training Loss: 0.0393\nEpoch [88/100], Validation Loss: 0.0354, Validation Accuracy: 98.67%\n\nEpoch [89/100], Training Loss: 0.0388\nEpoch [89/100], Validation Loss: 0.0350, Validation Accuracy: 98.68%\n\nEpoch [90/100], Training Loss: 0.0383\nEpoch [90/100], Validation Loss: 0.0347, Validation Accuracy: 98.69%\n\nEpoch [91/100], Training Loss: 0.0379\nEpoch [91/100], Validation Loss: 0.0345, Validation Accuracy: 98.71%\n\nEpoch [92/100], Training Loss: 0.0374\nEpoch [92/100], Validation Loss: 0.0343, Validation Accuracy: 98.73%\n\nEpoch [93/100], Training Loss: 0.0369\nEpoch [93/100], Validation Loss: 0.0341, Validation Accuracy: 98.73%\n\nEpoch [94/100], Training Loss: 0.0364\nEpoch [94/100], Validation Loss: 0.0339, Validation Accuracy: 98.74%\n\nEpoch [95/100], Training Loss: 0.0360\nEpoch [95/100], Validation Loss: 0.0337, Validation Accuracy: 98.74%\n\nEpoch [96/100], Training Loss: 0.0356\nEpoch [96/100], Validation Loss: 0.0336, Validation Accuracy: 98.73%\n\nEpoch [97/100], Training Loss: 0.0351\nEpoch [97/100], Validation Loss: 0.0334, Validation Accuracy: 98.74%\n\nEpoch [98/100], Training Loss: 0.0347\nEpoch [98/100], Validation Loss: 0.0332, Validation Accuracy: 98.75%\n\nEpoch [99/100], Training Loss: 0.0343\nEpoch [99/100], Validation Loss: 0.0331, Validation Accuracy: 98.75%\n\nEpoch [100/100], Training Loss: 0.0339\nEpoch [100/100], Validation Loss: 0.0329, Validation Accuracy: 98.77%\n\n","output_type":"stream"}],"execution_count":385},{"cell_type":"code","source":"model.enable_aug = False","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-16T07:59:20.616424Z","iopub.execute_input":"2024-11-16T07:59:20.616807Z","iopub.status.idle":"2024-11-16T07:59:20.621371Z","shell.execute_reply.started":"2024-11-16T07:59:20.616768Z","shell.execute_reply":"2024-11-16T07:59:20.620399Z"}},"outputs":[],"execution_count":386},{"cell_type":"code","source":"patience = 10\nbest_val_loss = float('inf')\nno_improvement_epochs = 0\n\nfor epoch in range(epochs):\n    model.train()\n    running_loss = 0.0\n    num_batches = 0\n    \n    for data, target in get_batches(train_data, train_targets, batch_size):\n        optimizer.zero_grad()\n        output = model(data)\n        loss = criterion(output, target)\n        loss.backward()\n        optimizer.step()\n        running_loss += loss.item()\n        num_batches += 1\n\n    print(f\"Epoch [{epoch + 1}/{epochs}], Training Loss: {running_loss / num_batches:.4f}\")\n\n    model.eval()\n    val_loss = 0.0\n    correct = 0\n    total = 0\n    num_batches = 0\n\n    with torch.no_grad():\n        for data, target in get_batches(test_data, test_targets, batch_size):\n            outputs = model(data)\n            loss = criterion(outputs, target)\n            val_loss += loss.item()\n            _, predicted = torch.max(outputs, 1)\n            total += target.size(0)\n            num_batches += 1\n            correct += (predicted == target).sum().item()\n\n    accuracy = 100 * correct / total\n    val_loss /= num_batches\n    print(f\"Epoch [{epoch + 1}/{epochs}], Validation Loss: {val_loss:.4f}, Validation Accuracy: {accuracy:.2f}%\")\n    print()\n\n    if val_loss < best_val_loss:\n        best_val_loss = val_loss\n        no_improvement_epochs = 0\n    else:\n        no_improvement_epochs += 1\n\n    if no_improvement_epochs >= patience:\n        print(f\"Early stopping triggered after {epoch + 1} epochs.\")\n        break","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-16T07:59:23.199543Z","iopub.execute_input":"2024-11-16T07:59:23.199927Z","iopub.status.idle":"2024-11-16T08:00:38.716517Z","shell.execute_reply.started":"2024-11-16T07:59:23.199888Z","shell.execute_reply":"2024-11-16T08:00:38.715500Z"}},"outputs":[{"name":"stdout","text":"Epoch [1/100], Training Loss: 0.0503\nEpoch [1/100], Validation Loss: 0.0402, Validation Accuracy: 98.58%\n\nEpoch [2/100], Training Loss: 0.0421\nEpoch [2/100], Validation Loss: 0.0413, Validation Accuracy: 98.58%\n\nEpoch [3/100], Training Loss: 0.0389\nEpoch [3/100], Validation Loss: 0.0354, Validation Accuracy: 98.74%\n\nEpoch [4/100], Training Loss: 0.0367\nEpoch [4/100], Validation Loss: 0.0332, Validation Accuracy: 98.81%\n\nEpoch [5/100], Training Loss: 0.0348\nEpoch [5/100], Validation Loss: 0.0329, Validation Accuracy: 98.79%\n\nEpoch [6/100], Training Loss: 0.0339\nEpoch [6/100], Validation Loss: 0.0328, Validation Accuracy: 98.81%\n\nEpoch [7/100], Training Loss: 0.0330\nEpoch [7/100], Validation Loss: 0.0325, Validation Accuracy: 98.84%\n\nEpoch [8/100], Training Loss: 0.0323\nEpoch [8/100], Validation Loss: 0.0321, Validation Accuracy: 98.81%\n\nEpoch [9/100], Training Loss: 0.0315\nEpoch [9/100], Validation Loss: 0.0320, Validation Accuracy: 98.83%\n\nEpoch [10/100], Training Loss: 0.0309\nEpoch [10/100], Validation Loss: 0.0318, Validation Accuracy: 98.82%\n\nEpoch [11/100], Training Loss: 0.0303\nEpoch [11/100], Validation Loss: 0.0316, Validation Accuracy: 98.83%\n\nEpoch [12/100], Training Loss: 0.0297\nEpoch [12/100], Validation Loss: 0.0314, Validation Accuracy: 98.83%\n\nEpoch [13/100], Training Loss: 0.0292\nEpoch [13/100], Validation Loss: 0.0312, Validation Accuracy: 98.85%\n\nEpoch [14/100], Training Loss: 0.0287\nEpoch [14/100], Validation Loss: 0.0310, Validation Accuracy: 98.86%\n\nEpoch [15/100], Training Loss: 0.0283\nEpoch [15/100], Validation Loss: 0.0308, Validation Accuracy: 98.87%\n\nEpoch [16/100], Training Loss: 0.0279\nEpoch [16/100], Validation Loss: 0.0307, Validation Accuracy: 98.89%\n\nEpoch [17/100], Training Loss: 0.0274\nEpoch [17/100], Validation Loss: 0.0305, Validation Accuracy: 98.89%\n\nEpoch [18/100], Training Loss: 0.0271\nEpoch [18/100], Validation Loss: 0.0304, Validation Accuracy: 98.90%\n\nEpoch [19/100], Training Loss: 0.0267\nEpoch [19/100], Validation Loss: 0.0303, Validation Accuracy: 98.90%\n\nEpoch [20/100], Training Loss: 0.0263\nEpoch [20/100], Validation Loss: 0.0302, Validation Accuracy: 98.90%\n\nEpoch [21/100], Training Loss: 0.0259\nEpoch [21/100], Validation Loss: 0.0301, Validation Accuracy: 98.91%\n\nEpoch [22/100], Training Loss: 0.0256\nEpoch [22/100], Validation Loss: 0.0300, Validation Accuracy: 98.91%\n\nEpoch [23/100], Training Loss: 0.0253\nEpoch [23/100], Validation Loss: 0.0299, Validation Accuracy: 98.91%\n\nEpoch [24/100], Training Loss: 0.0249\nEpoch [24/100], Validation Loss: 0.0298, Validation Accuracy: 98.92%\n\nEpoch [25/100], Training Loss: 0.0246\nEpoch [25/100], Validation Loss: 0.0298, Validation Accuracy: 98.92%\n\nEpoch [26/100], Training Loss: 0.0243\nEpoch [26/100], Validation Loss: 0.0297, Validation Accuracy: 98.94%\n\nEpoch [27/100], Training Loss: 0.0240\nEpoch [27/100], Validation Loss: 0.0296, Validation Accuracy: 98.94%\n\nEpoch [28/100], Training Loss: 0.0237\nEpoch [28/100], Validation Loss: 0.0296, Validation Accuracy: 98.94%\n\nEpoch [29/100], Training Loss: 0.0234\nEpoch [29/100], Validation Loss: 0.0295, Validation Accuracy: 98.93%\n\nEpoch [30/100], Training Loss: 0.0231\nEpoch [30/100], Validation Loss: 0.0295, Validation Accuracy: 98.92%\n\nEpoch [31/100], Training Loss: 0.0228\nEpoch [31/100], Validation Loss: 0.0294, Validation Accuracy: 98.91%\n\nEpoch [32/100], Training Loss: 0.0225\nEpoch [32/100], Validation Loss: 0.0294, Validation Accuracy: 98.91%\n\nEpoch [33/100], Training Loss: 0.0222\nEpoch [33/100], Validation Loss: 0.0293, Validation Accuracy: 98.90%\n\nEpoch [34/100], Training Loss: 0.0220\nEpoch [34/100], Validation Loss: 0.0293, Validation Accuracy: 98.90%\n\nEpoch [35/100], Training Loss: 0.0217\nEpoch [35/100], Validation Loss: 0.0293, Validation Accuracy: 98.90%\n\nEpoch [36/100], Training Loss: 0.0215\nEpoch [36/100], Validation Loss: 0.0292, Validation Accuracy: 98.90%\n\nEpoch [37/100], Training Loss: 0.0212\nEpoch [37/100], Validation Loss: 0.0292, Validation Accuracy: 98.89%\n\nEpoch [38/100], Training Loss: 0.0209\nEpoch [38/100], Validation Loss: 0.0292, Validation Accuracy: 98.88%\n\nEpoch [39/100], Training Loss: 0.0207\nEpoch [39/100], Validation Loss: 0.0291, Validation Accuracy: 98.88%\n\nEpoch [40/100], Training Loss: 0.0205\nEpoch [40/100], Validation Loss: 0.0291, Validation Accuracy: 98.88%\n\nEpoch [41/100], Training Loss: 0.0202\nEpoch [41/100], Validation Loss: 0.0291, Validation Accuracy: 98.88%\n\nEpoch [42/100], Training Loss: 0.0200\nEpoch [42/100], Validation Loss: 0.0291, Validation Accuracy: 98.88%\n\nEpoch [43/100], Training Loss: 0.0197\nEpoch [43/100], Validation Loss: 0.0291, Validation Accuracy: 98.88%\n\nEpoch [44/100], Training Loss: 0.0195\nEpoch [44/100], Validation Loss: 0.0291, Validation Accuracy: 98.89%\n\nEpoch [45/100], Training Loss: 0.0193\nEpoch [45/100], Validation Loss: 0.0291, Validation Accuracy: 98.90%\n\nEpoch [46/100], Training Loss: 0.0191\nEpoch [46/100], Validation Loss: 0.0291, Validation Accuracy: 98.89%\n\nEpoch [47/100], Training Loss: 0.0188\nEpoch [47/100], Validation Loss: 0.0291, Validation Accuracy: 98.90%\n\nEpoch [48/100], Training Loss: 0.0186\nEpoch [48/100], Validation Loss: 0.0291, Validation Accuracy: 98.91%\n\nEpoch [49/100], Training Loss: 0.0184\nEpoch [49/100], Validation Loss: 0.0290, Validation Accuracy: 98.92%\n\nEpoch [50/100], Training Loss: 0.0182\nEpoch [50/100], Validation Loss: 0.0290, Validation Accuracy: 98.92%\n\nEpoch [51/100], Training Loss: 0.0180\nEpoch [51/100], Validation Loss: 0.0290, Validation Accuracy: 98.92%\n\nEpoch [52/100], Training Loss: 0.0178\nEpoch [52/100], Validation Loss: 0.0291, Validation Accuracy: 98.92%\n\nEpoch [53/100], Training Loss: 0.0175\nEpoch [53/100], Validation Loss: 0.0291, Validation Accuracy: 98.92%\n\nEpoch [54/100], Training Loss: 0.0173\nEpoch [54/100], Validation Loss: 0.0290, Validation Accuracy: 98.92%\n\nEpoch [55/100], Training Loss: 0.0171\nEpoch [55/100], Validation Loss: 0.0291, Validation Accuracy: 98.94%\n\nEpoch [56/100], Training Loss: 0.0169\nEpoch [56/100], Validation Loss: 0.0291, Validation Accuracy: 98.96%\n\nEpoch [57/100], Training Loss: 0.0167\nEpoch [57/100], Validation Loss: 0.0291, Validation Accuracy: 98.96%\n\nEpoch [58/100], Training Loss: 0.0165\nEpoch [58/100], Validation Loss: 0.0291, Validation Accuracy: 98.96%\n\nEpoch [59/100], Training Loss: 0.0163\nEpoch [59/100], Validation Loss: 0.0291, Validation Accuracy: 98.96%\n\nEarly stopping triggered after 59 epochs.\n","output_type":"stream"}],"execution_count":387},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\n\nclass SimpleCNN(nn.Module):\n    def __init__(self):\n        super(SimpleCNN, self).__init__()\n        self.conv1 = nn.Conv2d(1, 32, kernel_size=4, stride=1, padding=3)\n        self.fc1 = nn.Linear(32 * 5 * 5, 10)\n        \n        self.pool = nn.MaxPool2d(2, 2)\n        self.relu = nn.ReLU()\n\n        nn.init.constant_(self.fc1.weight, 0)\n        nn.init.constant_(self.fc1.bias, 0)\n\n    def forward(self, x):\n        # x = x.view(x.size(0), 1, 7, 4, 7, 4).mean(dim=(3, 5))\n        x = x.view(x.size(0), 1, 7, 4, 7, 4).max(dim=3)[0].max(dim=4)[0]\n\n        x = self.relu(self.conv1(x))\n        x = self.pool(x)\n\n        x = x.view(-1, 32 * 5 * 5)\n        x = self.fc1(x)\n        return x\n\nbatch_size = 6000\nlearning_rate = 0.001 * 5\nepochs = 100\n\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nmodel = SimpleCNN().to(device)\ncriterion = nn.CrossEntropyLoss()\noptimizer = optim.Adam(model.parameters(), lr=learning_rate)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-16T06:57:54.627761Z","iopub.execute_input":"2024-11-16T06:57:54.628841Z","iopub.status.idle":"2024-11-16T06:57:54.640951Z","shell.execute_reply.started":"2024-11-16T06:57:54.628796Z","shell.execute_reply":"2024-11-16T06:57:54.640058Z"}},"outputs":[],"execution_count":341},{"cell_type":"code","source":"patience = 10\nbest_val_loss = float('inf')\nno_improvement_epochs = 0\n\nfor epoch in range(epochs):\n    model.train()\n    running_loss = 0.0\n    num_batches = 0\n    \n    for data, target in get_batches(train_data, train_targets, batch_size):\n        optimizer.zero_grad()\n        output = model(data)\n        loss = criterion(output, target)\n        loss.backward()\n        optimizer.step()\n        running_loss += loss.item()\n        num_batches += 1\n\n    print(f\"Epoch [{epoch + 1}/{epochs}], Training Loss: {running_loss / num_batches:.4f}\")\n\n    model.eval()\n    val_loss = 0.0\n    correct = 0\n    total = 0\n    num_batches = 0\n\n    with torch.no_grad():\n        for data, target in get_batches(test_data, test_targets, batch_size):\n            outputs = model(data)\n            loss = criterion(outputs, target)\n            val_loss += loss.item()\n            _, predicted = torch.max(outputs, 1)\n            total += target.size(0)\n            num_batches += 1\n            correct += (predicted == target).sum().item()\n\n    accuracy = 100 * correct / total\n    val_loss /= num_batches\n    print(f\"Epoch [{epoch + 1}/{epochs}], Validation Loss: {val_loss:.4f}, Validation Accuracy: {accuracy:.2f}%\")\n    print()\n\n    if val_loss < best_val_loss:\n        best_val_loss = val_loss\n        no_improvement_epochs = 0\n    else:\n        no_improvement_epochs += 1\n\n    if no_improvement_epochs >= patience:\n        print(f\"Early stopping triggered after {epoch + 1} epochs.\")\n        break","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-16T06:57:55.850164Z","iopub.execute_input":"2024-11-16T06:57:55.850556Z","iopub.status.idle":"2024-11-16T06:58:02.646445Z","shell.execute_reply.started":"2024-11-16T06:57:55.850518Z","shell.execute_reply":"2024-11-16T06:58:02.645395Z"}},"outputs":[{"name":"stdout","text":"Epoch [1/100], Training Loss: 1.8729\nEpoch [1/100], Validation Loss: 1.3403, Validation Accuracy: 69.92%\n\nEpoch [2/100], Training Loss: 1.0685\nEpoch [2/100], Validation Loss: 0.7984, Validation Accuracy: 75.89%\n\nEpoch [3/100], Training Loss: 0.7187\nEpoch [3/100], Validation Loss: 0.6105, Validation Accuracy: 81.45%\n\nEpoch [4/100], Training Loss: 0.5833\nEpoch [4/100], Validation Loss: 0.5233, Validation Accuracy: 83.99%\n\nEpoch [5/100], Training Loss: 0.5124\nEpoch [5/100], Validation Loss: 0.4765, Validation Accuracy: 85.15%\n\nEpoch [6/100], Training Loss: 0.4697\nEpoch [6/100], Validation Loss: 0.4443, Validation Accuracy: 86.02%\n\nEpoch [7/100], Training Loss: 0.4417\nEpoch [7/100], Validation Loss: 0.4215, Validation Accuracy: 86.56%\n\nEpoch [8/100], Training Loss: 0.4209\nEpoch [8/100], Validation Loss: 0.4052, Validation Accuracy: 86.98%\n\nEpoch [9/100], Training Loss: 0.4051\nEpoch [9/100], Validation Loss: 0.3929, Validation Accuracy: 87.51%\n\nEpoch [10/100], Training Loss: 0.3921\nEpoch [10/100], Validation Loss: 0.3825, Validation Accuracy: 87.76%\n\nEpoch [11/100], Training Loss: 0.3812\nEpoch [11/100], Validation Loss: 0.3727, Validation Accuracy: 88.07%\n\nEpoch [12/100], Training Loss: 0.3716\nEpoch [12/100], Validation Loss: 0.3636, Validation Accuracy: 88.38%\n\nEpoch [13/100], Training Loss: 0.3631\nEpoch [13/100], Validation Loss: 0.3561, Validation Accuracy: 88.66%\n\nEpoch [14/100], Training Loss: 0.3558\nEpoch [14/100], Validation Loss: 0.3498, Validation Accuracy: 88.78%\n\nEpoch [15/100], Training Loss: 0.3495\nEpoch [15/100], Validation Loss: 0.3442, Validation Accuracy: 88.89%\n\nEpoch [16/100], Training Loss: 0.3437\nEpoch [16/100], Validation Loss: 0.3394, Validation Accuracy: 89.12%\n\nEpoch [17/100], Training Loss: 0.3386\nEpoch [17/100], Validation Loss: 0.3351, Validation Accuracy: 89.24%\n\nEpoch [18/100], Training Loss: 0.3339\nEpoch [18/100], Validation Loss: 0.3312, Validation Accuracy: 89.38%\n\nEpoch [19/100], Training Loss: 0.3297\nEpoch [19/100], Validation Loss: 0.3276, Validation Accuracy: 89.42%\n\nEpoch [20/100], Training Loss: 0.3258\nEpoch [20/100], Validation Loss: 0.3242, Validation Accuracy: 89.54%\n\nEpoch [21/100], Training Loss: 0.3222\nEpoch [21/100], Validation Loss: 0.3211, Validation Accuracy: 89.65%\n\nEpoch [22/100], Training Loss: 0.3188\nEpoch [22/100], Validation Loss: 0.3182, Validation Accuracy: 89.68%\n\nEpoch [23/100], Training Loss: 0.3156\nEpoch [23/100], Validation Loss: 0.3156, Validation Accuracy: 89.79%\n\nEpoch [24/100], Training Loss: 0.3127\nEpoch [24/100], Validation Loss: 0.3132, Validation Accuracy: 89.85%\n\nEpoch [25/100], Training Loss: 0.3099\nEpoch [25/100], Validation Loss: 0.3110, Validation Accuracy: 89.89%\n\nEpoch [26/100], Training Loss: 0.3073\nEpoch [26/100], Validation Loss: 0.3089, Validation Accuracy: 89.91%\n\nEpoch [27/100], Training Loss: 0.3048\nEpoch [27/100], Validation Loss: 0.3069, Validation Accuracy: 90.01%\n\nEpoch [28/100], Training Loss: 0.3024\nEpoch [28/100], Validation Loss: 0.3051, Validation Accuracy: 90.09%\n\nEpoch [29/100], Training Loss: 0.3001\nEpoch [29/100], Validation Loss: 0.3033, Validation Accuracy: 90.16%\n\nEpoch [30/100], Training Loss: 0.2979\nEpoch [30/100], Validation Loss: 0.3017, Validation Accuracy: 90.19%\n\nEpoch [31/100], Training Loss: 0.2958\nEpoch [31/100], Validation Loss: 0.3001, Validation Accuracy: 90.22%\n\nEpoch [32/100], Training Loss: 0.2938\nEpoch [32/100], Validation Loss: 0.2986, Validation Accuracy: 90.30%\n\nEpoch [33/100], Training Loss: 0.2919\nEpoch [33/100], Validation Loss: 0.2972, Validation Accuracy: 90.28%\n\nEpoch [34/100], Training Loss: 0.2901\nEpoch [34/100], Validation Loss: 0.2958, Validation Accuracy: 90.30%\n\nEpoch [35/100], Training Loss: 0.2884\nEpoch [35/100], Validation Loss: 0.2945, Validation Accuracy: 90.33%\n\nEpoch [36/100], Training Loss: 0.2867\nEpoch [36/100], Validation Loss: 0.2933, Validation Accuracy: 90.39%\n\nEpoch [37/100], Training Loss: 0.2851\nEpoch [37/100], Validation Loss: 0.2921, Validation Accuracy: 90.44%\n\nEpoch [38/100], Training Loss: 0.2836\nEpoch [38/100], Validation Loss: 0.2909, Validation Accuracy: 90.41%\n\nEpoch [39/100], Training Loss: 0.2821\nEpoch [39/100], Validation Loss: 0.2898, Validation Accuracy: 90.40%\n\nEpoch [40/100], Training Loss: 0.2807\nEpoch [40/100], Validation Loss: 0.2888, Validation Accuracy: 90.42%\n\nEpoch [41/100], Training Loss: 0.2793\nEpoch [41/100], Validation Loss: 0.2878, Validation Accuracy: 90.49%\n\nEpoch [42/100], Training Loss: 0.2780\nEpoch [42/100], Validation Loss: 0.2869, Validation Accuracy: 90.53%\n\nEpoch [43/100], Training Loss: 0.2767\nEpoch [43/100], Validation Loss: 0.2860, Validation Accuracy: 90.51%\n\nEpoch [44/100], Training Loss: 0.2755\nEpoch [44/100], Validation Loss: 0.2852, Validation Accuracy: 90.53%\n\nEpoch [45/100], Training Loss: 0.2743\nEpoch [45/100], Validation Loss: 0.2843, Validation Accuracy: 90.58%\n\nEpoch [46/100], Training Loss: 0.2731\nEpoch [46/100], Validation Loss: 0.2836, Validation Accuracy: 90.63%\n\nEpoch [47/100], Training Loss: 0.2720\nEpoch [47/100], Validation Loss: 0.2828, Validation Accuracy: 90.64%\n\nEpoch [48/100], Training Loss: 0.2709\nEpoch [48/100], Validation Loss: 0.2821, Validation Accuracy: 90.65%\n\nEpoch [49/100], Training Loss: 0.2698\nEpoch [49/100], Validation Loss: 0.2814, Validation Accuracy: 90.65%\n\nEpoch [50/100], Training Loss: 0.2688\nEpoch [50/100], Validation Loss: 0.2807, Validation Accuracy: 90.67%\n\nEpoch [51/100], Training Loss: 0.2678\nEpoch [51/100], Validation Loss: 0.2800, Validation Accuracy: 90.71%\n\nEpoch [52/100], Training Loss: 0.2668\nEpoch [52/100], Validation Loss: 0.2794, Validation Accuracy: 90.73%\n\nEpoch [53/100], Training Loss: 0.2659\nEpoch [53/100], Validation Loss: 0.2788, Validation Accuracy: 90.73%\n\nEpoch [54/100], Training Loss: 0.2649\nEpoch [54/100], Validation Loss: 0.2782, Validation Accuracy: 90.75%\n\nEpoch [55/100], Training Loss: 0.2640\nEpoch [55/100], Validation Loss: 0.2777, Validation Accuracy: 90.79%\n\nEpoch [56/100], Training Loss: 0.2632\nEpoch [56/100], Validation Loss: 0.2772, Validation Accuracy: 90.76%\n\nEpoch [57/100], Training Loss: 0.2624\nEpoch [57/100], Validation Loss: 0.2767, Validation Accuracy: 90.81%\n\nEpoch [58/100], Training Loss: 0.2616\nEpoch [58/100], Validation Loss: 0.2763, Validation Accuracy: 90.81%\n\nEpoch [59/100], Training Loss: 0.2607\nEpoch [59/100], Validation Loss: 0.2758, Validation Accuracy: 90.83%\n\nEpoch [60/100], Training Loss: 0.2600\nEpoch [60/100], Validation Loss: 0.2754, Validation Accuracy: 90.83%\n\nEpoch [61/100], Training Loss: 0.2592\nEpoch [61/100], Validation Loss: 0.2750, Validation Accuracy: 90.83%\n\nEpoch [62/100], Training Loss: 0.2584\nEpoch [62/100], Validation Loss: 0.2746, Validation Accuracy: 90.83%\n\nEpoch [63/100], Training Loss: 0.2577\nEpoch [63/100], Validation Loss: 0.2742, Validation Accuracy: 90.83%\n\nEpoch [64/100], Training Loss: 0.2570\nEpoch [64/100], Validation Loss: 0.2739, Validation Accuracy: 90.87%\n\nEpoch [65/100], Training Loss: 0.2563\nEpoch [65/100], Validation Loss: 0.2735, Validation Accuracy: 90.86%\n\nEpoch [66/100], Training Loss: 0.2556\nEpoch [66/100], Validation Loss: 0.2732, Validation Accuracy: 90.85%\n\nEpoch [67/100], Training Loss: 0.2550\nEpoch [67/100], Validation Loss: 0.2729, Validation Accuracy: 90.87%\n\nEpoch [68/100], Training Loss: 0.2543\nEpoch [68/100], Validation Loss: 0.2726, Validation Accuracy: 90.88%\n\nEpoch [69/100], Training Loss: 0.2537\nEpoch [69/100], Validation Loss: 0.2723, Validation Accuracy: 90.93%\n\nEpoch [70/100], Training Loss: 0.2531\nEpoch [70/100], Validation Loss: 0.2719, Validation Accuracy: 90.90%\n\nEpoch [71/100], Training Loss: 0.2525\nEpoch [71/100], Validation Loss: 0.2717, Validation Accuracy: 90.91%\n\nEpoch [72/100], Training Loss: 0.2519\nEpoch [72/100], Validation Loss: 0.2714, Validation Accuracy: 90.92%\n\nEpoch [73/100], Training Loss: 0.2513\nEpoch [73/100], Validation Loss: 0.2712, Validation Accuracy: 90.93%\n\nEpoch [74/100], Training Loss: 0.2507\nEpoch [74/100], Validation Loss: 0.2709, Validation Accuracy: 90.96%\n\nEpoch [75/100], Training Loss: 0.2502\nEpoch [75/100], Validation Loss: 0.2706, Validation Accuracy: 90.99%\n\nEpoch [76/100], Training Loss: 0.2496\nEpoch [76/100], Validation Loss: 0.2704, Validation Accuracy: 91.01%\n\nEpoch [77/100], Training Loss: 0.2491\nEpoch [77/100], Validation Loss: 0.2702, Validation Accuracy: 91.02%\n\nEpoch [78/100], Training Loss: 0.2485\nEpoch [78/100], Validation Loss: 0.2699, Validation Accuracy: 91.06%\n\nEpoch [79/100], Training Loss: 0.2480\nEpoch [79/100], Validation Loss: 0.2696, Validation Accuracy: 91.06%\n\nEpoch [80/100], Training Loss: 0.2475\nEpoch [80/100], Validation Loss: 0.2694, Validation Accuracy: 91.07%\n\nEpoch [81/100], Training Loss: 0.2469\nEpoch [81/100], Validation Loss: 0.2692, Validation Accuracy: 91.08%\n\nEpoch [82/100], Training Loss: 0.2464\nEpoch [82/100], Validation Loss: 0.2690, Validation Accuracy: 91.08%\n\nEpoch [83/100], Training Loss: 0.2460\nEpoch [83/100], Validation Loss: 0.2689, Validation Accuracy: 91.09%\n\nEpoch [84/100], Training Loss: 0.2455\nEpoch [84/100], Validation Loss: 0.2687, Validation Accuracy: 91.07%\n\nEpoch [85/100], Training Loss: 0.2450\nEpoch [85/100], Validation Loss: 0.2685, Validation Accuracy: 91.10%\n\nEpoch [86/100], Training Loss: 0.2445\nEpoch [86/100], Validation Loss: 0.2683, Validation Accuracy: 91.10%\n\nEpoch [87/100], Training Loss: 0.2441\nEpoch [87/100], Validation Loss: 0.2681, Validation Accuracy: 91.10%\n\nEpoch [88/100], Training Loss: 0.2436\nEpoch [88/100], Validation Loss: 0.2679, Validation Accuracy: 91.10%\n\nEpoch [89/100], Training Loss: 0.2432\nEpoch [89/100], Validation Loss: 0.2677, Validation Accuracy: 91.17%\n\nEpoch [90/100], Training Loss: 0.2427\nEpoch [90/100], Validation Loss: 0.2676, Validation Accuracy: 91.17%\n\nEpoch [91/100], Training Loss: 0.2423\nEpoch [91/100], Validation Loss: 0.2674, Validation Accuracy: 91.17%\n\nEpoch [92/100], Training Loss: 0.2418\nEpoch [92/100], Validation Loss: 0.2672, Validation Accuracy: 91.18%\n\nEpoch [93/100], Training Loss: 0.2414\nEpoch [93/100], Validation Loss: 0.2671, Validation Accuracy: 91.17%\n\nEpoch [94/100], Training Loss: 0.2410\nEpoch [94/100], Validation Loss: 0.2669, Validation Accuracy: 91.18%\n\nEpoch [95/100], Training Loss: 0.2406\nEpoch [95/100], Validation Loss: 0.2668, Validation Accuracy: 91.23%\n\nEpoch [96/100], Training Loss: 0.2402\nEpoch [96/100], Validation Loss: 0.2666, Validation Accuracy: 91.22%\n\nEpoch [97/100], Training Loss: 0.2398\nEpoch [97/100], Validation Loss: 0.2665, Validation Accuracy: 91.21%\n\nEpoch [98/100], Training Loss: 0.2394\nEpoch [98/100], Validation Loss: 0.2663, Validation Accuracy: 91.24%\n\nEpoch [99/100], Training Loss: 0.2390\nEpoch [99/100], Validation Loss: 0.2661, Validation Accuracy: 91.24%\n\nEpoch [100/100], Training Loss: 0.2386\nEpoch [100/100], Validation Loss: 0.2659, Validation Accuracy: 91.27%\n\n","output_type":"stream"}],"execution_count":342},{"cell_type":"code","source":"for param in model.parameters():\n    param.requires_grad = False\n\nmodel.eval()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-16T06:58:20.834517Z","iopub.execute_input":"2024-11-16T06:58:20.834917Z","iopub.status.idle":"2024-11-16T06:58:20.841961Z","shell.execute_reply.started":"2024-11-16T06:58:20.834878Z","shell.execute_reply":"2024-11-16T06:58:20.841018Z"}},"outputs":[{"execution_count":343,"output_type":"execute_result","data":{"text/plain":"SimpleCNN(\n  (conv1): Conv2d(1, 32, kernel_size=(4, 4), stride=(1, 1), padding=(3, 3))\n  (fc1): Linear(in_features=800, out_features=10, bias=True)\n  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n  (relu): ReLU()\n)"},"metadata":{}}],"execution_count":343},{"cell_type":"code","source":"class SimpleCNN_2(nn.Module):\n    def __init__(self):\n        super(SimpleCNN_2, self).__init__()\n        self.conv1 = nn.Conv2d(1, 32, kernel_size=3, stride=1, padding=2)\n        self.fc1 = nn.Linear(32 * 8 * 8, 10)\n\n        self.pool = nn.MaxPool2d(2, 2)\n        self.relu = nn.ReLU()\n        self.aug_pool = nn.AvgPool2d(kernel_size=2, stride=1, padding=1)\n\n        nn.init.constant_(self.fc1.weight, 0)\n        nn.init.constant_(self.fc1.bias, 0)\n        \n    def forward(self, x):\n        with torch.no_grad():\n            to_add = model(x)\n            \n        # x = x.view(x.size(0), 1, 14, 2, 14, 2).mean(dim=(3, 5))\n        # x = x.view(x.size(0), 1, 14, 2, 14, 2).max(dim=3)[0].max(dim=4)[0]\n        \n        x = self.relu(self.conv1(x))\n        x = self.pool(x)\n\n        x = x.view(-1, 32 * 8 * 8)\n        x = self.fc1(x)\n\n        return x + to_add\n\nbatch_size = 6000\nlearning_rate = 0.001 * 1\nepochs = 100\n\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nmodel_2 = SimpleCNN_2().to(device)\ncriterion = nn.CrossEntropyLoss()\noptimizer = optim.Adam(model_2.parameters(), lr=learning_rate)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-16T07:03:37.507586Z","iopub.execute_input":"2024-11-16T07:03:37.507965Z","iopub.status.idle":"2024-11-16T07:03:37.520538Z","shell.execute_reply.started":"2024-11-16T07:03:37.507927Z","shell.execute_reply":"2024-11-16T07:03:37.519623Z"}},"outputs":[],"execution_count":356},{"cell_type":"code","source":"patience = 10\nbest_val_loss = float('inf')\nno_improvement_epochs = 0\n\nfor epoch in range(epochs):\n    model_2.train()\n    running_loss = 0.0\n    num_batches = 0\n    \n    for data, target in get_batches(train_data, train_targets, batch_size):\n        optimizer.zero_grad()\n        output = model_2(data)\n        loss = criterion(output, target)\n        loss.backward()\n        optimizer.step()\n        running_loss += loss.item()\n        num_batches += 1\n\n    print(f\"Epoch [{epoch + 1}/{epochs}], Training Loss: {running_loss / num_batches:.4f}\")\n\n    model.eval()\n    val_loss = 0.0\n    correct = 0\n    total = 0\n    num_batches = 0\n\n    with torch.no_grad():\n        for data, target in get_batches(test_data, test_targets, batch_size):\n            outputs = model_2(data)\n            loss = criterion(outputs, target)\n            val_loss += loss.item()\n            _, predicted = torch.max(outputs, 1)\n            total += target.size(0)\n            num_batches += 1\n            correct += (predicted == target).sum().item()\n\n    accuracy = 100 * correct / total\n    val_loss /= num_batches\n    print(f\"Epoch [{epoch + 1}/{epochs}], Validation Loss: {val_loss:.4f}, Validation Accuracy: {accuracy:.2f}%\")\n    print()\n\n    if val_loss < best_val_loss:\n        best_val_loss = val_loss\n        no_improvement_epochs = 0\n    else:\n        no_improvement_epochs += 1\n\n    if no_improvement_epochs >= patience:\n        print(f\"Early stopping triggered after {epoch + 1} epochs.\")\n        break","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-16T07:03:47.152989Z","iopub.execute_input":"2024-11-16T07:03:47.153541Z","iopub.status.idle":"2024-11-16T07:04:02.545201Z","shell.execute_reply.started":"2024-11-16T07:03:47.153497Z","shell.execute_reply":"2024-11-16T07:04:02.544268Z"}},"outputs":[{"name":"stdout","text":"Epoch [1/100], Training Loss: 0.2326\nEpoch [1/100], Validation Loss: 0.2577, Validation Accuracy: 91.66%\n\nEpoch [2/100], Training Loss: 0.2236\nEpoch [2/100], Validation Loss: 0.2466, Validation Accuracy: 91.93%\n\nEpoch [3/100], Training Loss: 0.2141\nEpoch [3/100], Validation Loss: 0.2375, Validation Accuracy: 92.18%\n\nEpoch [4/100], Training Loss: 0.2043\nEpoch [4/100], Validation Loss: 0.2263, Validation Accuracy: 92.67%\n\nEpoch [5/100], Training Loss: 0.1946\nEpoch [5/100], Validation Loss: 0.2160, Validation Accuracy: 93.05%\n\nEpoch [6/100], Training Loss: 0.1851\nEpoch [6/100], Validation Loss: 0.2059, Validation Accuracy: 93.50%\n\nEpoch [7/100], Training Loss: 0.1761\nEpoch [7/100], Validation Loss: 0.1964, Validation Accuracy: 93.78%\n\nEpoch [8/100], Training Loss: 0.1678\nEpoch [8/100], Validation Loss: 0.1879, Validation Accuracy: 94.02%\n\nEpoch [9/100], Training Loss: 0.1604\nEpoch [9/100], Validation Loss: 0.1800, Validation Accuracy: 94.25%\n\nEpoch [10/100], Training Loss: 0.1538\nEpoch [10/100], Validation Loss: 0.1731, Validation Accuracy: 94.40%\n\nEpoch [11/100], Training Loss: 0.1480\nEpoch [11/100], Validation Loss: 0.1668, Validation Accuracy: 94.61%\n\nEpoch [12/100], Training Loss: 0.1427\nEpoch [12/100], Validation Loss: 0.1611, Validation Accuracy: 94.77%\n\nEpoch [13/100], Training Loss: 0.1380\nEpoch [13/100], Validation Loss: 0.1560, Validation Accuracy: 94.93%\n\nEpoch [14/100], Training Loss: 0.1336\nEpoch [14/100], Validation Loss: 0.1513, Validation Accuracy: 95.07%\n\nEpoch [15/100], Training Loss: 0.1297\nEpoch [15/100], Validation Loss: 0.1471, Validation Accuracy: 95.18%\n\nEpoch [16/100], Training Loss: 0.1261\nEpoch [16/100], Validation Loss: 0.1432, Validation Accuracy: 95.32%\n\nEpoch [17/100], Training Loss: 0.1227\nEpoch [17/100], Validation Loss: 0.1397, Validation Accuracy: 95.43%\n\nEpoch [18/100], Training Loss: 0.1196\nEpoch [18/100], Validation Loss: 0.1365, Validation Accuracy: 95.46%\n\nEpoch [19/100], Training Loss: 0.1168\nEpoch [19/100], Validation Loss: 0.1335, Validation Accuracy: 95.55%\n\nEpoch [20/100], Training Loss: 0.1141\nEpoch [20/100], Validation Loss: 0.1307, Validation Accuracy: 95.70%\n\nEpoch [21/100], Training Loss: 0.1116\nEpoch [21/100], Validation Loss: 0.1281, Validation Accuracy: 95.78%\n\nEpoch [22/100], Training Loss: 0.1092\nEpoch [22/100], Validation Loss: 0.1256, Validation Accuracy: 95.86%\n\nEpoch [23/100], Training Loss: 0.1069\nEpoch [23/100], Validation Loss: 0.1233, Validation Accuracy: 95.93%\n\nEpoch [24/100], Training Loss: 0.1048\nEpoch [24/100], Validation Loss: 0.1212, Validation Accuracy: 95.98%\n\nEpoch [25/100], Training Loss: 0.1028\nEpoch [25/100], Validation Loss: 0.1191, Validation Accuracy: 96.04%\n\nEpoch [26/100], Training Loss: 0.1009\nEpoch [26/100], Validation Loss: 0.1172, Validation Accuracy: 96.13%\n\nEpoch [27/100], Training Loss: 0.0991\nEpoch [27/100], Validation Loss: 0.1153, Validation Accuracy: 96.17%\n\nEpoch [28/100], Training Loss: 0.0974\nEpoch [28/100], Validation Loss: 0.1136, Validation Accuracy: 96.26%\n\nEpoch [29/100], Training Loss: 0.0957\nEpoch [29/100], Validation Loss: 0.1119, Validation Accuracy: 96.33%\n\nEpoch [30/100], Training Loss: 0.0942\nEpoch [30/100], Validation Loss: 0.1103, Validation Accuracy: 96.43%\n\nEpoch [31/100], Training Loss: 0.0926\nEpoch [31/100], Validation Loss: 0.1088, Validation Accuracy: 96.45%\n\nEpoch [32/100], Training Loss: 0.0912\nEpoch [32/100], Validation Loss: 0.1073, Validation Accuracy: 96.45%\n\nEpoch [33/100], Training Loss: 0.0898\nEpoch [33/100], Validation Loss: 0.1060, Validation Accuracy: 96.48%\n\nEpoch [34/100], Training Loss: 0.0885\nEpoch [34/100], Validation Loss: 0.1047, Validation Accuracy: 96.55%\n\nEpoch [35/100], Training Loss: 0.0872\nEpoch [35/100], Validation Loss: 0.1034, Validation Accuracy: 96.60%\n\nEpoch [36/100], Training Loss: 0.0859\nEpoch [36/100], Validation Loss: 0.1022, Validation Accuracy: 96.64%\n\nEpoch [37/100], Training Loss: 0.0847\nEpoch [37/100], Validation Loss: 0.1011, Validation Accuracy: 96.65%\n\nEpoch [38/100], Training Loss: 0.0836\nEpoch [38/100], Validation Loss: 0.1000, Validation Accuracy: 96.71%\n\nEpoch [39/100], Training Loss: 0.0824\nEpoch [39/100], Validation Loss: 0.0989, Validation Accuracy: 96.72%\n\nEpoch [40/100], Training Loss: 0.0814\nEpoch [40/100], Validation Loss: 0.0979, Validation Accuracy: 96.75%\n\nEpoch [41/100], Training Loss: 0.0803\nEpoch [41/100], Validation Loss: 0.0970, Validation Accuracy: 96.77%\n\nEpoch [42/100], Training Loss: 0.0793\nEpoch [42/100], Validation Loss: 0.0960, Validation Accuracy: 96.82%\n\nEpoch [43/100], Training Loss: 0.0783\nEpoch [43/100], Validation Loss: 0.0951, Validation Accuracy: 96.89%\n\nEpoch [44/100], Training Loss: 0.0773\nEpoch [44/100], Validation Loss: 0.0943, Validation Accuracy: 96.91%\n\nEpoch [45/100], Training Loss: 0.0764\nEpoch [45/100], Validation Loss: 0.0934, Validation Accuracy: 96.93%\n\nEpoch [46/100], Training Loss: 0.0755\nEpoch [46/100], Validation Loss: 0.0926, Validation Accuracy: 96.94%\n\nEpoch [47/100], Training Loss: 0.0746\nEpoch [47/100], Validation Loss: 0.0919, Validation Accuracy: 96.95%\n\nEpoch [48/100], Training Loss: 0.0738\nEpoch [48/100], Validation Loss: 0.0911, Validation Accuracy: 96.97%\n\nEpoch [49/100], Training Loss: 0.0730\nEpoch [49/100], Validation Loss: 0.0904, Validation Accuracy: 96.99%\n\nEpoch [50/100], Training Loss: 0.0722\nEpoch [50/100], Validation Loss: 0.0897, Validation Accuracy: 96.97%\n\nEpoch [51/100], Training Loss: 0.0714\nEpoch [51/100], Validation Loss: 0.0891, Validation Accuracy: 96.99%\n\nEpoch [52/100], Training Loss: 0.0706\nEpoch [52/100], Validation Loss: 0.0884, Validation Accuracy: 97.01%\n\nEpoch [53/100], Training Loss: 0.0699\nEpoch [53/100], Validation Loss: 0.0878, Validation Accuracy: 97.02%\n\nEpoch [54/100], Training Loss: 0.0691\nEpoch [54/100], Validation Loss: 0.0872, Validation Accuracy: 97.02%\n\nEpoch [55/100], Training Loss: 0.0684\nEpoch [55/100], Validation Loss: 0.0866, Validation Accuracy: 97.04%\n\nEpoch [56/100], Training Loss: 0.0678\nEpoch [56/100], Validation Loss: 0.0861, Validation Accuracy: 97.04%\n\nEpoch [57/100], Training Loss: 0.0671\nEpoch [57/100], Validation Loss: 0.0855, Validation Accuracy: 97.05%\n\nEpoch [58/100], Training Loss: 0.0664\nEpoch [58/100], Validation Loss: 0.0850, Validation Accuracy: 97.05%\n\nEpoch [59/100], Training Loss: 0.0658\nEpoch [59/100], Validation Loss: 0.0845, Validation Accuracy: 97.08%\n\nEpoch [60/100], Training Loss: 0.0652\nEpoch [60/100], Validation Loss: 0.0840, Validation Accuracy: 97.08%\n\nEpoch [61/100], Training Loss: 0.0646\nEpoch [61/100], Validation Loss: 0.0835, Validation Accuracy: 97.12%\n\nEpoch [62/100], Training Loss: 0.0640\nEpoch [62/100], Validation Loss: 0.0830, Validation Accuracy: 97.13%\n\nEpoch [63/100], Training Loss: 0.0634\nEpoch [63/100], Validation Loss: 0.0826, Validation Accuracy: 97.16%\n\nEpoch [64/100], Training Loss: 0.0628\nEpoch [64/100], Validation Loss: 0.0822, Validation Accuracy: 97.19%\n\nEpoch [65/100], Training Loss: 0.0623\nEpoch [65/100], Validation Loss: 0.0817, Validation Accuracy: 97.19%\n\nEpoch [66/100], Training Loss: 0.0617\nEpoch [66/100], Validation Loss: 0.0813, Validation Accuracy: 97.21%\n\nEpoch [67/100], Training Loss: 0.0612\nEpoch [67/100], Validation Loss: 0.0809, Validation Accuracy: 97.23%\n\nEpoch [68/100], Training Loss: 0.0607\nEpoch [68/100], Validation Loss: 0.0806, Validation Accuracy: 97.23%\n\nEpoch [69/100], Training Loss: 0.0602\nEpoch [69/100], Validation Loss: 0.0802, Validation Accuracy: 97.25%\n\nEpoch [70/100], Training Loss: 0.0597\nEpoch [70/100], Validation Loss: 0.0798, Validation Accuracy: 97.27%\n\nEpoch [71/100], Training Loss: 0.0592\nEpoch [71/100], Validation Loss: 0.0795, Validation Accuracy: 97.30%\n\nEpoch [72/100], Training Loss: 0.0587\nEpoch [72/100], Validation Loss: 0.0791, Validation Accuracy: 97.31%\n\nEpoch [73/100], Training Loss: 0.0582\nEpoch [73/100], Validation Loss: 0.0788, Validation Accuracy: 97.31%\n\nEpoch [74/100], Training Loss: 0.0578\nEpoch [74/100], Validation Loss: 0.0785, Validation Accuracy: 97.32%\n\nEpoch [75/100], Training Loss: 0.0573\nEpoch [75/100], Validation Loss: 0.0782, Validation Accuracy: 97.35%\n\nEpoch [76/100], Training Loss: 0.0569\nEpoch [76/100], Validation Loss: 0.0779, Validation Accuracy: 97.35%\n\nEpoch [77/100], Training Loss: 0.0565\nEpoch [77/100], Validation Loss: 0.0776, Validation Accuracy: 97.36%\n\nEpoch [78/100], Training Loss: 0.0561\nEpoch [78/100], Validation Loss: 0.0773, Validation Accuracy: 97.37%\n\nEpoch [79/100], Training Loss: 0.0556\nEpoch [79/100], Validation Loss: 0.0770, Validation Accuracy: 97.37%\n\nEpoch [80/100], Training Loss: 0.0552\nEpoch [80/100], Validation Loss: 0.0768, Validation Accuracy: 97.41%\n\nEpoch [81/100], Training Loss: 0.0548\nEpoch [81/100], Validation Loss: 0.0765, Validation Accuracy: 97.41%\n\nEpoch [82/100], Training Loss: 0.0545\nEpoch [82/100], Validation Loss: 0.0763, Validation Accuracy: 97.41%\n\nEpoch [83/100], Training Loss: 0.0541\nEpoch [83/100], Validation Loss: 0.0760, Validation Accuracy: 97.42%\n\nEpoch [84/100], Training Loss: 0.0537\nEpoch [84/100], Validation Loss: 0.0758, Validation Accuracy: 97.44%\n\nEpoch [85/100], Training Loss: 0.0533\nEpoch [85/100], Validation Loss: 0.0756, Validation Accuracy: 97.44%\n\nEpoch [86/100], Training Loss: 0.0529\nEpoch [86/100], Validation Loss: 0.0753, Validation Accuracy: 97.44%\n\nEpoch [87/100], Training Loss: 0.0526\nEpoch [87/100], Validation Loss: 0.0751, Validation Accuracy: 97.44%\n\nEpoch [88/100], Training Loss: 0.0522\nEpoch [88/100], Validation Loss: 0.0749, Validation Accuracy: 97.46%\n\nEpoch [89/100], Training Loss: 0.0519\nEpoch [89/100], Validation Loss: 0.0747, Validation Accuracy: 97.45%\n\nEpoch [90/100], Training Loss: 0.0515\nEpoch [90/100], Validation Loss: 0.0745, Validation Accuracy: 97.44%\n\nEpoch [91/100], Training Loss: 0.0512\nEpoch [91/100], Validation Loss: 0.0743, Validation Accuracy: 97.45%\n\nEpoch [92/100], Training Loss: 0.0509\nEpoch [92/100], Validation Loss: 0.0741, Validation Accuracy: 97.45%\n\nEpoch [93/100], Training Loss: 0.0505\nEpoch [93/100], Validation Loss: 0.0739, Validation Accuracy: 97.45%\n\nEpoch [94/100], Training Loss: 0.0502\nEpoch [94/100], Validation Loss: 0.0738, Validation Accuracy: 97.45%\n\nEpoch [95/100], Training Loss: 0.0499\nEpoch [95/100], Validation Loss: 0.0736, Validation Accuracy: 97.50%\n\nEpoch [96/100], Training Loss: 0.0496\nEpoch [96/100], Validation Loss: 0.0734, Validation Accuracy: 97.49%\n\nEpoch [97/100], Training Loss: 0.0493\nEpoch [97/100], Validation Loss: 0.0732, Validation Accuracy: 97.48%\n\nEpoch [98/100], Training Loss: 0.0490\nEpoch [98/100], Validation Loss: 0.0731, Validation Accuracy: 97.50%\n\nEpoch [99/100], Training Loss: 0.0487\nEpoch [99/100], Validation Loss: 0.0729, Validation Accuracy: 97.52%\n\nEpoch [100/100], Training Loss: 0.0484\nEpoch [100/100], Validation Loss: 0.0728, Validation Accuracy: 97.53%\n\n","output_type":"stream"}],"execution_count":357},{"cell_type":"code","source":"for name, param in model_2.named_parameters():\n    print(name)\n    print(param)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-16T05:11:35.218247Z","iopub.execute_input":"2024-11-16T05:11:35.218609Z","iopub.status.idle":"2024-11-16T05:11:35.262506Z","shell.execute_reply.started":"2024-11-16T05:11:35.218574Z","shell.execute_reply":"2024-11-16T05:11:35.261562Z"}},"outputs":[{"name":"stdout","text":"conv1.weight\nParameter containing:\ntensor([[[[-0.2534,  0.1643,  0.2416],\n          [ 0.2667,  0.3875, -0.0827],\n          [ 0.4322, -0.0697, -0.3061]]],\n\n\n        [[[-0.0567, -0.0488,  0.2234],\n          [-0.0501, -0.4290, -0.2631],\n          [ 0.4309,  0.2795,  0.0157]]],\n\n\n        [[[ 0.0650,  0.1302,  0.0565],\n          [-0.2735, -0.1238,  0.3030],\n          [ 0.0147, -0.4208,  0.2622]]],\n\n\n        [[[ 0.3591, -0.2588,  0.2400],\n          [-0.0104, -0.2382,  0.0424],\n          [ 0.2966, -0.4755, -0.0138]]],\n\n\n        [[[ 0.1702, -0.0504, -0.2481],\n          [-0.3263, -0.2601,  0.3250],\n          [ 0.0112,  0.3293,  0.0563]]],\n\n\n        [[[-0.1517, -0.1362,  0.3226],\n          [ 0.1773,  0.2505, -0.1789],\n          [-0.2960,  0.3635,  0.1897]]],\n\n\n        [[[ 0.0162, -0.2020, -0.7532],\n          [ 0.3767, -0.1904, -0.7365],\n          [ 0.2741, -0.0306, -0.5199]]],\n\n\n        [[[-0.2556,  0.2600, -0.2233],\n          [-0.0364,  0.4169, -0.2390],\n          [ 0.0252, -0.0921, -0.1873]]],\n\n\n        [[[-0.1111, -0.3193, -0.1991],\n          [ 0.0678, -0.0744, -0.0319],\n          [ 0.1802, -0.0903, -0.0233]]],\n\n\n        [[[-0.6811, -0.3219,  0.0642],\n          [ 0.1328,  0.2473,  0.2635],\n          [ 0.0726, -0.1942,  0.0455]]],\n\n\n        [[[-0.3248, -0.0012, -0.0908],\n          [ 0.0875,  0.3966, -0.1301],\n          [-0.0483,  0.0249,  0.4961]]],\n\n\n        [[[-0.2066,  0.4164,  0.5470],\n          [-0.3148,  0.3438, -0.0660],\n          [-0.3335,  0.0612,  0.0424]]],\n\n\n        [[[-0.2841,  0.0250, -0.4067],\n          [-0.8195, -0.5995, -0.0937],\n          [-0.0771,  0.1211,  0.1880]]],\n\n\n        [[[ 0.3070,  0.0024, -0.0835],\n          [-0.4031,  0.4031,  0.3994],\n          [-0.0077, -0.5043, -0.3521]]],\n\n\n        [[[-0.1952,  0.2306,  0.2615],\n          [-0.1491, -0.4067,  0.0926],\n          [-0.3663, -0.2862,  0.3140]]],\n\n\n        [[[-0.1610,  0.4047,  0.2362],\n          [ 0.3223, -0.4245,  0.1143],\n          [ 0.1228,  0.3680,  0.4900]]],\n\n\n        [[[ 0.1232,  0.3107, -0.2776],\n          [-0.2042,  0.4280, -0.2662],\n          [-0.0194,  0.0644, -0.1549]]],\n\n\n        [[[-0.6095, -0.0760, -0.5358],\n          [ 0.0491,  0.4507, -0.1789],\n          [ 0.1555,  0.2459, -0.5226]]],\n\n\n        [[[-0.1353, -0.2724, -0.0733],\n          [-0.4106,  0.1383,  0.3091],\n          [ 0.0881,  0.3308,  0.2524]]],\n\n\n        [[[-0.2042, -0.1958,  0.4010],\n          [ 0.0653,  0.2035,  0.4159],\n          [-0.6998, -0.7280, -0.4195]]],\n\n\n        [[[ 0.1535, -0.4327,  0.3184],\n          [ 0.2522,  0.2798, -0.0861],\n          [-0.3770,  0.0189,  0.1875]]],\n\n\n        [[[-0.0029,  0.3994, -0.0246],\n          [ 0.3552,  0.1516, -0.1227],\n          [-0.1810, -0.2063, -0.0494]]],\n\n\n        [[[-0.2029, -0.0292,  0.2274],\n          [-0.1262, -0.3765,  0.3649],\n          [-0.4122,  0.3894,  0.2225]]],\n\n\n        [[[-0.3551,  0.0772,  0.4053],\n          [-0.2230, -0.4756, -0.3382],\n          [ 0.4878, -0.0598,  0.4304]]],\n\n\n        [[[-0.0523,  0.2796, -0.0860],\n          [-0.2557,  0.1314,  0.0579],\n          [-0.2691,  0.3378, -0.1856]]],\n\n\n        [[[ 0.2455, -0.1611,  0.3357],\n          [-0.4697, -0.3190,  0.2453],\n          [ 0.0873,  0.2057,  0.2384]]],\n\n\n        [[[-0.1056,  0.4904,  0.4609],\n          [ 0.0953, -0.3359,  0.0715],\n          [-0.3800,  0.1048,  0.0574]]],\n\n\n        [[[-0.2723, -0.5516, -0.3195],\n          [-0.2163,  0.1263,  0.4052],\n          [ 0.2080,  0.0393,  0.1479]]],\n\n\n        [[[-0.3031, -0.0912, -0.0986],\n          [ 0.3201,  0.4281,  0.0535],\n          [-0.2571, -0.2251, -0.2436]]],\n\n\n        [[[-0.4521,  0.3847,  0.4295],\n          [-0.5549, -0.2193, -0.0610],\n          [ 0.1551,  0.0031, -0.0342]]],\n\n\n        [[[-0.2691,  0.0749,  0.3267],\n          [ 0.3932,  0.2754,  0.1596],\n          [ 0.1589,  0.4624,  0.4735]]],\n\n\n        [[[-0.0316,  0.3628,  0.0577],\n          [-0.3815, -0.0643,  0.2967],\n          [ 0.1155, -0.3922, -0.0705]]]], device='cuda:0', requires_grad=True)\nconv1.bias\nParameter containing:\ntensor([-0.1791, -0.0143, -0.0874,  0.1708, -0.0521, -0.2709, -0.0032, -0.0007,\n        -0.3152,  0.2685, -0.0005,  0.0971,  0.2668,  0.2470,  0.3558, -0.1816,\n        -0.0094,  0.2676,  0.1655,  0.1934,  0.0894, -0.1667,  0.0245,  0.2128,\n        -0.0229,  0.1464, -0.1549,  0.0004,  0.1598, -0.1523, -0.1142,  0.2368],\n       device='cuda:0', requires_grad=True)\nconv2.weight\nParameter containing:\ntensor([[[[-0.0844,  0.0971,  0.1540],\n          [ 0.0165, -0.0698, -0.1185],\n          [ 0.2329, -0.1635,  0.0943]],\n\n         [[ 0.2992,  0.1881, -0.0164],\n          [ 0.2831,  0.3245, -0.4063],\n          [-0.0957, -0.0261,  0.1164]],\n\n         [[-0.0509, -0.0784, -0.0249],\n          [-0.1330, -0.2307,  0.1058],\n          [-0.0971, -0.2612,  0.0751]],\n\n         ...,\n\n         [[-0.0240, -0.0129, -0.1196],\n          [ 0.1256,  0.1560, -0.1004],\n          [-0.2447, -0.0639, -0.3787]],\n\n         [[ 0.0400,  0.0176,  0.0548],\n          [ 0.0362, -0.0503, -0.0358],\n          [-0.1278, -0.0924,  0.0133]],\n\n         [[-0.0199, -0.1236,  0.0742],\n          [ 0.0232,  0.0386, -0.0683],\n          [-0.1044, -0.1329, -0.0300]]],\n\n\n        [[[-0.0633,  0.0393, -0.0317],\n          [ 0.0008,  0.0197, -0.0577],\n          [ 0.0498,  0.0027, -0.0518]],\n\n         [[-0.0127,  0.0179,  0.0163],\n          [-0.0053, -0.0350, -0.0439],\n          [ 0.0086,  0.0383,  0.0429]],\n\n         [[-0.0048, -0.0023, -0.0217],\n          [ 0.0054,  0.0441,  0.0530],\n          [ 0.0362,  0.0195, -0.0227]],\n\n         ...,\n\n         [[ 0.0488, -0.0356,  0.0531],\n          [-0.0180,  0.0480,  0.0267],\n          [ 0.0436, -0.0110, -0.0531]],\n\n         [[ 0.0382,  0.0166, -0.0159],\n          [-0.0398,  0.0466, -0.0270],\n          [-0.0394, -0.0320, -0.0391]],\n\n         [[-0.0283,  0.0402, -0.0474],\n          [ 0.0299,  0.0079, -0.0620],\n          [-0.0527,  0.0192, -0.0155]]],\n\n\n        [[[-0.1609, -0.0011,  0.0485],\n          [-0.0598,  0.1117, -0.0464],\n          [-0.1260, -0.0544,  0.1179]],\n\n         [[-0.2961, -0.1495, -0.1034],\n          [ 0.1001,  0.1981,  0.1575],\n          [ 0.3716,  0.2248,  0.0557]],\n\n         [[-0.0918,  0.0732, -0.2907],\n          [-0.1232, -0.2533, -0.2723],\n          [-0.0764,  0.0016, -0.0937]],\n\n         ...,\n\n         [[ 0.1506,  0.1586,  0.3057],\n          [ 0.0848, -0.0472, -0.0504],\n          [ 0.3421, -0.0714, -0.0038]],\n\n         [[-0.0061, -0.0314,  0.1033],\n          [ 0.0132,  0.1005, -0.0206],\n          [-0.0499, -0.0654, -0.0239]],\n\n         [[ 0.0569, -0.0238, -0.0101],\n          [ 0.0178, -0.0139, -0.0381],\n          [ 0.1159, -0.0258, -0.0293]]],\n\n\n        ...,\n\n\n        [[[-0.0545, -0.0344, -0.0521],\n          [-0.0639, -0.0275,  0.0127],\n          [ 0.1133, -0.0104, -0.0075]],\n\n         [[ 0.1866,  0.2872,  0.1102],\n          [ 0.0379,  0.1759,  0.2044],\n          [ 0.0417,  0.1457,  0.2130]],\n\n         [[-0.0687, -0.0510, -0.1637],\n          [-0.0756, -0.0662, -0.2770],\n          [-0.0835,  0.0730, -0.0379]],\n\n         ...,\n\n         [[ 0.1323,  0.2235,  0.2849],\n          [-0.1572, -0.0023,  0.2894],\n          [ 0.0799,  0.0122,  0.0874]],\n\n         [[ 0.0353, -0.0695,  0.0286],\n          [-0.0419, -0.0494,  0.0128],\n          [ 0.0191, -0.0143, -0.0635]],\n\n         [[-0.0240,  0.0456,  0.0144],\n          [-0.0610, -0.0134, -0.0464],\n          [-0.0204, -0.0017, -0.0664]]],\n\n\n        [[[-0.0579,  0.1583, -0.0162],\n          [-0.0374, -0.0481,  0.0305],\n          [ 0.1423, -0.1203, -0.2085]],\n\n         [[-0.1613, -0.1840, -0.1229],\n          [-0.1903,  0.1736,  0.3612],\n          [-0.4615, -0.3412,  0.0059]],\n\n         [[ 0.1651, -0.1437,  0.0523],\n          [ 0.2103,  0.0123,  0.0541],\n          [-0.0237,  0.0180, -0.0275]],\n\n         ...,\n\n         [[-0.1716, -0.1734, -0.3023],\n          [-0.1347, -0.0918, -0.0804],\n          [ 0.0264,  0.1103,  0.0242]],\n\n         [[ 0.0833, -0.0531,  0.0427],\n          [ 0.1231,  0.0530,  0.0408],\n          [-0.1490, -0.1078, -0.0799]],\n\n         [[-0.0030, -0.0690, -0.0061],\n          [ 0.1014,  0.0338,  0.0949],\n          [-0.0465,  0.0396,  0.0420]]],\n\n\n        [[[-0.1857, -0.0186,  0.0809],\n          [-0.0210, -0.0668,  0.0897],\n          [ 0.0410, -0.1034, -0.0038]],\n\n         [[-0.2777, -0.0773, -0.0355],\n          [ 0.0161, -0.0738,  0.0330],\n          [-0.2644, -0.4709, -0.0832]],\n\n         [[ 0.2762,  0.2593, -0.0432],\n          [-0.0169,  0.2077, -0.0675],\n          [ 0.0727,  0.1725,  0.0237]],\n\n         ...,\n\n         [[-0.1408, -0.1293,  0.2627],\n          [-0.0160, -0.0575,  0.2518],\n          [-0.0960, -0.1868,  0.1336]],\n\n         [[ 0.0771,  0.0657,  0.0298],\n          [ 0.0165,  0.0357,  0.0769],\n          [-0.0038,  0.0344,  0.0015]],\n\n         [[ 0.1094,  0.1110, -0.0023],\n          [ 0.0120,  0.0477,  0.1049],\n          [ 0.0081,  0.0152,  0.1020]]]], device='cuda:0', requires_grad=True)\nconv2.bias\nParameter containing:\ntensor([-0.0092, -0.0180, -0.0330,  0.0091,  0.0298, -0.0751, -0.0531, -0.0244,\n         0.0406,  0.0571,  0.0096, -0.0613,  0.0506,  0.0125,  0.0317,  0.0227,\n        -0.0226, -0.0098,  0.0478, -0.0744,  0.0036, -0.0080, -0.0626,  0.0372,\n         0.0370, -0.0556, -0.0061, -0.0054, -0.0197, -0.0508, -0.0455,  0.0144,\n        -0.0206,  0.0082, -0.0146,  0.0338, -0.0338, -0.0185, -0.0424,  0.0092,\n        -0.0085, -0.0155,  0.0305,  0.0266,  0.0146,  0.0419,  0.0036, -0.0070,\n         0.0329, -0.0517, -0.0219, -0.0669,  0.0408, -0.0425, -0.0379, -0.0231,\n         0.0308, -0.0081, -0.0539,  0.0303, -0.0507, -0.0281,  0.0332, -0.0157],\n       device='cuda:0', requires_grad=True)\nfc1.weight\nParameter containing:\ntensor([[-0.0163, -0.0320, -0.0249,  ..., -0.0353,  0.0252,  0.0030],\n        [-0.1089,  0.0171, -0.0684,  ..., -0.0025, -0.0242,  0.0511],\n        [-0.0721, -0.0992, -0.1730,  ..., -0.0210, -0.0630,  0.0058],\n        ...,\n        [-0.0908, -0.0054, -0.2168,  ..., -0.0080, -0.0744,  0.0079],\n        [-0.0751, -0.0290, -0.1436,  ...,  0.0448, -0.0187, -0.0009],\n        [ 0.0222,  0.0313, -0.0007,  ..., -0.0322,  0.0087,  0.0266]],\n       device='cuda:0', requires_grad=True)\nfc1.bias\nParameter containing:\ntensor([-0.0181,  0.0040,  0.0109,  0.0082,  0.0211,  0.0211,  0.0155,  0.0023,\n        -0.0042,  0.0191,  0.1602,  0.0140, -0.0026,  0.1060,  0.0150, -0.0322,\n        -0.0578, -0.0465, -0.0125, -0.0062,  0.0172, -0.0085, -0.0257, -0.0209,\n        -0.0080, -0.0029, -0.0330,  0.0299,  0.0039, -0.0213,  0.0300, -0.0124],\n       device='cuda:0', requires_grad=True)\nfc2.weight\nParameter containing:\ntensor([[ 0.0000e+00, -2.3044e-01, -1.1645e-01, -1.3312e-01, -1.9410e-01,\n         -7.4734e-02, -1.6919e-02, -1.4358e-01,  1.0953e-03,  1.7916e-01,\n          2.7216e-01, -1.6185e-01,  8.8651e-02,  9.5808e-02, -5.8169e-02,\n         -9.9634e-02, -2.6786e-02, -1.7600e-01, -1.4197e-01,  0.0000e+00,\n          1.6073e-01, -4.3525e-02, -2.9225e-01, -1.1841e-01,  1.1806e-01,\n         -3.4606e-02, -9.7262e-02, -6.6600e-02, -1.1016e-01, -8.2469e-02,\n          2.0553e-01, -5.1777e-03],\n        [ 0.0000e+00,  1.5093e-01, -1.5756e-02,  1.3588e-01, -3.0931e-02,\n          6.0466e-02,  1.0469e-01,  2.8443e-01,  5.6536e-02,  2.0951e-02,\n          1.7545e-02,  1.1782e-01,  5.1984e-02,  9.4864e-03,  1.1059e-01,\n         -4.1141e-01, -6.1930e-03, -1.4544e-01, -4.9231e-02,  0.0000e+00,\n          1.0581e-01,  1.5436e-01,  2.3295e-02,  4.8591e-02,  2.4592e-02,\n          3.1125e-02,  6.9276e-02,  8.2287e-02,  4.4423e-02,  7.7177e-02,\n          6.5374e-02, -2.9729e-03],\n        [ 0.0000e+00, -8.8644e-02, -3.5172e-02,  6.8793e-02, -4.9155e-02,\n          1.0788e-01, -3.5033e-02,  6.7285e-02,  8.1602e-02, -1.2303e-01,\n          2.0864e-01,  2.1871e-02,  2.1128e-01, -8.2582e-02, -1.9345e-02,\n         -7.7684e-02, -4.0410e-02, -1.1412e-01,  1.5856e-01,  0.0000e+00,\n         -1.7524e-02, -3.0027e-01, -7.5165e-02, -6.2222e-02,  6.2279e-02,\n         -2.3918e-02, -8.5424e-02,  2.5028e-01,  1.2961e-01,  3.5160e-03,\n          1.3937e-02,  3.3581e-03],\n        [ 0.0000e+00,  6.5856e-02,  1.3412e-01,  9.0231e-02,  1.5021e-01,\n          1.1453e-01,  1.1930e-01, -2.0075e-01,  1.1593e-01,  1.8236e-01,\n         -2.4653e-01,  7.7094e-02,  1.1995e-01, -1.7963e-01,  1.3131e-01,\n          1.6478e-02,  1.6158e-02,  1.2828e-01,  1.1405e-01,  0.0000e+00,\n          7.5371e-02,  1.0736e-01,  1.3264e-01,  1.2459e-01,  7.4165e-02,\n          1.2443e-01,  1.5768e-01,  9.9896e-02,  1.1166e-01,  1.2122e-01,\n          1.0445e-01,  4.6474e-03],\n        [ 0.0000e+00, -2.2207e-02, -5.0537e-02, -3.8986e-02, -3.4188e-02,\n         -7.4577e-02, -1.0829e-01,  5.1886e-01, -1.1753e-01, -1.6806e-01,\n          7.2991e-02, -4.6898e-02, -1.3707e-01,  9.7398e-02, -9.7031e-02,\n         -3.2693e-01, -2.3451e-02, -4.5248e-02, -1.0008e-01,  0.0000e+00,\n         -1.4634e-01, -5.3253e-02, -2.3145e-02, -1.2340e-01, -1.3300e-01,\n         -1.2941e-01, -4.4199e-02, -9.5214e-02, -1.1889e-01, -9.1350e-02,\n         -1.3132e-01, -4.0274e-03],\n        [ 0.0000e+00,  4.4072e-02, -4.8431e-02, -6.0593e-02, -5.6215e-02,\n         -1.7516e-01, -1.4346e-01,  1.8529e-01, -2.8733e-02, -7.5894e-02,\n         -3.5102e-01, -5.3939e-02, -1.0410e-01,  9.0142e-02, -6.0575e-02,\n          3.7835e-01,  5.7673e-03,  8.7770e-02, -1.5739e-01,  0.0000e+00,\n         -1.2236e-01,  9.7540e-02, -1.6561e-01, -8.9346e-02, -1.4185e-01,\n         -1.0645e-01, -2.3513e-02, -1.2517e-01, -1.0721e-01, -1.2140e-01,\n         -6.4783e-02,  4.8882e-03],\n        [ 0.0000e+00, -2.0077e-01, -2.9463e-01, -1.9894e-01, -2.5397e-01,\n         -1.4183e-01,  1.7567e-01, -3.8899e-01, -1.4815e-01,  1.9285e-01,\n          1.6693e-01, -2.7343e-01, -1.6198e-01,  1.4333e-01, -5.1752e-02,\n          1.3852e-01, -1.2581e-02, -1.3727e-01, -2.2353e-01,  0.0000e+00,\n         -1.1013e-01, -2.7600e-02, -4.3252e-02,  2.0877e-02, -4.9096e-02,\n         -2.1163e-01, -1.1132e-01, -1.0640e-01, -8.9175e-02, -1.9091e-01,\n         -8.2076e-02, -3.5517e-03],\n        [ 0.0000e+00,  2.7250e-01, -3.9302e-02,  2.1113e-01, -9.9226e-02,\n          3.8206e-02,  1.4850e-01,  2.3147e-01, -1.8041e-01,  6.8609e-02,\n         -8.8264e-02,  1.5767e-01,  6.8632e-02, -1.9445e-01,  1.1890e-01,\n          7.7265e-02,  7.4515e-03, -1.9690e-01,  9.7706e-02,  0.0000e+00,\n          9.9243e-02,  3.1188e-01,  1.6638e-01,  4.5621e-02,  1.5626e-01,\n          3.3682e-02,  2.0448e-02,  1.8358e-01, -4.6858e-04, -2.1204e-02,\n          1.1792e-01, -7.5749e-03],\n        [ 0.0000e+00, -2.7926e-01, -8.1330e-02, -2.2567e-01, -1.6396e-02,\n         -7.8353e-02, -1.7264e-01, -1.7377e-01, -2.2836e-01, -1.8709e-01,\n          2.8157e-01, -1.8254e-01, -1.1344e-01,  5.6047e-02, -1.8375e-01,\n          1.8834e-01,  8.7923e-03, -7.9986e-02, -7.9183e-04,  0.0000e+00,\n         -1.2604e-01, -3.2661e-01, -1.6792e-02, -1.3948e-01, -1.0897e-01,\n         -7.9588e-02, -1.4727e-01, -7.3615e-02, -9.8244e-02, -1.1826e-01,\n         -2.5185e-01, -5.3915e-03],\n        [ 0.0000e+00,  4.0132e-02,  9.5385e-02, -4.8614e-02,  1.1269e-01,\n          1.1745e-02, -3.6732e-02, -5.1462e-01,  1.5448e-01, -1.2366e-01,\n         -4.8914e-02,  5.6348e-02, -1.7413e-01,  1.3657e-01, -2.5442e-02,\n         -1.6534e-01,  1.0072e-02,  1.7190e-01, -3.7397e-02,  0.0000e+00,\n          3.1990e-02, -2.4619e-02,  3.9282e-02,  9.7260e-02, -3.7754e-02,\n          8.7382e-02, -7.2078e-03, -2.5179e-01,  2.3444e-02,  1.1922e-01,\n         -3.1303e-02, -3.8371e-03]], device='cuda:0', requires_grad=True)\nfc2.bias\nParameter containing:\ntensor([ 0.0472,  0.0287, -0.0290, -0.0463,  0.0214, -0.0290,  0.0316, -0.0171,\n         0.0175,  0.0585], device='cuda:0', requires_grad=True)\n","output_type":"stream"}],"execution_count":208},{"cell_type":"code","source":"for param in model_2.parameters():\n    param.requires_grad = False\n\nmodel_2.eval()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-16T05:09:12.890394Z","iopub.execute_input":"2024-11-16T05:09:12.891321Z","iopub.status.idle":"2024-11-16T05:09:12.898360Z","shell.execute_reply.started":"2024-11-16T05:09:12.891260Z","shell.execute_reply":"2024-11-16T05:09:12.897377Z"}},"outputs":[{"execution_count":202,"output_type":"execute_result","data":{"text/plain":"SimpleCNN_2(\n  (conv1): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n  (conv2): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n  (fc1): Linear(in_features=576, out_features=32, bias=True)\n  (fc2): Linear(in_features=32, out_features=10, bias=True)\n  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n  (relu): ReLU()\n)"},"metadata":{}}],"execution_count":202},{"cell_type":"code","source":"class SimpleCNN_3(nn.Module):\n    def __init__(self):\n        super(SimpleCNN_3, self).__init__()\n        self.conv1 = nn.Conv2d(1, 32, kernel_size=3, stride=1, padding=1)\n        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1)\n        self.fc1 = nn.Linear(64 * 7 * 7, 128)\n        self.fc2 = nn.Linear(128, 10)\n        self.pool = nn.MaxPool2d(2, 2)\n        self.relu = nn.ReLU()\n\n        nn.init.constant_(self.fc2.weight, 0)\n        nn.init.constant_(self.fc2.bias, 0)\n\n    def forward(self, x):\n        with torch.no_grad():\n            to_add = model_2(x)\n            \n        x = self.relu(self.conv1(x))\n        x = self.pool(x)\n        x = self.relu(self.conv2(x))\n        x = self.pool(x)\n        x = x.view(-1, 64 * 7 * 7)\n        x = self.relu(self.fc1(x))\n        x = self.fc2(x)\n        return to_add + x","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-16T05:09:23.349876Z","iopub.execute_input":"2024-11-16T05:09:23.350717Z","iopub.status.idle":"2024-11-16T05:09:23.359496Z","shell.execute_reply.started":"2024-11-16T05:09:23.350676Z","shell.execute_reply":"2024-11-16T05:09:23.358563Z"}},"outputs":[],"execution_count":203},{"cell_type":"code","source":"batch_size = 6000\nlearning_rate = 0.001\nepochs = 100\n\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nmodel_3 = SimpleCNN_3().to(device)\ncriterion = nn.CrossEntropyLoss()\noptimizer = optim.Adam(model_3.parameters(), lr=learning_rate)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-16T05:09:27.911833Z","iopub.execute_input":"2024-11-16T05:09:27.912511Z","iopub.status.idle":"2024-11-16T05:09:27.923445Z","shell.execute_reply.started":"2024-11-16T05:09:27.912466Z","shell.execute_reply":"2024-11-16T05:09:27.922599Z"}},"outputs":[],"execution_count":204},{"cell_type":"code","source":"patience = 10\nbest_val_loss = float('inf')\nno_improvement_epochs = 0\n\nfor epoch in range(epochs):\n    model_3.train()\n    running_loss = 0.0\n    num_batches = 0\n    \n    for data, target in get_batches(train_data, train_targets, batch_size):\n        optimizer.zero_grad()\n        output = model_3(data)\n        loss = criterion(output, target)\n        loss.backward()\n        optimizer.step()\n        running_loss += loss.item()\n        num_batches += 1\n\n    print(f\"Epoch [{epoch + 1}/{epochs}], Training Loss: {running_loss / num_batches:.4f}\")\n\n    model_3.eval()\n    val_loss = 0.0\n    correct = 0\n    total = 0\n    num_batches = 0\n\n    with torch.no_grad():\n        for data, target in get_batches(test_data, test_targets, batch_size):\n            outputs = model_3(data)\n            loss = criterion(outputs, target)\n            val_loss += loss.item()\n            _, predicted = torch.max(outputs, 1)\n            total += target.size(0)\n            num_batches += 1\n            correct += (predicted == target).sum().item()\n\n    accuracy = 100 * correct / total\n    val_loss /= num_batches\n    print(f\"Epoch [{epoch + 1}/{epochs}], Validation Loss: {val_loss:.4f}, Validation Accuracy: {accuracy:.2f}%\")\n    print()\n\n    if val_loss < best_val_loss:\n        best_val_loss = val_loss\n        no_improvement_epochs = 0\n    else:\n        no_improvement_epochs += 1\n\n    if no_improvement_epochs >= patience:\n        print(f\"Early stopping triggered after {epoch + 1} epochs.\")\n        break","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-16T05:09:29.838595Z","iopub.execute_input":"2024-11-16T05:09:29.839240Z","iopub.status.idle":"2024-11-16T05:10:18.822895Z","shell.execute_reply.started":"2024-11-16T05:09:29.839198Z","shell.execute_reply":"2024-11-16T05:10:18.821888Z"}},"outputs":[{"name":"stdout","text":"Epoch [1/100], Training Loss: 0.0249\nEpoch [1/100], Validation Loss: 0.0436, Validation Accuracy: 98.40%\n\nEpoch [2/100], Training Loss: 0.0247\nEpoch [2/100], Validation Loss: 0.0435, Validation Accuracy: 98.42%\n\nEpoch [3/100], Training Loss: 0.0246\nEpoch [3/100], Validation Loss: 0.0435, Validation Accuracy: 98.41%\n\nEpoch [4/100], Training Loss: 0.0246\nEpoch [4/100], Validation Loss: 0.0435, Validation Accuracy: 98.41%\n\nEpoch [5/100], Training Loss: 0.0246\nEpoch [5/100], Validation Loss: 0.0435, Validation Accuracy: 98.42%\n\nEpoch [6/100], Training Loss: 0.0245\nEpoch [6/100], Validation Loss: 0.0434, Validation Accuracy: 98.43%\n\nEpoch [7/100], Training Loss: 0.0244\nEpoch [7/100], Validation Loss: 0.0432, Validation Accuracy: 98.43%\n\nEpoch [8/100], Training Loss: 0.0243\nEpoch [8/100], Validation Loss: 0.0430, Validation Accuracy: 98.46%\n\nEpoch [9/100], Training Loss: 0.0240\nEpoch [9/100], Validation Loss: 0.0427, Validation Accuracy: 98.49%\n\nEpoch [10/100], Training Loss: 0.0237\nEpoch [10/100], Validation Loss: 0.0426, Validation Accuracy: 98.46%\n\nEpoch [11/100], Training Loss: 0.0233\nEpoch [11/100], Validation Loss: 0.0426, Validation Accuracy: 98.47%\n\nEpoch [12/100], Training Loss: 0.0229\nEpoch [12/100], Validation Loss: 0.0426, Validation Accuracy: 98.41%\n\nEpoch [13/100], Training Loss: 0.0225\nEpoch [13/100], Validation Loss: 0.0423, Validation Accuracy: 98.44%\n\nEpoch [14/100], Training Loss: 0.0222\nEpoch [14/100], Validation Loss: 0.0413, Validation Accuracy: 98.52%\n\nEpoch [15/100], Training Loss: 0.0218\nEpoch [15/100], Validation Loss: 0.0411, Validation Accuracy: 98.56%\n\nEpoch [16/100], Training Loss: 0.0211\nEpoch [16/100], Validation Loss: 0.0411, Validation Accuracy: 98.49%\n\nEpoch [17/100], Training Loss: 0.0204\nEpoch [17/100], Validation Loss: 0.0410, Validation Accuracy: 98.52%\n\nEpoch [18/100], Training Loss: 0.0197\nEpoch [18/100], Validation Loss: 0.0405, Validation Accuracy: 98.52%\n\nEpoch [19/100], Training Loss: 0.0185\nEpoch [19/100], Validation Loss: 0.0392, Validation Accuracy: 98.61%\n\nEpoch [20/100], Training Loss: 0.0174\nEpoch [20/100], Validation Loss: 0.0390, Validation Accuracy: 98.60%\n\nEpoch [21/100], Training Loss: 0.0164\nEpoch [21/100], Validation Loss: 0.0391, Validation Accuracy: 98.61%\n\nEpoch [22/100], Training Loss: 0.0153\nEpoch [22/100], Validation Loss: 0.0391, Validation Accuracy: 98.62%\n\nEpoch [23/100], Training Loss: 0.0143\nEpoch [23/100], Validation Loss: 0.0391, Validation Accuracy: 98.60%\n\nEpoch [24/100], Training Loss: 0.0133\nEpoch [24/100], Validation Loss: 0.0389, Validation Accuracy: 98.64%\n\nEpoch [25/100], Training Loss: 0.0126\nEpoch [25/100], Validation Loss: 0.0394, Validation Accuracy: 98.63%\n\nEpoch [26/100], Training Loss: 0.0120\nEpoch [26/100], Validation Loss: 0.0420, Validation Accuracy: 98.55%\n\nEpoch [27/100], Training Loss: 0.0114\nEpoch [27/100], Validation Loss: 0.0372, Validation Accuracy: 98.71%\n\nEpoch [28/100], Training Loss: 0.0103\nEpoch [28/100], Validation Loss: 0.0414, Validation Accuracy: 98.60%\n\nEpoch [29/100], Training Loss: 0.0099\nEpoch [29/100], Validation Loss: 0.0412, Validation Accuracy: 98.56%\n\nEpoch [30/100], Training Loss: 0.0098\nEpoch [30/100], Validation Loss: 0.0402, Validation Accuracy: 98.66%\n\nEpoch [31/100], Training Loss: 0.0108\nEpoch [31/100], Validation Loss: 0.0358, Validation Accuracy: 98.73%\n\nEpoch [32/100], Training Loss: 0.0099\nEpoch [32/100], Validation Loss: 0.0362, Validation Accuracy: 98.73%\n\nEpoch [33/100], Training Loss: 0.0084\nEpoch [33/100], Validation Loss: 0.0359, Validation Accuracy: 98.76%\n\nEpoch [34/100], Training Loss: 0.0075\nEpoch [34/100], Validation Loss: 0.0367, Validation Accuracy: 98.74%\n\nEpoch [35/100], Training Loss: 0.0069\nEpoch [35/100], Validation Loss: 0.0356, Validation Accuracy: 98.79%\n\nEpoch [36/100], Training Loss: 0.0063\nEpoch [36/100], Validation Loss: 0.0363, Validation Accuracy: 98.74%\n\nEpoch [37/100], Training Loss: 0.0059\nEpoch [37/100], Validation Loss: 0.0350, Validation Accuracy: 98.78%\n\nEpoch [38/100], Training Loss: 0.0055\nEpoch [38/100], Validation Loss: 0.0358, Validation Accuracy: 98.77%\n\nEpoch [39/100], Training Loss: 0.0050\nEpoch [39/100], Validation Loss: 0.0349, Validation Accuracy: 98.81%\n\nEpoch [40/100], Training Loss: 0.0048\nEpoch [40/100], Validation Loss: 0.0353, Validation Accuracy: 98.77%\n\nEpoch [41/100], Training Loss: 0.0044\nEpoch [41/100], Validation Loss: 0.0350, Validation Accuracy: 98.78%\n\nEpoch [42/100], Training Loss: 0.0042\nEpoch [42/100], Validation Loss: 0.0351, Validation Accuracy: 98.80%\n\nEpoch [43/100], Training Loss: 0.0039\nEpoch [43/100], Validation Loss: 0.0353, Validation Accuracy: 98.78%\n\nEpoch [44/100], Training Loss: 0.0038\nEpoch [44/100], Validation Loss: 0.0355, Validation Accuracy: 98.78%\n\nEpoch [45/100], Training Loss: 0.0036\nEpoch [45/100], Validation Loss: 0.0360, Validation Accuracy: 98.80%\n\nEpoch [46/100], Training Loss: 0.0036\nEpoch [46/100], Validation Loss: 0.0373, Validation Accuracy: 98.78%\n\nEpoch [47/100], Training Loss: 0.0036\nEpoch [47/100], Validation Loss: 0.0392, Validation Accuracy: 98.81%\n\nEpoch [48/100], Training Loss: 0.0037\nEpoch [48/100], Validation Loss: 0.0420, Validation Accuracy: 98.73%\n\nEpoch [49/100], Training Loss: 0.0040\nEpoch [49/100], Validation Loss: 0.0366, Validation Accuracy: 98.83%\n\nEarly stopping triggered after 49 epochs.\n","output_type":"stream"}],"execution_count":205},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}