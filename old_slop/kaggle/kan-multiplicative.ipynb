{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":84894,"databundleVersionId":9709193,"sourceType":"competition"},{"sourceId":408,"sourceType":"datasetVersion","datasetId":180},{"sourceId":668,"sourceType":"datasetVersion","datasetId":308},{"sourceId":7949759,"sourceType":"datasetVersion","datasetId":4675026},{"sourceId":9738619,"sourceType":"datasetVersion","datasetId":5960716}],"dockerImageVersionId":30787,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import sys\nimport time\nimport math\nimport itertools\n\nimport numpy as np\nimport pandas as pd\n\nimport matplotlib.pyplot as plt\nfrom scipy.special import logit\nfrom scipy.stats import norm\n\nimport tensorflow as tf\nfrom keras import layers, models, datasets\n\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport torch.nn.functional as F\nfrom torch.utils.data import TensorDataset, DataLoader\nfrom torch.optim.lr_scheduler import StepLR, LambdaLR\nimport torch.autograd.profiler as profiler\n\nfrom sklearn.model_selection import train_test_split, LeaveOneOut, StratifiedKFold, cross_val_predict\nfrom sklearn.preprocessing import StandardScaler, MinMaxScaler, LabelEncoder, PowerTransformer\nfrom sklearn.metrics import f1_score, log_loss, accuracy_score\nfrom sklearn.linear_model import LogisticRegression\nfrom scipy.stats import norm\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(device)","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-01-30T22:39:30.661992Z","iopub.execute_input":"2025-01-30T22:39:30.662867Z","iopub.status.idle":"2025-01-30T22:39:30.671830Z","shell.execute_reply.started":"2025-01-30T22:39:30.662819Z","shell.execute_reply":"2025-01-30T22:39:30.670944Z"}},"outputs":[{"name":"stdout","text":"cuda\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"import torch\n\nbatch_size, features = 2, 3\nx = torch.arange(batch_size * features, dtype=torch.float).reshape(batch_size, features)\n\nx_i = x.unsqueeze(2).expand(batch_size, features, features)\nx_j = x.unsqueeze(1).expand(batch_size, features, features)\n\npairs_4d = torch.stack([x_i, x_j], dim=-1)\npairs_2d = pairs_4d.view(batch_size, features * features, 2)\n\nprint(pairs_4d.shape)\nprint(\"Input shape:\", x.shape)\nprint(x)\nprint(\"Pairs shape:\", pairs_2d.shape)\nprint(\"Pairs: \", pairs_2d)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-30T22:39:30.844695Z","iopub.execute_input":"2025-01-30T22:39:30.845517Z","iopub.status.idle":"2025-01-30T22:39:30.855260Z","shell.execute_reply.started":"2025-01-30T22:39:30.845473Z","shell.execute_reply":"2025-01-30T22:39:30.854248Z"}},"outputs":[{"name":"stdout","text":"torch.Size([2, 3, 3, 2])\nInput shape: torch.Size([2, 3])\ntensor([[0., 1., 2.],\n        [3., 4., 5.]])\nPairs shape: torch.Size([2, 9, 2])\nPairs:  tensor([[[0., 0.],\n         [0., 1.],\n         [0., 2.],\n         [1., 0.],\n         [1., 1.],\n         [1., 2.],\n         [2., 0.],\n         [2., 1.],\n         [2., 2.]],\n\n        [[3., 3.],\n         [3., 4.],\n         [3., 5.],\n         [4., 3.],\n         [4., 4.],\n         [4., 5.],\n         [5., 3.],\n         [5., 4.],\n         [5., 5.]]])\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"idx = torch.combinations(torch.arange(4), r=2, with_replacement=True)\nprint(idx.shape)\nprint(idx)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-30T22:39:31.753486Z","iopub.execute_input":"2025-01-30T22:39:31.754165Z","iopub.status.idle":"2025-01-30T22:39:31.766635Z","shell.execute_reply.started":"2025-01-30T22:39:31.754133Z","shell.execute_reply":"2025-01-30T22:39:31.765752Z"}},"outputs":[{"name":"stdout","text":"torch.Size([10, 2])\ntensor([[0, 0],\n        [0, 1],\n        [0, 2],\n        [0, 3],\n        [1, 1],\n        [1, 2],\n        [1, 3],\n        [2, 2],\n        [2, 3],\n        [3, 3]])\n","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"import torch\n\nB, N = 2, 4\nx = torch.randn(B, N)\n\nM = N*(N+1)//2\npairs = torch.rand(B, M, 2)\nprint(pairs)\n\npairs_flat = pairs.view(B, -1)\nprint(pairs_flat.shape)\nprint(pairs_flat)\n\npairs_unflat = pairs_flat.view(B, M, 2)\nprint(pairs_unflat)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-30T22:39:31.976530Z","iopub.execute_input":"2025-01-30T22:39:31.976787Z","iopub.status.idle":"2025-01-30T22:39:31.987722Z","shell.execute_reply.started":"2025-01-30T22:39:31.976764Z","shell.execute_reply":"2025-01-30T22:39:31.986957Z"}},"outputs":[{"name":"stdout","text":"tensor([[[0.9495, 0.9842],\n         [0.9676, 0.4880],\n         [0.7591, 0.9547],\n         [0.7442, 0.4111],\n         [0.4154, 0.6210],\n         [0.5671, 0.7571],\n         [0.9082, 0.7438],\n         [0.8985, 0.9118],\n         [0.6309, 0.3738],\n         [0.2816, 0.6959]],\n\n        [[0.6708, 0.9513],\n         [0.4167, 0.2213],\n         [0.7704, 0.3702],\n         [0.5432, 0.5037],\n         [0.9517, 0.9136],\n         [0.0110, 0.1411],\n         [0.6949, 0.5253],\n         [0.6954, 0.3850],\n         [0.9487, 0.8844],\n         [0.5325, 0.6924]]])\ntorch.Size([2, 20])\ntensor([[0.9495, 0.9842, 0.9676, 0.4880, 0.7591, 0.9547, 0.7442, 0.4111, 0.4154,\n         0.6210, 0.5671, 0.7571, 0.9082, 0.7438, 0.8985, 0.9118, 0.6309, 0.3738,\n         0.2816, 0.6959],\n        [0.6708, 0.9513, 0.4167, 0.2213, 0.7704, 0.3702, 0.5432, 0.5037, 0.9517,\n         0.9136, 0.0110, 0.1411, 0.6949, 0.5253, 0.6954, 0.3850, 0.9487, 0.8844,\n         0.5325, 0.6924]])\ntensor([[[0.9495, 0.9842],\n         [0.9676, 0.4880],\n         [0.7591, 0.9547],\n         [0.7442, 0.4111],\n         [0.4154, 0.6210],\n         [0.5671, 0.7571],\n         [0.9082, 0.7438],\n         [0.8985, 0.9118],\n         [0.6309, 0.3738],\n         [0.2816, 0.6959]],\n\n        [[0.6708, 0.9513],\n         [0.4167, 0.2213],\n         [0.7704, 0.3702],\n         [0.5432, 0.5037],\n         [0.9517, 0.9136],\n         [0.0110, 0.1411],\n         [0.6949, 0.5253],\n         [0.6954, 0.3850],\n         [0.9487, 0.8844],\n         [0.5325, 0.6924]]])\n","output_type":"stream"}],"execution_count":6},{"cell_type":"code","source":"def calculate_metrics(model, data_tensor, labels_tensor, batch_size=1024, num_features=22):\n    model.eval()\n    all_preds = []\n    all_labels = []\n\n    with torch.no_grad():\n        for start_idx in range(0, len(data_tensor), batch_size):\n            end_idx = min(start_idx + batch_size, len(data_tensor))\n            inputs = data_tensor[start_idx:end_idx].view(-1, num_features)\n            labels = labels_tensor[start_idx:end_idx]\n\n            outputs = model(inputs)\n            _, preds = torch.max(outputs, 1)\n            all_preds.extend(preds.cpu().numpy())\n            all_labels.extend(labels.cpu().numpy())\n\n    accuracy = accuracy_score(all_labels, all_preds)\n    f1 = f1_score(all_labels, all_preds, average='weighted')\n    return accuracy, f1","metadata":{"execution":{"iopub.status.busy":"2025-01-30T22:39:34.071064Z","iopub.execute_input":"2025-01-30T22:39:34.071950Z","iopub.status.idle":"2025-01-30T22:39:34.077434Z","shell.execute_reply.started":"2025-01-30T22:39:34.071914Z","shell.execute_reply":"2025-01-30T22:39:34.076631Z"},"trusted":true},"outputs":[],"execution_count":7},{"cell_type":"code","source":"class CustomDataLoader:\n    def __init__(self, features, labels, validation_size=0.2, random_state=42, classification=True):        \n        if validation_size > 0.0:\n            stratify = labels if classification else None\n            train_data, val_data, train_labels, val_labels = train_test_split(\n                features, labels, test_size=validation_size, stratify=stratify, random_state=random_state\n            )\n            \n            self.val_data_tensor = torch.tensor(val_data).float().to(device)\n            \n            if classification:\n                self.val_labels_tensor = torch.tensor(val_labels).long().to(device)\n\n            else:\n                self.val_labels_tensor =torch.tensor(val_labels).float().to(device)\n        else:\n            train_data, train_labels = features, labels\n            self.val_data_tensor, self.val_labels_tensor = None, None\n        \n        self.train_data_tensor = torch.tensor(train_data).float().to(device)\n\n        if classification:\n            self.train_labels_tensor = torch.tensor(train_labels).long().to(device)\n        else:\n            self.train_labels_tensor = torch.tensor(train_labels).float().to(device)\n\n        torch.manual_seed(random_state)\n        indices = torch.randperm(len(self.train_data_tensor))\n\n        self.train_data_tensor = self.train_data_tensor[indices]\n        self.train_labels_tensor = self.train_labels_tensor[indices]","metadata":{"execution":{"iopub.status.busy":"2025-01-30T22:39:34.253140Z","iopub.execute_input":"2025-01-30T22:39:34.253958Z","iopub.status.idle":"2025-01-30T22:39:34.260814Z","shell.execute_reply.started":"2025-01-30T22:39:34.253924Z","shell.execute_reply":"2025-01-30T22:39:34.259926Z"},"trusted":true},"outputs":[],"execution_count":8},{"cell_type":"code","source":"def evaluate_model(model, custom_train_loader, criterion, optimizer, num_epochs, scheduler, batch_size=1024, num_features=22, early_stopping_patience=10):\n    best_val_loss = float('inf')\n    best_epoch = 0\n    patience_counter = 0\n    \n    for epoch in range(num_epochs):\n        running_loss = 0.0\n        model.train()\n        i = 0\n        total_loss = 0\n        num_items = 0\n\n        for start_idx in range(0, len(custom_train_loader.train_data_tensor), batch_size):\n            end_idx = min(start_idx + batch_size, len(custom_train_loader.train_data_tensor))\n            inputs = custom_train_loader.train_data_tensor[start_idx:end_idx].view(-1, num_features)\n            labels = custom_train_loader.train_labels_tensor[start_idx:end_idx]\n\n            optimizer.zero_grad()\n            outputs = model(inputs)\n            loss = criterion(outputs, labels, model)\n            loss.backward()\n            optimizer.step()\n            scheduler.step()\n            running_loss += loss.item() * len(labels)\n            total_loss += loss.item() * len(labels)\n            num_items += len(labels)\n\n            i += 1\n\n        if epoch % 10 == 0:\n            model.eval()\n\n            train_reg_loss = 0.0\n            val_loss = 0.0\n            with torch.no_grad():\n                for start_idx in range(0, len(custom_train_loader.train_data_tensor), batch_size):\n                    end_idx = min(start_idx + batch_size, len(custom_train_loader.train_data_tensor))\n                    inputs = custom_train_loader.train_data_tensor[start_idx:end_idx].view(-1, num_features)\n                    labels = custom_train_loader.train_labels_tensor[start_idx:end_idx]\n        \n                    outputs = model(inputs)\n                    train_reg_loss += criterion.regular_loss(outputs, labels).item() * len(labels)\n\n                for start_idx in range(0, len(custom_train_loader.val_data_tensor), batch_size):\n                    end_idx = min(start_idx + batch_size, len(custom_train_loader.val_data_tensor))\n                    val_inputs = custom_train_loader.val_data_tensor[start_idx:end_idx].view(-1, num_features)\n                    val_labels = custom_train_loader.val_labels_tensor[start_idx:end_idx]\n    \n                    val_outputs = model(val_inputs)\n                    val_loss += criterion.regular_loss(val_outputs, val_labels).item() * len(val_labels)\n    \n            avg_train_loss = running_loss / len(custom_train_loader.train_data_tensor)\n            avg_val_loss = val_loss / len(custom_train_loader.val_data_tensor)\n    \n            train_accuracy, train_f1 = calculate_metrics(model, custom_train_loader.train_data_tensor, custom_train_loader.train_labels_tensor, batch_size, num_features)\n            val_accuracy, val_f1 = calculate_metrics(model, custom_train_loader.val_data_tensor, custom_train_loader.val_labels_tensor, batch_size, num_features)\n    \n            print(f'Epoch {epoch + 1}, Training Loss: {avg_train_loss}, Validation Loss: {avg_val_loss}')\n            print(f'Epoch {epoch + 1}, Training Loss: {train_reg_loss / len(custom_train_loader.train_data_tensor)}, Validation Loss: {avg_val_loss}')\n            print(f'Training Accuracy: {train_accuracy}, Training F1 Score: {train_f1}')\n            print(f'Validation Accuracy: {val_accuracy}, Validation F1 Score: {val_f1}')\n            for param_group in optimizer.param_groups:\n                print(\"Learning Rate:\", param_group['lr'])\n                \n            print()\n            \n            if avg_val_loss < best_val_loss:\n                best_val_loss = avg_val_loss\n                best_epoch = epoch + 1\n                patience_counter = 0\n            else:\n                patience_counter += 10\n                if patience_counter >= early_stopping_patience:\n                    print(f'Early stopping triggered after {epoch + 1} epochs.')\n                    print(f'Best Validation Loss: {best_val_loss} from Epoch {best_epoch}')\n                    break\n\n    if patience_counter < early_stopping_patience:\n        print(f'Best Validation Loss after {num_epochs} epochs: {best_val_loss} from Epoch {best_epoch}')","metadata":{"execution":{"iopub.status.busy":"2025-01-30T22:39:35.310941Z","iopub.execute_input":"2025-01-30T22:39:35.311617Z","iopub.status.idle":"2025-01-30T22:39:35.323403Z","shell.execute_reply.started":"2025-01-30T22:39:35.311583Z","shell.execute_reply":"2025-01-30T22:39:35.322543Z"},"trusted":true},"outputs":[],"execution_count":9},{"cell_type":"code","source":"data_dl = pd.read_csv('/kaggle/input/playground-series-s4e10/train.csv')\ndata_og = pd.read_csv('/kaggle/input/loan-approval-prediction/credit_risk_dataset.csv')\n\ndata_dl = data_dl.drop([\"id\"], axis=1)\n\nmedian_emp_length = data_og['person_emp_length'].median()\nmedian_int_rate = data_og['loan_int_rate'].median()\n\ndata_dl['source'] = 0\ndata_og['source'] = 1\n\ndata = pd.concat([data_dl, data_og], ignore_index=True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-30T22:39:37.325587Z","iopub.execute_input":"2025-01-30T22:39:37.325903Z","iopub.status.idle":"2025-01-30T22:39:37.548869Z","shell.execute_reply.started":"2025-01-30T22:39:37.325876Z","shell.execute_reply":"2025-01-30T22:39:37.547922Z"}},"outputs":[],"execution_count":10},{"cell_type":"code","source":"data['person_emp_length_missing'] = data['person_emp_length'].isna().astype(int)\ndata['loan_int_rate_missing'] = data['loan_int_rate'].isna().astype(int)\n\ndata['person_emp_length'] = data['person_emp_length'].fillna(median_emp_length)\ndata['loan_int_rate'] = data['loan_int_rate'].fillna(median_int_rate)\n\n# grade_mapping = {'A': 7, 'B': 6, 'C': 5, 'D': 4, 'E': 3, 'F': 2, 'G': 1}\n# data['loan_grade'] = data['loan_grade'].map(grade_mapping)\n\n# purpose_mapping = {\n#     'DEBTCONSOLIDATION': 1,\n#     'HOMEIMPROVEMENT': 2,\n#     'MEDICAL': 3,\n#     'PERSONAL': 4,\n#     'EDUCATION': 5,\n#     'VENTURE': 6\n# }\n# data['loan_intent'] = data['loan_intent'].map(purpose_mapping)\n\n# home_ownership_mapping = {\n#     'OWN': 1,\n#     'MORTGAGE': 2,\n#     'OTHER': 3,\n#     'RENT': 4\n# }\n# data['person_home_ownership'] = data['person_home_ownership'].map(home_ownership_mapping)\n\nX = data.drop([\"loan_status\"], axis=1)\nX = pd.get_dummies(X, drop_first=True)\ny = data[\"loan_status\"]\n\ncolumn_to_log = [\n    'person_age',\n    'person_income',\n]\n\ncolumn_to_sqrt = [\n    'person_emp_length',\n    'loan_percent_income',\n]\n\nfor col in column_to_log:\n    if (X[col] <= 0).any():\n        print(f\"Column '{col}' contains non-positive values. Adding 1 to avoid log of non-positive numbers.\")\n        X[col] = np.log(X[col] + 1)\n    else:\n        X[col] = np.log(X[col])\n\nfor col in column_to_sqrt:\n    if (X[col] < 0).any():\n        print(f\"Column '{col}' contains negative values. Setting negative values to NaN before applying sqrt.\")\n        X[col] = np.sqrt(X[col].clip(lower=0))\n    else:\n        X[col] = np.sqrt(X[col])\n\nprint(data.isnull().sum())\nprint(X.columns)\nprint(X.shape, y.shape)\nprint(X.columns.get_loc('source'))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-30T22:39:37.560285Z","iopub.execute_input":"2025-01-30T22:39:37.560571Z","iopub.status.idle":"2025-01-30T22:39:37.657834Z","shell.execute_reply.started":"2025-01-30T22:39:37.560546Z","shell.execute_reply":"2025-01-30T22:39:37.656959Z"}},"outputs":[{"name":"stdout","text":"person_age                    0\nperson_income                 0\nperson_home_ownership         0\nperson_emp_length             0\nloan_intent                   0\nloan_grade                    0\nloan_amnt                     0\nloan_int_rate                 0\nloan_percent_income           0\ncb_person_default_on_file     0\ncb_person_cred_hist_length    0\nloan_status                   0\nsource                        0\nperson_emp_length_missing     0\nloan_int_rate_missing         0\ndtype: int64\nIndex(['person_age', 'person_income', 'person_emp_length', 'loan_amnt',\n       'loan_int_rate', 'loan_percent_income', 'cb_person_cred_hist_length',\n       'source', 'person_emp_length_missing', 'loan_int_rate_missing',\n       'person_home_ownership_OTHER', 'person_home_ownership_OWN',\n       'person_home_ownership_RENT', 'loan_intent_EDUCATION',\n       'loan_intent_HOMEIMPROVEMENT', 'loan_intent_MEDICAL',\n       'loan_intent_PERSONAL', 'loan_intent_VENTURE', 'loan_grade_B',\n       'loan_grade_C', 'loan_grade_D', 'loan_grade_E', 'loan_grade_F',\n       'loan_grade_G', 'cb_person_default_on_file_Y'],\n      dtype='object')\n(91226, 25) (91226,)\n7\n","output_type":"stream"}],"execution_count":11},{"cell_type":"code","source":"x_scaler = StandardScaler()\nx_scaled = x_scaler.fit_transform(X)\n\nlabel_encoder = LabelEncoder()\ny_encoded = label_encoder.fit_transform(y)\n\nprint(x_scaled.shape)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-30T22:39:39.356676Z","iopub.execute_input":"2025-01-30T22:39:39.357014Z","iopub.status.idle":"2025-01-30T22:39:39.468398Z","shell.execute_reply.started":"2025-01-30T22:39:39.356984Z","shell.execute_reply":"2025-01-30T22:39:39.467419Z"}},"outputs":[{"name":"stdout","text":"(91226, 25)\n","output_type":"stream"}],"execution_count":12},{"cell_type":"code","source":"x_scaled = x_scaler.transform(X)\n\nlabel_encoder = LabelEncoder()\ny_encoded = label_encoder.fit_transform(y)\n\nprint(x_scaled.shape)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-30T22:39:39.572293Z","iopub.execute_input":"2025-01-30T22:39:39.572585Z","iopub.status.idle":"2025-01-30T22:39:39.653552Z","shell.execute_reply.started":"2025-01-30T22:39:39.572558Z","shell.execute_reply":"2025-01-30T22:39:39.652705Z"}},"outputs":[{"name":"stdout","text":"(91226, 25)\n","output_type":"stream"}],"execution_count":13},{"cell_type":"code","source":"feature_means = x_scaled.mean(axis=0)\nfeature_variances = x_scaled.var(axis=0)\nfeature_mins = x_scaled.min(axis=0)\nfeature_maxs = x_scaled.max(axis=0)\n\nfeature_stats_scaled_full = pd.DataFrame({\n    'Mean': feature_means,\n    'Variance': feature_variances,\n    'Min': feature_mins,\n    'Max': feature_maxs\n})\n\nprint(\"Mean, Variance, Min, and Max of Scaled Features:\")\nprint(feature_stats_scaled_full)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-30T22:39:40.063566Z","iopub.execute_input":"2025-01-30T22:39:40.063890Z","iopub.status.idle":"2025-01-30T22:39:40.087834Z","shell.execute_reply.started":"2025-01-30T22:39:40.063861Z","shell.execute_reply":"2025-01-30T22:39:40.086886Z"}},"outputs":[{"name":"stdout","text":"Mean, Variance, Min, and Max of Scaled Features:\n            Mean  Variance       Min        Max\n0  -3.289997e-16       1.0 -1.552712   8.591255\n1  -1.420680e-16       1.0 -5.315235   9.344027\n2   3.987875e-17       1.0 -1.859550   8.913061\n3  -1.333446e-16       1.0 -1.513249   4.385625\n4   9.327889e-16       1.0 -1.759487   4.065809\n5  -5.358707e-16       1.0 -3.245038   4.413749\n6   6.480297e-17       1.0 -0.943500   5.989958\n7  -1.944089e-16       1.0 -0.745361   1.341632\n8  -4.610980e-17       1.0 -0.099539  10.046317\n9   6.729539e-17       1.0 -0.188056   5.317578\n10  4.673291e-18       1.0 -0.046402  21.550842\n11  7.477266e-17       1.0 -0.258691   3.865621\n12 -6.698384e-17       1.0 -1.031790   0.969189\n13  3.489391e-17       1.0 -0.508188   1.967776\n14 -7.103402e-17       1.0 -0.348605   2.868576\n15  1.217003e-17       1.0 -0.478658   2.089176\n16  3.115527e-19       1.0 -0.453072   2.207155\n17  8.801365e-18       1.0 -0.456460   2.190774\n18  2.141925e-17       1.0 -0.714835   1.398923\n19  1.947205e-18       1.0 -0.487098   2.052974\n20  4.984844e-17       1.0 -0.323861   3.087747\n21 -1.386410e-17       1.0 -0.148680   6.725861\n22  1.869316e-18       1.0 -0.065524  15.261482\n23 -5.452173e-18       1.0 -0.032626  30.650844\n24  2.928596e-17       1.0 -0.433778   2.305326\n","output_type":"stream"}],"execution_count":14},{"cell_type":"code","source":"class CustomLoss(nn.Module):\n    def __init__(self, criterion, f1_lambda, f2_lambda, l1_lambda, l2_lambda, wa_lambda):\n        super(CustomLoss, self).__init__()\n        self.criterion = criterion\n        self.f1_lambda = f1_lambda\n        self.f2_lambda = f2_lambda\n        self.l1_lambda = l1_lambda\n        self.l2_lambda = l2_lambda\n        self.wa_lambda = wa_lambda\n        self.i = 0\n\n    def forward(self, outputs, labels, model): \n        f1_loss = 0.0\n        f2_loss = 0.0\n        l1_loss = 0.0\n        l2_loss = 0.0\n\n        # for name, module in model.named_modules():\n        #     if isinstance(module, CustomActivation):\n        #         f1_loss += (module.a ** 2).sum() + (module.b ** 2).sum()\n        #         f2_loss += ((module.a - module.b) ** 2).sum()\n\n        #     if isinstance(module, nn.Linear):\n        #         l1_loss += torch.norm(module.weight, 1)\n        #         l2_loss += torch.norm(module.weight, 2) ** 2\n\n        total_loss = (self.criterion(outputs, labels)\n                      + self.f1_lambda * f1_loss\n                      + self.f2_lambda * f2_loss\n                      + self.l1_lambda * l1_loss\n                      + self.l2_lambda * l2_loss)\n        self.i += 1\n\n        return total_loss\n\n    def compute_gradient_magnitude(self, model):\n        total_abs_sum = 0.0\n        for param in model.parameters():\n            if param.grad is not None:\n                total_abs_sum += param.grad.abs().sum().item()\n        self.grad_magnitude = total_abs_sum\n\n    def regular_loss(self, outputs, labels):\n        return self.criterion(outputs, labels)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-30T22:39:41.746853Z","iopub.execute_input":"2025-01-30T22:39:41.747504Z","iopub.status.idle":"2025-01-30T22:39:41.754387Z","shell.execute_reply.started":"2025-01-30T22:39:41.747469Z","shell.execute_reply":"2025-01-30T22:39:41.753587Z"}},"outputs":[],"execution_count":15},{"cell_type":"code","source":"# class CustomActivation1d_2(nn.Module):\n#     def __init__(self, num_features, init_identity=True):\n#         super(CustomActivation1d_2, self).__init__()\n#         num_control_points = 2\n        \n#         self.a = nn.Parameter(torch.zeros(num_features, num_control_points))\n#         self.b = nn.Parameter(torch.zeros(num_features, num_control_points))\n\n#         self.local_bias = nn.Parameter(torch.zeros(num_features, num_control_points))\n#         self.global_bias = nn.Parameter(torch.zeros(1, num_features))\n\n#         with torch.no_grad():\n#             random_tensor = torch.randn(num_features) / 2\n#             self.a[:, 0] = random_tensor\n#             self.b[:, 0] = random_tensor\n\n#             random_tensor = torch.randn(num_features) / 2\n#             self.a[:, 1] = random_tensor\n#             self.b[:, 1] = random_tensor\n\n#             self.local_bias[:, 0] = norm.ppf(1/3)\n#             self.local_bias[:, 1] = norm.ppf(2/3)\n\n#         \"\"\"\n#         with torch.no_grad():\n#             if init_identity:\n#         \"\"\"\n        \n#     def forward(self, x):\n#         x = x.unsqueeze(-1) + self.local_bias\n#         x = torch.where(x < 0, self.a * x, self.b * x)\n#         x = x.sum(dim=-1) + self.global_bias            \n#         return x","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-27T08:22:12.533142Z","iopub.execute_input":"2025-01-27T08:22:12.533485Z","iopub.status.idle":"2025-01-27T08:22:12.542111Z","shell.execute_reply.started":"2025-01-27T08:22:12.533447Z","shell.execute_reply":"2025-01-27T08:22:12.541251Z"},"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class CustomActivation1d(nn.Module):\n    def __init__(self, num_features, init_identity=False):\n        super(CustomActivation1d, self).__init__()\n        self.a = nn.Parameter(torch.cat([torch.zeros(25), torch.ones(num_features - 25)]))\n        self.b = nn.Parameter(torch.cat([torch.zeros(25), torch.ones(num_features - 25)]))\n\n        random_tensor = torch.randn(num_features)\n        self.a = nn.Parameter(random_tensor)\n        self.b = nn.Parameter(random_tensor)\n\n        self.a = nn.Parameter(torch.zeros(num_features))\n        self.b = nn.Parameter(torch.zeros(num_features))\n\n        self.local_bias = nn.Parameter(torch.zeros(num_features))\n        self.global_bias = nn.Parameter(torch.tensor(0.0))\n\n    def forward(self, x):\n        x = x + self.local_bias\n        x = torch.where(x < 0, self.a * x, self.b * x)\n        return x.sum(dim=-1) + self.global_bias","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-30T22:39:43.215112Z","iopub.execute_input":"2025-01-30T22:39:43.215940Z","iopub.status.idle":"2025-01-30T22:39:43.222121Z","shell.execute_reply.started":"2025-01-30T22:39:43.215905Z","shell.execute_reply":"2025-01-30T22:39:43.221267Z"}},"outputs":[],"execution_count":16},{"cell_type":"code","source":"class CustomActivation1d_2(nn.Module):\n    def __init__(self, num_features, init_identity=False):\n        super(CustomActivation1d_2, self).__init__()\n        random_tensor = torch.randn(num_features)\n        self.a = nn.Parameter(random_tensor)\n        self.b = nn.Parameter(random_tensor)\n\n        self.a = nn.Parameter(torch.zeros(num_features))\n        self.b = nn.Parameter(torch.zeros(num_features))\n\n        self.a = nn.Parameter(torch.tensor([1 if i % 2 == 0 else 0 for i in range(num_features)], dtype=torch.float32))\n        self.b = nn.Parameter(torch.tensor([1 if i % 2 == 0 else 0 for i in range(num_features)], dtype=torch.float32))\n\n        self.local_bias = nn.Parameter(torch.zeros(num_features))\n        self.global_bias = nn.Parameter(torch.zeros(num_features))\n\n    def forward(self, x):\n        x = x + self.local_bias\n        x = torch.where(x < 0, self.a * x, self.b * x)\n        return x + self.global_bias","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-30T22:39:43.422328Z","iopub.execute_input":"2025-01-30T22:39:43.423072Z","iopub.status.idle":"2025-01-30T22:39:43.429533Z","shell.execute_reply.started":"2025-01-30T22:39:43.423043Z","shell.execute_reply":"2025-01-30T22:39:43.428541Z"}},"outputs":[],"execution_count":17},{"cell_type":"code","source":"import torch\n\n# Example shapes\nbatch_size = 4\nx_features = 3\ny_features = 5\n\n# Random tensors\nx = torch.rand(batch_size, x_features)  # Shape (batch_size, x_features)\ny = torch.rand(batch_size, y_features)  # Shape (batch_size, y_features)\n\n# Compute the outer product for each batch\n# Add an extra dimension to align dimensions for broadcasting\nouter_product = x.unsqueeze(2) * y.unsqueeze(1)  # Shape (batch_size, x_features, y_features)\n\n# Reshape to (batch_size, x_features * y_features)\nresult = outer_product.view(batch_size, -1)\n\n# Resulting shape\nprint(result.shape)  # (batch_size, x_features * y_features)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-30T22:46:28.269644Z","iopub.execute_input":"2025-01-30T22:46:28.270268Z","iopub.status.idle":"2025-01-30T22:46:28.276678Z","shell.execute_reply.started":"2025-01-30T22:46:28.270220Z","shell.execute_reply":"2025-01-30T22:46:28.275841Z"}},"outputs":[{"name":"stdout","text":"torch.Size([4, 15])\n","output_type":"stream"}],"execution_count":33},{"cell_type":"code","source":"class CustomActivation(nn.Module):\n    def __init__(self, num_features):\n        super(CustomActivation, self).__init__()\n        self.a = nn.Parameter(torch.zeros(num_features, num_features))\n        self.b = nn.Parameter(torch.zeros(num_features, num_features))\n        \n        self.local_bias = nn.Parameter(torch.zeros(num_features, num_features))\n        self.global_bias = nn.Parameter(torch.zeros(num_features))\n\n        with torch.no_grad():\n            self.a.fill_diagonal_(1)\n            self.b.fill_diagonal_(1)\n            \n    def forward(self, x):\n        batch_size, num_features = x.shape\n        x = x.unsqueeze(-1).expand(-1, -1, num_features)\n        x = x + self.local_bias\n        x = torch.where(x < 0, self.a * x, self.b * x)\n        return x.sum(dim=1) + self.global_bias","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-27T08:22:12.571221Z","iopub.execute_input":"2025-01-27T08:22:12.57148Z","iopub.status.idle":"2025-01-27T08:22:12.579119Z","shell.execute_reply.started":"2025-01-27T08:22:12.571456Z","shell.execute_reply":"2025-01-27T08:22:12.578184Z"},"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class CustomLinear(nn.Module):\n    def __init__(self, num_features, num_outputs, init_identity=False):\n        super(CustomLinear, self).__init__()\n        \n        if init_identity and num_features != num_outputs:\n            raise ValueError(\"For identity initialization, num_features must equal num_outputs.\")\n\n        self.linear = nn.Linear(num_features, num_outputs, bias=True)\n        \n        with torch.no_grad():\n            self.linear.bias.zero_()\n\n            if init_identity:\n                self.linear.weight.copy_(torch.eye(num_features, num_outputs))\n            else:\n                self.linear.weight.zero_()\n\n    def forward(self, x):\n        return self.linear(x)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-27T08:22:12.580184Z","iopub.execute_input":"2025-01-27T08:22:12.580508Z","iopub.status.idle":"2025-01-27T08:22:12.592478Z","shell.execute_reply.started":"2025-01-27T08:22:12.580472Z","shell.execute_reply":"2025-01-27T08:22:12.591823Z"},"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\"\"\"\nclass TabularDenseNet(nn.Module):\n    def __init__(self, input_size, output_size, num_control_points, num_layers, window_size):\n        super(TabularDenseNet, self).__init__()\n        self.layers = nn.ModuleList()\n        \n        if num_layers % 2 == 1:\n            self.layers.append(CustomLinear(input_size, input_size, init_identity=True))\n            # self.layers.append(CustomActivation(input_size, window_size, num_control_points, init_identity=True))\n            num_layers -= 1\n            input_size *= 2\n            \n        for i in range(num_layers):\n            if i % 2 == 0:\n                self.layers.append(CustomLinear(input_size, input_size, init_identity=True))\n            else:\n                # self.layers.append(CustomActivation(input_size, window_size, num_control_points, init_identity=True))\n                self.layers.append(CustomLinear(input_size, input_size, init_identity=True))\n\n            input_size *= 2\n\n        self.final = CustomLinear(input_size, output_size, init_identity=False)\n        self.final_act = CustomActivation(output_size, window_size, num_control_points, init_identity=True)\n        \n    def forward(self, x):\n        outputs = [x]\n\n        for layer in self.layers:\n            concatenated_outputs = torch.cat(outputs, dim=-1)\n            outputs.append(F.relu(layer(concatenated_outputs)))\n\n        concatenated_outputs = torch.cat(outputs, dim=-1)\n        x = self.final(concatenated_outputs)\n        x = self.final_act(x)\n        return x\n\"\"\"\nprint(\"\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-27T08:22:12.593358Z","iopub.execute_input":"2025-01-27T08:22:12.593628Z","iopub.status.idle":"2025-01-27T08:22:12.603124Z","shell.execute_reply.started":"2025-01-27T08:22:12.593604Z","shell.execute_reply":"2025-01-27T08:22:12.602357Z"},"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class TabularDenseNet(nn.Module):\n    def __init__(self, input_size, output_size, num_control_points, num_layers):\n        super(TabularDenseNet, self).__init__()\n        self.layers = nn.ModuleList()\n        for i in range(num_layers):\n            self.layers.append(CustomActivation(input_size))\n            input_size *= 2\n\n        self.final_layer = CustomActivation1d(input_size)\n\n    def forward(self, x):\n        outputs = [x]\n\n        for layer in self.layers:\n            concatenated_outputs = torch.cat(outputs, dim=-1)\n            outputs.append(layer(concatenated_outputs))\n\n        final_out = self.final_layer(torch.cat(outputs, dim=-1))\n        return torch.stack([final_out, -final_out], dim=-1)\n\n    # def forward(self, x):\n    #     outputs = [x]\n    #     summed_total = torch.zeros(x.size(0), device=x.device)\n    #     summed_total += self.bias\n\n    #     for layer, layer_norm in zip(self.layers, self.layer_norms):\n    #         concatenated_outputs = torch.cat(outputs, dim=-1)\n    #         inter_features = layer_norm(layer(concatenated_outputs))\n    #         summed_feature = inter_features.sum(dim=-1)\n\n    #         outputs.append(inter_features)\n    #         outputs.append(summed_feature.unsqueeze(-1))\n    #         summed_total += summed_feature\n\n    #     print(torch.cat(outputs, dim=-1).shape)\n    #     return torch.stack([summed_total, -summed_total], dim=-1)\n\n    # def forward(self, x):\n    #     batch_size = x.size(0)\n    #     device = x.device\n    #     L = self.num_layers\n    #     D = self.input_dim\n\n    #     total_features = 6656\n    #     concatenated_features = torch.zeros(batch_size, total_features, device=device)\n\n    #     concatenated_features[:, :D] = x\n\n    #     current_pos = D\n    #     summed_total = self.bias.expand(batch_size)\n\n    #     for i, (layer, layer_norm) in enumerate(zip(self.layers, self.layer_norms)):\n    #         current_features = concatenated_features[:, :current_pos]\n    #         inter_features = (layer(current_features))\n    #         summed_feature = inter_features.sum(dim=-1, keepdim=True)\n\n    #         concatenated_features[:, current_pos:current_pos + current_pos] = inter_features\n    #         concatenated_features[:, current_pos + current_pos + 1] = summed_feature.squeeze(-1)\n\n    #         current_pos += current_pos + 1\n    #         summed_total = summed_total + summed_feature.squeeze(-1)\n\n    #     return torch.stack([summed_total, -summed_total], dim=-1)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-27T08:22:12.604041Z","iopub.execute_input":"2025-01-27T08:22:12.604356Z","iopub.status.idle":"2025-01-27T08:22:12.618234Z","shell.execute_reply.started":"2025-01-27T08:22:12.604321Z","shell.execute_reply":"2025-01-27T08:22:12.617448Z"},"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class TabularDenseNet(nn.Module):\n    def __init__(self, input_size, output_size, num_layers):\n        super(TabularDenseNet, self).__init__()\n        self.layers = nn.ModuleList()\n\n        input_size += 625\n        input_size += input_size * 25\n\n        self.final_weight = nn.Parameter(torch.zeros(input_size))\n        self.bias = nn.Parameter(torch.tensor(0.0))\n\n    def forward(self, x):\n        outputs = [x]\n\n        for i in range (2):\n            concatenated_outputs = torch.cat(outputs, dim=-1)\n            outer_product = concatenated_outputs.unsqueeze(2) * x.unsqueeze(1)\n            result = outer_product.view(x.size(0), -1)\n            outputs.append(result)\n\n        final_out = ((torch.cat(outputs, dim=-1) * self.final_weight).sum(dim=-1) + self.bias)\n        return torch.stack([final_out, -final_out], dim=-1)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-30T22:49:05.556823Z","iopub.execute_input":"2025-01-30T22:49:05.557262Z","iopub.status.idle":"2025-01-30T22:49:05.563783Z","shell.execute_reply.started":"2025-01-30T22:49:05.557226Z","shell.execute_reply":"2025-01-30T22:49:05.562845Z"}},"outputs":[],"execution_count":47},{"cell_type":"code","source":"class TabularDenseNet(nn.Module):\n    def __init__(self, input_size, output_size, num_layers):\n        super(TabularDenseNet, self).__init__()\n        self.layers = nn.ModuleList()\n\n        for i in range(num_layers):\n            input_size += ((input_size + 1) * input_size) // 2\n\n        self.final_layer = nn.Linear(input_size, output_size, bias=True)\n        nn.init.zeros_(self.final_layer.weight)\n        nn.init.zeros_(self.final_layer.bias)\n\n    def forward(self, x):\n        outputs = [x]\n\n        for _ in range(1):  # (loop is unnecessary, but keeping it as per your code)\n            concatenated_outputs = torch.cat(outputs, dim=-1)  # [batch, features]\n\n            # Generate pairs manually instead of using torch.combinations\n            batch_size, feature_size = concatenated_outputs.shape\n            idx = torch.triu_indices(feature_size, feature_size, offset=0)  # Upper triangle indices\n            pairs_first = concatenated_outputs[:, idx[0]]  # Select first elements\n            pairs_second = concatenated_outputs[:, idx[1]]  # Select second elements\n            \n            result = pairs_first * pairs_second  # Element-wise multiplication\n            outputs.append(result)\n\n        final_out = self.final_layer(torch.cat(outputs, dim=-1))\n        return final_out","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-30T22:10:32.605772Z","iopub.execute_input":"2025-01-30T22:10:32.606389Z","iopub.status.idle":"2025-01-30T22:10:32.613616Z","shell.execute_reply.started":"2025-01-30T22:10:32.606355Z","shell.execute_reply":"2025-01-30T22:10:32.612818Z"},"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":52},{"cell_type":"code","source":"# import torch\n# import torch.nn as nn\n\n# # 1. Create a random input tensor of shape (batch_size, num_features).\n# batch_size = 2\n# num_features = 4\n# concatenated_outputs = torch.randn(batch_size, num_features)\n# print(\"concatenated_outputs:\", concatenated_outputs.shape)\n\n# # 2. Generate all (i, j) combinations with i <= j.\n# idx = torch.combinations(torch.arange(num_features), r=2, with_replacement=True)\n# # idx has shape (M, 2), where M = num_features * (num_features + 1) // 2\n# print(\"idx shape:\", idx.shape)\n\n# # 3. Gather the pairs. pairs has shape (B, M, 2).\n# pairs = concatenated_outputs[:, idx]  \n# print(\"pairs shape:\", pairs.shape)\n# print(pairs)\n\n# # 4. Flatten from (B, M, 2) -> (B, M*2).\n# pairs_flat = pairs.view(pairs.size(0), -1)  \n# print(\"pairs_flat shape:\", pairs_flat.shape)\n\n# # 5. Reshape (unflatten) back to (B, M, 2).\n# pairs_unflat = pairs_flat.view(pairs.size(0), pairs.size(1), 2)\n# print(\"pairs_unflat shape:\", pairs_unflat.shape)\n# print(pairs_unflat)\n\n# # 6. Sum across the last dimension to get (B, M).\n# summed_output = pairs_unflat.sum(dim=-1)\n# print(\"summed_output shape:\", summed_output.shape)\n# print(\"summed_output:\", summed_output)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-27T08:22:12.632105Z","iopub.execute_input":"2025-01-27T08:22:12.632382Z","iopub.status.idle":"2025-01-27T08:22:12.645719Z","shell.execute_reply.started":"2025-01-27T08:22:12.632355Z","shell.execute_reply":"2025-01-27T08:22:12.644892Z"},"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"custom_train_loader = CustomDataLoader(x_scaled, y_encoded, validation_size=0.2, random_state=0, classification=True)\nprint(custom_train_loader.train_data_tensor.shape)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-30T22:49:34.663519Z","iopub.execute_input":"2025-01-30T22:49:34.663862Z","iopub.status.idle":"2025-01-30T22:49:34.721031Z","shell.execute_reply.started":"2025-01-30T22:49:34.663830Z","shell.execute_reply":"2025-01-30T22:49:34.720097Z"}},"outputs":[{"name":"stdout","text":"torch.Size([72980, 25])\n","output_type":"stream"}],"execution_count":52},{"cell_type":"code","source":"num_features = 25\nnum_classes = 2\nnum_epochs = 10000\nbatch_size = 72980 * 1","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-30T22:49:35.366926Z","iopub.execute_input":"2025-01-30T22:49:35.367749Z","iopub.status.idle":"2025-01-30T22:49:35.371577Z","shell.execute_reply.started":"2025-01-30T22:49:35.367715Z","shell.execute_reply":"2025-01-30T22:49:35.370621Z"}},"outputs":[],"execution_count":53},{"cell_type":"code","source":"torch.cuda.empty_cache()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-30T22:49:36.811480Z","iopub.execute_input":"2025-01-30T22:49:36.811832Z","iopub.status.idle":"2025-01-30T22:49:36.838102Z","shell.execute_reply.started":"2025-01-30T22:49:36.811803Z","shell.execute_reply":"2025-01-30T22:49:36.837047Z"}},"outputs":[],"execution_count":54},{"cell_type":"code","source":"def custom_lr_lambda(step):\n    num_step_threshold = 100\n\n    if step < num_step_threshold:\n        return step / num_step_threshold\n    if step == num_step_threshold:\n        print(\"here\")\n    return 0.99999 ** (step - num_step_threshold)\n\nstart_time = time.time()\nmodel = TabularDenseNet(num_features, num_classes, 1).to(device)\n\noptimizer = optim.Rprop(model.parameters(), lr=0.001 * 0.001 * 0.001 * 0.001 * 0.001 * 0.001 * 1.0)\nscheduler = LambdaLR(optimizer, lr_lambda=custom_lr_lambda)\n\ncriterion = CustomLoss(nn.CrossEntropyLoss(), 0.0, 0.0, 0.0, 0.0, 0.0)\nevaluate_model(model, custom_train_loader, criterion, optimizer, 2000, scheduler, batch_size, num_features, early_stopping_patience=10000)\n\nprint(f\"Execution time: {(time.time() - start_time):.6f} seconds\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-30T22:49:38.331916Z","iopub.execute_input":"2025-01-30T22:49:38.332738Z","iopub.status.idle":"2025-01-30T22:54:46.342271Z","shell.execute_reply.started":"2025-01-30T22:49:38.332703Z","shell.execute_reply":"2025-01-30T22:54:46.341278Z"}},"outputs":[{"name":"stdout","text":"Epoch 1, Training Loss: 0.6931450963020325, Validation Loss: 0.692583441734314\nEpoch 1, Training Loss: 0.6925780177116394, Validation Loss: 0.692583441734314\nTraining Accuracy: 0.80841326390792, Training F1 Score: 0.8229904065941791\nValidation Accuracy: 0.8094376849720487, Validation F1 Score: 0.8239895162241755\nLearning Rate: 1.0000000000000002e-20\n\nEpoch 11, Training Loss: 0.6803534626960754, Validation Loss: 0.6778711080551147\nEpoch 11, Training Loss: 0.6776260137557983, Validation Loss: 0.6778711080551147\nTraining Accuracy: 0.8151137297889833, Training F1 Score: 0.828331048459051\nValidation Accuracy: 0.8153019839964923, Validation F1 Score: 0.8286464457730224\nLearning Rate: 1.1000000000000004e-19\n\nEpoch 21, Training Loss: 0.6211879253387451, Validation Loss: 0.6118537187576294\nEpoch 21, Training Loss: 0.609385073184967, Validation Loss: 0.6118537187576294\nTraining Accuracy: 0.8429432721293505, Training F1 Score: 0.8512029316641893\nValidation Accuracy: 0.8400745368847967, Validation F1 Score: 0.8490737830661481\nLearning Rate: 2.1000000000000004e-19\n\nEpoch 31, Training Loss: 0.4269401431083679, Validation Loss: 0.41223591566085815\nEpoch 31, Training Loss: 0.3998059928417206, Validation Loss: 0.41223591566085815\nTraining Accuracy: 0.8914360098657166, Training F1 Score: 0.8911949211839493\nValidation Accuracy: 0.8875369944097337, Validation F1 Score: 0.8880221509461784\nLearning Rate: 3.100000000000001e-19\n\nEpoch 41, Training Loss: 0.22933433949947357, Validation Loss: 0.2477281242609024\nEpoch 41, Training Loss: 0.2235047072172165, Validation Loss: 0.2477281242609024\nTraining Accuracy: 0.9301178405042477, Training F1 Score: 0.9254824630860312\nValidation Accuracy: 0.9269428915926778, Validation F1 Score: 0.9224745042249525\nLearning Rate: 4.100000000000001e-19\n\nEpoch 51, Training Loss: 0.20072810351848602, Validation Loss: 0.22712858021259308\nEpoch 51, Training Loss: 0.19989389181137085, Validation Loss: 0.22712858021259308\nTraining Accuracy: 0.9368731159221705, Training F1 Score: 0.9334387041627653\nValidation Accuracy: 0.9318206730242244, Validation F1 Score: 0.9283805875394319\nLearning Rate: 5.100000000000002e-19\n\nEpoch 61, Training Loss: 0.19638748466968536, Validation Loss: 0.2254071980714798\nEpoch 61, Training Loss: 0.19611522555351257, Validation Loss: 0.2254071980714798\nTraining Accuracy: 0.9379281995067141, Training F1 Score: 0.9345797991282114\nValidation Accuracy: 0.9323687383536118, Validation F1 Score: 0.9290722992592815\nLearning Rate: 6.100000000000001e-19\n\nEpoch 71, Training Loss: 0.19428108632564545, Validation Loss: 0.2253570556640625\nEpoch 71, Training Loss: 0.1941380798816681, Validation Loss: 0.2253570556640625\nTraining Accuracy: 0.9384351877226638, Training F1 Score: 0.9351881423730514\nValidation Accuracy: 0.9324235448865504, Validation F1 Score: 0.9292274911340034\nLearning Rate: 7.100000000000002e-19\n\nEpoch 81, Training Loss: 0.19295473396778107, Validation Loss: 0.22672036290168762\nEpoch 81, Training Loss: 0.1928524374961853, Validation Loss: 0.22672036290168762\nTraining Accuracy: 0.9387092354069608, Training F1 Score: 0.9354499042373471\nValidation Accuracy: 0.9327523840841828, Validation F1 Score: 0.9295262142898785\nLearning Rate: 8.100000000000003e-19\n\nEpoch 91, Training Loss: 0.1920190453529358, Validation Loss: 0.22885213792324066\nEpoch 91, Training Loss: 0.19194801151752472, Validation Loss: 0.22885213792324066\nTraining Accuracy: 0.9392436283913401, Training F1 Score: 0.9360881043812993\nValidation Accuracy: 0.932916803682999, Validation F1 Score: 0.9297497621375181\nLearning Rate: 9.100000000000004e-19\n\nhere\nEpoch 101, Training Loss: 0.19141091406345367, Validation Loss: 0.23042748868465424\nEpoch 101, Training Loss: 0.19135473668575287, Validation Loss: 0.23042748868465424\nTraining Accuracy: 0.9394628665387778, Training F1 Score: 0.9362739795106159\nValidation Accuracy: 0.933136029814754, Validation F1 Score: 0.9299906601516373\nLearning Rate: 9.999900000000003e-19\n\nEpoch 111, Training Loss: 0.1908881962299347, Validation Loss: 0.2335764467716217\nEpoch 111, Training Loss: 0.19084355235099792, Validation Loss: 0.2335764467716217\nTraining Accuracy: 0.9394354617703481, Training F1 Score: 0.9362345725632221\nValidation Accuracy: 0.9332456428806314, Validation F1 Score: 0.9301054295612249\nLearning Rate: 9.998900054998358e-19\n\nEpoch 121, Training Loss: 0.19043754041194916, Validation Loss: 0.23478476703166962\nEpoch 121, Training Loss: 0.19039444625377655, Validation Loss: 0.23478476703166962\nTraining Accuracy: 0.9395724856124966, Training F1 Score: 0.9363998983209965\nValidation Accuracy: 0.93330044941357, Validation F1 Score: 0.9301684576560938\nLearning Rate: 9.997900209986712e-19\n\nEpoch 131, Training Loss: 0.19008179008960724, Validation Loss: 0.23662185668945312\nEpoch 131, Training Loss: 0.19005650281906128, Validation Loss: 0.23662185668945312\nTraining Accuracy: 0.9394902713072074, Training F1 Score: 0.9363501831811135\nValidation Accuracy: 0.9334648690123862, Validation F1 Score: 0.9303687081357306\nLearning Rate: 9.99690046495507e-19\n\nEpoch 141, Training Loss: 0.189804345369339, Validation Loss: 0.23930442333221436\nEpoch 141, Training Loss: 0.18977738916873932, Validation Loss: 0.23930442333221436\nTraining Accuracy: 0.9398191285283639, Training F1 Score: 0.9366568708368938\nValidation Accuracy: 0.9336292886112024, Validation F1 Score: 0.9305687580946265\nLearning Rate: 9.995900819893431e-19\n\nEpoch 151, Training Loss: 0.18954351544380188, Validation Loss: 0.24239543080329895\nEpoch 151, Training Loss: 0.1895217001438141, Validation Loss: 0.24239543080329895\nTraining Accuracy: 0.9400383666758015, Training F1 Score: 0.9368954578933816\nValidation Accuracy: 0.932916803682999, Validation F1 Score: 0.9298177676432243\nLearning Rate: 9.9949012747918e-19\n\nEpoch 161, Training Loss: 0.1893189400434494, Validation Loss: 0.2444104552268982\nEpoch 161, Training Loss: 0.18929094076156616, Validation Loss: 0.2444104552268982\nTraining Accuracy: 0.9400109619073719, Training F1 Score: 0.9368744439216785\nValidation Accuracy: 0.9329716102159378, Validation F1 Score: 0.9298920376034726\nLearning Rate: 9.993901829640183e-19\n\nEpoch 171, Training Loss: 0.18913820385932922, Validation Loss: 0.24692532420158386\nEpoch 171, Training Loss: 0.1891237199306488, Validation Loss: 0.24692532420158386\nTraining Accuracy: 0.9401205809810907, Training F1 Score: 0.9370002035835282\nValidation Accuracy: 0.9326427710183054, Validation F1 Score: 0.9295707413831846\nLearning Rate: 9.992902484428582e-19\n\nEpoch 181, Training Loss: 0.18896858394145966, Validation Loss: 0.24957220256328583\nEpoch 181, Training Loss: 0.1889510303735733, Validation Loss: 0.24957220256328583\nTraining Accuracy: 0.9399698547547273, Training F1 Score: 0.9368455337457914\nValidation Accuracy: 0.9326427710183054, Validation F1 Score: 0.9295707413831846\nLearning Rate: 9.991903239147006e-19\n\nEpoch 191, Training Loss: 0.18880261480808258, Validation Loss: 0.25237980484962463\nEpoch 191, Training Loss: 0.188792422413826, Validation Loss: 0.25237980484962463\nTraining Accuracy: 0.939997259523157, Training F1 Score: 0.93689779221415\nValidation Accuracy: 0.9326975775512442, Validation F1 Score: 0.9296336995290057\nLearning Rate: 9.990904093785461e-19\n\nEpoch 201, Training Loss: 0.18867868185043335, Validation Loss: 0.2539949119091034\nEpoch 201, Training Loss: 0.18866479396820068, Validation Loss: 0.2539949119091034\nTraining Accuracy: 0.9402164976705947, Training F1 Score: 0.9371127991659761\nValidation Accuracy: 0.9324783514194892, Validation F1 Score: 0.9294384635157603\nLearning Rate: 9.989905048333957e-19\n\nEpoch 211, Training Loss: 0.18854819238185883, Validation Loss: 0.25533193349838257\nEpoch 211, Training Loss: 0.18853847682476044, Validation Loss: 0.25533193349838257\nTraining Accuracy: 0.9402713072074541, Training F1 Score: 0.9371626755530483\nValidation Accuracy: 0.9327523840841828, Validation F1 Score: 0.9296966459890516\nLearning Rate: 9.988906102782501e-19\n\nEpoch 221, Training Loss: 0.18844297528266907, Validation Loss: 0.25687405467033386\nEpoch 221, Training Loss: 0.1884298175573349, Validation Loss: 0.25687405467033386\nTraining Accuracy: 0.940367223896958, Training F1 Score: 0.9372752315739709\nValidation Accuracy: 0.9329716102159378, Validation F1 Score: 0.9299145782844872\nLearning Rate: 9.987907257121107e-19\n\nEpoch 231, Training Loss: 0.1883280724287033, Validation Loss: 0.2582200765609741\nEpoch 231, Training Loss: 0.18831583857536316, Validation Loss: 0.2582200765609741\nTraining Accuracy: 0.9404083310496026, Training F1 Score: 0.9373094184175806\nValidation Accuracy: 0.932916803682999, Validation F1 Score: 0.9298741654676267\nLearning Rate: 9.986908511339784e-19\n\nEpoch 241, Training Loss: 0.18821679055690765, Validation Loss: 0.25959092378616333\nEpoch 241, Training Loss: 0.188205286860466, Validation Loss: 0.25959092378616333\nTraining Accuracy: 0.9402850095916689, Training F1 Score: 0.9371809801531713\nValidation Accuracy: 0.9330264167488764, Validation F1 Score: 0.929988750164575\nLearning Rate: 9.985909865428544e-19\n\nEpoch 251, Training Loss: 0.18812435865402222, Validation Loss: 0.26087188720703125\nEpoch 251, Training Loss: 0.1881173998117447, Validation Loss: 0.26087188720703125\nTraining Accuracy: 0.9402713072074541, Training F1 Score: 0.9371574869378264\nValidation Accuracy: 0.93330044941357, Validation F1 Score: 0.9302584151857898\nLearning Rate: 9.984911319377401e-19\n\nEpoch 261, Training Loss: 0.18804199993610382, Validation Loss: 0.26221829652786255\nEpoch 261, Training Loss: 0.1880318522453308, Validation Loss: 0.26221829652786255\nTraining Accuracy: 0.9403535215127432, Training F1 Score: 0.9372543479555225\nValidation Accuracy: 0.9330812232818152, Validation F1 Score: 0.9300516548889726\nLearning Rate: 9.983912873176372e-19\n\nEpoch 271, Training Loss: 0.18795163929462433, Validation Loss: 0.2632546126842499\nEpoch 271, Training Loss: 0.18794578313827515, Validation Loss: 0.2632546126842499\nTraining Accuracy: 0.9404768429706769, Training F1 Score: 0.9373905344571068\nValidation Accuracy: 0.9330812232818152, Validation F1 Score: 0.9300179436196725\nLearning Rate: 9.982914526815468e-19\n\nEpoch 281, Training Loss: 0.18787959218025208, Validation Loss: 0.2643831968307495\nEpoch 281, Training Loss: 0.18787167966365814, Validation Loss: 0.2643831968307495\nTraining Accuracy: 0.9404768429706769, Training F1 Score: 0.9374008588438726\nValidation Accuracy: 0.9331908363476926, Validation F1 Score: 0.9301550215653249\nLearning Rate: 9.98191628028471e-19\n\nEpoch 291, Training Loss: 0.18779130280017853, Validation Loss: 0.26633861660957336\nEpoch 291, Training Loss: 0.1877799928188324, Validation Loss: 0.26633861660957336\nTraining Accuracy: 0.9403946286653878, Training F1 Score: 0.9373118121369532\nValidation Accuracy: 0.933136029814754, Validation F1 Score: 0.9300921119099242\nLearning Rate: 9.980918133574109e-19\n\nEpoch 301, Training Loss: 0.1877259761095047, Validation Loss: 0.26827114820480347\nEpoch 301, Training Loss: 0.18771928548812866, Validation Loss: 0.26827114820480347\nTraining Accuracy: 0.9405316525075363, Training F1 Score: 0.9374688065426018\nValidation Accuracy: 0.9330264167488764, Validation F1 Score: 0.9299549964590699\nLearning Rate: 9.97992008667369e-19\n\nEpoch 311, Training Loss: 0.18766066431999207, Validation Loss: 0.2699754238128662\nEpoch 311, Training Loss: 0.1876545250415802, Validation Loss: 0.2699754238128662\nTraining Accuracy: 0.9405316525075363, Training F1 Score: 0.9374791032002022\nValidation Accuracy: 0.9330264167488764, Validation F1 Score: 0.929966257582963\nLearning Rate: 9.97892213957347e-19\n\nEpoch 321, Training Loss: 0.18759988248348236, Validation Loss: 0.2700662314891815\nEpoch 321, Training Loss: 0.18759414553642273, Validation Loss: 0.2700662314891815\nTraining Accuracy: 0.9405042477391066, Training F1 Score: 0.9374296798112559\nValidation Accuracy: 0.93330044941357, Validation F1 Score: 0.9302584151857898\nLearning Rate: 9.97792429226347e-19\n\nEpoch 331, Training Loss: 0.18753939867019653, Validation Loss: 0.2702649235725403\nEpoch 331, Training Loss: 0.18753184378147125, Validation Loss: 0.2702649235725403\nTraining Accuracy: 0.9406275691970403, Training F1 Score: 0.93757351938994\nValidation Accuracy: 0.933519675545325, Validation F1 Score: 0.9304652834121112\nLearning Rate: 9.976926544733711e-19\n\nEpoch 341, Training Loss: 0.18748052418231964, Validation Loss: 0.27071431279182434\nEpoch 341, Training Loss: 0.18747678399085999, Validation Loss: 0.27071431279182434\nTraining Accuracy: 0.9406138668128254, Training F1 Score: 0.937578377874502\nValidation Accuracy: 0.9331908363476926, Validation F1 Score: 0.9301438028853555\nLearning Rate: 9.975928896974214e-19\n\nEpoch 351, Training Loss: 0.18743537366390228, Validation Loss: 0.27109524607658386\nEpoch 351, Training Loss: 0.18742994964122772, Validation Loss: 0.27109524607658386\nTraining Accuracy: 0.9406823787338997, Training F1 Score: 0.9376465455991796\nValidation Accuracy: 0.93330044941357, Validation F1 Score: 0.9302359850886558\nLearning Rate: 9.974931348975003e-19\n\nEpoch 361, Training Loss: 0.18738700449466705, Validation Loss: 0.2718743085861206\nEpoch 361, Training Loss: 0.18738184869289398, Validation Loss: 0.2718743085861206\nTraining Accuracy: 0.9406960811181145, Training F1 Score: 0.937667357314423\nValidation Accuracy: 0.933136029814754, Validation F1 Score: 0.9301033348615232\nLearning Rate: 9.973933900726106e-19\n\nEpoch 371, Training Loss: 0.18733422458171844, Validation Loss: 0.27318257093429565\nEpoch 371, Training Loss: 0.1873287707567215, Validation Loss: 0.27318257093429565\nTraining Accuracy: 0.9407234858865443, Training F1 Score: 0.9376987227243169\nValidation Accuracy: 0.933136029814754, Validation F1 Score: 0.9300921119099242\nLearning Rate: 9.972936552217542e-19\n\nEpoch 381, Training Loss: 0.18728932738304138, Validation Loss: 0.2736527919769287\nEpoch 381, Training Loss: 0.18728476762771606, Validation Loss: 0.2736527919769287\nTraining Accuracy: 0.9406823787338997, Training F1 Score: 0.9376542366146222\nValidation Accuracy: 0.933136029814754, Validation F1 Score: 0.9300808790926474\nLearning Rate: 9.971939303439343e-19\n\nEpoch 391, Training Loss: 0.1872435361146927, Validation Loss: 0.2745923697948456\nEpoch 391, Training Loss: 0.1872386932373047, Validation Loss: 0.2745923697948456\nTraining Accuracy: 0.9406412715812551, Training F1 Score: 0.9376148772177797\nValidation Accuracy: 0.933136029814754, Validation F1 Score: 0.9300921119099242\nLearning Rate: 9.970942154381533e-19\n\nEpoch 401, Training Loss: 0.1872057318687439, Validation Loss: 0.27595287561416626\nEpoch 401, Training Loss: 0.18720144033432007, Validation Loss: 0.27595287561416626\nTraining Accuracy: 0.9406275691970403, Training F1 Score: 0.9375914975572524\nValidation Accuracy: 0.9330812232818152, Validation F1 Score: 0.9300066867652002\nLearning Rate: 9.969945105034144e-19\n\nEpoch 411, Training Loss: 0.1871589720249176, Validation Loss: 0.27681463956832886\nEpoch 411, Training Loss: 0.18715541064739227, Validation Loss: 0.27681463956832886\nTraining Accuracy: 0.9407919978076186, Training F1 Score: 0.9377566542024345\nValidation Accuracy: 0.9331908363476926, Validation F1 Score: 0.9301438028853555\nLearning Rate: 9.968948155387201e-19\n\nEpoch 421, Training Loss: 0.18712390959262848, Validation Loss: 0.2773360013961792\nEpoch 421, Training Loss: 0.18712061643600464, Validation Loss: 0.2773360013961792\nTraining Accuracy: 0.9407371882707591, Training F1 Score: 0.9377067221174975\nValidation Accuracy: 0.9330812232818152, Validation F1 Score: 0.9300179436196725\nLearning Rate: 9.967951305430739e-19\n\nEpoch 431, Training Loss: 0.18708989024162292, Validation Loss: 0.2770498991012573\nEpoch 431, Training Loss: 0.187085822224617, Validation Loss: 0.2770498991012573\nTraining Accuracy: 0.9407234858865443, Training F1 Score: 0.9376936000657814\nValidation Accuracy: 0.9330812232818152, Validation F1 Score: 0.9300179436196725\nLearning Rate: 9.966954555154783e-19\n\nEpoch 441, Training Loss: 0.1870536357164383, Validation Loss: 0.2783566415309906\nEpoch 441, Training Loss: 0.1870499551296234, Validation Loss: 0.2783566415309906\nTraining Accuracy: 0.9406823787338997, Training F1 Score: 0.9376439807944035\nValidation Accuracy: 0.933136029814754, Validation F1 Score: 0.9300921119099242\nLearning Rate: 9.965957904549372e-19\n\nEpoch 451, Training Loss: 0.18702340126037598, Validation Loss: 0.2786860466003418\nEpoch 451, Training Loss: 0.1870199739933014, Validation Loss: 0.2786860466003418\nTraining Accuracy: 0.9408057001918334, Training F1 Score: 0.9377800167092408\nValidation Accuracy: 0.933136029814754, Validation F1 Score: 0.9300921119099242\nLearning Rate: 9.964961353604537e-19\n\nEpoch 461, Training Loss: 0.18699078261852264, Validation Loss: 0.27935168147087097\nEpoch 461, Training Loss: 0.18698745965957642, Validation Loss: 0.27935168147087097\nTraining Accuracy: 0.9407645930391888, Training F1 Score: 0.9377252821255575\nValidation Accuracy: 0.9330264167488764, Validation F1 Score: 0.9299549964590699\nLearning Rate: 9.963964902310311e-19\n\nEpoch 471, Training Loss: 0.18695275485515594, Validation Loss: 0.2804264426231384\nEpoch 471, Training Loss: 0.1869492083787918, Validation Loss: 0.2804264426231384\nTraining Accuracy: 0.9406823787338997, Training F1 Score: 0.9376516735091392\nValidation Accuracy: 0.9330812232818152, Validation F1 Score: 0.9300291905849214\nLearning Rate: 9.962968550656731e-19\n\nEpoch 481, Training Loss: 0.18692107498645782, Validation Loss: 0.2806035578250885\nEpoch 481, Training Loss: 0.1869174838066101, Validation Loss: 0.2806035578250885\nTraining Accuracy: 0.9406823787338997, Training F1 Score: 0.9376542366146222\nValidation Accuracy: 0.932916803682999, Validation F1 Score: 0.9298629057194651\nLearning Rate: 9.961972298633832e-19\n\nEpoch 491, Training Loss: 0.18688839673995972, Validation Loss: 0.2809920310974121\nEpoch 491, Training Loss: 0.18688571453094482, Validation Loss: 0.2809920310974121\nTraining Accuracy: 0.9406275691970403, Training F1 Score: 0.9375914975572524\nValidation Accuracy: 0.9330264167488764, Validation F1 Score: 0.929988750164575\nLearning Rate: 9.960976146231654e-19\n\nEpoch 501, Training Loss: 0.18685805797576904, Validation Loss: 0.2814021706581116\nEpoch 501, Training Loss: 0.18685536086559296, Validation Loss: 0.2814021706581116\nTraining Accuracy: 0.9406960811181145, Training F1 Score: 0.9376647945179615\nValidation Accuracy: 0.9329716102159378, Validation F1 Score: 0.9299483151185355\nLearning Rate: 9.959980093440233e-19\n\nEpoch 511, Training Loss: 0.18682563304901123, Validation Loss: 0.28165704011917114\nEpoch 511, Training Loss: 0.18682396411895752, Validation Loss: 0.28165704011917114\nTraining Accuracy: 0.9406823787338997, Training F1 Score: 0.9376491098373748\nValidation Accuracy: 0.932916803682999, Validation F1 Score: 0.9298629057194651\nLearning Rate: 9.958984140249608e-19\n\nEpoch 521, Training Loss: 0.1868017166852951, Validation Loss: 0.28230929374694824\nEpoch 521, Training Loss: 0.18679963052272797, Validation Loss: 0.28230929374694824\nTraining Accuracy: 0.9407371882707591, Training F1 Score: 0.9377092834204267\nValidation Accuracy: 0.9328619971500602, Validation F1 Score: 0.9298225038812378\nLearning Rate: 9.957988286649822e-19\n\nEpoch 531, Training Loss: 0.18677841126918793, Validation Loss: 0.2829291522502899\nEpoch 531, Training Loss: 0.18677647411823273, Validation Loss: 0.2829291522502899\nTraining Accuracy: 0.9408057001918334, Training F1 Score: 0.9377646564793702\nValidation Accuracy: 0.9328071906171216, Validation F1 Score: 0.9297708490132532\nLearning Rate: 9.956992532630914e-19\n\nEpoch 541, Training Loss: 0.18675509095191956, Validation Loss: 0.2832506000995636\nEpoch 541, Training Loss: 0.1867530196905136, Validation Loss: 0.2832506000995636\nTraining Accuracy: 0.940750890654974, Training F1 Score: 0.9377147209351264\nValidation Accuracy: 0.9327523840841828, Validation F1 Score: 0.9297079283773706\nLearning Rate: 9.955996878182926e-19\n\nEpoch 551, Training Loss: 0.18672911822795868, Validation Loss: 0.2833881378173828\nEpoch 551, Training Loss: 0.1867257058620453, Validation Loss: 0.2833881378173828\nTraining Accuracy: 0.9407919978076186, Training F1 Score: 0.9377566542024345\nValidation Accuracy: 0.932533157952428, Validation F1 Score: 0.9294787773696358\nLearning Rate: 9.955001323295903e-19\n\nEpoch 561, Training Loss: 0.18670779466629028, Validation Loss: 0.2838926911354065\nEpoch 561, Training Loss: 0.18670541048049927, Validation Loss: 0.2838926911354065\nTraining Accuracy: 0.9406138668128254, Training F1 Score: 0.93757581095817\nValidation Accuracy: 0.9326975775512442, Validation F1 Score: 0.9296562826984298\nLearning Rate: 9.95400586795989e-19\n\nEpoch 571, Training Loss: 0.1866866797208786, Validation Loss: 0.28473609685897827\nEpoch 571, Training Loss: 0.18668460845947266, Validation Loss: 0.28473609685897827\nTraining Accuracy: 0.9406686763496849, Training F1 Score: 0.9376359889701417\nValidation Accuracy: 0.9326427710183054, Validation F1 Score: 0.9295933528735032\nLearning Rate: 9.953010512164932e-19\n\nEpoch 581, Training Loss: 0.18666516244411469, Validation Loss: 0.28537359833717346\nEpoch 581, Training Loss: 0.1866629719734192, Validation Loss: 0.28537359833717346\nTraining Accuracy: 0.9406686763496849, Training F1 Score: 0.93763855295089\nValidation Accuracy: 0.9325879644853666, Validation F1 Score: 0.9295304113767815\nLearning Rate: 9.952015255901071e-19\n\nEpoch 591, Training Loss: 0.18664582073688507, Validation Loss: 0.28602463006973267\nEpoch 591, Training Loss: 0.1866438388824463, Validation Loss: 0.28602463006973267\nTraining Accuracy: 0.9407234858865443, Training F1 Score: 0.9376987227243169\nValidation Accuracy: 0.9326427710183054, Validation F1 Score: 0.9295820520949829\nLearning Rate: 9.951020099158362e-19\n\nEpoch 601, Training Loss: 0.1866258680820465, Validation Loss: 0.2870338261127472\nEpoch 601, Training Loss: 0.18662337958812714, Validation Loss: 0.2870338261127472\nTraining Accuracy: 0.9406275691970403, Training F1 Score: 0.9376017583194212\nValidation Accuracy: 0.9325879644853666, Validation F1 Score: 0.9295304113767815\nLearning Rate: 9.950025041926846e-19\n\nEpoch 611, Training Loss: 0.18660269677639008, Validation Loss: 0.28787553310394287\nEpoch 611, Training Loss: 0.1866002380847931, Validation Loss: 0.28787553310394287\nTraining Accuracy: 0.9406686763496849, Training F1 Score: 0.9376411163653009\nValidation Accuracy: 0.9328619971500602, Validation F1 Score: 0.9298450022158679\nLearning Rate: 9.949030084196577e-19\n\nEpoch 621, Training Loss: 0.18658193945884705, Validation Loss: 0.288440078496933\nEpoch 621, Training Loss: 0.18657930195331573, Validation Loss: 0.288440078496933\nTraining Accuracy: 0.9407097835023294, Training F1 Score: 0.93768304038611\nValidation Accuracy: 0.932916803682999, Validation F1 Score: 0.9299078854270172\nLearning Rate: 9.948035225957602e-19\n\nEpoch 631, Training Loss: 0.18655937910079956, Validation Loss: 0.28888222575187683\nEpoch 631, Training Loss: 0.1865570992231369, Validation Loss: 0.28888222575187683\nTraining Accuracy: 0.9406686763496849, Training F1 Score: 0.937643679213524\nValidation Accuracy: 0.9328071906171216, Validation F1 Score: 0.9297708490132532\nLearning Rate: 9.947040467199976e-19\n\nEpoch 641, Training Loss: 0.18654043972492218, Validation Loss: 0.2891230881214142\nEpoch 641, Training Loss: 0.18653903901576996, Validation Loss: 0.2891230881214142\nTraining Accuracy: 0.940750890654974, Training F1 Score: 0.937730085200757\nValidation Accuracy: 0.9326427710183054, Validation F1 Score: 0.9295933528735032\nLearning Rate: 9.946045807913749e-19\n\nEpoch 651, Training Loss: 0.18652178347110748, Validation Loss: 0.28938961029052734\nEpoch 651, Training Loss: 0.18651969730854034, Validation Loss: 0.28938961029052734\nTraining Accuracy: 0.9407371882707591, Training F1 Score: 0.9377144043291602\nValidation Accuracy: 0.9326427710183054, Validation F1 Score: 0.9296046437290924\nLearning Rate: 9.945051248088975e-19\n\nEpoch 661, Training Loss: 0.18650242686271667, Validation Loss: 0.2898784875869751\nEpoch 661, Training Loss: 0.18649953603744507, Validation Loss: 0.2898784875869751\nTraining Accuracy: 0.9407645930391888, Training F1 Score: 0.9377534372169398\nValidation Accuracy: 0.932533157952428, Validation F1 Score: 0.9294900865992781\nLearning Rate: 9.944056787715708e-19\n\nEpoch 671, Training Loss: 0.1864834725856781, Validation Loss: 0.29044708609580994\nEpoch 671, Training Loss: 0.18648168444633484, Validation Loss: 0.29044708609580994\nTraining Accuracy: 0.9407919978076186, Training F1 Score: 0.9377822350715699\nValidation Accuracy: 0.9325879644853666, Validation F1 Score: 0.9295304113767815\nLearning Rate: 9.943062426784003e-19\n\nEpoch 681, Training Loss: 0.18646551668643951, Validation Loss: 0.2908540964126587\nEpoch 681, Training Loss: 0.18646344542503357, Validation Loss: 0.2908540964126587\nTraining Accuracy: 0.9408468073444779, Training F1 Score: 0.9378321695511055\nValidation Accuracy: 0.9325879644853666, Validation F1 Score: 0.9295417163836064\nLearning Rate: 9.942068165283917e-19\n\nEpoch 691, Training Loss: 0.18644946813583374, Validation Loss: 0.2911088466644287\nEpoch 691, Training Loss: 0.18644732236862183, Validation Loss: 0.2911088466644287\nTraining Accuracy: 0.940750890654974, Training F1 Score: 0.937740316737439\nValidation Accuracy: 0.9326975775512442, Validation F1 Score: 0.9296675594134364\nLearning Rate: 9.941074003205508e-19\n\nEpoch 701, Training Loss: 0.18643124401569366, Validation Loss: 0.2917321026325226\nEpoch 701, Training Loss: 0.18642941117286682, Validation Loss: 0.2917321026325226\nTraining Accuracy: 0.9407371882707591, Training F1 Score: 0.9377297534891131\nValidation Accuracy: 0.9326427710183054, Validation F1 Score: 0.9296271957127923\nLearning Rate: 9.940079940538831e-19\n\nEpoch 711, Training Loss: 0.1864154040813446, Validation Loss: 0.29183948040008545\nEpoch 711, Training Loss: 0.18641361594200134, Validation Loss: 0.29183948040008545\nTraining Accuracy: 0.9407782954234036, Training F1 Score: 0.9377742240158908\nValidation Accuracy: 0.9328071906171216, Validation F1 Score: 0.9297933558280417\nLearning Rate: 9.939085977273949e-19\n\nEpoch 721, Training Loss: 0.18639840185642242, Validation Loss: 0.2922390103340149\nEpoch 721, Training Loss: 0.18639634549617767, Validation Loss: 0.2922390103340149\nTraining Accuracy: 0.9407919978076186, Training F1 Score: 0.9377873444685453\nValidation Accuracy: 0.9327523840841828, Validation F1 Score: 0.9297417161428773\nLearning Rate: 9.93809211340092e-19\n\nEpoch 731, Training Loss: 0.18637995421886444, Validation Loss: 0.29259708523750305\nEpoch 731, Training Loss: 0.1863788366317749, Validation Loss: 0.29259708523750305\nTraining Accuracy: 0.9407919978076186, Training F1 Score: 0.9377873444685453\nValidation Accuracy: 0.9326427710183054, Validation F1 Score: 0.9296159246720829\nLearning Rate: 9.937098348909806e-19\n\nEpoch 741, Training Loss: 0.1863650530576706, Validation Loss: 0.2926471531391144\nEpoch 741, Training Loss: 0.18636342883110046, Validation Loss: 0.2926471531391144\nTraining Accuracy: 0.9407782954234036, Training F1 Score: 0.9377767781772433\nValidation Accuracy: 0.9326427710183054, Validation F1 Score: 0.9296271957127923\nLearning Rate: 9.93610468379067e-19\n\nEpoch 751, Training Loss: 0.1863483339548111, Validation Loss: 0.29288747906684875\nEpoch 751, Training Loss: 0.18634693324565887, Validation Loss: 0.29288747906684875\nTraining Accuracy: 0.9406960811181145, Training F1 Score: 0.9376955107473629\nValidation Accuracy: 0.9326975775512442, Validation F1 Score: 0.929690083155621\nLearning Rate: 9.935111118033575e-19\n\nEpoch 761, Training Loss: 0.18633227050304413, Validation Loss: 0.29282036423683167\nEpoch 761, Training Loss: 0.18633104860782623, Validation Loss: 0.29282036423683167\nTraining Accuracy: 0.9407645930391888, Training F1 Score: 0.9377662123897622\nValidation Accuracy: 0.9326427710183054, Validation F1 Score: 0.9296271957127923\nLearning Rate: 9.934117651628583e-19\n\nEpoch 771, Training Loss: 0.18631874024868011, Validation Loss: 0.2928999066352844\nEpoch 771, Training Loss: 0.18631713092327118, Validation Loss: 0.2928999066352844\nTraining Accuracy: 0.9407371882707591, Training F1 Score: 0.9377399749691416\nValidation Accuracy: 0.9326427710183054, Validation F1 Score: 0.9296271957127923\nLearning Rate: 9.933124284565761e-19\n\nEpoch 781, Training Loss: 0.18630245327949524, Validation Loss: 0.2932946979999542\nEpoch 781, Training Loss: 0.18630054593086243, Validation Loss: 0.2932946979999542\nTraining Accuracy: 0.9407371882707591, Training F1 Score: 0.9377374204454595\nValidation Accuracy: 0.932533157952428, Validation F1 Score: 0.9295013859001904\nLearning Rate: 9.932131016835178e-19\n\nEpoch 791, Training Loss: 0.18628430366516113, Validation Loss: 0.2934610843658447\nEpoch 791, Training Loss: 0.18628305196762085, Validation Loss: 0.2934610843658447\nTraining Accuracy: 0.9407782954234036, Training F1 Score: 0.9377793317746466\nValidation Accuracy: 0.9324235448865504, Validation F1 Score: 0.9293755294694638\nLearning Rate: 9.931137848426896e-19\n\nEpoch 801, Training Loss: 0.1862720400094986, Validation Loss: 0.2937871217727661\nEpoch 801, Training Loss: 0.18627096712589264, Validation Loss: 0.2937871217727661\nTraining Accuracy: 0.9407919978076186, Training F1 Score: 0.9377975564951817\nValidation Accuracy: 0.932533157952428, Validation F1 Score: 0.9295239547571476\nLearning Rate: 9.930144779330986e-19\n\nEpoch 811, Training Loss: 0.18625608086585999, Validation Loss: 0.2940141558647156\nEpoch 811, Training Loss: 0.1862543523311615, Validation Loss: 0.2940141558647156\nTraining Accuracy: 0.9407782954234036, Training F1 Score: 0.9377767781772433\nValidation Accuracy: 0.9324783514194892, Validation F1 Score: 0.9294497670311153\nLearning Rate: 9.929151809537517e-19\n\nEpoch 821, Training Loss: 0.18624207377433777, Validation Loss: 0.2942794859409332\nEpoch 821, Training Loss: 0.186241015791893, Validation Loss: 0.2942794859409332\nTraining Accuracy: 0.9408057001918334, Training F1 Score: 0.937815778176233\nValidation Accuracy: 0.9323687383536118, Validation F1 Score: 0.9293352374321397\nLearning Rate: 9.92815893903656e-19\n\nEpoch 831, Training Loss: 0.18622639775276184, Validation Loss: 0.2945019006729126\nEpoch 831, Training Loss: 0.18622471392154694, Validation Loss: 0.2945019006729126\nTraining Accuracy: 0.9408605097286928, Training F1 Score: 0.9378580575329802\nValidation Accuracy: 0.9324235448865504, Validation F1 Score: 0.9293755294694638\nLearning Rate: 9.927166167818184e-19\n\nEpoch 841, Training Loss: 0.18621350824832916, Validation Loss: 0.29462578892707825\nEpoch 841, Training Loss: 0.186212420463562, Validation Loss: 0.29462578892707825\nTraining Accuracy: 0.9407919978076186, Training F1 Score: 0.9377898983208954\nValidation Accuracy: 0.9324783514194892, Validation F1 Score: 0.9294497670311153\nLearning Rate: 9.926173495872465e-19\n\nEpoch 851, Training Loss: 0.18620209395885468, Validation Loss: 0.29494157433509827\nEpoch 851, Training Loss: 0.18620078265666962, Validation Loss: 0.29494157433509827\nTraining Accuracy: 0.9408057001918334, Training F1 Score: 0.9378106761618779\nValidation Accuracy: 0.9323687383536118, Validation F1 Score: 0.9293239155669222\nLearning Rate: 9.92518092318947e-19\n\nEpoch 861, Training Loss: 0.1861879527568817, Validation Loss: 0.29524579644203186\nEpoch 861, Training Loss: 0.18618643283843994, Validation Loss: 0.29524579644203186\nTraining Accuracy: 0.9408331049602631, Training F1 Score: 0.9378420176231531\nValidation Accuracy: 0.932313931820673, Validation F1 Score: 0.9292723083564055\nLearning Rate: 9.92418844975928e-19\n\nEpoch 871, Training Loss: 0.18617349863052368, Validation Loss: 0.2954833209514618\nEpoch 871, Training Loss: 0.18617229163646698, Validation Loss: 0.2954833209514618\nTraining Accuracy: 0.940750890654974, Training F1 Score: 0.9377530934546204\nValidation Accuracy: 0.9324783514194892, Validation F1 Score: 0.9294384635157603\nLearning Rate: 9.923196075571966e-19\n\nEpoch 881, Training Loss: 0.1861657351255417, Validation Loss: 0.2955779731273651\nEpoch 881, Training Loss: 0.1861647516489029, Validation Loss: 0.2955779731273651\nTraining Accuracy: 0.9408194025760482, Training F1 Score: 0.937826347258202\nValidation Accuracy: 0.9323687383536118, Validation F1 Score: 0.9293239155669222\nLearning Rate: 9.922203800617604e-19\n\nEpoch 891, Training Loss: 0.18615113198757172, Validation Loss: 0.2959575653076172\nEpoch 891, Training Loss: 0.18614883720874786, Validation Loss: 0.2959575653076172\nTraining Accuracy: 0.9408331049602631, Training F1 Score: 0.9378394675155067\nValidation Accuracy: 0.9323687383536118, Validation F1 Score: 0.9293239155669222\nLearning Rate: 9.921211624886273e-19\n\nEpoch 901, Training Loss: 0.18614239990711212, Validation Loss: 0.29621583223342896\nEpoch 901, Training Loss: 0.18614189326763153, Validation Loss: 0.29621583223342896\nTraining Accuracy: 0.9408194025760482, Training F1 Score: 0.937826347258202\nValidation Accuracy: 0.9324235448865504, Validation F1 Score: 0.929386847128298\nLearning Rate: 9.920219548368051e-19\n\nEpoch 911, Training Loss: 0.1861339807510376, Validation Loss: 0.29639410972595215\nEpoch 911, Training Loss: 0.1861329823732376, Validation Loss: 0.29639410972595215\nTraining Accuracy: 0.9408605097286928, Training F1 Score: 0.9378708077956298\nValidation Accuracy: 0.9324235448865504, Validation F1 Score: 0.929386847128298\nLearning Rate: 9.919227571053017e-19\n\nEpoch 921, Training Loss: 0.1861206293106079, Validation Loss: 0.29677948355674744\nEpoch 921, Training Loss: 0.1861194372177124, Validation Loss: 0.29677948355674744\nTraining Accuracy: 0.940750890654974, Training F1 Score: 0.9377658560739854\nValidation Accuracy: 0.9323687383536118, Validation F1 Score: 0.9293239155669222\nLearning Rate: 9.91823569293125e-19\n\nEpoch 931, Training Loss: 0.18611016869544983, Validation Loss: 0.29689741134643555\nEpoch 931, Training Loss: 0.18610894680023193, Validation Loss: 0.29689741134643555\nTraining Accuracy: 0.9408468073444779, Training F1 Score: 0.9378653315876597\nValidation Accuracy: 0.9323687383536118, Validation F1 Score: 0.9293352374321397\nLearning Rate: 9.917243913992832e-19\n\nEpoch 941, Training Loss: 0.18609727919101715, Validation Loss: 0.2971002459526062\nEpoch 941, Training Loss: 0.18609619140625, Validation Loss: 0.2971002459526062\nTraining Accuracy: 0.9408331049602631, Training F1 Score: 0.9378471161496502\nValidation Accuracy: 0.9323687383536118, Validation F1 Score: 0.9293352374321397\nLearning Rate: 9.916252234227844e-19\n\nEpoch 951, Training Loss: 0.18608640134334564, Validation Loss: 0.2972835898399353\nEpoch 951, Training Loss: 0.18608492612838745, Validation Loss: 0.2972835898399353\nTraining Accuracy: 0.9408057001918334, Training F1 Score: 0.9378234269749346\nValidation Accuracy: 0.932313931820673, Validation F1 Score: 0.9292836344225375\nLearning Rate: 9.915260653626371e-19\n\nEpoch 961, Training Loss: 0.18607179820537567, Validation Loss: 0.2975257933139801\nEpoch 961, Training Loss: 0.18607060611248016, Validation Loss: 0.2975257933139801\nTraining Accuracy: 0.9408468073444779, Training F1 Score: 0.9378755161567892\nValidation Accuracy: 0.9323687383536118, Validation F1 Score: 0.9293352374321397\nLearning Rate: 9.914269172178496e-19\n\nEpoch 971, Training Loss: 0.18606342375278473, Validation Loss: 0.2976589500904083\nEpoch 971, Training Loss: 0.18606260418891907, Validation Loss: 0.2976589500904083\nTraining Accuracy: 0.9408057001918334, Training F1 Score: 0.9378234269749346\nValidation Accuracy: 0.9323687383536118, Validation F1 Score: 0.9293352374321397\nLearning Rate: 9.913277789874304e-19\n\nEpoch 981, Training Loss: 0.18605129420757294, Validation Loss: 0.29787564277648926\nEpoch 981, Training Loss: 0.18604974448680878, Validation Loss: 0.29787564277648926\nTraining Accuracy: 0.9408468073444779, Training F1 Score: 0.9378627840399184\nValidation Accuracy: 0.9324235448865504, Validation F1 Score: 0.9293981548526223\nLearning Rate: 9.912286506703883e-19\n\nEpoch 991, Training Loss: 0.1860409826040268, Validation Loss: 0.29796114563941956\nEpoch 991, Training Loss: 0.18603962659835815, Validation Loss: 0.29796114563941956\nTraining Accuracy: 0.9408194025760482, Training F1 Score: 0.9378365455471629\nValidation Accuracy: 0.9322591252877342, Validation F1 Score: 0.9292093676180843\nLearning Rate: 9.911295322657315e-19\n\nEpoch 1001, Training Loss: 0.18603119254112244, Validation Loss: 0.2979726195335388\nEpoch 1001, Training Loss: 0.18603022396564484, Validation Loss: 0.2979726195335388\nTraining Accuracy: 0.9408057001918334, Training F1 Score: 0.9378208779382137\nValidation Accuracy: 0.9321495122218568, Validation F1 Score: 0.9290948196692461\nLearning Rate: 9.910304237724694e-19\n\nEpoch 1011, Training Loss: 0.18602196872234344, Validation Loss: 0.29833856225013733\nEpoch 1011, Training Loss: 0.18602094054222107, Validation Loss: 0.29833856225013733\nTraining Accuracy: 0.9408057001918334, Training F1 Score: 0.9378234269749346\nValidation Accuracy: 0.9322043187547956, Validation F1 Score: 0.929146415209835\nLearning Rate: 9.909313251896106e-19\n\nEpoch 1021, Training Loss: 0.18601150810718536, Validation Loss: 0.2985856533050537\nEpoch 1021, Training Loss: 0.1860104501247406, Validation Loss: 0.2985856533050537\nTraining Accuracy: 0.9407782954234036, Training F1 Score: 0.9377997402695433\nValidation Accuracy: 0.9322043187547956, Validation F1 Score: 0.9291577695845131\nLearning Rate: 9.908322365161643e-19\n\nEpoch 1031, Training Loss: 0.18600237369537354, Validation Loss: 0.2987748086452484\nEpoch 1031, Training Loss: 0.18600130081176758, Validation Loss: 0.2987748086452484\nTraining Accuracy: 0.9407919978076186, Training F1 Score: 0.937810308852034\nValidation Accuracy: 0.9321495122218568, Validation F1 Score: 0.9290948196692461\nLearning Rate: 9.907331577511392e-19\n\nEpoch 1041, Training Loss: 0.18599198758602142, Validation Loss: 0.2990110218524933\nEpoch 1041, Training Loss: 0.1859910786151886, Validation Loss: 0.2990110218524933\nTraining Accuracy: 0.9407782954234036, Training F1 Score: 0.9377895405277415\nValidation Accuracy: 0.9322043187547956, Validation F1 Score: 0.9291691139924525\nLearning Rate: 9.906340888935448e-19\n\nEpoch 1051, Training Loss: 0.18598289787769318, Validation Loss: 0.29927223920822144\nEpoch 1051, Training Loss: 0.18598198890686035, Validation Loss: 0.29927223920822144\nTraining Accuracy: 0.9408331049602631, Training F1 Score: 0.9378471161496502\nValidation Accuracy: 0.9321495122218568, Validation F1 Score: 0.9291061782341887\nLearning Rate: 9.905350299423904e-19\n\nEpoch 1061, Training Loss: 0.1859743744134903, Validation Loss: 0.2991824746131897\nEpoch 1061, Training Loss: 0.18597356975078583, Validation Loss: 0.2991824746131897\nTraining Accuracy: 0.9407782954234036, Training F1 Score: 0.9377946415244601\nValidation Accuracy: 0.9322043187547956, Validation F1 Score: 0.9291691139924525\nLearning Rate: 9.904359808966854e-19\n\nEpoch 1071, Training Loss: 0.18596482276916504, Validation Loss: 0.2990352213382721\nEpoch 1071, Training Loss: 0.18596398830413818, Validation Loss: 0.2990352213382721\nTraining Accuracy: 0.9407919978076186, Training F1 Score: 0.9378128576346977\nValidation Accuracy: 0.9322043187547956, Validation F1 Score: 0.9291691139924525\nLearning Rate: 9.903369417554393e-19\n\nEpoch 1081, Training Loss: 0.18595653772354126, Validation Loss: 0.29914531111717224\nEpoch 1081, Training Loss: 0.18595586717128754, Validation Loss: 0.29914531111717224\nTraining Accuracy: 0.9407782954234036, Training F1 Score: 0.9377946415244601\nValidation Accuracy: 0.9322591252877342, Validation F1 Score: 0.9292207078331642\nLearning Rate: 9.902379125176616e-19\n\nEpoch 1091, Training Loss: 0.18594761192798615, Validation Loss: 0.2994547486305237\nEpoch 1091, Training Loss: 0.1859465390443802, Validation Loss: 0.2994547486305237\nTraining Accuracy: 0.9407782954234036, Training F1 Score: 0.937804836764179\nValidation Accuracy: 0.9322043187547956, Validation F1 Score: 0.9291691139924525\nLearning Rate: 9.901388931823621e-19\n\nEpoch 1101, Training Loss: 0.18593867123126984, Validation Loss: 0.299603134393692\nEpoch 1101, Training Loss: 0.185937762260437, Validation Loss: 0.299603134393692\nTraining Accuracy: 0.9408057001918334, Training F1 Score: 0.9378259754490073\nValidation Accuracy: 0.9322043187547956, Validation F1 Score: 0.9291577695845131\nLearning Rate: 9.900398837485506e-19\n\nEpoch 1111, Training Loss: 0.18592816591262817, Validation Loss: 0.299753874540329\nEpoch 1111, Training Loss: 0.18592731654644012, Validation Loss: 0.299753874540329\nTraining Accuracy: 0.9407645930391888, Training F1 Score: 0.9377815239915165\nValidation Accuracy: 0.9322591252877342, Validation F1 Score: 0.9292207078331642\nLearning Rate: 9.899408842152367e-19\n\nEpoch 1121, Training Loss: 0.1859191656112671, Validation Loss: 0.29984384775161743\nEpoch 1121, Training Loss: 0.18591830134391785, Validation Loss: 0.29984384775161743\nTraining Accuracy: 0.9408057001918334, Training F1 Score: 0.9378259754490073\nValidation Accuracy: 0.9320947056889182, Validation F1 Score: 0.9290432308126205\nLearning Rate: 9.898418945814308e-19\n\nEpoch 1131, Training Loss: 0.18591006100177765, Validation Loss: 0.3001116216182709\nEpoch 1131, Training Loss: 0.18590928614139557, Validation Loss: 0.3001116216182709\nTraining Accuracy: 0.9407645930391888, Training F1 Score: 0.9377815239915165\nValidation Accuracy: 0.9320947056889182, Validation F1 Score: 0.9290432308126205\nLearning Rate: 9.897429148461429e-19\n\nEpoch 1141, Training Loss: 0.18590180575847626, Validation Loss: 0.30020371079444885\nEpoch 1141, Training Loss: 0.1859009861946106, Validation Loss: 0.30020371079444885\nTraining Accuracy: 0.9407645930391888, Training F1 Score: 0.9377815239915165\nValidation Accuracy: 0.9320947056889182, Validation F1 Score: 0.9290432308126205\nLearning Rate: 9.89643945008383e-19\n\nEpoch 1151, Training Loss: 0.18589399755001068, Validation Loss: 0.30042120814323425\nEpoch 1151, Training Loss: 0.18589328229427338, Validation Loss: 0.30042120814323425\nTraining Accuracy: 0.9407371882707591, Training F1 Score: 0.9377603908682219\nValidation Accuracy: 0.9320947056889182, Validation F1 Score: 0.9290545935623673\nLearning Rate: 9.895449850671618e-19\n\nEpoch 1161, Training Loss: 0.1858852356672287, Validation Loss: 0.30067887902259827\nEpoch 1161, Training Loss: 0.18588438630104065, Validation Loss: 0.30067887902259827\nTraining Accuracy: 0.9407645930391888, Training F1 Score: 0.9377891721903635\nValidation Accuracy: 0.9320398991559794, Validation F1 Score: 0.9289916486352134\nLearning Rate: 9.894460350214891e-19\n\nEpoch 1171, Training Loss: 0.18587446212768555, Validation Loss: 0.30081987380981445\nEpoch 1171, Training Loss: 0.18587486445903778, Validation Loss: 0.30081987380981445\nTraining Accuracy: 0.9408331049602631, Training F1 Score: 0.9378547597196328\nValidation Accuracy: 0.9320947056889182, Validation F1 Score: 0.9290318580800208\nLearning Rate: 9.89347094870376e-19\n\nEpoch 1181, Training Loss: 0.18587037920951843, Validation Loss: 0.3009685277938843\nEpoch 1181, Training Loss: 0.18586990237236023, Validation Loss: 0.3009685277938843\nTraining Accuracy: 0.9407097835023294, Training F1 Score: 0.9377367102476976\nValidation Accuracy: 0.9322043187547956, Validation F1 Score: 0.9291691139924525\nLearning Rate: 9.892481646128325e-19\n\nEpoch 1191, Training Loss: 0.18586257100105286, Validation Loss: 0.30113616585731506\nEpoch 1191, Training Loss: 0.18586181104183197, Validation Loss: 0.30113616585731506\nTraining Accuracy: 0.9407234858865443, Training F1 Score: 0.9377523742589834\nValidation Accuracy: 0.9320398991559794, Validation F1 Score: 0.9289916486352134\nLearning Rate: 9.891492442478697e-19\n\nEpoch 1201, Training Loss: 0.18585559725761414, Validation Loss: 0.3013417422771454\nEpoch 1201, Training Loss: 0.1858547478914261, Validation Loss: 0.3013417422771454\nTraining Accuracy: 0.9407097835023294, Training F1 Score: 0.9377367102476976\nValidation Accuracy: 0.9321495122218568, Validation F1 Score: 0.9291061782341887\nLearning Rate: 9.890503337744983e-19\n\nEpoch 1211, Training Loss: 0.18584445118904114, Validation Loss: 0.30150166153907776\nEpoch 1211, Training Loss: 0.18584638833999634, Validation Loss: 0.30150166153907776\nTraining Accuracy: 0.9406412715812551, Training F1 Score: 0.9376787943580175\nValidation Accuracy: 0.9320947056889182, Validation F1 Score: 0.9290545935623673\nLearning Rate: 9.889514331917292e-19\n\nEpoch 1221, Training Loss: 0.18584184348583221, Validation Loss: 0.3015299439430237\nEpoch 1221, Training Loss: 0.18584150075912476, Validation Loss: 0.3015299439430237\nTraining Accuracy: 0.9407097835023294, Training F1 Score: 0.9377341601783347\nValidation Accuracy: 0.9320947056889182, Validation F1 Score: 0.9290432308126205\nLearning Rate: 9.888525424985732e-19\n\nEpoch 1231, Training Loss: 0.18583542108535767, Validation Loss: 0.3015557825565338\nEpoch 1231, Training Loss: 0.1858343780040741, Validation Loss: 0.3015557825565338\nTraining Accuracy: 0.940750890654974, Training F1 Score: 0.9377760560315386\nValidation Accuracy: 0.9320947056889182, Validation F1 Score: 0.9290432308126205\nLearning Rate: 9.887536616940414e-19\n\nEpoch 1241, Training Loss: 0.18582738935947418, Validation Loss: 0.301704466342926\nEpoch 1241, Training Loss: 0.18582648038864136, Validation Loss: 0.301704466342926\nTraining Accuracy: 0.9407097835023294, Training F1 Score: 0.9377367102476976\nValidation Accuracy: 0.9320398991559794, Validation F1 Score: 0.928980271720408\nLearning Rate: 9.886547907771453e-19\n\nEpoch 1251, Training Loss: 0.18581993877887726, Validation Loss: 0.30198052525520325\nEpoch 1251, Training Loss: 0.18581923842430115, Validation Loss: 0.30198052525520325\nTraining Accuracy: 0.9406960811181145, Training F1 Score: 0.9377184945662578\nValidation Accuracy: 0.9319850926230406, Validation F1 Score: 0.9289173009502064\nLearning Rate: 9.88555929746896e-19\n\nEpoch 1261, Training Loss: 0.1858132928609848, Validation Loss: 0.3020893335342407\nEpoch 1261, Training Loss: 0.1858125925064087, Validation Loss: 0.3020893335342407\nTraining Accuracy: 0.9407371882707591, Training F1 Score: 0.9377654892122237\nValidation Accuracy: 0.9320398991559794, Validation F1 Score: 0.928980271720408\nLearning Rate: 9.88457078602305e-19\n\nEpoch 1271, Training Loss: 0.18580558896064758, Validation Loss: 0.3022210896015167\nEpoch 1271, Training Loss: 0.1858052760362625, Validation Loss: 0.3022210896015167\nTraining Accuracy: 0.9407645930391888, Training F1 Score: 0.9377917204647425\nValidation Accuracy: 0.9320398991559794, Validation F1 Score: 0.928980271720408\nLearning Rate: 9.883582373423832e-19\n\nEpoch 1281, Training Loss: 0.1857975423336029, Validation Loss: 0.3023277819156647\nEpoch 1281, Training Loss: 0.18579627573490143, Validation Loss: 0.3023277819156647\nTraining Accuracy: 0.9407782954234036, Training F1 Score: 0.937804836764179\nValidation Accuracy: 0.9320398991559794, Validation F1 Score: 0.928980271720408\nLearning Rate: 9.88259405966143e-19\n\nEpoch 1291, Training Loss: 0.1857895404100418, Validation Loss: 0.3025357723236084\nEpoch 1291, Training Loss: 0.18578878045082092, Validation Loss: 0.3025357723236084\nTraining Accuracy: 0.940750890654974, Training F1 Score: 0.9377760560315386\nValidation Accuracy: 0.9322591252877342, Validation F1 Score: 0.9292320380947455\nLearning Rate: 9.881605844725955e-19\n\nEpoch 1301, Training Loss: 0.18578264117240906, Validation Loss: 0.3026484549045563\nEpoch 1301, Training Loss: 0.18578188121318817, Validation Loss: 0.3026484549045563\nTraining Accuracy: 0.9408057001918334, Training F1 Score: 0.9378361637217821\nValidation Accuracy: 0.9322043187547956, Validation F1 Score: 0.9291691139924525\nLearning Rate: 9.880617728607528e-19\n\nEpoch 1311, Training Loss: 0.18577557802200317, Validation Loss: 0.30278831720352173\nEpoch 1311, Training Loss: 0.1857748180627823, Validation Loss: 0.30278831720352173\nTraining Accuracy: 0.9408057001918334, Training F1 Score: 0.9378336174968194\nValidation Accuracy: 0.9320947056889182, Validation F1 Score: 0.9290545935623673\nLearning Rate: 9.879629711296266e-19\n\nEpoch 1321, Training Loss: 0.18576844036579132, Validation Loss: 0.30301526188850403\nEpoch 1321, Training Loss: 0.18576790392398834, Validation Loss: 0.30301526188850403\nTraining Accuracy: 0.9407645930391888, Training F1 Score: 0.9377993619142436\nValidation Accuracy: 0.9319850926230406, Validation F1 Score: 0.928928692040728\nLearning Rate: 9.87864179278229e-19\n\nEpoch 1331, Training Loss: 0.185761958360672, Validation Loss: 0.3031010925769806\nEpoch 1331, Training Loss: 0.18576116859912872, Validation Loss: 0.3031010925769806\nTraining Accuracy: 0.9407782954234036, Training F1 Score: 0.9378124772891376\nValidation Accuracy: 0.931930286090102, Validation F1 Score: 0.9288657237715701\nLearning Rate: 9.877653973055717e-19\n\nEpoch 1341, Training Loss: 0.18575534224510193, Validation Loss: 0.30316880345344543\nEpoch 1341, Training Loss: 0.1857547014951706, Validation Loss: 0.30316880345344543\nTraining Accuracy: 0.9407234858865443, Training F1 Score: 0.9377472752988982\nValidation Accuracy: 0.9320947056889182, Validation F1 Score: 0.9290432308126205\nLearning Rate: 9.876666252106673e-19\n\nEpoch 1351, Training Loss: 0.18574877083301544, Validation Loss: 0.30329060554504395\nEpoch 1351, Training Loss: 0.1857481449842453, Validation Loss: 0.30329060554504395\nTraining Accuracy: 0.9407645930391888, Training F1 Score: 0.93779426817675\nValidation Accuracy: 0.9320947056889182, Validation F1 Score: 0.9290432308126205\nLearning Rate: 9.875678629925282e-19\n\nEpoch 1361, Training Loss: 0.18574258685112, Validation Loss: 0.3035567104816437\nEpoch 1361, Training Loss: 0.1857420951128006, Validation Loss: 0.3035567104816437\nTraining Accuracy: 0.9407782954234036, Training F1 Score: 0.9378150230068514\nValidation Accuracy: 0.9319850926230406, Validation F1 Score: 0.928928692040728\nLearning Rate: 9.874691106501662e-19\n\nEpoch 1371, Training Loss: 0.18573476374149323, Validation Loss: 0.3037337064743042\nEpoch 1371, Training Loss: 0.18573346734046936, Validation Loss: 0.3037337064743042\nTraining Accuracy: 0.9407371882707591, Training F1 Score: 0.9377654892122237\nValidation Accuracy: 0.9319850926230406, Validation F1 Score: 0.9289173009502064\nLearning Rate: 9.87370368182594e-19\n\nEpoch 1381, Training Loss: 0.18572670221328735, Validation Loss: 0.30376002192497253\nEpoch 1381, Training Loss: 0.1857261061668396, Validation Loss: 0.30376002192497253\nTraining Accuracy: 0.9408057001918334, Training F1 Score: 0.9378336174968194\nValidation Accuracy: 0.9320398991559794, Validation F1 Score: 0.9289688848094892\nLearning Rate: 9.872716355888246e-19\n\nEpoch 1391, Training Loss: 0.18572023510932922, Validation Loss: 0.30374231934547424\nEpoch 1391, Training Loss: 0.18571963906288147, Validation Loss: 0.30374231934547424\nTraining Accuracy: 0.9408057001918334, Training F1 Score: 0.9378336174968194\nValidation Accuracy: 0.9321495122218568, Validation F1 Score: 0.9291061782341887\nLearning Rate: 9.871729128678699e-19\n\nEpoch 1401, Training Loss: 0.18571288883686066, Validation Loss: 0.3038453757762909\nEpoch 1401, Training Loss: 0.18571223318576813, Validation Loss: 0.3038453757762909\nTraining Accuracy: 0.9408331049602631, Training F1 Score: 0.937870031683132\nValidation Accuracy: 0.9321495122218568, Validation F1 Score: 0.9291061782341887\nLearning Rate: 9.870742000187433e-19\n\nEpoch 1411, Training Loss: 0.18570531904697418, Validation Loss: 0.30400463938713074\nEpoch 1411, Training Loss: 0.18570496141910553, Validation Loss: 0.30400463938713074\nTraining Accuracy: 0.9407645930391888, Training F1 Score: 0.9378044534040301\nValidation Accuracy: 0.9320947056889182, Validation F1 Score: 0.9290432308126205\nLearning Rate: 9.869754970404574e-19\n\nEpoch 1421, Training Loss: 0.1856989562511444, Validation Loss: 0.30427417159080505\nEpoch 1421, Training Loss: 0.18569836020469666, Validation Loss: 0.30427417159080505\nTraining Accuracy: 0.9407782954234036, Training F1 Score: 0.937817568162842\nValidation Accuracy: 0.9320398991559794, Validation F1 Score: 0.928980271720408\nLearning Rate: 9.86876803932025e-19\n\nEpoch 1431, Training Loss: 0.18569281697273254, Validation Loss: 0.3046515882015228\nEpoch 1431, Training Loss: 0.18569229543209076, Validation Loss: 0.3046515882015228\nTraining Accuracy: 0.9407782954234036, Training F1 Score: 0.937817568162842\nValidation Accuracy: 0.9322043187547956, Validation F1 Score: 0.9291577695845131\nLearning Rate: 9.867781206924594e-19\n\nEpoch 1441, Training Loss: 0.18568605184555054, Validation Loss: 0.3047362267971039\nEpoch 1441, Training Loss: 0.18568538129329681, Validation Loss: 0.3047362267971039\nTraining Accuracy: 0.9407782954234036, Training F1 Score: 0.9378150230068514\nValidation Accuracy: 0.9320398991559794, Validation F1 Score: 0.9289916486352134\nLearning Rate: 9.866794473207738e-19\n\nEpoch 1451, Training Loss: 0.18567946553230286, Validation Loss: 0.30479303002357483\nEpoch 1451, Training Loss: 0.1856786459684372, Validation Loss: 0.30479303002357483\nTraining Accuracy: 0.9407645930391888, Training F1 Score: 0.9377917204647425\nValidation Accuracy: 0.9320947056889182, Validation F1 Score: 0.9290432308126205\nLearning Rate: 9.865807838159814e-19\n\nEpoch 1461, Training Loss: 0.18567322194576263, Validation Loss: 0.30510005354881287\nEpoch 1461, Training Loss: 0.18567276000976562, Validation Loss: 0.30510005354881287\nTraining Accuracy: 0.9407782954234036, Training F1 Score: 0.937809931009553\nValidation Accuracy: 0.9321495122218568, Validation F1 Score: 0.9291061782341887\nLearning Rate: 9.864821301770951e-19\n\nEpoch 1471, Training Loss: 0.18566715717315674, Validation Loss: 0.30525198578834534\nEpoch 1471, Training Loss: 0.18566672503948212, Validation Loss: 0.30525198578834534\nTraining Accuracy: 0.9408057001918334, Training F1 Score: 0.9378361637217821\nValidation Accuracy: 0.9320398991559794, Validation F1 Score: 0.928980271720408\nLearning Rate: 9.86383486403129e-19\n\nEpoch 1481, Training Loss: 0.18566207587718964, Validation Loss: 0.3054237961769104\nEpoch 1481, Training Loss: 0.1856614500284195, Validation Loss: 0.3054237961769104\nTraining Accuracy: 0.9408194025760482, Training F1 Score: 0.9378492807511503\nValidation Accuracy: 0.9320398991559794, Validation F1 Score: 0.928980271720408\nLearning Rate: 9.862848524930967e-19\n\nEpoch 1491, Training Loss: 0.18565590679645538, Validation Loss: 0.3054434061050415\nEpoch 1491, Training Loss: 0.18565529584884644, Validation Loss: 0.3054434061050415\nTraining Accuracy: 0.9408468073444779, Training F1 Score: 0.9378780608943542\nValidation Accuracy: 0.9320398991559794, Validation F1 Score: 0.9289916486352134\nLearning Rate: 9.861862284460112e-19\n\nEpoch 1501, Training Loss: 0.1856502741575241, Validation Loss: 0.3055528700351715\nEpoch 1501, Training Loss: 0.18564976751804352, Validation Loss: 0.3055528700351715\nTraining Accuracy: 0.9407919978076186, Training F1 Score: 0.9378281385221946\nValidation Accuracy: 0.9319850926230406, Validation F1 Score: 0.9289400731322826\nLearning Rate: 9.860876142608867e-19\n\nEpoch 1511, Training Loss: 0.18564459681510925, Validation Loss: 0.30569005012512207\nEpoch 1511, Training Loss: 0.18564414978027344, Validation Loss: 0.30569005012512207\nTraining Accuracy: 0.9408605097286928, Training F1 Score: 0.9378911789621\nValidation Accuracy: 0.9319850926230406, Validation F1 Score: 0.928928692040728\nLearning Rate: 9.859890099367371e-19\n\nEpoch 1521, Training Loss: 0.1856384575366974, Validation Loss: 0.3058842718601227\nEpoch 1521, Training Loss: 0.18563775718212128, Validation Loss: 0.3058842718601227\nTraining Accuracy: 0.9408194025760482, Training F1 Score: 0.9378620019088609\nValidation Accuracy: 0.9320398991559794, Validation F1 Score: 0.9289916486352134\nLearning Rate: 9.858904154725761e-19\n\nEpoch 1531, Training Loss: 0.18563351035118103, Validation Loss: 0.3060074746608734\nEpoch 1531, Training Loss: 0.18563292920589447, Validation Loss: 0.3060074746608734\nTraining Accuracy: 0.9408194025760482, Training F1 Score: 0.9378543708987512\nValidation Accuracy: 0.9319850926230406, Validation F1 Score: 0.928928692040728\nLearning Rate: 9.857918308674179e-19\n\nEpoch 1541, Training Loss: 0.18562808632850647, Validation Loss: 0.3060104250907898\nEpoch 1541, Training Loss: 0.1856275200843811, Validation Loss: 0.3060104250907898\nTraining Accuracy: 0.9408194025760482, Training F1 Score: 0.9378543708987512\nValidation Accuracy: 0.9319850926230406, Validation F1 Score: 0.928928692040728\nLearning Rate: 9.856932561202765e-19\n\nEpoch 1551, Training Loss: 0.18562297523021698, Validation Loss: 0.3060053288936615\nEpoch 1551, Training Loss: 0.185622438788414, Validation Loss: 0.3060053288936615\nTraining Accuracy: 0.9408468073444779, Training F1 Score: 0.9378856917380961\nValidation Accuracy: 0.9319850926230406, Validation F1 Score: 0.9289400731322826\nLearning Rate: 9.855946912301663e-19\n\nEpoch 1561, Training Loss: 0.18561752140522003, Validation Loss: 0.3061060905456543\nEpoch 1561, Training Loss: 0.18561668694019318, Validation Loss: 0.3061060905456543\nTraining Accuracy: 0.9408742121129077, Training F1 Score: 0.9379119264709682\nValidation Accuracy: 0.931930286090102, Validation F1 Score: 0.9288771190362379\nLearning Rate: 9.854961361961016e-19\n\nEpoch 1571, Training Loss: 0.1856115609407425, Validation Loss: 0.30616557598114014\nEpoch 1571, Training Loss: 0.18561096489429474, Validation Loss: 0.30616557598114014\nTraining Accuracy: 0.9408468073444779, Training F1 Score: 0.9378882342301857\nValidation Accuracy: 0.9318754795571632, Validation F1 Score: 0.9288141532688357\nLearning Rate: 9.853975910170969e-19\n\nEpoch 1581, Training Loss: 0.18560631573200226, Validation Loss: 0.3061319887638092\nEpoch 1581, Training Loss: 0.18560583889484406, Validation Loss: 0.3061319887638092\nTraining Accuracy: 0.9407782954234036, Training F1 Score: 0.9378201127572581\nValidation Accuracy: 0.931930286090102, Validation F1 Score: 0.9288771190362379\nLearning Rate: 9.852990556921664e-19\n\nEpoch 1591, Training Loss: 0.18560099601745605, Validation Loss: 0.30613186955451965\nEpoch 1591, Training Loss: 0.18560044467449188, Validation Loss: 0.30613186955451965\nTraining Accuracy: 0.9408605097286928, Training F1 Score: 0.9378988088801291\nValidation Accuracy: 0.9318206730242244, Validation F1 Score: 0.9287511758227319\nLearning Rate: 9.852005302203252e-19\n\nEpoch 1601, Training Loss: 0.1855957955121994, Validation Loss: 0.3061351776123047\nEpoch 1601, Training Loss: 0.18559524416923523, Validation Loss: 0.3061351776123047\nTraining Accuracy: 0.9407782954234036, Training F1 Score: 0.9378226567902475\nValidation Accuracy: 0.9318754795571632, Validation F1 Score: 0.9288141532688357\nLearning Rate: 9.851020146005878e-19\n\nEpoch 1611, Training Loss: 0.18559135496616364, Validation Loss: 0.30619192123413086\nEpoch 1611, Training Loss: 0.18559078872203827, Validation Loss: 0.30619192123413086\nTraining Accuracy: 0.9408605097286928, Training F1 Score: 0.9378988088801291\nValidation Accuracy: 0.9319850926230406, Validation F1 Score: 0.928928692040728\nLearning Rate: 9.85003508831969e-19\n\nEpoch 1621, Training Loss: 0.18558616936206818, Validation Loss: 0.30624428391456604\nEpoch 1621, Training Loss: 0.18558570742607117, Validation Loss: 0.30624428391456604\nTraining Accuracy: 0.9408331049602631, Training F1 Score: 0.9378649432756146\nValidation Accuracy: 0.9320398991559794, Validation F1 Score: 0.928980271720408\nLearning Rate: 9.849050129134838e-19\n\nEpoch 1631, Training Loss: 0.1855805367231369, Validation Loss: 0.30626657605171204\nEpoch 1631, Training Loss: 0.1855800300836563, Validation Loss: 0.30626657605171204\nTraining Accuracy: 0.9408605097286928, Training F1 Score: 0.9378988088801291\nValidation Accuracy: 0.9319850926230406, Validation F1 Score: 0.928928692040728\nLearning Rate: 9.848065268441472e-19\n\nEpoch 1641, Training Loss: 0.185575470328331, Validation Loss: 0.30631688237190247\nEpoch 1641, Training Loss: 0.18557488918304443, Validation Loss: 0.30631688237190247\nTraining Accuracy: 0.9408194025760482, Training F1 Score: 0.9378518261058022\nValidation Accuracy: 0.9318754795571632, Validation F1 Score: 0.9288141532688357\nLearning Rate: 9.847080506229746e-19\n\nEpoch 1651, Training Loss: 0.18557068705558777, Validation Loss: 0.30631813406944275\nEpoch 1651, Training Loss: 0.1855701357126236, Validation Loss: 0.30631813406944275\nTraining Accuracy: 0.9409016168813373, Training F1 Score: 0.9379330788010732\nValidation Accuracy: 0.931930286090102, Validation F1 Score: 0.9288657237715701\nLearning Rate: 9.846095842489807e-19\n\nEpoch 1661, Training Loss: 0.18556614220142365, Validation Loss: 0.3063525855541229\nEpoch 1661, Training Loss: 0.1855655312538147, Validation Loss: 0.3063525855541229\nTraining Accuracy: 0.9408057001918334, Training F1 Score: 0.9378412544861348\nValidation Accuracy: 0.9320398991559794, Validation F1 Score: 0.9289916486352134\nLearning Rate: 9.845111277211813e-19\n\nEpoch 1671, Training Loss: 0.18556155264377594, Validation Loss: 0.3063698709011078\nEpoch 1671, Training Loss: 0.1855611801147461, Validation Loss: 0.3063698709011078\nTraining Accuracy: 0.9408742121129077, Training F1 Score: 0.9379144683460471\nValidation Accuracy: 0.9319850926230406, Validation F1 Score: 0.9289173009502064\nLearning Rate: 9.844126810385915e-19\n\nEpoch 1681, Training Loss: 0.18555767834186554, Validation Loss: 0.3063071072101593\nEpoch 1681, Training Loss: 0.1855572909116745, Validation Loss: 0.3063071072101593\nTraining Accuracy: 0.9408194025760482, Training F1 Score: 0.9378569151301454\nValidation Accuracy: 0.931930286090102, Validation F1 Score: 0.9288657237715701\nLearning Rate: 9.84314244200227e-19\n\nEpoch 1691, Training Loss: 0.18555191159248352, Validation Loss: 0.30641594529151917\nEpoch 1691, Training Loss: 0.1855522245168686, Validation Loss: 0.30641594529151917\nTraining Accuracy: 0.9408879144971225, Training F1 Score: 0.9379225023832796\nValidation Accuracy: 0.9319850926230406, Validation F1 Score: 0.928905899850299\nLearning Rate: 9.842158172051035e-19\n\nEpoch 1701, Training Loss: 0.18554982542991638, Validation Loss: 0.3064422905445099\nEpoch 1701, Training Loss: 0.18554958701133728, Validation Loss: 0.3064422905445099\nTraining Accuracy: 0.9408742121129077, Training F1 Score: 0.9379144683460471\nValidation Accuracy: 0.931930286090102, Validation F1 Score: 0.9288657237715701\nLearning Rate: 9.841174000522365e-19\n\nEpoch 1711, Training Loss: 0.18554627895355225, Validation Loss: 0.3065190017223358\nEpoch 1711, Training Loss: 0.1855458915233612, Validation Loss: 0.3065190017223358\nTraining Accuracy: 0.9408331049602631, Training F1 Score: 0.9378725750447904\nValidation Accuracy: 0.9320398991559794, Validation F1 Score: 0.928980271720408\nLearning Rate: 9.840189927406418e-19\n\nEpoch 1721, Training Loss: 0.185541033744812, Validation Loss: 0.3065427243709564\nEpoch 1721, Training Loss: 0.18554049730300903, Validation Loss: 0.3065427243709564\nTraining Accuracy: 0.9408468073444779, Training F1 Score: 0.9378882342301857\nValidation Accuracy: 0.9320398991559794, Validation F1 Score: 0.928980271720408\nLearning Rate: 9.839205952693356e-19\n\nEpoch 1731, Training Loss: 0.1855357438325882, Validation Loss: 0.30656784772872925\nEpoch 1731, Training Loss: 0.18553532660007477, Validation Loss: 0.30656784772872925\nTraining Accuracy: 0.9409016168813373, Training F1 Score: 0.9379381629993805\nValidation Accuracy: 0.9320398991559794, Validation F1 Score: 0.928980271720408\nLearning Rate: 9.838222076373338e-19\n\nEpoch 1741, Training Loss: 0.1855311244726181, Validation Loss: 0.3066811263561249\nEpoch 1741, Training Loss: 0.18553072214126587, Validation Loss: 0.3066811263561249\nTraining Accuracy: 0.9408331049602631, Training F1 Score: 0.9378776600846462\nValidation Accuracy: 0.9319850926230406, Validation F1 Score: 0.9289173009502064\nLearning Rate: 9.837238298436525e-19\n\nEpoch 1751, Training Loss: 0.18552647531032562, Validation Loss: 0.3067694306373596\nEpoch 1751, Training Loss: 0.18552608788013458, Validation Loss: 0.3067694306373596\nTraining Accuracy: 0.9408742121129077, Training F1 Score: 0.9379170096602387\nValidation Accuracy: 0.9319850926230406, Validation F1 Score: 0.9289173009502064\nLearning Rate: 9.836254618873078e-19\n\nEpoch 1761, Training Loss: 0.1855221539735794, Validation Loss: 0.30676400661468506\nEpoch 1761, Training Loss: 0.18552188575267792, Validation Loss: 0.30676400661468506\nTraining Accuracy: 0.9408468073444779, Training F1 Score: 0.9378882342301857\nValidation Accuracy: 0.9319850926230406, Validation F1 Score: 0.9289173009502064\nLearning Rate: 9.83527103767316e-19\n\nEpoch 1771, Training Loss: 0.1855190545320511, Validation Loss: 0.30673855543136597\nEpoch 1771, Training Loss: 0.18551874160766602, Validation Loss: 0.30673855543136597\nTraining Accuracy: 0.9408057001918334, Training F1 Score: 0.9378488864209522\nValidation Accuracy: 0.9318754795571632, Validation F1 Score: 0.928802743820392\nLearning Rate: 9.834287554826936e-19\n\nEpoch 1781, Training Loss: 0.18551477789878845, Validation Loss: 0.3066938817501068\nEpoch 1781, Training Loss: 0.1855144053697586, Validation Loss: 0.3066938817501068\nTraining Accuracy: 0.9408194025760482, Training F1 Score: 0.9378645444564782\nValidation Accuracy: 0.9319850926230406, Validation F1 Score: 0.9289173009502064\nLearning Rate: 9.833304170324571e-19\n\nEpoch 1791, Training Loss: 0.18551066517829895, Validation Loss: 0.3066665530204773\nEpoch 1791, Training Loss: 0.18551020324230194, Validation Loss: 0.3066665530204773\nTraining Accuracy: 0.9408742121129077, Training F1 Score: 0.9379195504136918\nValidation Accuracy: 0.9319850926230406, Validation F1 Score: 0.928905899850299\nLearning Rate: 9.832320884156233e-19\n\nEpoch 1801, Training Loss: 0.1855057030916214, Validation Loss: 0.3068884313106537\nEpoch 1801, Training Loss: 0.18550509214401245, Validation Loss: 0.3068884313106537\nTraining Accuracy: 0.9408057001918334, Training F1 Score: 0.9378514292766934\nValidation Accuracy: 0.9322043187547956, Validation F1 Score: 0.929146415209835\nLearning Rate: 9.831337696312085e-19\n\nEpoch 1811, Training Loss: 0.1855003535747528, Validation Loss: 0.3071746528148651\nEpoch 1811, Training Loss: 0.1854996681213379, Validation Loss: 0.3071746528148651\nTraining Accuracy: 0.9408331049602631, Training F1 Score: 0.9378827428808729\nValidation Accuracy: 0.9322043187547956, Validation F1 Score: 0.929146415209835\nLearning Rate: 9.830354606782299e-19\n\nEpoch 1821, Training Loss: 0.18549714982509613, Validation Loss: 0.30737778544425964\nEpoch 1821, Training Loss: 0.18549694120883942, Validation Loss: 0.30737778544425964\nTraining Accuracy: 0.9408468073444779, Training F1 Score: 0.9378958583410516\nValidation Accuracy: 0.9320947056889182, Validation F1 Score: 0.9290318580800208\nLearning Rate: 9.829371615557041e-19\n\nEpoch 1831, Training Loss: 0.1854945719242096, Validation Loss: 0.30758991837501526\nEpoch 1831, Training Loss: 0.18549419939517975, Validation Loss: 0.30758991837501526\nTraining Accuracy: 0.9408194025760482, Training F1 Score: 0.9378670864431321\nValidation Accuracy: 0.9320398991559794, Validation F1 Score: 0.928980271720408\nLearning Rate: 9.828388722626482e-19\n\nEpoch 1841, Training Loss: 0.18549023568630219, Validation Loss: 0.30774009227752686\nEpoch 1841, Training Loss: 0.1854899674654007, Validation Loss: 0.30774009227752686\nTraining Accuracy: 0.9408605097286928, Training F1 Score: 0.9379038926864477\nValidation Accuracy: 0.9320947056889182, Validation F1 Score: 0.9290318580800208\nLearning Rate: 9.827405927980794e-19\n\nEpoch 1851, Training Loss: 0.18548494577407837, Validation Loss: 0.3080904185771942\nEpoch 1851, Training Loss: 0.18548466265201569, Validation Loss: 0.3080904185771942\nTraining Accuracy: 0.9408331049602631, Training F1 Score: 0.9378776600846462\nValidation Accuracy: 0.9319850926230406, Validation F1 Score: 0.9289173009502064\nLearning Rate: 9.826423231610148e-19\n\nEpoch 1861, Training Loss: 0.1854812204837799, Validation Loss: 0.3082263171672821\nEpoch 1861, Training Loss: 0.18548080325126648, Validation Loss: 0.3082263171672821\nTraining Accuracy: 0.9408331049602631, Training F1 Score: 0.9378802017631392\nValidation Accuracy: 0.9320398991559794, Validation F1 Score: 0.9289688848094892\nLearning Rate: 9.825440633504717e-19\n\nEpoch 1871, Training Loss: 0.18547682464122772, Validation Loss: 0.3083338737487793\nEpoch 1871, Training Loss: 0.18547649681568146, Validation Loss: 0.3083338737487793\nTraining Accuracy: 0.9407782954234036, Training F1 Score: 0.9378201127572581\nValidation Accuracy: 0.9320398991559794, Validation F1 Score: 0.9289688848094892\nLearning Rate: 9.824458133654674e-19\n\nEpoch 1881, Training Loss: 0.18547311425209045, Validation Loss: 0.3085469901561737\nEpoch 1881, Training Loss: 0.18547260761260986, Validation Loss: 0.3085469901561737\nTraining Accuracy: 0.9407919978076186, Training F1 Score: 0.9378357713814407\nValidation Accuracy: 0.9320398991559794, Validation F1 Score: 0.9289688848094892\nLearning Rate: 9.823475732050196e-19\n\nEpoch 1891, Training Loss: 0.18546876311302185, Validation Loss: 0.30856838822364807\nEpoch 1891, Training Loss: 0.18546822667121887, Validation Loss: 0.30856838822364807\nTraining Accuracy: 0.9407782954234036, Training F1 Score: 0.9378150230068514\nValidation Accuracy: 0.931930286090102, Validation F1 Score: 0.9288543184946659\nLearning Rate: 9.822493428681458e-19\n\nEpoch 1901, Training Loss: 0.185464546084404, Validation Loss: 0.3086053729057312\nEpoch 1901, Training Loss: 0.18546415865421295, Validation Loss: 0.3086053729057312\nTraining Accuracy: 0.9407919978076186, Training F1 Score: 0.9378332276564455\nValidation Accuracy: 0.9319850926230406, Validation F1 Score: 0.928905899850299\nLearning Rate: 9.821511223538636e-19\n\nEpoch 1911, Training Loss: 0.18546080589294434, Validation Loss: 0.3086691200733185\nEpoch 1911, Training Loss: 0.18546034395694733, Validation Loss: 0.3086691200733185\nTraining Accuracy: 0.9408057001918334, Training F1 Score: 0.9378488864209522\nValidation Accuracy: 0.9320947056889182, Validation F1 Score: 0.9290432308126205\nLearning Rate: 9.820529116611909e-19\n\nEpoch 1921, Training Loss: 0.18545661866664886, Validation Loss: 0.3087446689605713\nEpoch 1921, Training Loss: 0.18545618653297424, Validation Loss: 0.3087446689605713\nTraining Accuracy: 0.9407919978076186, Training F1 Score: 0.9378332276564455\nValidation Accuracy: 0.9320398991559794, Validation F1 Score: 0.928980271720408\nLearning Rate: 9.819547107891454e-19\n\nEpoch 1931, Training Loss: 0.18545275926589966, Validation Loss: 0.3088315427303314\nEpoch 1931, Training Loss: 0.18545226752758026, Validation Loss: 0.3088315427303314\nTraining Accuracy: 0.9408194025760482, Training F1 Score: 0.9378645444564782\nValidation Accuracy: 0.9319850926230406, Validation F1 Score: 0.9289173009502064\nLearning Rate: 9.818565197367454e-19\n\nEpoch 1941, Training Loss: 0.1854487806558609, Validation Loss: 0.3089042007923126\nEpoch 1941, Training Loss: 0.18544818460941315, Validation Loss: 0.3089042007923126\nTraining Accuracy: 0.9408468073444779, Training F1 Score: 0.9378958583410516\nValidation Accuracy: 0.9320398991559794, Validation F1 Score: 0.928980271720408\nLearning Rate: 9.817583385030087e-19\n\nEpoch 1951, Training Loss: 0.18544481694698334, Validation Loss: 0.3090111017227173\nEpoch 1951, Training Loss: 0.18544448912143707, Validation Loss: 0.3090111017227173\nTraining Accuracy: 0.9408742121129077, Training F1 Score: 0.9379170096602387\nValidation Accuracy: 0.9321495122218568, Validation F1 Score: 0.9290834511243127\nLearning Rate: 9.816601670869536e-19\n\nEpoch 1961, Training Loss: 0.18544135987758636, Validation Loss: 0.3090810179710388\nEpoch 1961, Training Loss: 0.18544107675552368, Validation Loss: 0.3090810179710388\nTraining Accuracy: 0.9408605097286928, Training F1 Score: 0.9379089742495852\nValidation Accuracy: 0.9320398991559794, Validation F1 Score: 0.9289688848094892\nLearning Rate: 9.815620054875983e-19\n\nEpoch 1971, Training Loss: 0.18543758988380432, Validation Loss: 0.3090556263923645\nEpoch 1971, Training Loss: 0.18543696403503418, Validation Loss: 0.3090556263923645\nTraining Accuracy: 0.9408468073444779, Training F1 Score: 0.9378933175315156\nValidation Accuracy: 0.9320947056889182, Validation F1 Score: 0.9290204753541664\nLearning Rate: 9.814638537039611e-19\n\nEpoch 1981, Training Loss: 0.18543344736099243, Validation Loss: 0.3090747892856598\nEpoch 1981, Training Loss: 0.1854330152273178, Validation Loss: 0.3090747892856598\nTraining Accuracy: 0.9408331049602631, Training F1 Score: 0.9378776600846462\nValidation Accuracy: 0.9321495122218568, Validation F1 Score: 0.9290834511243127\nLearning Rate: 9.813657117350607e-19\n\nEpoch 1991, Training Loss: 0.18542948365211487, Validation Loss: 0.3091840147972107\nEpoch 1991, Training Loss: 0.18542931973934174, Validation Loss: 0.3091840147972107\nTraining Accuracy: 0.9408194025760482, Training F1 Score: 0.9378594588001326\nValidation Accuracy: 0.9321495122218568, Validation F1 Score: 0.9290720725889885\nLearning Rate: 9.812675795799156e-19\n\nBest Validation Loss after 2000 epochs: 0.2253570556640625 from Epoch 71\nExecution time: 308.003923 seconds\n","output_type":"stream"}],"execution_count":55},{"cell_type":"code","source":"def custom_lr_lambda(step):\n    num_step_threshold = 100\n\n    if step < num_step_threshold:\n        return step / num_step_threshold\n    if step == num_step_threshold:\n        print(\"here\")\n    return 0.99999 ** (step - num_step_threshold)\n\nstart_time = time.time()\nmodel = TabularDenseNet(num_features, num_classes, 1).to(device)\n\noptimizer = optim.Rprop(model.parameters(), lr=0.001 * 0.001 * 0.001 * 0.001 * 0.001 * 0.001 * 1.0)\nscheduler = LambdaLR(optimizer, lr_lambda=custom_lr_lambda)\n\ncriterion = CustomLoss(nn.CrossEntropyLoss(), 0.0, 0.0, 0.0, 0.0, 0.0)\nevaluate_model(model, custom_train_loader, criterion, optimizer, 2000, scheduler, batch_size, num_features, early_stopping_patience=10000)\n\nprint(f\"Execution time: {(time.time() - start_time):.6f} seconds\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-30T22:19:24.020145Z","iopub.execute_input":"2025-01-30T22:19:24.020937Z","iopub.status.idle":"2025-01-30T22:20:08.670098Z","shell.execute_reply.started":"2025-01-30T22:19:24.020903Z","shell.execute_reply":"2025-01-30T22:20:08.669127Z"},"collapsed":true,"jupyter":{"source_hidden":true,"outputs_hidden":true}},"outputs":[{"name":"stdout","text":"Epoch 1, Training Loss: 0.6931450963020325, Validation Loss: 0.6931193470954895\nEpoch 1, Training Loss: 0.6931175589561462, Validation Loss: 0.6931193470954895\nTraining Accuracy: 0.8623184434091532, Training F1 Score: 0.8428494257227417\nValidation Accuracy: 0.8647374767072235, Validation F1 Score: 0.8461320946083078\nLearning Rate: 1.0000000000000002e-20\n\nEpoch 11, Training Loss: 0.6924352645874023, Validation Loss: 0.6922627091407776\nEpoch 11, Training Loss: 0.6922658085823059, Validation Loss: 0.6922627091407776\nTraining Accuracy: 0.8623184434091532, Training F1 Score: 0.8428494257227417\nValidation Accuracy: 0.8647374767072235, Validation F1 Score: 0.8461320946083078\nLearning Rate: 1.1000000000000004e-19\n\nEpoch 21, Training Loss: 0.6880935430526733, Validation Loss: 0.6870555281639099\nEpoch 21, Training Loss: 0.6870748400688171, Validation Loss: 0.6870555281639099\nTraining Accuracy: 0.8622773362565086, Training F1 Score: 0.842815759969005\nValidation Accuracy: 0.864847089773101, Validation F1 Score: 0.8462225901123941\nLearning Rate: 2.1000000000000004e-19\n\nEpoch 31, Training Loss: 0.6634219884872437, Validation Loss: 0.65794837474823\nEpoch 31, Training Loss: 0.6579297184944153, Validation Loss: 0.65794837474823\nTraining Accuracy: 0.8630172650041107, Training F1 Score: 0.8437742987703166\nValidation Accuracy: 0.8658884138989368, Validation F1 Score: 0.8475260278073936\nLearning Rate: 3.100000000000001e-19\n\nEpoch 41, Training Loss: 0.5469522476196289, Validation Loss: 0.5262554287910461\nEpoch 41, Training Loss: 0.5257114768028259, Validation Loss: 0.5262554287910461\nTraining Accuracy: 0.8713894217593862, Training F1 Score: 0.8508020532863824\nValidation Accuracy: 0.8732872958456648, Validation F1 Score: 0.8543168020635504\nLearning Rate: 4.100000000000001e-19\n\nEpoch 51, Training Loss: 0.32828301191329956, Validation Loss: 0.3117453455924988\nEpoch 51, Training Loss: 0.3121935725212097, Validation Loss: 0.3117453455924988\nTraining Accuracy: 0.8864346396272952, Training F1 Score: 0.8701049384124095\nValidation Accuracy: 0.8876466074756111, Validation F1 Score: 0.8719068682104149\nLearning Rate: 5.100000000000002e-19\n\nEpoch 61, Training Loss: 0.23085857927799225, Validation Loss: 0.23099085688591003\nEpoch 61, Training Loss: 0.22903847694396973, Validation Loss: 0.23099085688591003\nTraining Accuracy: 0.9233762674705399, Training F1 Score: 0.9184712764956426\nValidation Accuracy: 0.922503562424641, Validation F1 Score: 0.9179387569394869\nLearning Rate: 6.100000000000001e-19\n\nEpoch 71, Training Loss: 0.22381183505058289, Validation Loss: 0.22468680143356323\nEpoch 71, Training Loss: 0.22362753748893738, Validation Loss: 0.22468680143356323\nTraining Accuracy: 0.9274869827349959, Training F1 Score: 0.9232830794862968\nValidation Accuracy: 0.926723665460923, Validation F1 Score: 0.9228294003742405\nLearning Rate: 7.100000000000002e-19\n\nEpoch 81, Training Loss: 0.22258496284484863, Validation Loss: 0.22363513708114624\nEpoch 81, Training Loss: 0.2225545346736908, Validation Loss: 0.22363513708114624\nTraining Accuracy: 0.9274047684297068, Training F1 Score: 0.9229735594728545\nValidation Accuracy: 0.927107311191494, Validation F1 Score: 0.9229700805555949\nLearning Rate: 8.100000000000003e-19\n\nEpoch 91, Training Loss: 0.22211040556430817, Validation Loss: 0.22335469722747803\nEpoch 91, Training Loss: 0.22208265960216522, Validation Loss: 0.22335469722747803\nTraining Accuracy: 0.9276377089613593, Training F1 Score: 0.9234538708237024\nValidation Accuracy: 0.9276553765208813, Validation F1 Score: 0.9238169429827547\nLearning Rate: 9.100000000000004e-19\n\nhere\nEpoch 101, Training Loss: 0.22187171876430511, Validation Loss: 0.22322504222393036\nEpoch 101, Training Loss: 0.22185111045837402, Validation Loss: 0.22322504222393036\nTraining Accuracy: 0.9278980542614415, Training F1 Score: 0.9238144361530435\nValidation Accuracy: 0.92771018305382, Validation F1 Score: 0.9239564474833798\nLearning Rate: 9.999900000000003e-19\n\nEpoch 111, Training Loss: 0.2217210829257965, Validation Loss: 0.2231731414794922\nEpoch 111, Training Loss: 0.22170977294445038, Validation Loss: 0.2231731414794922\nTraining Accuracy: 0.9276514113455742, Training F1 Score: 0.9234732161935808\nValidation Accuracy: 0.9273813438561876, Validation F1 Score: 0.9235093430927391\nLearning Rate: 9.998900054998358e-19\n\nEpoch 121, Training Loss: 0.22162938117980957, Validation Loss: 0.22318659722805023\nEpoch 121, Training Loss: 0.22162482142448425, Validation Loss: 0.22318659722805023\nTraining Accuracy: 0.9276240065771444, Training F1 Score: 0.9234442287754725\nValidation Accuracy: 0.9269428915926778, Validation F1 Score: 0.9230602772616775\nLearning Rate: 9.997900209986712e-19\n\nEpoch 131, Training Loss: 0.22158017754554749, Validation Loss: 0.2232980877161026\nEpoch 131, Training Loss: 0.22157591581344604, Validation Loss: 0.2232980877161026\nTraining Accuracy: 0.9278021375719375, Training F1 Score: 0.9236922156189525\nValidation Accuracy: 0.927107311191494, Validation F1 Score: 0.9232652252011435\nLearning Rate: 9.99690046495507e-19\n\nEpoch 141, Training Loss: 0.22153818607330322, Validation Loss: 0.22338609397411346\nEpoch 141, Training Loss: 0.22153519093990326, Validation Loss: 0.22338609397411346\nTraining Accuracy: 0.9277747328035079, Training F1 Score: 0.9236536040081929\nValidation Accuracy: 0.927107311191494, Validation F1 Score: 0.9232652252011435\nLearning Rate: 9.995900819893431e-19\n\nEpoch 151, Training Loss: 0.22151033580303192, Validation Loss: 0.2234780341386795\nEpoch 151, Training Loss: 0.22150753438472748, Validation Loss: 0.2234780341386795\nTraining Accuracy: 0.9277336256508633, Training F1 Score: 0.923605322746077\nValidation Accuracy: 0.9273813438561876, Validation F1 Score: 0.9235473732510573\nLearning Rate: 9.9949012747918e-19\n\nEpoch 161, Training Loss: 0.22148656845092773, Validation Loss: 0.2235666811466217\nEpoch 161, Training Loss: 0.22148466110229492, Validation Loss: 0.2235666811466217\nTraining Accuracy: 0.9277336256508633, Training F1 Score: 0.9236117604696217\nValidation Accuracy: 0.9272717307903102, Validation F1 Score: 0.9234573086408259\nLearning Rate: 9.993901829640183e-19\n\nEpoch 171, Training Loss: 0.22146743535995483, Validation Loss: 0.2236185520887375\nEpoch 171, Training Loss: 0.22146540880203247, Validation Loss: 0.2236185520887375\nTraining Accuracy: 0.9276788161140038, Training F1 Score: 0.9235441597717953\nValidation Accuracy: 0.9271621177244328, Validation F1 Score: 0.9233419466342558\nLearning Rate: 9.992902484428582e-19\n\nEpoch 181, Training Loss: 0.22145025432109833, Validation Loss: 0.2236476093530655\nEpoch 181, Training Loss: 0.22144870460033417, Validation Loss: 0.2236476093530655\nTraining Accuracy: 0.9276103041929296, Training F1 Score: 0.9234701179889991\nValidation Accuracy: 0.9272717307903102, Validation F1 Score: 0.9234446464550599\nLearning Rate: 9.991903239147006e-19\n\nEpoch 191, Training Loss: 0.22143475711345673, Validation Loss: 0.22369863092899323\nEpoch 191, Training Loss: 0.2214331030845642, Validation Loss: 0.22369863092899323\nTraining Accuracy: 0.9276240065771444, Training F1 Score: 0.923489441551911\nValidation Accuracy: 0.9273813438561876, Validation F1 Score: 0.9235600275455571\nLearning Rate: 9.990904093785461e-19\n\nEpoch 201, Training Loss: 0.221420556306839, Validation Loss: 0.22369086742401123\nEpoch 201, Training Loss: 0.22141897678375244, Validation Loss: 0.22369086742401123\nTraining Accuracy: 0.9277610304192929, Training F1 Score: 0.9236342930446185\nValidation Accuracy: 0.9272717307903102, Validation F1 Score: 0.9234446464550599\nLearning Rate: 9.989905048333957e-19\n\nEpoch 211, Training Loss: 0.2214062213897705, Validation Loss: 0.2237226963043213\nEpoch 211, Training Loss: 0.22140517830848694, Validation Loss: 0.2237226963043213\nTraining Accuracy: 0.9278158399561524, Training F1 Score: 0.9237018781694402\nValidation Accuracy: 0.9274361503891264, Validation F1 Score: 0.9236240362680289\nLearning Rate: 9.988906102782501e-19\n\nEpoch 221, Training Loss: 0.22139376401901245, Validation Loss: 0.22375985980033875\nEpoch 221, Training Loss: 0.22139272093772888, Validation Loss: 0.22375985980033875\nTraining Accuracy: 0.9277336256508633, Training F1 Score: 0.9236214116912818\nValidation Accuracy: 0.9274909569220651, Validation F1 Score: 0.9237132472098366\nLearning Rate: 9.987907257121107e-19\n\nEpoch 231, Training Loss: 0.22138307988643646, Validation Loss: 0.22378617525100708\nEpoch 231, Training Loss: 0.22138197720050812, Validation Loss: 0.22378617525100708\nTraining Accuracy: 0.9277610304192929, Training F1 Score: 0.9236471607493087\nValidation Accuracy: 0.9273813438561876, Validation F1 Score: 0.9235853025684325\nLearning Rate: 9.986908511339784e-19\n\nEpoch 241, Training Loss: 0.22137343883514404, Validation Loss: 0.22383715212345123\nEpoch 241, Training Loss: 0.2213723361492157, Validation Loss: 0.22383715212345123\nTraining Accuracy: 0.9277473280350781, Training F1 Score: 0.9236375013965527\nValidation Accuracy: 0.9272169242573715, Validation F1 Score: 0.9234059592983695\nLearning Rate: 9.985909865428544e-19\n\nEpoch 251, Training Loss: 0.22136534750461578, Validation Loss: 0.22386378049850464\nEpoch 251, Training Loss: 0.2213646024465561, Validation Loss: 0.22386378049850464\nTraining Accuracy: 0.9276514113455742, Training F1 Score: 0.9235345190993494\nValidation Accuracy: 0.9273813438561876, Validation F1 Score: 0.9235726706473959\nLearning Rate: 9.984911319377401e-19\n\nEpoch 261, Training Loss: 0.22135774791240692, Validation Loss: 0.22387899458408356\nEpoch 261, Training Loss: 0.22135724127292633, Validation Loss: 0.22387899458408356\nTraining Accuracy: 0.9277062208824336, Training F1 Score: 0.9235795728878013\nValidation Accuracy: 0.9274361503891264, Validation F1 Score: 0.9236113971175293\nLearning Rate: 9.983912873176372e-19\n\nEpoch 271, Training Loss: 0.2213519662618637, Validation Loss: 0.22390301525592804\nEpoch 271, Training Loss: 0.22135138511657715, Validation Loss: 0.22390301525592804\nTraining Accuracy: 0.9277336256508633, Training F1 Score: 0.9236181953322941\nValidation Accuracy: 0.9273813438561876, Validation F1 Score: 0.9235726706473959\nLearning Rate: 9.982914526815468e-19\n\nEpoch 281, Training Loss: 0.2213459312915802, Validation Loss: 0.22393947839736938\nEpoch 281, Training Loss: 0.22134535014629364, Validation Loss: 0.22393947839736938\nTraining Accuracy: 0.9277884351877227, Training F1 Score: 0.923682553291382\nValidation Accuracy: 0.9272717307903102, Validation F1 Score: 0.9234573086408259\nLearning Rate: 9.98191628028471e-19\n\nEpoch 291, Training Loss: 0.22133971750736237, Validation Loss: 0.2239716500043869\nEpoch 291, Training Loss: 0.221339151263237, Validation Loss: 0.2239716500043869\nTraining Accuracy: 0.9278843518772266, Training F1 Score: 0.9237887357834057\nValidation Accuracy: 0.9272717307903102, Validation F1 Score: 0.9234319730597381\nLearning Rate: 9.980918133574109e-19\n\nEpoch 301, Training Loss: 0.2213340401649475, Validation Loss: 0.22401395440101624\nEpoch 301, Training Loss: 0.22133345901966095, Validation Loss: 0.22401395440101624\nTraining Accuracy: 0.9278021375719375, Training F1 Score: 0.9236954285716739\nValidation Accuracy: 0.9272169242573715, Validation F1 Score: 0.9234059592983695\nLearning Rate: 9.97992008667369e-19\n\nEpoch 311, Training Loss: 0.22132790088653564, Validation Loss: 0.22403882443904877\nEpoch 311, Training Loss: 0.22132731974124908, Validation Loss: 0.22403882443904877\nTraining Accuracy: 0.9278706494930118, Training F1 Score: 0.9237726510590983\nValidation Accuracy: 0.9272169242573715, Validation F1 Score: 0.923393293175183\nLearning Rate: 9.97892213957347e-19\n\nEpoch 321, Training Loss: 0.22132252156734467, Validation Loss: 0.22407028079032898\nEpoch 321, Training Loss: 0.22132188081741333, Validation Loss: 0.22407028079032898\nTraining Accuracy: 0.9278295423403672, Training F1 Score: 0.9237211804260816\nValidation Accuracy: 0.9272717307903102, Validation F1 Score: 0.9234446464550599\nLearning Rate: 9.97792429226347e-19\n\nEpoch 331, Training Loss: 0.22131726145744324, Validation Loss: 0.224076047539711\nEpoch 331, Training Loss: 0.22131671011447906, Validation Loss: 0.224076047539711\nTraining Accuracy: 0.9278295423403672, Training F1 Score: 0.9237179679791377\nValidation Accuracy: 0.9273265373232489, Validation F1 Score: 0.9235213117693056\nLearning Rate: 9.976926544733711e-19\n\nEpoch 341, Training Loss: 0.22131232917308807, Validation Loss: 0.2241063416004181\nEpoch 341, Training Loss: 0.22131189703941345, Validation Loss: 0.2241063416004181\nTraining Accuracy: 0.9277884351877227, Training F1 Score: 0.9236793400859036\nValidation Accuracy: 0.9273265373232489, Validation F1 Score: 0.9235339476336928\nLearning Rate: 9.975928896974214e-19\n\nEpoch 351, Training Loss: 0.22130848467350006, Validation Loss: 0.22411800920963287\nEpoch 351, Training Loss: 0.22130811214447021, Validation Loss: 0.22411800920963287\nTraining Accuracy: 0.9278295423403672, Training F1 Score: 0.9237147548182124\nValidation Accuracy: 0.9273265373232489, Validation F1 Score: 0.9235339476336928\nLearning Rate: 9.974931348975003e-19\n\nEpoch 361, Training Loss: 0.22130480408668518, Validation Loss: 0.22413238883018494\nEpoch 361, Training Loss: 0.22130447626113892, Validation Loss: 0.22413238883018494\nTraining Accuracy: 0.9278706494930118, Training F1 Score: 0.9237565997557998\nValidation Accuracy: 0.9272717307903102, Validation F1 Score: 0.9234825994311815\nLearning Rate: 9.973933900726106e-19\n\nEpoch 371, Training Loss: 0.22130070626735687, Validation Loss: 0.22415460646152496\nEpoch 371, Training Loss: 0.22130043804645538, Validation Loss: 0.22415460646152496\nTraining Accuracy: 0.9278843518772266, Training F1 Score: 0.9237759000330742\nValidation Accuracy: 0.9272717307903102, Validation F1 Score: 0.9234825994311815\nLearning Rate: 9.972936552217542e-19\n\nEpoch 381, Training Loss: 0.2212972491979599, Validation Loss: 0.22416622936725616\nEpoch 381, Training Loss: 0.22129692137241364, Validation Loss: 0.22416622936725616\nTraining Accuracy: 0.9278843518772266, Training F1 Score: 0.9237694778780343\nValidation Accuracy: 0.9272717307903102, Validation F1 Score: 0.9234825994311815\nLearning Rate: 9.971939303439343e-19\n\nEpoch 391, Training Loss: 0.221293643116951, Validation Loss: 0.22417767345905304\nEpoch 391, Training Loss: 0.22129330039024353, Validation Loss: 0.22417767345905304\nTraining Accuracy: 0.9278980542614415, Training F1 Score: 0.9237855676130357\nValidation Accuracy: 0.9273813438561876, Validation F1 Score: 0.9235979233205089\nLearning Rate: 9.970942154381533e-19\n\nEpoch 401, Training Loss: 0.2212906777858734, Validation Loss: 0.2241932451725006\nEpoch 401, Training Loss: 0.22129032015800476, Validation Loss: 0.2241932451725006\nTraining Accuracy: 0.9278980542614415, Training F1 Score: 0.9237791445373259\nValidation Accuracy: 0.9273265373232489, Validation F1 Score: 0.9235339476336928\nLearning Rate: 9.969945105034144e-19\n\nEpoch 411, Training Loss: 0.22128766775131226, Validation Loss: 0.22421269118785858\nEpoch 411, Training Loss: 0.22128741443157196, Validation Loss: 0.22421269118785858\nTraining Accuracy: 0.9278980542614415, Training F1 Score: 0.923775931928607\nValidation Accuracy: 0.9273813438561876, Validation F1 Score: 0.9235853025684325\nLearning Rate: 9.968948155387201e-19\n\nEpoch 421, Training Loss: 0.2212848663330078, Validation Loss: 0.22426392138004303\nEpoch 421, Training Loss: 0.22128452360630035, Validation Loss: 0.22426392138004303\nTraining Accuracy: 0.9279665661825157, Training F1 Score: 0.9238531747942167\nValidation Accuracy: 0.9273813438561876, Validation F1 Score: 0.9235853025684325\nLearning Rate: 9.967951305430739e-19\n\nEpoch 431, Training Loss: 0.22128218412399292, Validation Loss: 0.2242903709411621\nEpoch 431, Training Loss: 0.22128188610076904, Validation Loss: 0.2242903709411621\nTraining Accuracy: 0.9279391614140861, Training F1 Score: 0.9238209946857813\nValidation Accuracy: 0.9274361503891264, Validation F1 Score: 0.9236240362680289\nLearning Rate: 9.966954555154783e-19\n\nEpoch 441, Training Loss: 0.2212793231010437, Validation Loss: 0.22431989014148712\nEpoch 441, Training Loss: 0.22127893567085266, Validation Loss: 0.22431989014148712\nTraining Accuracy: 0.9279117566456564, Training F1 Score: 0.9237952354179904\nValidation Accuracy: 0.9273813438561876, Validation F1 Score: 0.9235600275455571\nLearning Rate: 9.965957904549372e-19\n\nEpoch 451, Training Loss: 0.22127720713615417, Validation Loss: 0.22433406114578247\nEpoch 451, Training Loss: 0.2212771326303482, Validation Loss: 0.22433406114578247\nTraining Accuracy: 0.9278980542614415, Training F1 Score: 0.9237887780804087\nValidation Accuracy: 0.9272717307903102, Validation F1 Score: 0.9234446464550599\nLearning Rate: 9.964961353604537e-19\n\nEpoch 461, Training Loss: 0.2212756872177124, Validation Loss: 0.22434616088867188\nEpoch 461, Training Loss: 0.2212754487991333, Validation Loss: 0.22434616088867188\nTraining Accuracy: 0.9279117566456564, Training F1 Score: 0.9237952354179904\nValidation Accuracy: 0.9273265373232489, Validation F1 Score: 0.923508664721606\nLearning Rate: 9.963964902310311e-19\n\nEpoch 471, Training Loss: 0.22127346694469452, Validation Loss: 0.22437362372875214\nEpoch 471, Training Loss: 0.2212733030319214, Validation Loss: 0.22437362372875214\nTraining Accuracy: 0.9278980542614415, Training F1 Score: 0.9237887780804087\nValidation Accuracy: 0.9273265373232489, Validation F1 Score: 0.923496006478734\nLearning Rate: 9.962968550656731e-19\n\nEpoch 481, Training Loss: 0.22127163410186768, Validation Loss: 0.22438564896583557\nEpoch 481, Training Loss: 0.2212713360786438, Validation Loss: 0.22438564896583557\nTraining Accuracy: 0.9279391614140861, Training F1 Score: 0.9238242051061035\nValidation Accuracy: 0.9274361503891264, Validation F1 Score: 0.9236113971175293\nLearning Rate: 9.961972298633832e-19\n\nEpoch 491, Training Loss: 0.22126930952072144, Validation Loss: 0.22441117465496063\nEpoch 491, Training Loss: 0.2212691754102707, Validation Loss: 0.22441117465496063\nTraining Accuracy: 0.9279391614140861, Training F1 Score: 0.9238177835517976\nValidation Accuracy: 0.9273265373232489, Validation F1 Score: 0.923508664721606\nLearning Rate: 9.960976146231654e-19\n\nEpoch 501, Training Loss: 0.2212674915790558, Validation Loss: 0.224445179104805\nEpoch 501, Training Loss: 0.22126732766628265, Validation Loss: 0.224445179104805\nTraining Accuracy: 0.9279391614140861, Training F1 Score: 0.9238209946857813\nValidation Accuracy: 0.9273813438561876, Validation F1 Score: 0.9235600275455571\nLearning Rate: 9.959980093440233e-19\n\nEpoch 511, Training Loss: 0.22126564383506775, Validation Loss: 0.2244471162557602\nEpoch 511, Training Loss: 0.221265509724617, Validation Loss: 0.2244471162557602\nTraining Accuracy: 0.9279391614140861, Training F1 Score: 0.9238209946857813\nValidation Accuracy: 0.9274361503891264, Validation F1 Score: 0.9236240362680289\nLearning Rate: 9.958984140249608e-19\n\nEpoch 521, Training Loss: 0.22126440703868866, Validation Loss: 0.2244565784931183\nEpoch 521, Training Loss: 0.22126424312591553, Validation Loss: 0.2244565784931183\nTraining Accuracy: 0.9279391614140861, Training F1 Score: 0.923814571703961\nValidation Accuracy: 0.9273813438561876, Validation F1 Score: 0.9235600275455571\nLearning Rate: 9.957988286649822e-19\n\nEpoch 531, Training Loss: 0.22126272320747375, Validation Loss: 0.22446082532405853\nEpoch 531, Training Loss: 0.22126255929470062, Validation Loss: 0.22446082532405853\nTraining Accuracy: 0.9279528637983009, Training F1 Score: 0.9238370851344088\nValidation Accuracy: 0.9273813438561876, Validation F1 Score: 0.9235600275455571\nLearning Rate: 9.956992532630914e-19\n\nEpoch 541, Training Loss: 0.22126074135303497, Validation Loss: 0.22447597980499268\nEpoch 541, Training Loss: 0.22126060724258423, Validation Loss: 0.22447597980499268\nTraining Accuracy: 0.9279391614140861, Training F1 Score: 0.9238242051061035\nValidation Accuracy: 0.9273265373232489, Validation F1 Score: 0.923496006478734\nLearning Rate: 9.955996878182926e-19\n\nEpoch 551, Training Loss: 0.22125928103923798, Validation Loss: 0.22449125349521637\nEpoch 551, Training Loss: 0.22125910222530365, Validation Loss: 0.22449125349521637\nTraining Accuracy: 0.9279528637983009, Training F1 Score: 0.9238370851344088\nValidation Accuracy: 0.9273265373232489, Validation F1 Score: 0.923496006478734\nLearning Rate: 9.955001323295903e-19\n\nEpoch 561, Training Loss: 0.2212577760219574, Validation Loss: 0.22450990974903107\nEpoch 561, Training Loss: 0.22125759720802307, Validation Loss: 0.22450990974903107\nTraining Accuracy: 0.9279391614140861, Training F1 Score: 0.9238209946857813\nValidation Accuracy: 0.9272717307903102, Validation F1 Score: 0.9234446464550599\nLearning Rate: 9.95400586795989e-19\n\nEpoch 571, Training Loss: 0.2212563008069992, Validation Loss: 0.22451597452163696\nEpoch 571, Training Loss: 0.22125618159770966, Validation Loss: 0.22451597452163696\nTraining Accuracy: 0.9279391614140861, Training F1 Score: 0.9238209946857813\nValidation Accuracy: 0.9272717307903102, Validation F1 Score: 0.9234446464550599\nLearning Rate: 9.953010512164932e-19\n\nEpoch 581, Training Loss: 0.2212548404932022, Validation Loss: 0.22453615069389343\nEpoch 581, Training Loss: 0.2212546467781067, Validation Loss: 0.22453615069389343\nTraining Accuracy: 0.9279528637983009, Training F1 Score: 0.9238370851344088\nValidation Accuracy: 0.9273265373232489, Validation F1 Score: 0.923496006478734\nLearning Rate: 9.952015255901071e-19\n\nEpoch 591, Training Loss: 0.22125335037708282, Validation Loss: 0.22457779943943024\nEpoch 591, Training Loss: 0.2212531864643097, Validation Loss: 0.22457779943943024\nTraining Accuracy: 0.9279391614140861, Training F1 Score: 0.9238177835517976\nValidation Accuracy: 0.9272717307903102, Validation F1 Score: 0.9234573086408259\nLearning Rate: 9.951020099158362e-19\n\nEpoch 601, Training Loss: 0.2212519496679306, Validation Loss: 0.22459043562412262\nEpoch 601, Training Loss: 0.22125183045864105, Validation Loss: 0.22459043562412262\nTraining Accuracy: 0.9279254590298712, Training F1 Score: 0.9238016913466564\nValidation Accuracy: 0.9272717307903102, Validation F1 Score: 0.9234446464550599\nLearning Rate: 9.950025041926846e-19\n\nEpoch 611, Training Loss: 0.22125068306922913, Validation Loss: 0.22459307312965393\nEpoch 611, Training Loss: 0.22125062346458435, Validation Loss: 0.22459307312965393\nTraining Accuracy: 0.9279528637983009, Training F1 Score: 0.9238338749678099\nValidation Accuracy: 0.9272169242573715, Validation F1 Score: 0.923393293175183\nLearning Rate: 9.949030084196577e-19\n\nEpoch 621, Training Loss: 0.22124946117401123, Validation Loss: 0.22461220622062683\nEpoch 621, Training Loss: 0.22124932706356049, Validation Loss: 0.22461220622062683\nTraining Accuracy: 0.9279665661825157, Training F1 Score: 0.9238499655948189\nValidation Accuracy: 0.9272169242573715, Validation F1 Score: 0.923393293175183\nLearning Rate: 9.948035225957602e-19\n\nEpoch 631, Training Loss: 0.2212482988834381, Validation Loss: 0.2246352732181549\nEpoch 631, Training Loss: 0.22124812006950378, Validation Loss: 0.2246352732181549\nTraining Accuracy: 0.9279665661825157, Training F1 Score: 0.923843545055661\nValidation Accuracy: 0.9273265373232489, Validation F1 Score: 0.923508664721606\nLearning Rate: 9.947040467199976e-19\n\nEpoch 641, Training Loss: 0.22124721109867096, Validation Loss: 0.22464215755462646\nEpoch 641, Training Loss: 0.2212470918893814, Validation Loss: 0.22464215755462646\nTraining Accuracy: 0.9280213757193752, Training F1 Score: 0.9239014917590619\nValidation Accuracy: 0.9272169242573715, Validation F1 Score: 0.923393293175183\nLearning Rate: 9.946045807913749e-19\n\nEpoch 651, Training Loss: 0.22124607861042023, Validation Loss: 0.2246657907962799\nEpoch 651, Training Loss: 0.22124604880809784, Validation Loss: 0.2246657907962799\nTraining Accuracy: 0.9279802685667307, Training F1 Score: 0.9238596368285219\nValidation Accuracy: 0.9272169242573715, Validation F1 Score: 0.923393293175183\nLearning Rate: 9.945051248088975e-19\n\nEpoch 661, Training Loss: 0.22124521434307098, Validation Loss: 0.22468701004981995\nEpoch 661, Training Loss: 0.22124512493610382, Validation Loss: 0.22468701004981995\nTraining Accuracy: 0.9280213757193752, Training F1 Score: 0.9239014917590619\nValidation Accuracy: 0.9272717307903102, Validation F1 Score: 0.9234446464550599\nLearning Rate: 9.944056787715708e-19\n\nEpoch 671, Training Loss: 0.2212441861629486, Validation Loss: 0.22471562027931213\nEpoch 671, Training Loss: 0.22124403715133667, Validation Loss: 0.22471562027931213\nTraining Accuracy: 0.9280076733351603, Training F1 Score: 0.9238886095694548\nValidation Accuracy: 0.9271621177244328, Validation F1 Score: 0.9233038691339129\nLearning Rate: 9.943062426784003e-19\n\nEpoch 681, Training Loss: 0.22124303877353668, Validation Loss: 0.22474370896816254\nEpoch 681, Training Loss: 0.2212429791688919, Validation Loss: 0.22474370896816254\nTraining Accuracy: 0.9279939709509455, Training F1 Score: 0.923875727812264\nValidation Accuracy: 0.9272169242573715, Validation F1 Score: 0.923367927280742\nLearning Rate: 9.942068165283917e-19\n\nEpoch 691, Training Loss: 0.2212420403957367, Validation Loss: 0.22476831078529358\nEpoch 691, Training Loss: 0.22124192118644714, Validation Loss: 0.22476831078529358\nTraining Accuracy: 0.9280213757193752, Training F1 Score: 0.9238982828623967\nValidation Accuracy: 0.9272717307903102, Validation F1 Score: 0.9234319730597381\nLearning Rate: 9.941074003205508e-19\n\nEpoch 701, Training Loss: 0.2212410718202591, Validation Loss: 0.2247922122478485\nEpoch 701, Training Loss: 0.22124087810516357, Validation Loss: 0.2247922122478485\nTraining Accuracy: 0.9279939709509455, Training F1 Score: 0.923875727812264\nValidation Accuracy: 0.9273813438561876, Validation F1 Score: 0.9235600275455571\nLearning Rate: 9.940079940538831e-19\n\nEpoch 711, Training Loss: 0.22124020755290985, Validation Loss: 0.22480738162994385\nEpoch 711, Training Loss: 0.22124013304710388, Validation Loss: 0.22480738162994385\nTraining Accuracy: 0.9279802685667307, Training F1 Score: 0.9238628464874112\nValidation Accuracy: 0.9273265373232489, Validation F1 Score: 0.923496006478734\nLearning Rate: 9.939085977273949e-19\n\nEpoch 721, Training Loss: 0.221239373087883, Validation Loss: 0.2248179018497467\nEpoch 721, Training Loss: 0.22123923897743225, Validation Loss: 0.2248179018497467\nTraining Accuracy: 0.9279665661825157, Training F1 Score: 0.9238499655948189\nValidation Accuracy: 0.9273813438561876, Validation F1 Score: 0.9235600275455571\nLearning Rate: 9.93809211340092e-19\n\nEpoch 731, Training Loss: 0.22123877704143524, Validation Loss: 0.22483371198177338\nEpoch 731, Training Loss: 0.22123870253562927, Validation Loss: 0.22483371198177338\nTraining Accuracy: 0.9279528637983009, Training F1 Score: 0.9238306640875892\nValidation Accuracy: 0.9273265373232489, Validation F1 Score: 0.923496006478734\nLearning Rate: 9.937098348909806e-19\n\nEpoch 741, Training Loss: 0.22123795747756958, Validation Loss: 0.22485412657260895\nEpoch 741, Training Loss: 0.22123782336711884, Validation Loss: 0.22485412657260895\nTraining Accuracy: 0.9280213757193752, Training F1 Score: 0.9239046999424972\nValidation Accuracy: 0.9273265373232489, Validation F1 Score: 0.923496006478734\nLearning Rate: 9.93610468379067e-19\n\nEpoch 751, Training Loss: 0.2212371826171875, Validation Loss: 0.22486256062984467\nEpoch 751, Training Loss: 0.22123712301254272, Validation Loss: 0.22486256062984467\nTraining Accuracy: 0.9280213757193752, Training F1 Score: 0.9239014917590619\nValidation Accuracy: 0.9272717307903102, Validation F1 Score: 0.9234319730597381\nLearning Rate: 9.935111118033575e-19\n\nEpoch 761, Training Loss: 0.2212364226579666, Validation Loss: 0.2248888611793518\nEpoch 761, Training Loss: 0.221236452460289, Validation Loss: 0.2248888611793518\nTraining Accuracy: 0.9280076733351603, Training F1 Score: 0.9238918180070128\nValidation Accuracy: 0.9272717307903102, Validation F1 Score: 0.9234319730597381\nLearning Rate: 9.934117651628583e-19\n\nEpoch 771, Training Loss: 0.22123588621616364, Validation Loss: 0.22490015625953674\nEpoch 771, Training Loss: 0.22123582661151886, Validation Loss: 0.22490015625953674\nTraining Accuracy: 0.9279939709509455, Training F1 Score: 0.9238789365038564\nValidation Accuracy: 0.9274361503891264, Validation F1 Score: 0.9236240362680289\nLearning Rate: 9.933124284565761e-19\n\nEpoch 781, Training Loss: 0.22123517096042633, Validation Loss: 0.22492507100105286\nEpoch 781, Training Loss: 0.22123517096042633, Validation Loss: 0.22492507100105286\nTraining Accuracy: 0.9279939709509455, Training F1 Score: 0.923875727812264\nValidation Accuracy: 0.9274361503891264, Validation F1 Score: 0.9236366642402419\nLearning Rate: 9.932131016835178e-19\n\nEpoch 791, Training Loss: 0.22123442590236664, Validation Loss: 0.22493933141231537\nEpoch 791, Training Loss: 0.22123433649539948, Validation Loss: 0.22493933141231537\nTraining Accuracy: 0.9279939709509455, Training F1 Score: 0.9238789365038564\nValidation Accuracy: 0.9273265373232489, Validation F1 Score: 0.923496006478734\nLearning Rate: 9.931137848426896e-19\n\nEpoch 801, Training Loss: 0.22123397886753082, Validation Loss: 0.22495102882385254\nEpoch 801, Training Loss: 0.2212340086698532, Validation Loss: 0.22495102882385254\nTraining Accuracy: 0.9279939709509455, Training F1 Score: 0.923875727812264\nValidation Accuracy: 0.9272717307903102, Validation F1 Score: 0.9234319730597381\nLearning Rate: 9.930144779330986e-19\n\nEpoch 811, Training Loss: 0.22123345732688904, Validation Loss: 0.2249528467655182\nEpoch 811, Training Loss: 0.2212333381175995, Validation Loss: 0.2249528467655182\nTraining Accuracy: 0.9280076733351603, Training F1 Score: 0.9238886095694548\nValidation Accuracy: 0.9273813438561876, Validation F1 Score: 0.9235600275455571\nLearning Rate: 9.929151809537517e-19\n\nEpoch 821, Training Loss: 0.22123292088508606, Validation Loss: 0.22495640814304352\nEpoch 821, Training Loss: 0.2212328165769577, Validation Loss: 0.22495640814304352\nTraining Accuracy: 0.9279939709509455, Training F1 Score: 0.923875727812264\nValidation Accuracy: 0.9273813438561876, Validation F1 Score: 0.9235600275455571\nLearning Rate: 9.92815893903656e-19\n\nEpoch 831, Training Loss: 0.22123205661773682, Validation Loss: 0.22498129308223724\nEpoch 831, Training Loss: 0.22123201191425323, Validation Loss: 0.22498129308223724\nTraining Accuracy: 0.9280076733351603, Training F1 Score: 0.9238886095694548\nValidation Accuracy: 0.9273813438561876, Validation F1 Score: 0.9235600275455571\nLearning Rate: 9.927166167818184e-19\n\nEpoch 841, Training Loss: 0.2212313711643219, Validation Loss: 0.2249949723482132\nEpoch 841, Training Loss: 0.22123131155967712, Validation Loss: 0.2249949723482132\nTraining Accuracy: 0.9280076733351603, Training F1 Score: 0.9238886095694548\nValidation Accuracy: 0.9273813438561876, Validation F1 Score: 0.9235600275455571\nLearning Rate: 9.926173495872465e-19\n\nEpoch 851, Training Loss: 0.22123081982135773, Validation Loss: 0.2249978631734848\nEpoch 851, Training Loss: 0.22123070061206818, Validation Loss: 0.2249978631734848\nTraining Accuracy: 0.9280076733351603, Training F1 Score: 0.9238918180070128\nValidation Accuracy: 0.9272717307903102, Validation F1 Score: 0.9234446464550599\nLearning Rate: 9.92518092318947e-19\n\nEpoch 861, Training Loss: 0.22123023867607117, Validation Loss: 0.22500669956207275\nEpoch 861, Training Loss: 0.221230149269104, Validation Loss: 0.22500669956207275\nTraining Accuracy: 0.9279939709509455, Training F1 Score: 0.923875727812264\nValidation Accuracy: 0.9273813438561876, Validation F1 Score: 0.9235600275455571\nLearning Rate: 9.92418844975928e-19\n\nEpoch 871, Training Loss: 0.22122962772846222, Validation Loss: 0.22501803934574127\nEpoch 871, Training Loss: 0.22122958302497864, Validation Loss: 0.22501803934574127\nTraining Accuracy: 0.9279802685667307, Training F1 Score: 0.9238628464874112\nValidation Accuracy: 0.9273265373232489, Validation F1 Score: 0.923496006478734\nLearning Rate: 9.923196075571966e-19\n\nEpoch 881, Training Loss: 0.22122910618782043, Validation Loss: 0.2250317931175232\nEpoch 881, Training Loss: 0.22122904658317566, Validation Loss: 0.2250317931175232\nTraining Accuracy: 0.9279939709509455, Training F1 Score: 0.923875727812264\nValidation Accuracy: 0.9273265373232489, Validation F1 Score: 0.923496006478734\nLearning Rate: 9.922203800617604e-19\n\nEpoch 891, Training Loss: 0.22122842073440552, Validation Loss: 0.22505508363246918\nEpoch 891, Training Loss: 0.22122839093208313, Validation Loss: 0.22505508363246918\nTraining Accuracy: 0.9279665661825157, Training F1 Score: 0.9238499655948189\nValidation Accuracy: 0.9274361503891264, Validation F1 Score: 0.9236240362680289\nLearning Rate: 9.921211624886273e-19\n\nEpoch 901, Training Loss: 0.22122791409492493, Validation Loss: 0.2250758409500122\nEpoch 901, Training Loss: 0.22122780978679657, Validation Loss: 0.2250758409500122\nTraining Accuracy: 0.9279939709509455, Training F1 Score: 0.9238789365038564\nValidation Accuracy: 0.9273813438561876, Validation F1 Score: 0.9235600275455571\nLearning Rate: 9.920219548368051e-19\n\nEpoch 911, Training Loss: 0.22122742235660553, Validation Loss: 0.22508767247200012\nEpoch 911, Training Loss: 0.22122739255428314, Validation Loss: 0.22508767247200012\nTraining Accuracy: 0.9279939709509455, Training F1 Score: 0.9238725184073614\nValidation Accuracy: 0.9274361503891264, Validation F1 Score: 0.9236240362680289\nLearning Rate: 9.919227571053017e-19\n\nEpoch 921, Training Loss: 0.22122681140899658, Validation Loss: 0.22510844469070435\nEpoch 921, Training Loss: 0.2212267965078354, Validation Loss: 0.22510844469070435\nTraining Accuracy: 0.9279665661825157, Training F1 Score: 0.9238531747942167\nValidation Accuracy: 0.9273813438561876, Validation F1 Score: 0.9235600275455571\nLearning Rate: 9.91823569293125e-19\n\nEpoch 931, Training Loss: 0.22122637927532196, Validation Loss: 0.22512727975845337\nEpoch 931, Training Loss: 0.22122624516487122, Validation Loss: 0.22512727975845337\nTraining Accuracy: 0.9279665661825157, Training F1 Score: 0.9238531747942167\nValidation Accuracy: 0.9274361503891264, Validation F1 Score: 0.9236240362680289\nLearning Rate: 9.917243913992832e-19\n\nEpoch 941, Training Loss: 0.2212258130311966, Validation Loss: 0.2251441925764084\nEpoch 941, Training Loss: 0.22122584283351898, Validation Loss: 0.2251441925764084\nTraining Accuracy: 0.9279528637983009, Training F1 Score: 0.9238338749678099\nValidation Accuracy: 0.9273813438561876, Validation F1 Score: 0.9235600275455571\nLearning Rate: 9.916252234227844e-19\n\nEpoch 951, Training Loss: 0.22122544050216675, Validation Loss: 0.22515733540058136\nEpoch 951, Training Loss: 0.22122538089752197, Validation Loss: 0.22515733540058136\nTraining Accuracy: 0.9279665661825157, Training F1 Score: 0.9238499655948189\nValidation Accuracy: 0.9273813438561876, Validation F1 Score: 0.9235600275455571\nLearning Rate: 9.915260653626371e-19\n\nEpoch 961, Training Loss: 0.22122499346733093, Validation Loss: 0.22518374025821686\nEpoch 961, Training Loss: 0.22122491896152496, Validation Loss: 0.22518374025821686\nTraining Accuracy: 0.9279802685667307, Training F1 Score: 0.9238628464874112\nValidation Accuracy: 0.9273813438561876, Validation F1 Score: 0.9235600275455571\nLearning Rate: 9.914269172178496e-19\n\nEpoch 971, Training Loss: 0.2212245911359787, Validation Loss: 0.225205659866333\nEpoch 971, Training Loss: 0.22122453153133392, Validation Loss: 0.225205659866333\nTraining Accuracy: 0.9279665661825157, Training F1 Score: 0.9238499655948189\nValidation Accuracy: 0.9273813438561876, Validation F1 Score: 0.9235600275455571\nLearning Rate: 9.913277789874304e-19\n\nEpoch 981, Training Loss: 0.22122417390346527, Validation Loss: 0.22521968185901642\nEpoch 981, Training Loss: 0.22122414410114288, Validation Loss: 0.22521968185901642\nTraining Accuracy: 0.9279528637983009, Training F1 Score: 0.9238370851344088\nValidation Accuracy: 0.9273813438561876, Validation F1 Score: 0.9235600275455571\nLearning Rate: 9.912286506703883e-19\n\nEpoch 991, Training Loss: 0.22122375667095184, Validation Loss: 0.22523874044418335\nEpoch 991, Training Loss: 0.22122369706630707, Validation Loss: 0.22523874044418335\nTraining Accuracy: 0.9279802685667307, Training F1 Score: 0.9238596368285219\nValidation Accuracy: 0.9274361503891264, Validation F1 Score: 0.9236240362680289\nLearning Rate: 9.911295322657315e-19\n\nEpoch 1001, Training Loss: 0.2212233543395996, Validation Loss: 0.2252558171749115\nEpoch 1001, Training Loss: 0.22122333943843842, Validation Loss: 0.2252558171749115\nTraining Accuracy: 0.9279802685667307, Training F1 Score: 0.9238628464874112\nValidation Accuracy: 0.9273813438561876, Validation F1 Score: 0.9235600275455571\nLearning Rate: 9.910304237724694e-19\n\nEpoch 1011, Training Loss: 0.2212228775024414, Validation Loss: 0.2252768725156784\nEpoch 1011, Training Loss: 0.2212228775024414, Validation Loss: 0.2252768725156784\nTraining Accuracy: 0.9279665661825157, Training F1 Score: 0.9238467556820308\nValidation Accuracy: 0.9274361503891264, Validation F1 Score: 0.9236240362680289\nLearning Rate: 9.909313251896106e-19\n\nEpoch 1021, Training Loss: 0.2212224006652832, Validation Loss: 0.22530998289585114\nEpoch 1021, Training Loss: 0.22122229635715485, Validation Loss: 0.22530998289585114\nTraining Accuracy: 0.9279528637983009, Training F1 Score: 0.9238338749678099\nValidation Accuracy: 0.9273813438561876, Validation F1 Score: 0.9235600275455571\nLearning Rate: 9.908322365161643e-19\n\nEpoch 1031, Training Loss: 0.22122198343276978, Validation Loss: 0.22531059384346008\nEpoch 1031, Training Loss: 0.22122201323509216, Validation Loss: 0.22531059384346008\nTraining Accuracy: 0.9279528637983009, Training F1 Score: 0.9238338749678099\nValidation Accuracy: 0.9273813438561876, Validation F1 Score: 0.9235600275455571\nLearning Rate: 9.907331577511392e-19\n\nEpoch 1041, Training Loss: 0.22122153639793396, Validation Loss: 0.22532165050506592\nEpoch 1041, Training Loss: 0.22122150659561157, Validation Loss: 0.22532165050506592\nTraining Accuracy: 0.9279802685667307, Training F1 Score: 0.9238628464874112\nValidation Accuracy: 0.9273813438561876, Validation F1 Score: 0.9235600275455571\nLearning Rate: 9.906340888935448e-19\n\nEpoch 1051, Training Loss: 0.2212209552526474, Validation Loss: 0.2253648042678833\nEpoch 1051, Training Loss: 0.22122082114219666, Validation Loss: 0.2253648042678833\nTraining Accuracy: 0.9279391614140861, Training F1 Score: 0.9238209946857813\nValidation Accuracy: 0.9273813438561876, Validation F1 Score: 0.9235726706473959\nLearning Rate: 9.905350299423904e-19\n\nEpoch 1061, Training Loss: 0.22122056782245636, Validation Loss: 0.2253577709197998\nEpoch 1061, Training Loss: 0.22122056782245636, Validation Loss: 0.2253577709197998\nTraining Accuracy: 0.9279528637983009, Training F1 Score: 0.9238338749678099\nValidation Accuracy: 0.9274361503891264, Validation F1 Score: 0.9236240362680289\nLearning Rate: 9.904359808966854e-19\n\nEpoch 1071, Training Loss: 0.22122031450271606, Validation Loss: 0.22535517811775208\nEpoch 1071, Training Loss: 0.2212202399969101, Validation Loss: 0.22535517811775208\nTraining Accuracy: 0.9279528637983009, Training F1 Score: 0.9238306640875892\nValidation Accuracy: 0.9274361503891264, Validation F1 Score: 0.9236240362680289\nLearning Rate: 9.903369417554393e-19\n\nEpoch 1081, Training Loss: 0.22121992707252502, Validation Loss: 0.2253665030002594\nEpoch 1081, Training Loss: 0.22121991217136383, Validation Loss: 0.2253665030002594\nTraining Accuracy: 0.9279528637983009, Training F1 Score: 0.9238306640875892\nValidation Accuracy: 0.9273813438561876, Validation F1 Score: 0.9235600275455571\nLearning Rate: 9.902379125176616e-19\n\nEpoch 1091, Training Loss: 0.22121953964233398, Validation Loss: 0.22538645565509796\nEpoch 1091, Training Loss: 0.2212195098400116, Validation Loss: 0.22538645565509796\nTraining Accuracy: 0.9279665661825157, Training F1 Score: 0.9238499655948189\nValidation Accuracy: 0.9274361503891264, Validation F1 Score: 0.9236240362680289\nLearning Rate: 9.901388931823621e-19\n\nEpoch 1101, Training Loss: 0.2212190181016922, Validation Loss: 0.22541987895965576\nEpoch 1101, Training Loss: 0.2212190330028534, Validation Loss: 0.22541987895965576\nTraining Accuracy: 0.9279665661825157, Training F1 Score: 0.9238499655948189\nValidation Accuracy: 0.9273813438561876, Validation F1 Score: 0.9235726706473959\nLearning Rate: 9.900398837485506e-19\n\nEpoch 1111, Training Loss: 0.22121869027614594, Validation Loss: 0.22542349994182587\nEpoch 1111, Training Loss: 0.22121866047382355, Validation Loss: 0.22542349994182587\nTraining Accuracy: 0.9279528637983009, Training F1 Score: 0.9238306640875892\nValidation Accuracy: 0.9273813438561876, Validation F1 Score: 0.9235726706473959\nLearning Rate: 9.899408842152367e-19\n\nEpoch 1121, Training Loss: 0.2212182730436325, Validation Loss: 0.22544518113136292\nEpoch 1121, Training Loss: 0.2212182581424713, Validation Loss: 0.22544518113136292\nTraining Accuracy: 0.9279254590298712, Training F1 Score: 0.9238081148358674\nValidation Accuracy: 0.9274361503891264, Validation F1 Score: 0.9236366642402419\nLearning Rate: 9.898418945814308e-19\n\nEpoch 1131, Training Loss: 0.22121797502040863, Validation Loss: 0.22544774413108826\nEpoch 1131, Training Loss: 0.22121796011924744, Validation Loss: 0.22544774413108826\nTraining Accuracy: 0.9279939709509455, Training F1 Score: 0.923875727812264\nValidation Accuracy: 0.9274361503891264, Validation F1 Score: 0.9236366642402419\nLearning Rate: 9.897429148461429e-19\n\nEpoch 1141, Training Loss: 0.22121773660182953, Validation Loss: 0.22545962035655975\nEpoch 1141, Training Loss: 0.22121772170066833, Validation Loss: 0.22545962035655975\nTraining Accuracy: 0.9279802685667307, Training F1 Score: 0.9238628464874112\nValidation Accuracy: 0.9274361503891264, Validation F1 Score: 0.9236366642402419\nLearning Rate: 9.89643945008383e-19\n\nEpoch 1151, Training Loss: 0.22121740877628326, Validation Loss: 0.22547924518585205\nEpoch 1151, Training Loss: 0.22121742367744446, Validation Loss: 0.22547924518585205\nTraining Accuracy: 0.9279665661825157, Training F1 Score: 0.9238499655948189\nValidation Accuracy: 0.9274361503891264, Validation F1 Score: 0.9236366642402419\nLearning Rate: 9.895449850671618e-19\n\nEpoch 1161, Training Loss: 0.2212170660495758, Validation Loss: 0.225502610206604\nEpoch 1161, Training Loss: 0.22121702134609222, Validation Loss: 0.225502610206604\nTraining Accuracy: 0.9279391614140861, Training F1 Score: 0.9238209946857813\nValidation Accuracy: 0.9274361503891264, Validation F1 Score: 0.9236366642402419\nLearning Rate: 9.894460350214891e-19\n\nEpoch 1171, Training Loss: 0.22121679782867432, Validation Loss: 0.22550977766513824\nEpoch 1171, Training Loss: 0.2212168127298355, Validation Loss: 0.22550977766513824\nTraining Accuracy: 0.9279528637983009, Training F1 Score: 0.9238370851344088\nValidation Accuracy: 0.9274361503891264, Validation F1 Score: 0.9236366642402419\nLearning Rate: 9.89347094870376e-19\n\nEpoch 1181, Training Loss: 0.22121641039848328, Validation Loss: 0.22554253041744232\nEpoch 1181, Training Loss: 0.22121639549732208, Validation Loss: 0.22554253041744232\nTraining Accuracy: 0.9279391614140861, Training F1 Score: 0.9238242051061035\nValidation Accuracy: 0.9273813438561876, Validation F1 Score: 0.9235853025684325\nLearning Rate: 9.892481646128325e-19\n\nEpoch 1191, Training Loss: 0.22121627628803253, Validation Loss: 0.22555269300937653\nEpoch 1191, Training Loss: 0.22121623158454895, Validation Loss: 0.22555269300937653\nTraining Accuracy: 0.9279528637983009, Training F1 Score: 0.9238370851344088\nValidation Accuracy: 0.9274909569220651, Validation F1 Score: 0.9237006455079518\nLearning Rate: 9.891492442478697e-19\n\nEpoch 1201, Training Loss: 0.2212158888578415, Validation Loss: 0.22558379173278809\nEpoch 1201, Training Loss: 0.2212158888578415, Validation Loss: 0.22558379173278809\nTraining Accuracy: 0.9279391614140861, Training F1 Score: 0.9238209946857813\nValidation Accuracy: 0.9273813438561876, Validation F1 Score: 0.9235979233205089\nLearning Rate: 9.890503337744983e-19\n\nEpoch 1211, Training Loss: 0.2212156057357788, Validation Loss: 0.22561293840408325\nEpoch 1211, Training Loss: 0.22121556103229523, Validation Loss: 0.22561293840408325\nTraining Accuracy: 0.9279254590298712, Training F1 Score: 0.9238081148358674\nValidation Accuracy: 0.9273813438561876, Validation F1 Score: 0.9235979233205089\nLearning Rate: 9.889514331917292e-19\n\nEpoch 1221, Training Loss: 0.22121533751487732, Validation Loss: 0.22562524676322937\nEpoch 1221, Training Loss: 0.22121523320674896, Validation Loss: 0.22562524676322937\nTraining Accuracy: 0.9279254590298712, Training F1 Score: 0.9238081148358674\nValidation Accuracy: 0.9274361503891264, Validation F1 Score: 0.9236492810460101\nLearning Rate: 9.888525424985732e-19\n\nEpoch 1231, Training Loss: 0.22121505439281464, Validation Loss: 0.2256440371274948\nEpoch 1231, Training Loss: 0.22121503949165344, Validation Loss: 0.2256440371274948\nTraining Accuracy: 0.9279254590298712, Training F1 Score: 0.9238081148358674\nValidation Accuracy: 0.9273813438561876, Validation F1 Score: 0.9235979233205089\nLearning Rate: 9.887536616940414e-19\n\nEpoch 1241, Training Loss: 0.22121478617191315, Validation Loss: 0.22564898431301117\nEpoch 1241, Training Loss: 0.22121469676494598, Validation Loss: 0.22564898431301117\nTraining Accuracy: 0.9278980542614415, Training F1 Score: 0.9237791445373259\nValidation Accuracy: 0.9273813438561876, Validation F1 Score: 0.9235979233205089\nLearning Rate: 9.886547907771453e-19\n\nEpoch 1251, Training Loss: 0.22121454775333405, Validation Loss: 0.22566713392734528\nEpoch 1251, Training Loss: 0.22121448814868927, Validation Loss: 0.22566713392734528\nTraining Accuracy: 0.9279117566456564, Training F1 Score: 0.9237920237767439\nValidation Accuracy: 0.9273813438561876, Validation F1 Score: 0.9235979233205089\nLearning Rate: 9.88555929746896e-19\n\nEpoch 1261, Training Loss: 0.2212141901254654, Validation Loss: 0.22567786276340485\nEpoch 1261, Training Loss: 0.221214160323143, Validation Loss: 0.22567786276340485\nTraining Accuracy: 0.9279391614140861, Training F1 Score: 0.9238209946857813\nValidation Accuracy: 0.9273813438561876, Validation F1 Score: 0.9235979233205089\nLearning Rate: 9.88457078602305e-19\n\nEpoch 1271, Training Loss: 0.22121398150920868, Validation Loss: 0.22567813098430634\nEpoch 1271, Training Loss: 0.2212139517068863, Validation Loss: 0.22567813098430634\nTraining Accuracy: 0.9279117566456564, Training F1 Score: 0.9237888114215644\nValidation Accuracy: 0.9273813438561876, Validation F1 Score: 0.9235979233205089\nLearning Rate: 9.883582373423832e-19\n\nEpoch 1281, Training Loss: 0.22121359407901764, Validation Loss: 0.22570309042930603\nEpoch 1281, Training Loss: 0.22121348977088928, Validation Loss: 0.22570309042930603\nTraining Accuracy: 0.9279117566456564, Training F1 Score: 0.9237888114215644\nValidation Accuracy: 0.9273813438561876, Validation F1 Score: 0.9235979233205089\nLearning Rate: 9.88259405966143e-19\n\nEpoch 1291, Training Loss: 0.2212131917476654, Validation Loss: 0.22571928799152374\nEpoch 1291, Training Loss: 0.2212131917476654, Validation Loss: 0.22571928799152374\nTraining Accuracy: 0.9279117566456564, Training F1 Score: 0.9237888114215644\nValidation Accuracy: 0.9273813438561876, Validation F1 Score: 0.9235979233205089\nLearning Rate: 9.881605844725955e-19\n\nEpoch 1301, Training Loss: 0.22121299803256989, Validation Loss: 0.2257232815027237\nEpoch 1301, Training Loss: 0.2212129831314087, Validation Loss: 0.2257232815027237\nTraining Accuracy: 0.9279117566456564, Training F1 Score: 0.9237888114215644\nValidation Accuracy: 0.9273813438561876, Validation F1 Score: 0.9235979233205089\nLearning Rate: 9.880617728607528e-19\n\nEpoch 1311, Training Loss: 0.22121283411979675, Validation Loss: 0.22572574019432068\nEpoch 1311, Training Loss: 0.22121281921863556, Validation Loss: 0.22572574019432068\nTraining Accuracy: 0.9279117566456564, Training F1 Score: 0.9237888114215644\nValidation Accuracy: 0.9273813438561876, Validation F1 Score: 0.9235979233205089\nLearning Rate: 9.879629711296266e-19\n\nEpoch 1321, Training Loss: 0.22121252119541168, Validation Loss: 0.2257477343082428\nEpoch 1321, Training Loss: 0.2212124615907669, Validation Loss: 0.2257477343082428\nTraining Accuracy: 0.9279391614140861, Training F1 Score: 0.9238177835517976\nValidation Accuracy: 0.9273813438561876, Validation F1 Score: 0.9235979233205089\nLearning Rate: 9.87864179278229e-19\n\nEpoch 1331, Training Loss: 0.22121235728263855, Validation Loss: 0.22574888169765472\nEpoch 1331, Training Loss: 0.22121229767799377, Validation Loss: 0.22574888169765472\nTraining Accuracy: 0.9279254590298712, Training F1 Score: 0.9238049034482085\nValidation Accuracy: 0.9273813438561876, Validation F1 Score: 0.9235979233205089\nLearning Rate: 9.877653973055717e-19\n\nEpoch 1341, Training Loss: 0.22121213376522064, Validation Loss: 0.225758358836174\nEpoch 1341, Training Loss: 0.22121211886405945, Validation Loss: 0.225758358836174\nTraining Accuracy: 0.9279254590298712, Training F1 Score: 0.9238049034482085\nValidation Accuracy: 0.9273813438561876, Validation F1 Score: 0.9235979233205089\nLearning Rate: 9.876666252106673e-19\n\nEpoch 1351, Training Loss: 0.22121194005012512, Validation Loss: 0.22577334940433502\nEpoch 1351, Training Loss: 0.22121182084083557, Validation Loss: 0.22577334940433502\nTraining Accuracy: 0.9279254590298712, Training F1 Score: 0.9238049034482085\nValidation Accuracy: 0.9273813438561876, Validation F1 Score: 0.9235979233205089\nLearning Rate: 9.875678629925282e-19\n\nEpoch 1361, Training Loss: 0.22121168673038483, Validation Loss: 0.22580021619796753\nEpoch 1361, Training Loss: 0.22121165692806244, Validation Loss: 0.22580021619796753\nTraining Accuracy: 0.9279254590298712, Training F1 Score: 0.9238049034482085\nValidation Accuracy: 0.9273813438561876, Validation F1 Score: 0.9235979233205089\nLearning Rate: 9.874691106501662e-19\n\nEpoch 1371, Training Loss: 0.2212114930152893, Validation Loss: 0.22581124305725098\nEpoch 1371, Training Loss: 0.2212114781141281, Validation Loss: 0.22581124305725098\nTraining Accuracy: 0.9279254590298712, Training F1 Score: 0.9238049034482085\nValidation Accuracy: 0.9273813438561876, Validation F1 Score: 0.9235979233205089\nLearning Rate: 9.87370368182594e-19\n\nEpoch 1381, Training Loss: 0.22121131420135498, Validation Loss: 0.22582805156707764\nEpoch 1381, Training Loss: 0.2212112694978714, Validation Loss: 0.22582805156707764\nTraining Accuracy: 0.9278980542614415, Training F1 Score: 0.923775931928607\nValidation Accuracy: 0.9273813438561876, Validation F1 Score: 0.9235979233205089\nLearning Rate: 9.872716355888246e-19\n\nEpoch 1391, Training Loss: 0.2212110161781311, Validation Loss: 0.22584784030914307\nEpoch 1391, Training Loss: 0.22121095657348633, Validation Loss: 0.22584784030914307\nTraining Accuracy: 0.9279117566456564, Training F1 Score: 0.9237888114215644\nValidation Accuracy: 0.9273813438561876, Validation F1 Score: 0.9235979233205089\nLearning Rate: 9.871729128678699e-19\n\nEpoch 1401, Training Loss: 0.22121082246303558, Validation Loss: 0.22585271298885345\nEpoch 1401, Training Loss: 0.22121082246303558, Validation Loss: 0.22585271298885345\nTraining Accuracy: 0.9278980542614415, Training F1 Score: 0.923775931928607\nValidation Accuracy: 0.9273813438561876, Validation F1 Score: 0.9235979233205089\nLearning Rate: 9.870742000187433e-19\n\nEpoch 1411, Training Loss: 0.22121065855026245, Validation Loss: 0.22586219012737274\nEpoch 1411, Training Loss: 0.22121062874794006, Validation Loss: 0.22586219012737274\nTraining Accuracy: 0.9279117566456564, Training F1 Score: 0.9237920237767439\nValidation Accuracy: 0.9273813438561876, Validation F1 Score: 0.9235979233205089\nLearning Rate: 9.869754970404574e-19\n\nEpoch 1421, Training Loss: 0.22121049463748932, Validation Loss: 0.22587499022483826\nEpoch 1421, Training Loss: 0.22121049463748932, Validation Loss: 0.22587499022483826\nTraining Accuracy: 0.9279254590298712, Training F1 Score: 0.9238016913466564\nValidation Accuracy: 0.9273813438561876, Validation F1 Score: 0.9235979233205089\nLearning Rate: 9.86876803932025e-19\n\nEpoch 1431, Training Loss: 0.22121036052703857, Validation Loss: 0.2258908450603485\nEpoch 1431, Training Loss: 0.2212103009223938, Validation Loss: 0.2258908450603485\nTraining Accuracy: 0.9279117566456564, Training F1 Score: 0.9237888114215644\nValidation Accuracy: 0.9273813438561876, Validation F1 Score: 0.9235979233205089\nLearning Rate: 9.867781206924594e-19\n\nEpoch 1441, Training Loss: 0.2212100476026535, Validation Loss: 0.22590987384319305\nEpoch 1441, Training Loss: 0.22120998799800873, Validation Loss: 0.22590987384319305\nTraining Accuracy: 0.9279117566456564, Training F1 Score: 0.9237888114215644\nValidation Accuracy: 0.9273813438561876, Validation F1 Score: 0.9235979233205089\nLearning Rate: 9.866794473207738e-19\n\nEpoch 1451, Training Loss: 0.2212098389863968, Validation Loss: 0.22592253983020782\nEpoch 1451, Training Loss: 0.2212097942829132, Validation Loss: 0.22592253983020782\nTraining Accuracy: 0.9278980542614415, Training F1 Score: 0.923775931928607\nValidation Accuracy: 0.9273813438561876, Validation F1 Score: 0.9235979233205089\nLearning Rate: 9.865807838159814e-19\n\nEpoch 1461, Training Loss: 0.22120970487594604, Validation Loss: 0.22592294216156006\nEpoch 1461, Training Loss: 0.22120961546897888, Validation Loss: 0.22592294216156006\nTraining Accuracy: 0.9278980542614415, Training F1 Score: 0.923775931928607\nValidation Accuracy: 0.9273813438561876, Validation F1 Score: 0.9235979233205089\nLearning Rate: 9.864821301770951e-19\n\nEpoch 1471, Training Loss: 0.22120946645736694, Validation Loss: 0.22593005001544952\nEpoch 1471, Training Loss: 0.22120948135852814, Validation Loss: 0.22593005001544952\nTraining Accuracy: 0.9279254590298712, Training F1 Score: 0.9238016913466564\nValidation Accuracy: 0.9273813438561876, Validation F1 Score: 0.9235979233205089\nLearning Rate: 9.86383486403129e-19\n\nEpoch 1481, Training Loss: 0.22120940685272217, Validation Loss: 0.22593894600868225\nEpoch 1481, Training Loss: 0.2212093323469162, Validation Loss: 0.22593894600868225\nTraining Accuracy: 0.9279254590298712, Training F1 Score: 0.9238016913466564\nValidation Accuracy: 0.9273813438561876, Validation F1 Score: 0.9235979233205089\nLearning Rate: 9.862848524930967e-19\n\nEpoch 1491, Training Loss: 0.22120918333530426, Validation Loss: 0.22596412897109985\nEpoch 1491, Training Loss: 0.22120903432369232, Validation Loss: 0.22596412897109985\nTraining Accuracy: 0.9279254590298712, Training F1 Score: 0.9238016913466564\nValidation Accuracy: 0.9273813438561876, Validation F1 Score: 0.9235979233205089\nLearning Rate: 9.861862284460112e-19\n\nEpoch 1501, Training Loss: 0.2212088704109192, Validation Loss: 0.2259775847196579\nEpoch 1501, Training Loss: 0.22120890021324158, Validation Loss: 0.2259775847196579\nTraining Accuracy: 0.9279117566456564, Training F1 Score: 0.9237888114215644\nValidation Accuracy: 0.9273813438561876, Validation F1 Score: 0.9235979233205089\nLearning Rate: 9.860876142608867e-19\n\nEpoch 1511, Training Loss: 0.22120873630046844, Validation Loss: 0.2259870022535324\nEpoch 1511, Training Loss: 0.22120872139930725, Validation Loss: 0.2259870022535324\nTraining Accuracy: 0.9279117566456564, Training F1 Score: 0.9237888114215644\nValidation Accuracy: 0.9273813438561876, Validation F1 Score: 0.9235979233205089\nLearning Rate: 9.859890099367371e-19\n\nEpoch 1521, Training Loss: 0.22120848298072815, Validation Loss: 0.22600822150707245\nEpoch 1521, Training Loss: 0.22120848298072815, Validation Loss: 0.22600822150707245\nTraining Accuracy: 0.9279117566456564, Training F1 Score: 0.9237888114215644\nValidation Accuracy: 0.9273813438561876, Validation F1 Score: 0.9235979233205089\nLearning Rate: 9.858904154725761e-19\n\nEpoch 1531, Training Loss: 0.2212081253528595, Validation Loss: 0.22604170441627502\nEpoch 1531, Training Loss: 0.22120818495750427, Validation Loss: 0.22604170441627502\nTraining Accuracy: 0.9279254590298712, Training F1 Score: 0.9238016913466564\nValidation Accuracy: 0.9273813438561876, Validation F1 Score: 0.9235979233205089\nLearning Rate: 9.857918308674179e-19\n\nEpoch 1541, Training Loss: 0.22120806574821472, Validation Loss: 0.22603894770145416\nEpoch 1541, Training Loss: 0.22120802104473114, Validation Loss: 0.22603894770145416\nTraining Accuracy: 0.9279117566456564, Training F1 Score: 0.9237888114215644\nValidation Accuracy: 0.9274361503891264, Validation F1 Score: 0.9236492810460101\nLearning Rate: 9.856932561202765e-19\n\nEpoch 1551, Training Loss: 0.221207857131958, Validation Loss: 0.22605422139167786\nEpoch 1551, Training Loss: 0.22120784223079681, Validation Loss: 0.22605422139167786\nTraining Accuracy: 0.9279254590298712, Training F1 Score: 0.9238016913466564\nValidation Accuracy: 0.9273813438561876, Validation F1 Score: 0.9235979233205089\nLearning Rate: 9.855946912301663e-19\n\nEpoch 1561, Training Loss: 0.22120767831802368, Validation Loss: 0.2260729819536209\nEpoch 1561, Training Loss: 0.22120767831802368, Validation Loss: 0.2260729819536209\nTraining Accuracy: 0.9279254590298712, Training F1 Score: 0.9238016913466564\nValidation Accuracy: 0.9273813438561876, Validation F1 Score: 0.9235979233205089\nLearning Rate: 9.854961361961016e-19\n\nEpoch 1571, Training Loss: 0.22120746970176697, Validation Loss: 0.2260795682668686\nEpoch 1571, Training Loss: 0.22120748460292816, Validation Loss: 0.2260795682668686\nTraining Accuracy: 0.9279117566456564, Training F1 Score: 0.9237888114215644\nValidation Accuracy: 0.9273813438561876, Validation F1 Score: 0.9235979233205089\nLearning Rate: 9.853975910170969e-19\n\nEpoch 1581, Training Loss: 0.2212074100971222, Validation Loss: 0.22609078884124756\nEpoch 1581, Training Loss: 0.2212074100971222, Validation Loss: 0.22609078884124756\nTraining Accuracy: 0.9279117566456564, Training F1 Score: 0.9237888114215644\nValidation Accuracy: 0.9273813438561876, Validation F1 Score: 0.9235979233205089\nLearning Rate: 9.852990556921664e-19\n\nEpoch 1591, Training Loss: 0.22120721638202667, Validation Loss: 0.22611093521118164\nEpoch 1591, Training Loss: 0.22120721638202667, Validation Loss: 0.22611093521118164\nTraining Accuracy: 0.9279254590298712, Training F1 Score: 0.9238016913466564\nValidation Accuracy: 0.9274361503891264, Validation F1 Score: 0.9236492810460101\nLearning Rate: 9.852005302203252e-19\n\nEpoch 1601, Training Loss: 0.22120703756809235, Validation Loss: 0.22612348198890686\nEpoch 1601, Training Loss: 0.22120702266693115, Validation Loss: 0.22612348198890686\nTraining Accuracy: 0.9279254590298712, Training F1 Score: 0.9238049034482085\nValidation Accuracy: 0.9274361503891264, Validation F1 Score: 0.9236492810460101\nLearning Rate: 9.851020146005878e-19\n\nEpoch 1611, Training Loss: 0.22120694816112518, Validation Loss: 0.2261388748884201\nEpoch 1611, Training Loss: 0.22120694816112518, Validation Loss: 0.2261388748884201\nTraining Accuracy: 0.9279254590298712, Training F1 Score: 0.9238049034482085\nValidation Accuracy: 0.9274361503891264, Validation F1 Score: 0.9236492810460101\nLearning Rate: 9.85003508831969e-19\n\nEpoch 1621, Training Loss: 0.2212066799402237, Validation Loss: 0.22616279125213623\nEpoch 1621, Training Loss: 0.2212066948413849, Validation Loss: 0.22616279125213623\nTraining Accuracy: 0.9279391614140861, Training F1 Score: 0.9238177835517976\nValidation Accuracy: 0.9274361503891264, Validation F1 Score: 0.9236492810460101\nLearning Rate: 9.849050129134838e-19\n\nEpoch 1631, Training Loss: 0.22120660543441772, Validation Loss: 0.22617052495479584\nEpoch 1631, Training Loss: 0.22120654582977295, Validation Loss: 0.22617052495479584\nTraining Accuracy: 0.9279117566456564, Training F1 Score: 0.9237888114215644\nValidation Accuracy: 0.9274361503891264, Validation F1 Score: 0.9236492810460101\nLearning Rate: 9.848065268441472e-19\n\nEpoch 1641, Training Loss: 0.2212064117193222, Validation Loss: 0.22617575526237488\nEpoch 1641, Training Loss: 0.221206396818161, Validation Loss: 0.22617575526237488\nTraining Accuracy: 0.9278980542614415, Training F1 Score: 0.923775931928607\nValidation Accuracy: 0.9274361503891264, Validation F1 Score: 0.9236492810460101\nLearning Rate: 9.847080506229746e-19\n\nEpoch 1651, Training Loss: 0.22120626270771027, Validation Loss: 0.22618688642978668\nEpoch 1651, Training Loss: 0.22120627760887146, Validation Loss: 0.22618688642978668\nTraining Accuracy: 0.9279254590298712, Training F1 Score: 0.9238049034482085\nValidation Accuracy: 0.9274361503891264, Validation F1 Score: 0.9236492810460101\nLearning Rate: 9.846095842489807e-19\n\nEpoch 1661, Training Loss: 0.2212061583995819, Validation Loss: 0.22620023787021637\nEpoch 1661, Training Loss: 0.2212062031030655, Validation Loss: 0.22620023787021637\nTraining Accuracy: 0.9279254590298712, Training F1 Score: 0.9238049034482085\nValidation Accuracy: 0.9274361503891264, Validation F1 Score: 0.9236492810460101\nLearning Rate: 9.845111277211813e-19\n\nEpoch 1671, Training Loss: 0.22120600938796997, Validation Loss: 0.22622442245483398\nEpoch 1671, Training Loss: 0.2212059646844864, Validation Loss: 0.22622442245483398\nTraining Accuracy: 0.9279117566456564, Training F1 Score: 0.9237920237767439\nValidation Accuracy: 0.9274361503891264, Validation F1 Score: 0.9236492810460101\nLearning Rate: 9.844126810385915e-19\n\nEpoch 1681, Training Loss: 0.22120583057403564, Validation Loss: 0.226246178150177\nEpoch 1681, Training Loss: 0.22120578587055206, Validation Loss: 0.226246178150177\nTraining Accuracy: 0.9279254590298712, Training F1 Score: 0.9238049034482085\nValidation Accuracy: 0.9274361503891264, Validation F1 Score: 0.9236492810460101\nLearning Rate: 9.84314244200227e-19\n\nEpoch 1691, Training Loss: 0.22120557725429535, Validation Loss: 0.22626015543937683\nEpoch 1691, Training Loss: 0.22120551764965057, Validation Loss: 0.22626015543937683\nTraining Accuracy: 0.9279391614140861, Training F1 Score: 0.9238177835517976\nValidation Accuracy: 0.9274361503891264, Validation F1 Score: 0.9236492810460101\nLearning Rate: 9.842158172051035e-19\n\nEpoch 1701, Training Loss: 0.22120550274848938, Validation Loss: 0.22627410292625427\nEpoch 1701, Training Loss: 0.22120548784732819, Validation Loss: 0.22627410292625427\nTraining Accuracy: 0.9279254590298712, Training F1 Score: 0.9238049034482085\nValidation Accuracy: 0.9274361503891264, Validation F1 Score: 0.9236492810460101\nLearning Rate: 9.841174000522365e-19\n\nEpoch 1711, Training Loss: 0.22120526432991028, Validation Loss: 0.2262818068265915\nEpoch 1711, Training Loss: 0.22120532393455505, Validation Loss: 0.2262818068265915\nTraining Accuracy: 0.9279254590298712, Training F1 Score: 0.9238049034482085\nValidation Accuracy: 0.9274361503891264, Validation F1 Score: 0.9236492810460101\nLearning Rate: 9.840189927406418e-19\n\nEpoch 1721, Training Loss: 0.22120516002178192, Validation Loss: 0.22628571093082428\nEpoch 1721, Training Loss: 0.22120511531829834, Validation Loss: 0.22628571093082428\nTraining Accuracy: 0.9279117566456564, Training F1 Score: 0.9237920237767439\nValidation Accuracy: 0.9273813438561876, Validation F1 Score: 0.9235979233205089\nLearning Rate: 9.839205952693356e-19\n\nEpoch 1731, Training Loss: 0.22120483219623566, Validation Loss: 0.22632458806037903\nEpoch 1731, Training Loss: 0.2212047576904297, Validation Loss: 0.22632458806037903\nTraining Accuracy: 0.9279254590298712, Training F1 Score: 0.9238049034482085\nValidation Accuracy: 0.9273813438561876, Validation F1 Score: 0.9235979233205089\nLearning Rate: 9.838222076373338e-19\n\nEpoch 1741, Training Loss: 0.22120468318462372, Validation Loss: 0.22632130980491638\nEpoch 1741, Training Loss: 0.22120468318462372, Validation Loss: 0.22632130980491638\nTraining Accuracy: 0.9279254590298712, Training F1 Score: 0.9238049034482085\nValidation Accuracy: 0.9273813438561876, Validation F1 Score: 0.9235979233205089\nLearning Rate: 9.837238298436525e-19\n\nEpoch 1751, Training Loss: 0.22120451927185059, Validation Loss: 0.22632813453674316\nEpoch 1751, Training Loss: 0.22120454907417297, Validation Loss: 0.22632813453674316\nTraining Accuracy: 0.9279254590298712, Training F1 Score: 0.9238049034482085\nValidation Accuracy: 0.9274361503891264, Validation F1 Score: 0.9236492810460101\nLearning Rate: 9.836254618873078e-19\n\nEpoch 1761, Training Loss: 0.22120438516139984, Validation Loss: 0.22633059322834015\nEpoch 1761, Training Loss: 0.22120438516139984, Validation Loss: 0.22633059322834015\nTraining Accuracy: 0.9279254590298712, Training F1 Score: 0.9238049034482085\nValidation Accuracy: 0.9274361503891264, Validation F1 Score: 0.9236492810460101\nLearning Rate: 9.83527103767316e-19\n\nEpoch 1771, Training Loss: 0.2212042659521103, Validation Loss: 0.2263481169939041\nEpoch 1771, Training Loss: 0.22120414674282074, Validation Loss: 0.2263481169939041\nTraining Accuracy: 0.9279117566456564, Training F1 Score: 0.9237888114215644\nValidation Accuracy: 0.9273813438561876, Validation F1 Score: 0.9235979233205089\nLearning Rate: 9.834287554826936e-19\n\nEpoch 1781, Training Loss: 0.22120407223701477, Validation Loss: 0.22636470198631287\nEpoch 1781, Training Loss: 0.2212039828300476, Validation Loss: 0.22636470198631287\nTraining Accuracy: 0.9279254590298712, Training F1 Score: 0.9238049034482085\nValidation Accuracy: 0.9273813438561876, Validation F1 Score: 0.9235979233205089\nLearning Rate: 9.833304170324571e-19\n\nEpoch 1791, Training Loss: 0.22120381891727448, Validation Loss: 0.22638018429279327\nEpoch 1791, Training Loss: 0.22120386362075806, Validation Loss: 0.22638018429279327\nTraining Accuracy: 0.9279254590298712, Training F1 Score: 0.9238049034482085\nValidation Accuracy: 0.9273813438561876, Validation F1 Score: 0.9235979233205089\nLearning Rate: 9.832320884156233e-19\n\nEpoch 1801, Training Loss: 0.22120366990566254, Validation Loss: 0.2263898104429245\nEpoch 1801, Training Loss: 0.22120362520217896, Validation Loss: 0.2263898104429245\nTraining Accuracy: 0.9279391614140861, Training F1 Score: 0.9238177835517976\nValidation Accuracy: 0.9273813438561876, Validation F1 Score: 0.9235979233205089\nLearning Rate: 9.831337696312085e-19\n\nEpoch 1811, Training Loss: 0.2212035059928894, Validation Loss: 0.2264070212841034\nEpoch 1811, Training Loss: 0.2212035208940506, Validation Loss: 0.2264070212841034\nTraining Accuracy: 0.9279391614140861, Training F1 Score: 0.9238177835517976\nValidation Accuracy: 0.9273813438561876, Validation F1 Score: 0.9235979233205089\nLearning Rate: 9.830354606782299e-19\n\nEpoch 1821, Training Loss: 0.22120338678359985, Validation Loss: 0.22642427682876587\nEpoch 1821, Training Loss: 0.22120332717895508, Validation Loss: 0.22642427682876587\nTraining Accuracy: 0.9279528637983009, Training F1 Score: 0.9238338749678099\nValidation Accuracy: 0.9273813438561876, Validation F1 Score: 0.9235979233205089\nLearning Rate: 9.829371615557041e-19\n\nEpoch 1831, Training Loss: 0.22120322287082672, Validation Loss: 0.22643840312957764\nEpoch 1831, Training Loss: 0.22120322287082672, Validation Loss: 0.22643840312957764\nTraining Accuracy: 0.9279254590298712, Training F1 Score: 0.9238049034482085\nValidation Accuracy: 0.9273813438561876, Validation F1 Score: 0.9235979233205089\nLearning Rate: 9.828388722626482e-19\n\nEpoch 1841, Training Loss: 0.22120310366153717, Validation Loss: 0.22645601630210876\nEpoch 1841, Training Loss: 0.2212030440568924, Validation Loss: 0.22645601630210876\nTraining Accuracy: 0.9279254590298712, Training F1 Score: 0.9238049034482085\nValidation Accuracy: 0.9273813438561876, Validation F1 Score: 0.9235979233205089\nLearning Rate: 9.827405927980794e-19\n\nEpoch 1851, Training Loss: 0.22120295464992523, Validation Loss: 0.22647373378276825\nEpoch 1851, Training Loss: 0.22120289504528046, Validation Loss: 0.22647373378276825\nTraining Accuracy: 0.9279391614140861, Training F1 Score: 0.9238177835517976\nValidation Accuracy: 0.9273813438561876, Validation F1 Score: 0.9235979233205089\nLearning Rate: 9.826423231610148e-19\n\nEpoch 1861, Training Loss: 0.2212027609348297, Validation Loss: 0.22649535536766052\nEpoch 1861, Training Loss: 0.22120271623134613, Validation Loss: 0.22649535536766052\nTraining Accuracy: 0.9279391614140861, Training F1 Score: 0.9238209946857813\nValidation Accuracy: 0.9273813438561876, Validation F1 Score: 0.9235979233205089\nLearning Rate: 9.825440633504717e-19\n\nEpoch 1871, Training Loss: 0.22120258212089539, Validation Loss: 0.2265244722366333\nEpoch 1871, Training Loss: 0.2212025672197342, Validation Loss: 0.2265244722366333\nTraining Accuracy: 0.9279254590298712, Training F1 Score: 0.9238081148358674\nValidation Accuracy: 0.9273813438561876, Validation F1 Score: 0.9235979233205089\nLearning Rate: 9.824458133654674e-19\n\nEpoch 1881, Training Loss: 0.22120244801044464, Validation Loss: 0.22652648389339447\nEpoch 1881, Training Loss: 0.22120240330696106, Validation Loss: 0.22652648389339447\nTraining Accuracy: 0.9279528637983009, Training F1 Score: 0.9238338749678099\nValidation Accuracy: 0.9273813438561876, Validation F1 Score: 0.9235979233205089\nLearning Rate: 9.823475732050196e-19\n\nEpoch 1891, Training Loss: 0.2212023138999939, Validation Loss: 0.22653372585773468\nEpoch 1891, Training Loss: 0.2212022840976715, Validation Loss: 0.22653372585773468\nTraining Accuracy: 0.9279528637983009, Training F1 Score: 0.9238338749678099\nValidation Accuracy: 0.9273813438561876, Validation F1 Score: 0.9235979233205089\nLearning Rate: 9.822493428681458e-19\n\nEpoch 1901, Training Loss: 0.22120220959186554, Validation Loss: 0.2265397012233734\nEpoch 1901, Training Loss: 0.22120216488838196, Validation Loss: 0.2265397012233734\nTraining Accuracy: 0.9279391614140861, Training F1 Score: 0.9238209946857813\nValidation Accuracy: 0.9273813438561876, Validation F1 Score: 0.9235979233205089\nLearning Rate: 9.821511223538636e-19\n\nEpoch 1911, Training Loss: 0.2212020456790924, Validation Loss: 0.22654949128627777\nEpoch 1911, Training Loss: 0.22120201587677002, Validation Loss: 0.22654949128627777\nTraining Accuracy: 0.9279528637983009, Training F1 Score: 0.9238370851344088\nValidation Accuracy: 0.9273813438561876, Validation F1 Score: 0.9235979233205089\nLearning Rate: 9.820529116611909e-19\n\nEpoch 1921, Training Loss: 0.22120198607444763, Validation Loss: 0.22654978930950165\nEpoch 1921, Training Loss: 0.22120198607444763, Validation Loss: 0.22654978930950165\nTraining Accuracy: 0.9279528637983009, Training F1 Score: 0.9238370851344088\nValidation Accuracy: 0.9273813438561876, Validation F1 Score: 0.9235979233205089\nLearning Rate: 9.819547107891454e-19\n\nEpoch 1931, Training Loss: 0.22120188176631927, Validation Loss: 0.22655604779720306\nEpoch 1931, Training Loss: 0.2212018370628357, Validation Loss: 0.22655604779720306\nTraining Accuracy: 0.9279391614140861, Training F1 Score: 0.9238209946857813\nValidation Accuracy: 0.9273813438561876, Validation F1 Score: 0.9235979233205089\nLearning Rate: 9.818565197367454e-19\n\nEpoch 1941, Training Loss: 0.22120174765586853, Validation Loss: 0.22657251358032227\nEpoch 1941, Training Loss: 0.22120174765586853, Validation Loss: 0.22657251358032227\nTraining Accuracy: 0.9279391614140861, Training F1 Score: 0.9238209946857813\nValidation Accuracy: 0.9273813438561876, Validation F1 Score: 0.9235979233205089\nLearning Rate: 9.817583385030087e-19\n\nEpoch 1951, Training Loss: 0.22120162844657898, Validation Loss: 0.22658604383468628\nEpoch 1951, Training Loss: 0.22120164334774017, Validation Loss: 0.22658604383468628\nTraining Accuracy: 0.9279528637983009, Training F1 Score: 0.9238370851344088\nValidation Accuracy: 0.9273813438561876, Validation F1 Score: 0.9235979233205089\nLearning Rate: 9.816601670869536e-19\n\nEpoch 1961, Training Loss: 0.22120150923728943, Validation Loss: 0.22660231590270996\nEpoch 1961, Training Loss: 0.22120147943496704, Validation Loss: 0.22660231590270996\nTraining Accuracy: 0.9279254590298712, Training F1 Score: 0.9238081148358674\nValidation Accuracy: 0.9273265373232489, Validation F1 Score: 0.9235339476336928\nLearning Rate: 9.815620054875983e-19\n\nEpoch 1971, Training Loss: 0.22120143473148346, Validation Loss: 0.22661109268665314\nEpoch 1971, Training Loss: 0.22120141983032227, Validation Loss: 0.22661109268665314\nTraining Accuracy: 0.9279254590298712, Training F1 Score: 0.9238081148358674\nValidation Accuracy: 0.9273813438561876, Validation F1 Score: 0.9235979233205089\nLearning Rate: 9.814638537039611e-19\n\nEpoch 1981, Training Loss: 0.2212013304233551, Validation Loss: 0.22663387656211853\nEpoch 1981, Training Loss: 0.2212013155221939, Validation Loss: 0.22663387656211853\nTraining Accuracy: 0.9279254590298712, Training F1 Score: 0.9238113255098247\nValidation Accuracy: 0.9273813438561876, Validation F1 Score: 0.9235979233205089\nLearning Rate: 9.813657117350607e-19\n\nEpoch 1991, Training Loss: 0.22120116651058197, Validation Loss: 0.22665375471115112\nEpoch 1991, Training Loss: 0.22120118141174316, Validation Loss: 0.22665375471115112\nTraining Accuracy: 0.9279254590298712, Training F1 Score: 0.9238081148358674\nValidation Accuracy: 0.9273813438561876, Validation F1 Score: 0.9235979233205089\nLearning Rate: 9.812675795799156e-19\n\nBest Validation Loss after 2000 epochs: 0.2231731414794922 from Epoch 111\nExecution time: 44.643428 seconds\n","output_type":"stream"}],"execution_count":79},{"cell_type":"code","source":"evaluate_model(model, custom_train_loader, criterion, optimizer, 1000, scheduler, batch_size, num_features, early_stopping_patience=10000)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-27T23:02:31.223423Z","iopub.execute_input":"2025-01-27T23:02:31.224109Z","iopub.status.idle":"2025-01-27T23:03:25.664451Z","shell.execute_reply.started":"2025-01-27T23:02:31.224078Z","shell.execute_reply":"2025-01-27T23:03:25.663515Z"},"collapsed":true,"jupyter":{"outputs_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def custom_lr_lambda(step):\n    num_step_threshold = 100\n\n    if step < num_step_threshold:\n        return step / num_step_threshold\n    if step == num_step_threshold:\n        print(\"here\")\n    return 0.99999 ** (step - num_step_threshold)\n\nstart_time = time.time()\nmodel = TabularDenseNet(num_features, num_classes, 1, 1).to(device)\n\noptimizer = optim.Adam(model.parameters(), lr=0.001 * 1.0)\nscheduler = LambdaLR(optimizer, lr_lambda=custom_lr_lambda)\n\ncriterion = CustomLoss(nn.CrossEntropyLoss(), 0.0, 0.0, 0.0, 0.0, 0.0)\nevaluate_model(model, custom_train_loader, criterion, optimizer, 1000, scheduler, batch_size, num_features, early_stopping_patience=10000)\n\nprint(f\"Execution time: {(time.time() - start_time):.6f} seconds\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-27T08:22:12.758255Z","iopub.execute_input":"2025-01-27T08:22:12.758587Z","iopub.status.idle":"2025-01-27T08:22:12.797647Z","shell.execute_reply.started":"2025-01-27T08:22:12.758548Z","shell.execute_reply":"2025-01-27T08:22:12.796055Z"},"jupyter":{"source_hidden":true,"outputs_hidden":true},"collapsed":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def custom_lr_lambda(step):\n    num_step_threshold = 100\n\n    if step < num_step_threshold:\n        return step / num_step_threshold\n    if step == num_step_threshold:\n        print(\"here\")\n    return 0.99999 ** (step - num_step_threshold)\n\nstart_time = time.time()\nmodel = TabularDenseNet(num_features, num_classes, 1, 1).to(device)\n\noptimizer = optim.Adam(model.parameters(), lr=0.001 * 1.0)\nscheduler = LambdaLR(optimizer, lr_lambda=custom_lr_lambda)\n\ncriterion = CustomLoss(nn.CrossEntropyLoss(), 0.0, 0.0, 0.0, 0.0, 0.0)\nevaluate_model(model, custom_train_loader, criterion, optimizer, 1000, scheduler, batch_size, num_features, early_stopping_patience=10000)\n\nprint(f\"Execution time: {(time.time() - start_time):.6f} seconds\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-27T08:22:12.798433Z","iopub.status.idle":"2025-01-27T08:22:12.798791Z","shell.execute_reply.started":"2025-01-27T08:22:12.798613Z","shell.execute_reply":"2025-01-27T08:22:12.798636Z"},"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def custom_lr_lambda(step):\n    num_step_threshold = 100\n\n    if step < num_step_threshold:\n        return step / num_step_threshold\n    if step == num_step_threshold:\n        print(\"here\")\n    return 0.99999 ** (step - num_step_threshold)\n\nstart_time = time.time()\nmodel = TabularDenseNet(num_features, num_classes, 1).to(device)\n\noptimizer = optim.Rprop(model.parameters(), lr=0.001 * 1.0)\nscheduler = LambdaLR(optimizer, lr_lambda=custom_lr_lambda)\n\ncriterion = CustomLoss(nn.CrossEntropyLoss(), 0.0, 0.0, 0.0, 0.0, 0.0)\nevaluate_model(model, custom_train_loader, criterion, optimizer, 1000, scheduler, batch_size, num_features, early_stopping_patience=10000)\n\nprint(f\"Execution time: {(time.time() - start_time):.6f} seconds\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-27T20:50:50.588727Z","iopub.execute_input":"2025-01-27T20:50:50.589068Z","iopub.status.idle":"2025-01-27T20:52:00.73568Z","shell.execute_reply.started":"2025-01-27T20:50:50.589039Z","shell.execute_reply":"2025-01-27T20:52:00.734778Z"},"jupyter":{"source_hidden":true,"outputs_hidden":true},"collapsed":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"torch.set_printoptions(sci_mode=False, precision=4)\n\nfor name, param in model.named_parameters():\n    print(name)\n    print(param)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-27T20:52:00.737345Z","iopub.execute_input":"2025-01-27T20:52:00.738338Z","iopub.status.idle":"2025-01-27T20:52:00.881143Z","shell.execute_reply.started":"2025-01-27T20:52:00.738294Z","shell.execute_reply":"2025-01-27T20:52:00.880271Z"},"jupyter":{"source_hidden":true,"outputs_hidden":true},"collapsed":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"evaluate_model(model, custom_train_loader, criterion, optimizer, 1500, scheduler, batch_size, num_features, early_stopping_patience=10000)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-27T20:52:12.554609Z","iopub.execute_input":"2025-01-27T20:52:12.554912Z","iopub.status.idle":"2025-01-27T20:53:56.301643Z","shell.execute_reply.started":"2025-01-27T20:52:12.554889Z","shell.execute_reply":"2025-01-27T20:53:56.300728Z"},"collapsed":true,"jupyter":{"outputs_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"torch.set_printoptions(sci_mode=False, precision=4, threshold=float('inf'))\n\nfor name, param in model.named_parameters():\n    print(name)\n    print(param)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-30T22:54:56.701176Z","iopub.execute_input":"2025-01-30T22:54:56.702305Z","iopub.status.idle":"2025-01-30T22:54:57.084920Z","shell.execute_reply.started":"2025-01-30T22:54:56.702248Z","shell.execute_reply":"2025-01-30T22:54:57.084011Z"},"collapsed":true,"jupyter":{"outputs_hidden":true}},"outputs":[{"name":"stdout","text":"final_weight\nParameter containing:\ntensor([     0.0012,      0.0214,      0.0035,     -0.0116,     -0.0081,\n             0.0168,      0.0008,     -0.0155,     -0.0101,     -0.0168,\n            -0.0081,     -0.0026,      0.0085,     -0.0214,     -0.0143,\n            -0.0078,     -0.0024,     -0.0051,     -0.0119,     -0.0113,\n            -0.0286,     -0.0089,     -0.0069,     -0.0896,     -0.0104,\n            -0.0089,     -0.0084,      0.0017,     -0.0163,      0.0038,\n            -0.0068,     -0.0034,      0.0020,      0.0037,     -0.0068,\n            -0.0036,      0.0061,     -0.0002,     -0.0133,      0.0085,\n            -0.0041,     -0.0011,      0.0010,      0.0077,     -0.0027,\n            -0.0016,      0.0007,     -0.0010,     -0.0010,      0.0077,\n            -0.0084,      0.0717,     -0.0116,      0.0085,      0.0019,\n             0.1308,      0.0193,      0.1563,     -0.0006,      0.0010,\n            -0.0104,      0.0021,     -0.0751,     -0.0101,      0.0016,\n             0.0091,     -0.0375,     -0.0206,      0.0004,      0.0055,\n            -0.0162,     -0.0158,     -0.0043,     -0.0188,      0.0014,\n             0.0017,     -0.0116,     -0.0056,      0.0032,      0.0001,\n            -0.0028,      0.0028,     -0.0135,     -0.0031,     -0.0012,\n            -0.0031,     -0.0066,      0.0091,     -0.0195,      0.0007,\n            -0.0124,     -0.0043,     -0.0019,     -0.0054,     -0.0030,\n             0.0056,      0.0048,      0.0046,     -0.0040,     -0.0011,\n            -0.0163,      0.0085,      0.0032,      0.0313,      0.0070,\n            -0.0072,     -0.0046,     -0.1401,     -0.0001,      0.0004,\n             0.0098,      0.0432,      0.0399,      0.0066,      0.0087,\n            -0.0663,      0.0643,      0.0332,      0.0008,     -0.0129,\n             0.0007,     -0.0008,      0.0422,      0.0106,     -0.0089,\n             0.0038,      0.0019,      0.0001,      0.0070,      0.0015,\n             0.0052,      0.0085,      0.0154,      0.0030,      0.0009,\n             0.0028,      0.0157,     -0.0057,      0.0187,      0.0138,\n             0.0025,      0.0212,      0.0158,      0.0061,      0.0125,\n            -0.0043,      0.0003,     -0.0010,     -0.0028,      0.0039,\n            -0.0068,      0.1308,     -0.0028,     -0.0072,      0.0052,\n             0.0112,      0.0248,      0.1077,      0.0035,      0.0017,\n            -0.0154,     -0.0300,     -0.1191,     -0.0118,     -0.0128,\n             0.0940,     -0.0387,     -0.0052,     -0.0024,      0.0044,\n            -0.0152,     -0.0015,     -0.0249,     -0.0100,      0.0175,\n            -0.0034,      0.0193,      0.0028,     -0.0046,      0.0085,\n             0.0248,      0.0002,      0.0044,     -0.0154,      0.0062,\n            -0.0478,     -0.0036,     -0.0077,     -0.0044,     -0.0118,\n             0.0044,      0.0133,      0.0007,     -0.0185,     -0.0054,\n             0.0030,      0.0038,      0.0010,     -0.0010,      0.0008,\n             0.0020,      0.1563,     -0.0135,     -0.1401,      0.0154,\n             0.1077,      0.0044,      0.0146,      0.0041,      0.0072,\n             0.0030,      0.1844,      0.0008,      0.0143,      0.0040,\n             0.0106,      0.0122,      0.0162,      0.0078,     -0.0009,\n             0.0016,      0.0019,      0.0022,      0.0027,     -0.0051,\n             0.0037,     -0.0006,     -0.0031,     -0.0001,      0.0030,\n             0.0035,     -0.0154,      0.0041,      0.0013,      0.0050,\n             0.0029,      0.0140,     -0.0010,      0.0065,      0.0068,\n             0.0018,      0.0049,      0.0035,      0.0007,     -0.0009,\n             0.0090,      0.0078,      0.0093,      0.0101,      0.0136,\n            -0.0068,      0.0010,     -0.0012,      0.0004,      0.0009,\n             0.0017,      0.0062,      0.0072,      0.0050,      0.0053,\n             0.0045,      0.0079,     -0.0022,      0.0080,      0.0076,\n             0.0051,      0.0068,      0.0079,     -0.0015,      0.0027,\n             0.0130,      0.0080,      0.0026,      0.0077,      0.0094,\n            -0.0036,     -0.0104,     -0.0031,      0.0098,      0.0028,\n            -0.0154,     -0.0478,      0.0030,      0.0029,      0.0045,\n             0.0061,     -0.0000,     -0.0022,      0.0185,      0.0113,\n             0.0014,      0.0030,     -0.0003,      0.0035,      0.0145,\n             0.0119,      0.0042,      0.0027,      0.0036,      0.0112,\n             0.0061,      0.0021,     -0.0066,      0.0432,      0.0157,\n            -0.0300,     -0.0036,      0.1844,      0.0140,      0.0079,\n            -0.0000,      0.0986,     -0.0892,      0.0641,      0.0332,\n             0.0211,      0.0478,      0.0463,      0.0041,      0.0045,\n             0.0136,     -0.0019,      0.0001,      0.0028,      0.0090,\n            -0.0002,     -0.0751,      0.0091,      0.0399,     -0.0057,\n            -0.1191,     -0.0077,      0.0008,     -0.0010,     -0.0022,\n            -0.0022,     -0.0892,      0.1031,     -0.0008,     -0.0045,\n             0.0016,     -0.0018,     -0.0159,      0.0009,      0.0171,\n            -0.0025,     -0.0001,     -0.0203,     -0.0029,      0.0063,\n            -0.0133,     -0.0101,     -0.0195,      0.0066,      0.0187,\n            -0.0118,     -0.0044,      0.0143,      0.0065,      0.0080,\n             0.0185,      0.0641,     -0.0008,      0.0148,     -0.0046,\n            -0.0023,     -0.0015,     -0.0071,     -0.0103,      0.0092,\n             0.0252,      0.0105,      0.0007,      0.0040,      0.0145,\n             0.0085,      0.0016,      0.0007,      0.0087,      0.0138,\n            -0.0128,     -0.0118,      0.0040,      0.0068,      0.0076,\n             0.0113,      0.0332,     -0.0045,     -0.0046,      0.0066,\n            -0.0025,     -0.0046,     -0.0065,     -0.0040,      0.0108,\n             0.0155,      0.0086,      0.0036,      0.0054,      0.0164,\n            -0.0041,      0.0091,     -0.0124,     -0.0663,      0.0025,\n             0.0940,      0.0044,      0.0106,      0.0018,      0.0051,\n             0.0014,      0.0211,      0.0016,     -0.0023,     -0.0025,\n             0.0114,     -0.0019,     -0.0059,      0.0030,      0.0064,\n             0.0091,      0.0117,      0.0002,      0.0023,      0.0198,\n            -0.0011,     -0.0375,     -0.0043,      0.0643,      0.0212,\n            -0.0387,      0.0133,      0.0122,      0.0049,      0.0068,\n             0.0030,      0.0478,     -0.0018,     -0.0015,     -0.0046,\n            -0.0019,      0.0139,     -0.0109,     -0.0023,      0.0057,\n             0.0152,      0.0078,      0.0022,      0.0113,      0.0205,\n             0.0010,     -0.0206,     -0.0019,      0.0332,      0.0158,\n            -0.0052,      0.0007,      0.0162,      0.0035,      0.0079,\n            -0.0003,      0.0463,     -0.0159,     -0.0071,     -0.0065,\n            -0.0059,     -0.0109,      0.0237,     -0.0001,     -0.0016,\n             0.0077,      0.0063,      0.0239,     -0.0003,      0.0063,\n             0.0077,      0.0004,     -0.0054,      0.0008,      0.0061,\n            -0.0024,     -0.0185,      0.0078,      0.0007,     -0.0015,\n             0.0035,      0.0041,      0.0009,     -0.0103,     -0.0040,\n             0.0030,     -0.0023,     -0.0001,      0.0236,     -0.0035,\n             0.0023,      0.0001,      0.0009,      0.0049,      0.0096,\n            -0.0027,      0.0055,     -0.0030,     -0.0129,      0.0125,\n             0.0044,     -0.0054,     -0.0009,     -0.0009,      0.0027,\n             0.0145,      0.0045,      0.0171,      0.0092,      0.0108,\n             0.0064,      0.0057,     -0.0016,     -0.0035,      0.0094,\n             0.0029,      0.0025,      0.0041,      0.0058,      0.0034,\n            -0.0016,     -0.0162,      0.0056,      0.0007,     -0.0043,\n            -0.0152,      0.0030,      0.0016,      0.0090,      0.0130,\n             0.0119,      0.0136,     -0.0025,      0.0252,      0.0155,\n             0.0091,      0.0152,      0.0077,      0.0023,      0.0029,\n             0.0005,      0.0082,      0.0089,      0.0291,      0.0040,\n             0.0007,     -0.0158,      0.0048,     -0.0008,      0.0003,\n            -0.0015,      0.0038,      0.0019,      0.0078,      0.0080,\n             0.0042,     -0.0019,     -0.0001,      0.0105,      0.0086,\n             0.0117,      0.0078,      0.0063,      0.0001,      0.0025,\n             0.0082,      0.0002,      0.0072,      0.0093,      0.0047,\n            -0.0010,     -0.0043,      0.0046,      0.0422,     -0.0010,\n            -0.0249,      0.0010,      0.0022,      0.0093,      0.0026,\n             0.0027,      0.0001,     -0.0203,      0.0007,      0.0036,\n             0.0002,      0.0022,      0.0239,      0.0009,      0.0041,\n             0.0089,      0.0072,      0.0008,      0.0076,      0.0009,\n            -0.0010,     -0.0188,     -0.0040,      0.0106,     -0.0028,\n            -0.0100,     -0.0010,      0.0027,      0.0101,      0.0077,\n             0.0036,      0.0028,     -0.0029,      0.0040,      0.0054,\n             0.0023,      0.0113,     -0.0003,      0.0049,      0.0058,\n             0.0291,      0.0093,      0.0076,      0.0006,      0.0024,\n             0.0077,      0.0014,     -0.0011,     -0.0089,      0.0039,\n             0.0175,      0.0008,     -0.0051,      0.0136,      0.0094,\n             0.0112,      0.0090,      0.0063,      0.0145,      0.0164,\n             0.0198,      0.0205,      0.0063,      0.0096,      0.0034,\n             0.0040,      0.0047,      0.0009,      0.0024,      0.0083,\n            -0.0089,     -0.0084,      0.0017,     -0.0163,      0.0038,\n            -0.0068,     -0.0034,      0.0020,      0.0037,     -0.0068,\n            -0.0036,      0.0061,     -0.0002,     -0.0133,      0.0085,\n            -0.0041,     -0.0011,      0.0010,      0.0077,     -0.0027,\n            -0.0016,      0.0007,     -0.0010,     -0.0010,      0.0077,\n            -0.0084,      0.0717,     -0.0116,      0.0085,      0.0019,\n             0.1308,      0.0193,      0.1563,     -0.0006,      0.0010,\n            -0.0104,      0.0021,     -0.0751,     -0.0101,      0.0016,\n             0.0091,     -0.0375,     -0.0206,      0.0004,      0.0055,\n            -0.0162,     -0.0158,     -0.0043,     -0.0188,      0.0014,\n             0.0017,     -0.0116,     -0.0056,      0.0032,      0.0001,\n            -0.0028,      0.0028,     -0.0135,     -0.0031,     -0.0012,\n            -0.0031,     -0.0066,      0.0091,     -0.0195,      0.0007,\n            -0.0124,     -0.0043,     -0.0019,     -0.0054,     -0.0030,\n             0.0056,      0.0048,      0.0046,     -0.0040,     -0.0011,\n            -0.0163,      0.0085,      0.0032,      0.0313,      0.0070,\n            -0.0072,     -0.0046,     -0.1401,     -0.0001,      0.0004,\n             0.0098,      0.0432,      0.0399,      0.0066,      0.0087,\n            -0.0663,      0.0643,      0.0332,      0.0008,     -0.0129,\n             0.0007,     -0.0008,      0.0422,      0.0106,     -0.0089,\n             0.0038,      0.0019,      0.0001,      0.0070,      0.0015,\n             0.0052,      0.0085,      0.0154,      0.0030,      0.0009,\n             0.0028,      0.0157,     -0.0057,      0.0187,      0.0138,\n             0.0025,      0.0212,      0.0158,      0.0061,      0.0125,\n            -0.0043,      0.0003,     -0.0010,     -0.0028,      0.0039,\n            -0.0068,      0.1308,     -0.0028,     -0.0072,      0.0052,\n             0.0112,      0.0248,      0.1077,      0.0035,      0.0017,\n            -0.0154,     -0.0300,     -0.1191,     -0.0118,     -0.0128,\n             0.0940,     -0.0387,     -0.0052,     -0.0024,      0.0044,\n            -0.0152,     -0.0015,     -0.0249,     -0.0100,      0.0175,\n            -0.0034,      0.0193,      0.0028,     -0.0046,      0.0085,\n             0.0248,      0.0002,      0.0044,     -0.0154,      0.0062,\n            -0.0478,     -0.0036,     -0.0077,     -0.0044,     -0.0118,\n             0.0044,      0.0133,      0.0007,     -0.0185,     -0.0054,\n             0.0030,      0.0038,      0.0010,     -0.0010,      0.0008,\n             0.0020,      0.1563,     -0.0135,     -0.1401,      0.0154,\n             0.1077,      0.0044,      0.0146,      0.0041,      0.0072,\n             0.0030,      0.1844,      0.0008,      0.0143,      0.0040,\n             0.0106,      0.0122,      0.0162,      0.0078,     -0.0009,\n             0.0016,      0.0019,      0.0022,      0.0027,     -0.0051,\n             0.0037,     -0.0006,     -0.0031,     -0.0001,      0.0030,\n             0.0035,     -0.0154,      0.0041,      0.0013,      0.0050,\n             0.0029,      0.0140,     -0.0010,      0.0065,      0.0068,\n             0.0018,      0.0049,      0.0035,      0.0007,     -0.0009,\n             0.0090,      0.0078,      0.0093,      0.0101,      0.0136,\n            -0.0068,      0.0010,     -0.0012,      0.0004,      0.0009,\n             0.0017,      0.0062,      0.0072,      0.0050,      0.0053,\n             0.0045,      0.0079,     -0.0022,      0.0080,      0.0076,\n             0.0051,      0.0068,      0.0079,     -0.0015,      0.0027,\n             0.0130,      0.0080,      0.0026,      0.0077,      0.0094,\n            -0.0036,     -0.0104,     -0.0031,      0.0098,      0.0028,\n            -0.0154,     -0.0478,      0.0030,      0.0029,      0.0045,\n             0.0061,     -0.0000,     -0.0022,      0.0185,      0.0113,\n             0.0014,      0.0030,     -0.0003,      0.0035,      0.0145,\n             0.0119,      0.0042,      0.0027,      0.0036,      0.0112,\n             0.0061,      0.0021,     -0.0066,      0.0432,      0.0157,\n            -0.0300,     -0.0036,      0.1844,      0.0140,      0.0079,\n            -0.0000,      0.0986,     -0.0892,      0.0641,      0.0332,\n             0.0211,      0.0478,      0.0463,      0.0041,      0.0045,\n             0.0136,     -0.0019,      0.0001,      0.0028,      0.0090,\n            -0.0002,     -0.0751,      0.0091,      0.0399,     -0.0057,\n            -0.1191,     -0.0077,      0.0008,     -0.0010,     -0.0022,\n            -0.0022,     -0.0892,      0.1031,     -0.0008,     -0.0045,\n             0.0016,     -0.0018,     -0.0159,      0.0009,      0.0171,\n            -0.0025,     -0.0001,     -0.0203,     -0.0029,      0.0063,\n            -0.0133,     -0.0101,     -0.0195,      0.0066,      0.0187,\n            -0.0118,     -0.0044,      0.0143,      0.0065,      0.0080,\n             0.0185,      0.0641,     -0.0008,      0.0148,     -0.0046,\n            -0.0023,     -0.0015,     -0.0071,     -0.0103,      0.0092,\n             0.0252,      0.0105,      0.0007,      0.0040,      0.0145,\n             0.0085,      0.0016,      0.0007,      0.0087,      0.0138,\n            -0.0128,     -0.0118,      0.0040,      0.0068,      0.0076,\n             0.0113,      0.0332,     -0.0045,     -0.0046,      0.0066,\n            -0.0025,     -0.0046,     -0.0065,     -0.0040,      0.0108,\n             0.0155,      0.0086,      0.0036,      0.0054,      0.0164,\n            -0.0041,      0.0091,     -0.0124,     -0.0663,      0.0025,\n             0.0940,      0.0044,      0.0106,      0.0018,      0.0051,\n             0.0014,      0.0211,      0.0016,     -0.0023,     -0.0025,\n             0.0114,     -0.0019,     -0.0059,      0.0030,      0.0064,\n             0.0091,      0.0117,      0.0002,      0.0023,      0.0198,\n            -0.0011,     -0.0375,     -0.0043,      0.0643,      0.0212,\n            -0.0387,      0.0133,      0.0122,      0.0049,      0.0068,\n             0.0030,      0.0478,     -0.0018,     -0.0015,     -0.0046,\n            -0.0019,      0.0139,     -0.0109,     -0.0023,      0.0057,\n             0.0152,      0.0078,      0.0022,      0.0113,      0.0205,\n             0.0010,     -0.0206,     -0.0019,      0.0332,      0.0158,\n            -0.0052,      0.0007,      0.0162,      0.0035,      0.0079,\n            -0.0003,      0.0463,     -0.0159,     -0.0071,     -0.0065,\n            -0.0059,     -0.0109,      0.0237,     -0.0001,     -0.0016,\n             0.0077,      0.0063,      0.0239,     -0.0003,      0.0063,\n             0.0077,      0.0004,     -0.0054,      0.0008,      0.0061,\n            -0.0024,     -0.0185,      0.0078,      0.0007,     -0.0015,\n             0.0035,      0.0041,      0.0009,     -0.0103,     -0.0040,\n             0.0030,     -0.0023,     -0.0001,      0.0236,     -0.0035,\n             0.0023,      0.0001,      0.0009,      0.0049,      0.0096,\n            -0.0027,      0.0055,     -0.0030,     -0.0129,      0.0125,\n             0.0044,     -0.0054,     -0.0009,     -0.0009,      0.0027,\n             0.0145,      0.0045,      0.0171,      0.0092,      0.0108,\n             0.0064,      0.0057,     -0.0016,     -0.0035,      0.0094,\n             0.0029,      0.0025,      0.0041,      0.0058,      0.0034,\n            -0.0016,     -0.0162,      0.0056,      0.0007,     -0.0043,\n            -0.0152,      0.0030,      0.0016,      0.0090,      0.0130,\n             0.0119,      0.0136,     -0.0025,      0.0252,      0.0155,\n             0.0091,      0.0152,      0.0077,      0.0023,      0.0029,\n             0.0005,      0.0082,      0.0089,      0.0291,      0.0040,\n             0.0007,     -0.0158,      0.0048,     -0.0008,      0.0003,\n            -0.0015,      0.0038,      0.0019,      0.0078,      0.0080,\n             0.0042,     -0.0019,     -0.0001,      0.0105,      0.0086,\n             0.0117,      0.0078,      0.0063,      0.0001,      0.0025,\n             0.0082,      0.0002,      0.0072,      0.0093,      0.0047,\n            -0.0010,     -0.0043,      0.0046,      0.0422,     -0.0010,\n            -0.0249,      0.0010,      0.0022,      0.0093,      0.0026,\n             0.0027,      0.0001,     -0.0203,      0.0007,      0.0036,\n             0.0002,      0.0022,      0.0239,      0.0009,      0.0041,\n             0.0089,      0.0072,      0.0008,      0.0076,      0.0009,\n            -0.0010,     -0.0188,     -0.0040,      0.0106,     -0.0028,\n            -0.0100,     -0.0010,      0.0027,      0.0101,      0.0077,\n             0.0036,      0.0028,     -0.0029,      0.0040,      0.0054,\n             0.0023,      0.0113,     -0.0003,      0.0049,      0.0058,\n             0.0291,      0.0093,      0.0076,      0.0006,      0.0024,\n             0.0077,      0.0014,     -0.0011,     -0.0089,      0.0039,\n             0.0175,      0.0008,     -0.0051,      0.0136,      0.0094,\n             0.0112,      0.0090,      0.0063,      0.0145,      0.0164,\n             0.0198,      0.0205,      0.0063,      0.0096,      0.0034,\n             0.0040,      0.0047,      0.0009,      0.0024,      0.0083,\n             0.0228,      0.0070,      0.0057,     -0.0061,     -0.0074,\n            -0.0067,     -0.0035,     -0.0034,      0.0181,      0.0250,\n             0.0127,      0.0152,     -0.0058,      0.0183,     -0.0357,\n             0.0001,      0.0122,      0.0034,     -0.0037,      0.0272,\n             0.0018,     -0.0096,     -0.0016,      0.0048,      0.0039,\n             0.0070,     -0.0136,     -0.0007,      0.0120,     -0.0207,\n             0.0028,      0.0000,      0.0012,      0.0151,      0.0094,\n            -0.0004,      0.0050,      0.0085,      0.0011,     -0.0095,\n            -0.0044,      0.0022,     -0.0095,      0.0155,      0.0107,\n             0.0132,      0.0011,     -0.0103,      0.0050,     -0.0033,\n             0.0057,     -0.0007,      0.0009,      0.0069,      0.0302,\n            -0.0057,     -0.0062,      0.0053,      0.0057,      0.0034,\n            -0.0023,      0.0006,      0.0027,      0.0025,      0.0053,\n             0.0057,      0.0038,     -0.0028,     -0.0169,     -0.0210,\n            -0.0258,     -0.0094,     -0.0069,     -0.0021,     -0.0017,\n            -0.0061,      0.0120,      0.0069,     -0.0124,      0.0021,\n             0.0023,     -0.0005,     -0.0082,     -0.0132,     -0.0187,\n             0.0012,     -0.0483,     -0.0132,      0.0036,      0.0117,\n             0.0024,      0.0038,      0.0152,      0.0120,      0.0114,\n             0.0044,      0.0106,     -0.0179,      0.0082,     -0.0017,\n            -0.0074,     -0.0207,      0.0302,      0.0021,      0.0037,\n            -0.0133,     -0.0033,      0.0009,      0.0069,     -0.0069,\n            -0.0047,     -0.0132,      0.0018,      0.0063,      0.0125,\n             0.0056,     -0.0101,      0.0126,     -0.0116,     -0.0107,\n            -0.0008,      0.0007,      0.0015,     -0.0010,     -0.0008,\n            -0.0067,      0.0028,     -0.0057,      0.0023,     -0.0133,\n             0.0122,      0.0031,      0.0009,      0.0052,      0.0175,\n             0.0168,     -0.0128,      0.0104,      0.0079,     -0.0076,\n             0.0013,      0.0019,     -0.0069,     -0.0059,      0.0036,\n             0.0043,     -0.0036,      0.0177,      0.0031,     -0.0009,\n            -0.0035,      0.0000,     -0.0062,     -0.0005,     -0.0033,\n             0.0031,     -0.0039,      0.0073,     -0.0166,     -0.0260,\n             0.0053,     -0.0070,      0.0053,     -0.0069,      0.0120,\n             0.0026,     -0.0099,      0.0050,      0.0074,     -0.0165,\n             0.0001,      0.0175,     -0.0043,      0.0034,      0.0017,\n            -0.0034,      0.0012,      0.0053,     -0.0082,      0.0009,\n             0.0009,      0.0073,      0.0018,      0.0012,     -0.0023,\n             0.0028,     -0.0141,     -0.0009,     -0.0009,      0.0004,\n             0.0007,      0.0046,     -0.0016,     -0.0021,      0.0007,\n            -0.0027,      0.0025,      0.0100,      0.0037,     -0.0040,\n             0.0181,      0.0151,      0.0057,     -0.0132,      0.0069,\n             0.0052,     -0.0166,      0.0012,      0.0024,      0.0041,\n            -0.0038,     -0.0009,      0.0063,     -0.0071,     -0.0018,\n            -0.0063,     -0.0135,     -0.0093,      0.0006,     -0.0135,\n            -0.0006,     -0.0006,     -0.0067,     -0.0037,      0.0090,\n             0.0250,      0.0094,      0.0034,     -0.0187,     -0.0069,\n             0.0175,     -0.0260,     -0.0023,      0.0041,     -0.0017,\n             0.0023,     -0.0287,      0.0032,     -0.0064,     -0.0009,\n            -0.0041,     -0.0007,     -0.0037,      0.0076,      0.0027,\n             0.0056,      0.0027,      0.0173,      0.0062,      0.0029,\n             0.0127,     -0.0004,     -0.0023,      0.0012,     -0.0047,\n             0.0168,      0.0053,      0.0028,     -0.0038,      0.0023,\n             0.0042,     -0.0178,      0.0005,     -0.0039,     -0.0033,\n             0.0044,      0.0086,      0.0023,     -0.0010,      0.0118,\n             0.0026,      0.0071,      0.0001,      0.0009,     -0.0098,\n             0.0152,      0.0050,      0.0006,     -0.0483,     -0.0132,\n            -0.0128,     -0.0070,     -0.0141,     -0.0009,     -0.0287,\n            -0.0178,      0.0089,     -0.0071,      0.0152,      0.0314,\n            -0.0304,     -0.0219,      0.0030,     -0.0085,      0.0037,\n            -0.0064,     -0.0269,      0.0024,     -0.0230,      0.0123,\n            -0.0058,      0.0085,      0.0027,     -0.0132,      0.0018,\n             0.0104,      0.0053,     -0.0009,      0.0063,      0.0032,\n             0.0005,     -0.0071,      0.0012,     -0.0048,     -0.0051,\n            -0.0033,     -0.0038,     -0.0152,      0.0002,     -0.0012,\n            -0.0048,      0.0069,     -0.0388,      0.0012,     -0.0066,\n             0.0183,      0.0011,      0.0025,      0.0036,      0.0063,\n             0.0079,     -0.0069,     -0.0009,     -0.0071,     -0.0064,\n            -0.0039,      0.0152,     -0.0048,     -0.0034,     -0.0088,\n             0.0055,      0.0096,      0.0021,      0.0011,     -0.0035,\n            -0.0049,     -0.0039,     -0.0048,      0.0059,      0.0007,\n            -0.0357,     -0.0095,      0.0053,      0.0117,      0.0125,\n            -0.0076,      0.0120,      0.0004,     -0.0018,     -0.0009,\n            -0.0033,      0.0314,     -0.0051,     -0.0088,      0.0130,\n            -0.0078,     -0.0075,     -0.0198,     -0.0140,     -0.0106,\n            -0.0113,     -0.0078,      0.0008,     -0.0086,     -0.0035,\n             0.0001,     -0.0044,      0.0057,      0.0024,      0.0056,\n             0.0013,      0.0026,      0.0007,     -0.0063,     -0.0041,\n             0.0044,     -0.0304,     -0.0033,      0.0055,     -0.0078,\n            -0.0013,      0.0036,      0.0007,      0.0012,      0.0010,\n            -0.0043,      0.0049,      0.0014,     -0.0001,     -0.0009,\n             0.0122,      0.0022,      0.0038,      0.0038,     -0.0101,\n             0.0019,     -0.0099,      0.0046,     -0.0135,     -0.0007,\n             0.0086,     -0.0219,     -0.0038,      0.0096,     -0.0075,\n             0.0036,     -0.0016,      0.0007,      0.0016,      0.0074,\n             0.0078,     -0.0003,      0.0029,      0.0031,     -0.0045,\n             0.0034,     -0.0095,     -0.0028,      0.0152,      0.0126,\n            -0.0069,      0.0050,     -0.0016,     -0.0093,     -0.0037,\n             0.0023,      0.0030,     -0.0152,      0.0021,     -0.0198,\n             0.0007,      0.0007,      0.0002,      0.0087,     -0.0076,\n            -0.0067,     -0.0035,      0.0109,     -0.0004,     -0.0007,\n            -0.0037,      0.0155,     -0.0169,      0.0120,     -0.0116,\n            -0.0059,      0.0074,     -0.0021,      0.0006,      0.0076,\n            -0.0010,     -0.0085,      0.0002,      0.0011,     -0.0140,\n             0.0012,      0.0016,      0.0087,      0.0052,      0.0011,\n             0.0014,      0.0039,      0.0045,     -0.0472,     -0.0003,\n             0.0272,      0.0107,     -0.0210,      0.0114,     -0.0107,\n             0.0036,     -0.0165,      0.0007,     -0.0135,      0.0027,\n             0.0118,      0.0037,     -0.0012,     -0.0035,     -0.0106,\n             0.0010,      0.0074,     -0.0076,      0.0011,     -0.0009,\n             0.0036,      0.0022,      0.0003,      0.0001,      0.0016,\n             0.0018,      0.0132,     -0.0258,      0.0044,     -0.0008,\n             0.0043,      0.0001,     -0.0027,     -0.0006,      0.0056,\n             0.0026,     -0.0064,     -0.0048,     -0.0049,     -0.0113,\n            -0.0043,      0.0078,     -0.0067,      0.0014,      0.0036,\n            -0.0010,     -0.0004,      0.0027,      0.0032,      0.0014,\n            -0.0096,      0.0011,     -0.0094,      0.0106,      0.0007,\n            -0.0036,      0.0175,      0.0025,     -0.0006,      0.0027,\n             0.0071,     -0.0269,      0.0069,     -0.0039,     -0.0078,\n             0.0049,     -0.0003,     -0.0035,      0.0039,      0.0022,\n            -0.0004,     -0.0022,      0.0031,      0.0022,     -0.0014,\n            -0.0016,     -0.0103,     -0.0069,     -0.0179,      0.0015,\n             0.0177,     -0.0043,      0.0100,     -0.0067,      0.0173,\n             0.0001,      0.0024,     -0.0388,     -0.0048,      0.0008,\n             0.0014,      0.0029,      0.0109,      0.0045,      0.0003,\n             0.0027,      0.0031,     -0.0024,      0.0010,     -0.0290,\n             0.0048,      0.0050,     -0.0021,      0.0082,     -0.0010,\n             0.0031,      0.0034,      0.0037,     -0.0037,      0.0062,\n             0.0009,     -0.0230,      0.0012,      0.0059,     -0.0086,\n            -0.0001,      0.0031,     -0.0004,     -0.0472,      0.0001,\n             0.0032,      0.0022,      0.0010,     -0.0003,     -0.0036,\n             0.0039,     -0.0033,     -0.0017,     -0.0017,     -0.0008,\n            -0.0009,      0.0017,     -0.0040,      0.0090,      0.0029,\n            -0.0098,      0.0123,     -0.0066,      0.0007,     -0.0035,\n            -0.0009,     -0.0045,     -0.0007,     -0.0003,      0.0016,\n             0.0014,     -0.0014,     -0.0290,     -0.0036,      0.0019,\n             0.0070,     -0.0136,     -0.0007,      0.0120,     -0.0207,\n             0.0028,      0.0000,      0.0012,      0.0151,      0.0094,\n            -0.0004,      0.0050,      0.0085,      0.0011,     -0.0095,\n            -0.0044,      0.0022,     -0.0095,      0.0155,      0.0107,\n             0.0132,      0.0011,     -0.0103,      0.0050,     -0.0033,\n            -0.0136,     -0.0341,     -0.0102,      0.0579,     -0.0115,\n            -0.0497,      0.0242,      0.0068,      0.0006,     -0.0087,\n            -0.0658,      0.0494,     -0.0184,      0.0114,     -0.0239,\n             0.0287,      0.0093,      0.0202,      0.0316,      0.0187,\n            -0.0069,     -0.0053,     -0.0929,     -0.0817,      0.0172,\n            -0.0007,     -0.0102,      0.0047,      0.0163,     -0.0146,\n            -0.0164,      0.0019,      0.0058,      0.1118,      0.0026,\n             0.0125,      0.0094,     -0.0078,     -0.0017,     -0.0064,\n            -0.0062,     -0.0144,     -0.0011,      0.0159,      0.0133,\n             0.0140,     -0.0096,     -0.0432,      0.0105,      0.0006,\n             0.0120,      0.0579,      0.0163,     -0.0861,      0.0033,\n             0.0756,     -0.0201,      0.0184,     -0.0045,      0.0060,\n            -0.0036,     -0.0326,     -0.0320,     -0.0023,      0.0034,\n             0.0181,     -0.0248,     -0.0226,     -0.0340,     -0.0012,\n             0.0168,      0.0088,     -0.0071,     -0.0075,     -0.0081,\n            -0.0207,     -0.0115,     -0.0146,      0.0033,      0.0022,\n             0.0127,      0.0185,     -0.0096,     -0.0013,      0.0044,\n            -0.0036,     -0.0162,     -0.0052,     -0.0049,      0.0024,\n            -0.0055,      0.0106,     -0.0034,      0.0090,      0.0088,\n             0.0140,      0.0112,      0.0029,     -0.0048,     -0.0034,\n             0.0028,     -0.0497,     -0.0164,      0.0756,      0.0127,\n            -0.0660,      0.0090,      0.0504,      0.0095,      0.0044,\n            -0.0779,      0.0178,      0.0018,      0.0104,     -0.0074,\n             0.0460,      0.0065,      0.0291,      0.0355,      0.0091,\n            -0.0158,     -0.0036,     -0.0372,     -0.0212,      0.0236,\n             0.0000,      0.0242,      0.0019,     -0.0201,      0.0185,\n             0.0090,      0.0006,     -0.0004,     -0.0156,     -0.0075,\n            -0.0420,      0.0250,     -0.0007,      0.0003,      0.0127,\n             0.0039,     -0.0027,      0.0074,     -0.0208,     -0.0053,\n            -0.0091,     -0.0044,     -0.0084,     -0.0171,     -0.0095,\n             0.0012,      0.0068,      0.0058,      0.0184,     -0.0096,\n             0.0504,     -0.0004,      0.1401,      0.0034,      0.0025,\n            -0.0223,      0.0536,      0.0047,     -0.0030,      0.0045,\n            -0.0047,     -0.0067,     -0.0093,     -0.0070,     -0.0041,\n            -0.0037,      0.0039,     -0.0823,     -0.0313,     -0.0072,\n             0.0151,      0.0006,      0.1118,     -0.0045,     -0.0013,\n             0.0095,     -0.0156,      0.0034,      0.0035,     -0.0021,\n            -0.0021,      0.0125,     -0.0147,     -0.0069,     -0.0149,\n            -0.0098,     -0.0062,     -0.0109,      0.0025,      0.0040,\n             0.0005,     -0.0008,     -0.0208,     -0.0019,      0.0006,\n             0.0094,     -0.0087,      0.0026,      0.0060,      0.0044,\n             0.0044,     -0.0075,      0.0025,     -0.0021,      0.0039,\n            -0.0012,      0.0068,      0.0001,     -0.0025,     -0.0034,\n            -0.0039,      0.0054,      0.0049,      0.0006,     -0.0057,\n             0.0012,      0.0003,     -0.0011,     -0.0012,     -0.0006,\n            -0.0004,     -0.0658,      0.0125,     -0.0036,     -0.0036,\n            -0.0779,     -0.0420,     -0.0223,     -0.0021,     -0.0012,\n             0.0073,     -0.0006,      0.0166,      0.0088,      0.0043,\n            -0.0340,      0.0127,      0.0229,      0.0003,     -0.0042,\n             0.0255,      0.0155,      0.0043,      0.0092,     -0.0102,\n             0.0050,      0.0494,      0.0094,     -0.0326,     -0.0162,\n             0.0178,      0.0250,      0.0536,      0.0125,      0.0068,\n            -0.0006,      0.0460,     -0.0053,      0.0609,      0.0540,\n             0.0082,      0.0124,     -0.0221,     -0.0172,      0.0120,\n            -0.0028,      0.0011,     -0.0018,     -0.0021,      0.0115,\n             0.0085,     -0.0184,     -0.0078,     -0.0320,     -0.0052,\n             0.0018,     -0.0007,      0.0047,     -0.0147,      0.0001,\n             0.0166,     -0.0053,      0.0435,      0.0091,      0.0213,\n            -0.0129,     -0.0066,     -0.0050,      0.0095,      0.0134,\n             0.0059,      0.0182,      0.0036,      0.0507,     -0.0036,\n             0.0011,      0.0114,     -0.0017,     -0.0023,     -0.0049,\n             0.0104,      0.0003,     -0.0030,     -0.0069,     -0.0025,\n             0.0088,      0.0609,      0.0091,      0.0116,     -0.0052,\n            -0.0325,     -0.0033,     -0.0047,      0.0043,      0.0023,\n             0.0111,      0.0035,      0.0112,      0.0054,     -0.0039,\n            -0.0095,     -0.0239,     -0.0064,      0.0034,      0.0024,\n            -0.0074,      0.0127,      0.0045,     -0.0149,     -0.0034,\n             0.0043,      0.0540,      0.0213,     -0.0052,      0.0070,\n            -0.0119,     -0.0026,     -0.0055,      0.0043,     -0.0008,\n             0.0035,      0.0055,      0.0249,     -0.0016,      0.0022,\n            -0.0044,      0.0287,     -0.0062,      0.0181,     -0.0055,\n             0.0460,      0.0039,     -0.0047,     -0.0098,     -0.0039,\n            -0.0340,      0.0082,     -0.0129,     -0.0325,     -0.0119,\n             0.0968,     -0.0073,     -0.0552,      0.0143,     -0.0001,\n            -0.0050,     -0.0014,     -0.0600,     -0.0175,      0.0045,\n             0.0022,      0.0093,     -0.0144,     -0.0248,      0.0106,\n             0.0065,     -0.0027,     -0.0067,     -0.0062,      0.0054,\n             0.0127,      0.0124,     -0.0066,     -0.0033,     -0.0026,\n            -0.0073,      0.0050,     -0.0001,     -0.0022,     -0.0114,\n            -0.0039,     -0.0053,      0.0418,      0.0138,     -0.0042,\n            -0.0095,      0.0202,     -0.0011,     -0.0226,     -0.0034,\n             0.0291,      0.0074,     -0.0093,     -0.0109,      0.0049,\n             0.0229,     -0.0221,     -0.0050,     -0.0047,     -0.0055,\n            -0.0552,     -0.0001,      0.0203,     -0.0037,     -0.0174,\n            -0.0037,     -0.0020,     -0.0281,      0.0309,     -0.0022,\n             0.0155,      0.0316,      0.0159,     -0.0340,      0.0090,\n             0.0355,     -0.0208,     -0.0070,      0.0025,      0.0006,\n             0.0003,     -0.0172,      0.0095,      0.0043,      0.0043,\n             0.0143,     -0.0022,     -0.0037,      0.0175,     -0.0330,\n            -0.0002,     -0.0005,     -0.0027,     -0.0001,     -0.0074,\n             0.0107,      0.0187,      0.0133,     -0.0012,      0.0088,\n             0.0091,     -0.0053,     -0.0041,      0.0040,     -0.0057,\n            -0.0042,      0.0120,      0.0134,      0.0023,     -0.0008,\n            -0.0001,     -0.0114,     -0.0174,     -0.0330,      0.0117,\n            -0.0008,     -0.0010,     -0.0307,     -0.0055,      0.0052,\n             0.0132,     -0.0069,      0.0140,      0.0168,      0.0140,\n            -0.0158,     -0.0091,     -0.0037,      0.0005,      0.0012,\n             0.0255,     -0.0028,      0.0059,      0.0111,      0.0035,\n            -0.0050,     -0.0039,     -0.0037,     -0.0002,     -0.0008,\n            -0.0001,      0.0056,      0.0039,      0.0143,      0.0079,\n             0.0011,     -0.0053,     -0.0096,      0.0088,      0.0112,\n            -0.0036,     -0.0044,      0.0039,     -0.0008,      0.0003,\n             0.0155,      0.0011,      0.0182,      0.0035,      0.0055,\n            -0.0014,     -0.0053,     -0.0020,     -0.0005,     -0.0010,\n             0.0056,      0.0004,      0.0020,      0.0082,      0.0027,\n            -0.0103,     -0.0929,     -0.0432,     -0.0071,      0.0029,\n            -0.0372,     -0.0084,     -0.0823,     -0.0208,     -0.0011,\n             0.0043,     -0.0018,      0.0036,      0.0112,      0.0249,\n            -0.0600,      0.0418,     -0.0281,     -0.0027,     -0.0307,\n             0.0039,      0.0020,      0.0047,      0.0039,     -0.0001,\n             0.0050,     -0.0817,      0.0105,     -0.0075,     -0.0048,\n            -0.0212,     -0.0171,     -0.0313,     -0.0019,     -0.0012,\n             0.0092,     -0.0021,      0.0507,      0.0054,     -0.0016,\n            -0.0175,      0.0138,      0.0309,     -0.0001,     -0.0055,\n             0.0143,      0.0082,      0.0039,      0.0151,     -0.0028,\n            -0.0033,      0.0172,      0.0006,     -0.0081,     -0.0034,\n             0.0236,     -0.0095,     -0.0072,      0.0006,     -0.0006,\n            -0.0102,      0.0115,     -0.0036,     -0.0039,      0.0022,\n             0.0045,     -0.0042,     -0.0022,     -0.0074,      0.0052,\n             0.0079,      0.0027,     -0.0001,     -0.0028,      0.0245,\n             0.0057,     -0.0007,      0.0009,      0.0069,      0.0302,\n            -0.0057,     -0.0062,      0.0053,      0.0057,      0.0034,\n            -0.0023,      0.0006,      0.0027,      0.0025,      0.0053,\n             0.0057,      0.0038,     -0.0028,     -0.0169,     -0.0210,\n            -0.0258,     -0.0094,     -0.0069,     -0.0021,     -0.0017,\n            -0.0007,     -0.0102,      0.0047,      0.0163,     -0.0146,\n            -0.0164,      0.0019,      0.0058,      0.1118,      0.0026,\n             0.0125,      0.0094,     -0.0078,     -0.0017,     -0.0064,\n            -0.0062,     -0.0144,     -0.0011,      0.0159,      0.0133,\n             0.0140,     -0.0096,     -0.0432,      0.0105,      0.0006,\n             0.0009,      0.0047,     -0.0180,     -0.0005,     -0.0093,\n             0.0035,     -0.0015,      0.0001,      0.0055,      0.0033,\n            -0.0063,      0.0269,     -0.0054,     -0.0022,      0.0021,\n             0.0001,     -0.0026,      0.0099,      0.0033,      0.0080,\n            -0.0069,     -0.0072,     -0.0226,     -0.0094,     -0.0040,\n             0.0069,      0.0163,     -0.0005,     -0.0276,      0.0162,\n             0.0122,     -0.0086,     -0.0063,     -0.2000,     -0.0026,\n            -0.0382,     -0.0134,      0.0120,      0.0042,      0.0076,\n             0.0084,      0.0189,     -0.0015,     -0.0235,     -0.0145,\n            -0.0171,      0.0052,     -0.0342,     -0.0093,     -0.0057,\n             0.0302,     -0.0146,     -0.0093,      0.0162,      0.0051,\n            -0.0174,     -0.0273,     -0.0017,      0.0050,      0.0023,\n             0.0041,     -0.0043,     -0.0039,     -0.0011,      0.0069,\n             0.0054,     -0.0006,      0.0025,     -0.0053,     -0.0087,\n            -0.0096,     -0.0172,      0.0013,      0.0034,      0.0048,\n            -0.0057,     -0.0164,      0.0035,      0.0122,     -0.0174,\n             0.0003,      0.0071,      0.0089,      0.0822,     -0.0006,\n             0.0305,      0.0452,     -0.0096,     -0.0024,     -0.0013,\n            -0.0042,     -0.0128,     -0.0010,      0.0188,      0.0138,\n             0.0150,     -0.0057,      0.0405,      0.0121,      0.0049,\n            -0.0062,      0.0019,     -0.0015,     -0.0086,     -0.0273,\n             0.0071,      0.0043,     -0.0026,     -0.0073,     -0.0010,\n            -0.0012,     -0.0005,     -0.0017,     -0.0033,     -0.0048,\n            -0.0041,     -0.0021,     -0.0025,      0.0174,      0.0190,\n             0.0236,      0.0123,      0.0278,     -0.0016,      0.0020,\n             0.0053,      0.0058,      0.0001,     -0.0063,     -0.0017,\n             0.0089,     -0.0026,      0.0019,      0.0058,      0.0005,\n             0.0073,     -0.0235,     -0.0012,     -0.0016,     -0.0001,\n             0.0018,     -0.0006,     -0.0012,      0.0026,      0.0028,\n             0.0053,      0.0012,      0.0103,      0.0082,     -0.0003,\n             0.0057,      0.1118,      0.0055,     -0.2000,      0.0050,\n             0.0822,     -0.0073,      0.0058,      0.0013,      0.0020,\n             0.0011,      0.0571,     -0.0113,      0.0110,     -0.0009,\n             0.0041,      0.0028,      0.0052,      0.0193,     -0.0003,\n            -0.0084,     -0.0059,     -0.0014,      0.0017,      0.0023,\n             0.0034,      0.0026,      0.0033,     -0.0026,      0.0023,\n            -0.0006,     -0.0010,      0.0005,      0.0020,      0.0005,\n            -0.0043,     -0.0078,      0.0019,      0.0007,     -0.0003,\n             0.0014,      0.0018,      0.0056,     -0.0022,     -0.0008,\n             0.0007,      0.0002,      0.0091,      0.0013,      0.0019,\n            -0.0023,      0.0125,     -0.0063,     -0.0382,      0.0041,\n             0.0305,     -0.0012,      0.0073,      0.0011,     -0.0043,\n             0.0023,      0.0067,     -0.0032,      0.0070,      0.0009,\n             0.0248,      0.0114,      0.0129,      0.0108,      0.0396,\n            -0.0039,     -0.0024,     -0.0041,      0.0030,      0.0158,\n             0.0006,      0.0094,      0.0269,     -0.0134,     -0.0043,\n             0.0452,     -0.0005,     -0.0235,      0.0571,     -0.0078,\n             0.0067,      0.0016,     -0.0056,     -0.0733,      0.0037,\n            -0.0433,     -0.0334,     -0.0084,      0.0059,     -0.0039,\n             0.0031,      0.0019,      0.0029,      0.0054,      0.0140,\n             0.0027,     -0.0078,     -0.0054,      0.0120,     -0.0039,\n            -0.0096,     -0.0017,     -0.0012,     -0.0113,      0.0019,\n            -0.0032,     -0.0056,      0.0060,      0.0018,      0.0036,\n            -0.0008,      0.0004,      0.0101,      0.0026,      0.0027,\n             0.0269,      0.0119,      0.0560,     -0.0055,     -0.0005,\n             0.0025,     -0.0017,     -0.0022,      0.0042,     -0.0011,\n            -0.0024,     -0.0033,     -0.0016,      0.0110,      0.0007,\n             0.0070,     -0.0733,      0.0018,      0.0006,      0.0003,\n             0.0051,      0.0003,      0.0012,     -0.0013,     -0.0052,\n             0.0018,     -0.0083,      0.0080,      0.0090,      0.0075,\n             0.0053,     -0.0064,      0.0021,      0.0076,      0.0069,\n            -0.0013,     -0.0048,     -0.0001,     -0.0009,     -0.0003,\n             0.0009,      0.0037,      0.0036,      0.0003,      0.0020,\n             0.0003,      0.0005,     -0.0024,     -0.0064,     -0.0073,\n            -0.0030,     -0.0089,      0.0079,      0.0012,      0.0049,\n             0.0057,     -0.0062,      0.0001,      0.0084,      0.0054,\n            -0.0042,     -0.0041,      0.0018,      0.0041,      0.0014,\n             0.0248,     -0.0433,     -0.0008,      0.0051,      0.0003,\n            -0.0001,      0.0021,      0.0016,     -0.0037,     -0.0067,\n            -0.0052,     -0.0146,     -0.0056,      0.0098,      0.0031,\n             0.0038,     -0.0144,     -0.0026,      0.0189,     -0.0006,\n            -0.0128,     -0.0021,     -0.0006,      0.0028,      0.0018,\n             0.0114,     -0.0334,      0.0004,      0.0003,      0.0005,\n             0.0021,     -0.0006,     -0.0003,     -0.0027,     -0.0042,\n             0.0027,     -0.0051,      0.0561,      0.0060,      0.0050,\n            -0.0028,     -0.0011,      0.0099,     -0.0015,      0.0025,\n            -0.0010,     -0.0025,     -0.0012,      0.0052,      0.0056,\n             0.0129,     -0.0084,      0.0101,      0.0012,     -0.0024,\n             0.0016,     -0.0003,      0.0020,     -0.0024,     -0.0055,\n             0.0058,     -0.0064,     -0.0298,      0.0038,      0.0067,\n            -0.0169,      0.0159,      0.0033,     -0.0235,     -0.0053,\n             0.0188,      0.0174,      0.0026,      0.0193,     -0.0022,\n             0.0108,      0.0059,      0.0026,     -0.0013,     -0.0064,\n            -0.0037,     -0.0027,     -0.0024,      0.0021,      0.0001,\n            -0.0076,     -0.0034,     -0.0080,      0.0019,      0.0021,\n            -0.0210,      0.0133,      0.0080,     -0.0145,     -0.0087,\n             0.0138,      0.0190,      0.0028,     -0.0003,     -0.0008,\n             0.0396,     -0.0039,      0.0027,     -0.0052,     -0.0073,\n            -0.0067,     -0.0042,     -0.0055,      0.0001,      0.0008,\n            -0.0093,     -0.0108,     -0.0068,      0.0013,     -0.0012,\n            -0.0258,      0.0140,     -0.0069,     -0.0171,     -0.0096,\n             0.0150,      0.0236,      0.0053,     -0.0084,      0.0007,\n            -0.0039,      0.0031,      0.0269,      0.0018,     -0.0030,\n            -0.0052,      0.0027,      0.0058,     -0.0076,     -0.0093,\n             0.0078,     -0.0097,     -0.0585,     -0.0046,     -0.0026,\n            -0.0094,     -0.0096,     -0.0072,      0.0052,     -0.0172,\n            -0.0057,      0.0123,      0.0012,     -0.0059,      0.0002,\n            -0.0024,      0.0019,      0.0119,     -0.0083,     -0.0089,\n            -0.0146,     -0.0051,     -0.0064,     -0.0034,     -0.0108,\n            -0.0097,      0.0072,     -0.0557,     -0.0139,     -0.0016,\n            -0.0069,     -0.0432,     -0.0226,     -0.0342,      0.0013,\n             0.0405,      0.0278,      0.0103,     -0.0014,      0.0091,\n            -0.0041,      0.0029,      0.0560,      0.0080,      0.0079,\n            -0.0056,      0.0561,     -0.0298,     -0.0080,     -0.0068,\n            -0.0585,     -0.0557,      0.0119,     -0.0136,     -0.0097,\n            -0.0021,      0.0105,     -0.0094,     -0.0093,      0.0034,\n             0.0121,     -0.0016,      0.0082,      0.0017,      0.0013,\n             0.0030,      0.0054,     -0.0055,      0.0090,      0.0012,\n             0.0098,      0.0060,      0.0038,      0.0019,      0.0013,\n            -0.0046,     -0.0139,     -0.0136,      0.0063,      0.0124,\n            -0.0017,      0.0006,     -0.0040,     -0.0057,      0.0048,\n             0.0049,      0.0020,     -0.0003,      0.0023,      0.0019,\n             0.0158,      0.0140,     -0.0005,      0.0075,      0.0049,\n             0.0031,      0.0050,      0.0067,      0.0021,     -0.0012,\n            -0.0026,     -0.0016,     -0.0097,      0.0124,      0.0015,\n            -0.0061,      0.0120,      0.0069,     -0.0124,      0.0021,\n             0.0023,     -0.0005,     -0.0082,     -0.0132,     -0.0187,\n             0.0012,     -0.0483,     -0.0132,      0.0036,      0.0117,\n             0.0024,      0.0038,      0.0152,      0.0120,      0.0114,\n             0.0044,      0.0106,     -0.0179,      0.0082,     -0.0017,\n             0.0120,      0.0579,      0.0163,     -0.0861,      0.0033,\n             0.0756,     -0.0201,      0.0184,     -0.0045,      0.0060,\n            -0.0036,     -0.0326,     -0.0320,     -0.0023,      0.0034,\n             0.0181,     -0.0248,     -0.0226,     -0.0340,     -0.0012,\n             0.0168,      0.0088,     -0.0071,     -0.0075,     -0.0081,\n             0.0069,      0.0163,     -0.0005,     -0.0276,      0.0162,\n             0.0122,     -0.0086,     -0.0063,     -0.2000,     -0.0026,\n            -0.0382,     -0.0134,      0.0120,      0.0042,      0.0076,\n             0.0084,      0.0189,     -0.0015,     -0.0235,     -0.0145,\n            -0.0171,      0.0052,     -0.0342,     -0.0093,     -0.0057,\n            -0.0124,     -0.0861,     -0.0276,      0.1021,      0.0097,\n            -0.0853,      0.0164,      0.0260,      0.0407,      0.0084,\n            -0.0508,      0.0114,      0.0331,      0.0025,     -0.0082,\n             0.0159,      0.0181,      0.0223,      0.0414,      0.0074,\n            -0.0210,     -0.0104,     -0.0039,     -0.0536,      0.0124,\n             0.0021,      0.0033,      0.0162,      0.0097,     -0.0038,\n            -0.0156,      0.0026,      0.0091,     -0.0088,     -0.0077,\n             0.0030,      0.0228,     -0.0198,      0.0118,     -0.0064,\n             0.0091,      0.0110,      0.0113,      0.0038,     -0.0073,\n            -0.0042,      0.0012,     -0.0023,     -0.0008,     -0.0041,\n             0.0023,      0.0756,      0.0122,     -0.0853,     -0.0156,\n             0.0705,     -0.0053,     -0.0112,     -0.0107,     -0.0079,\n             0.0003,      0.0474,     -0.0525,     -0.0061,     -0.0044,\n             0.0035,     -0.0206,     -0.0207,     -0.0394,     -0.0077,\n             0.0194,      0.0007,      0.0220,      0.0117,     -0.0058,\n            -0.0005,     -0.0201,     -0.0086,      0.0162,      0.0026,\n            -0.0053,      0.0034,      0.0040,      0.0109,      0.0124,\n             0.0084,      0.0256,      0.0020,     -0.0063,     -0.0150,\n            -0.0065,     -0.0025,     -0.0113,     -0.0067,     -0.0218,\n            -0.0141,     -0.0082,     -0.0063,      0.0087,      0.0155,\n            -0.0082,      0.0184,     -0.0063,      0.0260,      0.0091,\n            -0.0112,      0.0040,     -0.1661,     -0.0029,     -0.0040,\n             0.0137,      0.1491,     -0.0078,      0.0053,     -0.0008,\n             0.0074,      0.0081,      0.0172,      0.0043,     -0.0049,\n            -0.0020,     -0.0100,      0.0109,      0.0021,      0.0099,\n            -0.0132,     -0.0045,     -0.2000,      0.0407,     -0.0088,\n            -0.0107,      0.0109,     -0.0029,     -0.0028,      0.0056,\n             0.0015,      0.0084,      0.0232,      0.0057,      0.0194,\n             0.0127,      0.0102,      0.0317,     -0.0035,      0.0162,\n            -0.0006,      0.0079,     -0.0021,      0.0004,     -0.0158,\n            -0.0187,      0.0060,     -0.0026,      0.0084,     -0.0077,\n            -0.0079,      0.0124,     -0.0040,      0.0056,     -0.0047,\n             0.0019,     -0.0047,     -0.0016,     -0.0046,      0.0000,\n            -0.0026,     -0.0067,     -0.0137,     -0.0107,      0.0010,\n            -0.0049,      0.0046,     -0.0011,      0.0057,     -0.0019,\n             0.0012,     -0.0036,     -0.0382,     -0.0508,      0.0030,\n             0.0003,      0.0084,      0.0137,      0.0015,      0.0019,\n            -0.0035,     -0.0445,     -0.0814,     -0.0078,     -0.0035,\n             0.0226,     -0.0229,     -0.0092,      0.0034,      0.0094,\n             0.0010,     -0.0016,     -0.0744,     -0.0103,      0.0046,\n            -0.0483,     -0.0326,     -0.0134,      0.0114,      0.0228,\n             0.0474,      0.0256,      0.1491,      0.0084,     -0.0047,\n            -0.0445,      0.0170,     -0.0835,     -0.0800,     -0.0625,\n            -0.0105,      0.0113,      0.1788,      0.0404,     -0.0198,\n             0.0052,     -0.0152,     -0.0366,     -0.0534,     -0.0218,\n            -0.0132,     -0.0320,      0.0120,      0.0331,     -0.0198,\n            -0.0525,      0.0020,     -0.0078,      0.0232,     -0.0016,\n            -0.0814,     -0.0835,     -0.0279,     -0.0119,     -0.0037,\n             0.0067,      0.0060,     -0.0028,      0.0055,      0.0112,\n             0.0098,     -0.0078,     -0.0260,     -0.6972,      0.0016,\n             0.0036,     -0.0023,      0.0042,      0.0025,      0.0118,\n            -0.0061,     -0.0063,      0.0053,      0.0057,     -0.0046,\n            -0.0078,     -0.0800,     -0.0119,     -0.0170,     -0.0052,\n             0.0310,      0.0004,      0.0058,     -0.0101,     -0.0072,\n            -0.0122,     -0.0053,     -0.0058,     -0.0078,     -0.0005,\n             0.0117,      0.0034,      0.0076,     -0.0082,     -0.0064,\n            -0.0044,     -0.0150,     -0.0008,      0.0194,      0.0000,\n            -0.0035,     -0.0625,     -0.0037,     -0.0052,      0.0020,\n            -0.0008,     -0.0043,     -0.0039,     -0.0051,      0.0040,\n             0.0026,      0.0009,      0.0017,     -0.0106,     -0.0055,\n             0.0024,      0.0181,      0.0084,      0.0159,      0.0091,\n             0.0035,     -0.0065,      0.0074,      0.0127,     -0.0026,\n             0.0226,     -0.0105,      0.0067,      0.0310,     -0.0008,\n            -0.1437,      0.0081,      0.0072,     -0.0136,     -0.0015,\n             0.0063,     -0.0009,     -0.0017,      0.0516,     -0.0076,\n             0.0038,     -0.0248,      0.0189,      0.0181,      0.0110,\n            -0.0206,     -0.0025,      0.0081,      0.0102,     -0.0067,\n            -0.0229,      0.0113,      0.0060,      0.0004,     -0.0043,\n             0.0081,     -0.0038,      0.0008,     -0.0058,     -0.0024,\n            -0.0059,      0.0016,     -0.0508,     -0.0234,      0.0016,\n             0.0152,     -0.0226,     -0.0015,      0.0223,      0.0113,\n            -0.0207,     -0.0113,      0.0172,      0.0317,     -0.0137,\n            -0.0092,      0.1788,     -0.0028,      0.0058,     -0.0039,\n             0.0072,      0.0008,     -0.0091,     -0.0057,      0.0105,\n             0.0032,      0.0014,     -0.0047,     -0.0141,     -0.0024,\n             0.0120,     -0.0340,     -0.0235,      0.0414,      0.0038,\n            -0.0394,     -0.0067,      0.0043,     -0.0035,     -0.0107,\n             0.0034,      0.0404,      0.0055,     -0.0101,     -0.0051,\n            -0.0136,     -0.0058,     -0.0057,     -0.0135,      0.0173,\n             0.0003,      0.0012,     -0.0009,     -0.0003,      0.0314,\n             0.0114,     -0.0012,     -0.0145,      0.0074,     -0.0073,\n            -0.0077,     -0.0218,     -0.0049,      0.0162,      0.0010,\n             0.0094,     -0.0198,      0.0112,     -0.0072,      0.0040,\n            -0.0015,     -0.0024,      0.0105,      0.0173,     -0.0213,\n             0.0013,      0.0086,      0.0028,      0.0427,     -0.0087,\n             0.0044,      0.0168,     -0.0171,     -0.0210,     -0.0042,\n             0.0194,     -0.0141,     -0.0020,     -0.0006,     -0.0049,\n             0.0010,      0.0052,      0.0098,     -0.0122,      0.0026,\n             0.0063,     -0.0059,      0.0032,      0.0003,      0.0013,\n            -0.0013,      0.0009,     -0.0015,     -0.0006,     -0.0059,\n             0.0106,      0.0088,      0.0052,     -0.0104,      0.0012,\n             0.0007,     -0.0082,     -0.0100,      0.0079,      0.0046,\n            -0.0016,     -0.0152,     -0.0078,     -0.0053,      0.0009,\n            -0.0009,      0.0016,      0.0014,      0.0012,      0.0086,\n             0.0009,     -0.0021,     -0.0003,      0.0008,     -0.0012,\n            -0.0179,     -0.0071,     -0.0342,     -0.0039,     -0.0023,\n             0.0220,     -0.0063,      0.0109,     -0.0021,     -0.0011,\n            -0.0744,     -0.0366,     -0.0260,     -0.0058,      0.0017,\n            -0.0017,     -0.0508,     -0.0047,     -0.0009,      0.0028,\n            -0.0015,     -0.0003,      0.0012,     -0.0427,      0.0101,\n             0.0082,     -0.0075,     -0.0093,     -0.0536,     -0.0008,\n             0.0117,      0.0087,      0.0021,      0.0004,      0.0057,\n            -0.0103,     -0.0534,     -0.6972,     -0.0078,     -0.0106,\n             0.0516,     -0.0234,     -0.0141,     -0.0003,      0.0427,\n            -0.0006,      0.0008,     -0.0427,     -0.0251,      0.0049,\n            -0.0017,     -0.0081,     -0.0057,      0.0124,     -0.0041,\n            -0.0058,      0.0155,      0.0099,     -0.0158,     -0.0019,\n             0.0046,     -0.0218,      0.0016,     -0.0005,     -0.0055,\n            -0.0076,      0.0016,     -0.0024,      0.0314,     -0.0087,\n            -0.0059,     -0.0012,      0.0101,      0.0049,     -0.0070,\n            -0.0074,     -0.0207,      0.0302,      0.0021,      0.0037,\n            -0.0133,     -0.0033,      0.0009,      0.0069,     -0.0069,\n            -0.0047,     -0.0132,      0.0018,      0.0063,      0.0125,\n             0.0056,     -0.0101,      0.0126,     -0.0116,     -0.0107,\n            -0.0008,      0.0007,      0.0015,     -0.0010,     -0.0008,\n            -0.0207,     -0.0115,     -0.0146,      0.0033,      0.0022,\n             0.0127,      0.0185,     -0.0096,     -0.0013,      0.0044,\n            -0.0036,     -0.0162,     -0.0052,     -0.0049,      0.0024,\n            -0.0055,      0.0106,     -0.0034,      0.0090,      0.0088,\n             0.0140,      0.0112,      0.0029,     -0.0048,     -0.0034,\n             0.0302,     -0.0146,     -0.0093,      0.0162,      0.0051,\n            -0.0174,     -0.0273,     -0.0017,      0.0050,      0.0023,\n             0.0041,     -0.0043,     -0.0039,     -0.0011,      0.0069,\n             0.0054,     -0.0006,      0.0025,     -0.0053,     -0.0087,\n            -0.0096,     -0.0172,      0.0013,      0.0034,      0.0048,\n             0.0021,      0.0033,      0.0162,      0.0097,     -0.0038,\n            -0.0156,      0.0026,      0.0091,     -0.0088,     -0.0077,\n             0.0030,      0.0228,     -0.0198,      0.0118,     -0.0064,\n             0.0091,      0.0110,      0.0113,      0.0038,     -0.0073,\n            -0.0042,      0.0012,     -0.0023,     -0.0008,     -0.0041,\n             0.0037,      0.0022,      0.0051,     -0.0038,     -0.0285,\n            -0.0036,      0.0033,      0.0153,      0.0140,     -0.0057,\n             0.0199,      0.0322,      0.0214,      0.0001,     -0.0137,\n             0.0248,      0.0056,     -0.0007,      0.0041,     -0.0361,\n             0.0122,      0.0045,     -0.0054,      0.0020,      0.0081,\n            -0.0133,      0.0127,     -0.0174,     -0.0156,     -0.0036,\n             0.0449,      0.0049,     -0.0194,      0.0002,     -0.0028,\n            -0.0020,     -0.0213,      0.0143,     -0.0026,      0.0191,\n            -0.0048,     -0.0018,     -0.0109,     -0.0143,      0.0010,\n             0.0039,     -0.0015,     -0.0439,     -0.0046,      0.0021,\n            -0.0033,      0.0185,     -0.0273,      0.0026,      0.0033,\n             0.0049,      0.0060,      0.0061,     -0.0017,     -0.0012,\n            -0.0146,      0.0102,     -0.0017,     -0.0053,     -0.0006,\n             0.0028,      0.0129,     -0.0068,      0.0004,     -0.0032,\n            -0.0092,     -0.0112,      0.0175,     -0.0065,     -0.0081,\n             0.0009,     -0.0096,     -0.0017,      0.0091,      0.0153,\n            -0.0194,      0.0061,     -0.0049,     -0.0003,     -0.0050,\n             0.0111,      0.0156,      0.0008,     -0.0045,      0.0012,\n             0.0025,     -0.0002,     -0.0026,      0.0023,     -0.0019,\n            -0.0036,     -0.0113,      0.0013,     -0.0150,      0.0016,\n             0.0069,     -0.0013,      0.0050,     -0.0088,      0.0140,\n             0.0002,     -0.0017,     -0.0003,      0.0003,     -0.0030,\n            -0.0026,      0.0051,     -0.0003,     -0.0156,     -0.0091,\n            -0.0201,     -0.0107,     -0.0268,     -0.0018,      0.0406,\n            -0.0114,     -0.0058,      0.0007,     -0.0004,     -0.0199,\n            -0.0069,      0.0044,      0.0023,     -0.0077,     -0.0057,\n            -0.0028,     -0.0012,     -0.0050,     -0.0030,      0.0033,\n             0.0004,     -0.0052,      0.0025,     -0.0428,     -0.0200,\n            -0.0119,     -0.0279,     -0.0130,      0.0024,     -0.0132,\n             0.0027,     -0.0002,      0.0028,      0.0077,     -0.0033,\n            -0.0047,     -0.0036,      0.0041,      0.0030,      0.0199,\n            -0.0020,     -0.0146,      0.0111,     -0.0026,      0.0004,\n             0.0040,     -0.0084,      0.0158,     -0.0046,     -0.0001,\n             0.0026,      0.0039,     -0.0063,      0.0972,     -0.0011,\n             0.0035,      0.0009,     -0.0003,     -0.0013,      0.0001,\n            -0.0132,     -0.0162,     -0.0043,      0.0228,      0.0322,\n            -0.0213,      0.0102,      0.0156,      0.0051,     -0.0052,\n            -0.0084,     -0.0031,     -0.0017,      0.0086,      0.0164,\n            -0.0024,     -0.0205,     -0.0101,     -0.0658,      0.0231,\n            -0.0012,      0.0184,      0.0061,      0.0023,     -0.0316,\n             0.0018,     -0.0052,     -0.0039,     -0.0198,      0.0214,\n             0.0143,     -0.0017,      0.0008,     -0.0003,      0.0025,\n             0.0158,     -0.0017,     -0.0208,     -0.0080,     -0.0105,\n            -0.0098,     -0.0065,     -0.0202,     -0.0042,     -0.0161,\n            -0.0127,      0.0004,     -0.0015,      0.0071,     -0.0039,\n             0.0063,     -0.0049,     -0.0011,      0.0118,      0.0001,\n            -0.0026,     -0.0053,     -0.0045,     -0.0156,     -0.0428,\n            -0.0046,      0.0086,     -0.0080,     -0.0074,      0.0002,\n            -0.0026,      0.0003,     -0.0045,      0.0077,     -0.0012,\n             0.0073,      0.0007,      0.0235,     -0.0047,      0.0016,\n             0.0125,      0.0024,      0.0069,     -0.0064,     -0.0137,\n             0.0191,     -0.0006,      0.0012,     -0.0091,     -0.0200,\n            -0.0001,      0.0164,     -0.0105,      0.0002,     -0.0043,\n            -0.0026,      0.0020,     -0.0015,      0.0012,      0.0039,\n             0.0114,      0.0034,      0.0095,     -0.0028,      0.0040,\n             0.0056,     -0.0055,      0.0054,      0.0091,      0.0248,\n            -0.0048,      0.0028,      0.0025,     -0.0201,     -0.0119,\n             0.0026,     -0.0024,     -0.0098,     -0.0026,     -0.0026,\n            -0.0064,      0.0020,     -0.0062,     -0.0014,     -0.0054,\n            -0.0014,     -0.0193,      0.0017,      0.0020,     -0.0094,\n            -0.0101,      0.0106,     -0.0006,      0.0110,      0.0056,\n            -0.0018,      0.0129,     -0.0002,     -0.0107,     -0.0279,\n             0.0039,     -0.0205,     -0.0065,      0.0003,      0.0020,\n             0.0020,     -0.0098,      0.0038,      0.0035,      0.0009,\n             0.0096,     -0.0037,      0.0015,      0.0052,     -0.0012,\n             0.0126,     -0.0034,      0.0025,      0.0113,     -0.0007,\n            -0.0109,     -0.0068,     -0.0026,     -0.0268,     -0.0130,\n            -0.0063,     -0.0101,     -0.0202,     -0.0045,     -0.0015,\n            -0.0062,      0.0038,     -0.0068,      0.0069,     -0.0048,\n             0.0094,      0.0025,     -0.0065,     -0.0013,     -0.0070,\n            -0.0116,      0.0090,     -0.0053,      0.0038,      0.0041,\n            -0.0143,      0.0004,      0.0023,     -0.0018,      0.0024,\n             0.0972,     -0.0658,     -0.0042,      0.0077,      0.0012,\n            -0.0014,      0.0035,      0.0069,     -0.0339,     -0.0123,\n             0.0008,     -0.0003,      0.0041,      0.0085,      0.0257,\n            -0.0107,      0.0088,     -0.0087,     -0.0073,     -0.0361,\n             0.0010,     -0.0032,     -0.0019,      0.0406,     -0.0132,\n            -0.0011,      0.0231,     -0.0161,     -0.0012,      0.0039,\n            -0.0054,      0.0009,     -0.0044,     -0.0123,      0.0221,\n            -0.0057,     -0.0061,     -0.0090,     -0.0039,      0.0028,\n            -0.0008,      0.0140,     -0.0096,     -0.0042,      0.0122,\n             0.0039,     -0.0092,     -0.0036,     -0.0114,      0.0027,\n             0.0035,     -0.0012,     -0.0127,      0.0073,      0.0114,\n            -0.0014,      0.0096,      0.0094,      0.0008,     -0.0057,\n            -0.0001,      0.0001,      0.0059,      0.0065,     -0.0025,\n             0.0007,      0.0112,     -0.0172,      0.0012,      0.0045,\n            -0.0015,     -0.0112,     -0.0113,     -0.0058,     -0.0002,\n             0.0009,      0.0184,      0.0004,      0.0007,      0.0034,\n            -0.0193,     -0.0037,      0.0025,     -0.0003,     -0.0061,\n             0.0001,      0.0004,     -0.0003,     -0.0002,     -0.0031,\n             0.0015,      0.0029,      0.0013,     -0.0023,     -0.0054,\n            -0.0439,      0.0175,      0.0013,      0.0007,      0.0028,\n            -0.0003,      0.0061,     -0.0015,      0.0235,      0.0095,\n             0.0017,      0.0015,     -0.0065,      0.0041,     -0.0090,\n             0.0059,     -0.0003,      0.0007,      0.0009,      0.0039,\n            -0.0010,     -0.0048,      0.0034,     -0.0008,      0.0020,\n            -0.0046,     -0.0065,     -0.0150,     -0.0004,      0.0077,\n            -0.0013,      0.0023,      0.0071,     -0.0047,     -0.0028,\n             0.0020,      0.0052,     -0.0013,      0.0085,     -0.0039,\n             0.0065,     -0.0002,      0.0009,      0.0020,      0.0005,\n            -0.0008,     -0.0034,      0.0048,     -0.0041,      0.0081,\n             0.0021,     -0.0081,      0.0016,     -0.0199,     -0.0033,\n             0.0001,     -0.0316,     -0.0039,      0.0016,      0.0040,\n            -0.0094,     -0.0012,     -0.0070,      0.0257,      0.0028,\n            -0.0025,     -0.0031,      0.0039,      0.0005,      0.0038,\n            -0.0067,      0.0028,     -0.0057,      0.0023,     -0.0133,\n             0.0122,      0.0031,      0.0009,      0.0052,      0.0175,\n             0.0168,     -0.0128,      0.0104,      0.0079,     -0.0076,\n             0.0013,      0.0019,     -0.0069,     -0.0059,      0.0036,\n             0.0043,     -0.0036,      0.0177,      0.0031,     -0.0009,\n             0.0028,     -0.0497,     -0.0164,      0.0756,      0.0127,\n            -0.0660,      0.0090,      0.0504,      0.0095,      0.0044,\n            -0.0779,      0.0178,      0.0018,      0.0104,     -0.0074,\n             0.0460,      0.0065,      0.0291,      0.0355,      0.0091,\n            -0.0158,     -0.0036,     -0.0372,     -0.0212,      0.0236,\n            -0.0057,     -0.0164,      0.0035,      0.0122,     -0.0174,\n             0.0003,      0.0071,      0.0089,      0.0822,     -0.0006,\n             0.0305,      0.0452,     -0.0096,     -0.0024,     -0.0013,\n            -0.0042,     -0.0128,     -0.0010,      0.0188,      0.0138,\n             0.0150,     -0.0057,      0.0405,      0.0121,      0.0049,\n             0.0023,      0.0756,      0.0122,     -0.0853,     -0.0156,\n             0.0705,     -0.0053,     -0.0112,     -0.0107,     -0.0079,\n             0.0003,      0.0474,     -0.0525,     -0.0061,     -0.0044,\n             0.0035,     -0.0206,     -0.0207,     -0.0394,     -0.0077,\n             0.0194,      0.0007,      0.0220,      0.0117,     -0.0058,\n            -0.0133,      0.0127,     -0.0174,     -0.0156,     -0.0036,\n             0.0449,      0.0049,     -0.0194,      0.0002,     -0.0028,\n            -0.0020,     -0.0213,      0.0143,     -0.0026,      0.0191,\n            -0.0048,     -0.0018,     -0.0109,     -0.0143,      0.0010,\n             0.0039,     -0.0015,     -0.0439,     -0.0046,      0.0021,\n             0.0122,     -0.0660,      0.0003,      0.0705,      0.0449,\n            -0.1975,     -0.0057,      0.0458,      0.0082,      0.0073,\n            -0.0549,      0.0245,     -0.0563,      0.0151,      0.0155,\n             0.0177,      0.0067,      0.0137,      0.0388,      0.0114,\n            -0.0147,      0.0082,     -0.0092,     -0.0350,      0.0144,\n             0.0031,      0.0090,      0.0071,     -0.0053,      0.0049,\n            -0.0057,     -0.0001,      0.0022,     -0.0024,     -0.0114,\n            -0.0056,      0.0202,     -0.0025,     -0.0061,      0.0085,\n             0.0028,     -0.0037,      0.0056,      0.0037,      0.0118,\n             0.0103,      0.0042,     -0.0351,     -0.0127,     -0.0151,\n             0.0009,      0.0504,      0.0089,     -0.0112,     -0.0194,\n             0.0458,      0.0022,      0.2102,      0.0015,      0.0010,\n            -0.0171,     -0.1047,      0.0028,     -0.0054,      0.0003,\n            -0.0091,     -0.0092,     -0.0206,      0.0018,      0.0126,\n             0.0071,      0.0130,     -0.0030,     -0.1122,     -0.0095,\n             0.0052,      0.0095,      0.0822,     -0.0107,      0.0002,\n             0.0082,     -0.0024,      0.0015,      0.0019,     -0.0034,\n            -0.0018,     -0.0185,     -0.0204,     -0.0038,     -0.0172,\n            -0.0106,     -0.0037,     -0.0127,      0.0024,     -0.0105,\n            -0.0148,     -0.0038,      0.0050,     -0.0025,      0.0125,\n             0.0175,      0.0044,     -0.0006,     -0.0079,     -0.0028,\n             0.0073,     -0.0114,      0.0010,     -0.0034,      0.0017,\n            -0.0004,     -0.0181,     -0.0023,      0.0025,     -0.0016,\n             0.0006,      0.0072,      0.0108,      0.0084,     -0.0003,\n             0.0046,     -0.0007,     -0.0108,     -0.0012,     -0.0013,\n             0.0168,     -0.0779,      0.0305,      0.0003,     -0.0020,\n            -0.0549,     -0.0056,     -0.0171,     -0.0018,     -0.0004,\n             0.0094,      0.1075,      0.2261,      0.0063,      0.0200,\n            -0.0022,      0.0238,      0.0008,     -0.0033,     -0.0056,\n             0.0025,     -0.0014,      0.0254,      0.0092,     -0.0085,\n            -0.0128,      0.0178,      0.0452,      0.0474,     -0.0213,\n             0.0245,      0.0202,     -0.1047,     -0.0185,     -0.0181,\n             0.1075,      0.0019,      0.0520,      0.0154,      0.0433,\n             0.0039,      0.0107,     -0.0359,      0.0001,     -0.0153,\n            -0.0137,      0.0144,      0.0345,      0.0243,      0.0544,\n             0.0104,      0.0018,     -0.0096,     -0.0525,      0.0143,\n            -0.0563,     -0.0025,      0.0028,     -0.0204,     -0.0023,\n             0.2261,      0.0520,      0.0368,      0.0167,      0.0093,\n            -0.0050,     -0.0038,     -0.0008,     -0.0016,     -0.0040,\n             0.0008,      0.0191,     -0.0183,      0.0217,     -0.0020,\n             0.0079,      0.0104,     -0.0024,     -0.0061,     -0.0026,\n             0.0151,     -0.0061,     -0.0054,     -0.0038,      0.0025,\n             0.0063,      0.0154,      0.0167,      0.0036,      0.0027,\n            -0.0112,      0.0001,     -0.0027,      0.0077,      0.0060,\n             0.0086,     -0.0058,     -0.0095,      0.0008,     -0.0006,\n            -0.0076,     -0.0074,     -0.0013,     -0.0044,      0.0191,\n             0.0155,      0.0085,      0.0003,     -0.0172,     -0.0016,\n             0.0200,      0.0433,      0.0093,      0.0027,     -0.0010,\n            -0.0043,      0.0017,      0.0025,     -0.0007,     -0.0100,\n            -0.0129,     -0.0097,     -0.0072,      0.0125,      0.0051,\n             0.0013,      0.0460,     -0.0042,      0.0035,     -0.0048,\n             0.0177,      0.0028,     -0.0091,     -0.0106,      0.0006,\n            -0.0022,      0.0039,     -0.0050,     -0.0112,     -0.0043,\n             0.0933,     -0.0073,     -0.0265,      0.0115,      0.0024,\n            -0.0064,     -0.0121,     -0.0061,     -0.0488,      0.0067,\n             0.0019,      0.0065,     -0.0128,     -0.0206,     -0.0018,\n             0.0067,     -0.0037,     -0.0092,     -0.0037,      0.0072,\n             0.0238,      0.0107,     -0.0038,      0.0001,      0.0017,\n            -0.0073,     -0.0012,      0.0013,      0.0009,     -0.0026,\n             0.0006,     -0.0091,      0.0376,      0.0320,     -0.0017,\n            -0.0069,      0.0291,     -0.0010,     -0.0207,     -0.0109,\n             0.0137,      0.0056,     -0.0206,     -0.0127,      0.0108,\n             0.0008,     -0.0359,     -0.0008,     -0.0027,      0.0025,\n            -0.0265,      0.0013,      0.0091,     -0.0003,     -0.0022,\n            -0.0002,     -0.0054,      0.0159,     -0.0010,      0.0039,\n            -0.0059,      0.0355,      0.0188,     -0.0394,     -0.0143,\n             0.0388,      0.0037,      0.0018,      0.0024,      0.0084,\n            -0.0033,      0.0001,     -0.0016,      0.0077,     -0.0007,\n             0.0115,      0.0009,     -0.0003,      0.0167,     -0.0120,\n             0.0005,     -0.0008,     -0.0023,     -0.0039,     -0.0245,\n             0.0036,      0.0091,      0.0138,     -0.0077,      0.0010,\n             0.0114,      0.0118,      0.0126,     -0.0105,     -0.0003,\n            -0.0056,     -0.0153,     -0.0040,      0.0060,     -0.0100,\n             0.0024,     -0.0026,     -0.0022,     -0.0120,      0.0069,\n            -0.0014,     -0.0028,     -0.0038,     -0.0056,      0.0071,\n             0.0043,     -0.0158,      0.0150,      0.0194,      0.0039,\n            -0.0147,      0.0103,      0.0071,     -0.0148,      0.0046,\n             0.0025,     -0.0137,      0.0008,      0.0086,     -0.0129,\n            -0.0064,      0.0006,     -0.0002,      0.0005,     -0.0014,\n            -0.0001,     -0.0006,      0.0053,      0.0068,      0.0047,\n            -0.0037,     -0.0036,     -0.0057,      0.0007,     -0.0015,\n             0.0082,      0.0042,      0.0130,     -0.0038,     -0.0007,\n            -0.0014,      0.0144,      0.0191,     -0.0058,     -0.0097,\n            -0.0121,     -0.0091,     -0.0054,     -0.0008,     -0.0028,\n            -0.0006,      0.0006,      0.0008,      0.0007,      0.0011,\n             0.0177,     -0.0372,      0.0405,      0.0220,     -0.0439,\n            -0.0092,     -0.0351,     -0.0030,      0.0050,     -0.0108,\n             0.0254,      0.0345,     -0.0183,     -0.0095,     -0.0072,\n            -0.0061,      0.0376,      0.0159,     -0.0023,     -0.0038,\n             0.0053,      0.0008,     -0.0026,      0.0269,      0.0040,\n             0.0031,     -0.0212,      0.0121,      0.0117,     -0.0046,\n            -0.0350,     -0.0127,     -0.1122,     -0.0025,     -0.0012,\n             0.0092,      0.0243,      0.0217,      0.0008,      0.0125,\n            -0.0488,      0.0320,     -0.0010,     -0.0039,     -0.0056,\n             0.0068,      0.0007,      0.0269,      0.0043,     -0.0052,\n            -0.0009,      0.0236,      0.0049,     -0.0058,      0.0021,\n             0.0144,     -0.0151,     -0.0095,      0.0125,     -0.0013,\n            -0.0085,      0.0544,     -0.0020,     -0.0006,      0.0051,\n             0.0067,     -0.0017,      0.0039,     -0.0245,      0.0071,\n             0.0047,      0.0011,      0.0040,     -0.0052,      0.0155,\n            -0.0035,      0.0000,     -0.0062,     -0.0005,     -0.0033,\n             0.0031,     -0.0039,      0.0073,     -0.0166,     -0.0260,\n             0.0053,     -0.0070,      0.0053,     -0.0069,      0.0120,\n             0.0026,     -0.0099,      0.0050,      0.0074,     -0.0165,\n             0.0001,      0.0175,     -0.0043,      0.0034,      0.0017,\n             0.0000,      0.0242,      0.0019,     -0.0201,      0.0185,\n             0.0090,      0.0006,     -0.0004,     -0.0156,     -0.0075,\n            -0.0420,      0.0250,     -0.0007,      0.0003,      0.0127,\n             0.0039,     -0.0027,      0.0074,     -0.0208,     -0.0053,\n            -0.0091,     -0.0044,     -0.0084,     -0.0171,     -0.0095,\n            -0.0062,      0.0019,     -0.0015,     -0.0086,     -0.0273,\n             0.0071,      0.0043,     -0.0026,     -0.0073,     -0.0010,\n            -0.0012,     -0.0005,     -0.0017,     -0.0033,     -0.0048,\n            -0.0041,     -0.0021,     -0.0025,      0.0174,      0.0190,\n             0.0236,      0.0123,      0.0278,     -0.0016,      0.0020,\n            -0.0005,     -0.0201,     -0.0086,      0.0162,      0.0026,\n            -0.0053,      0.0034,      0.0040,      0.0109,      0.0124,\n             0.0084,      0.0256,      0.0020,     -0.0063,     -0.0150,\n            -0.0065,     -0.0025,     -0.0113,     -0.0067,     -0.0218,\n            -0.0141,     -0.0082,     -0.0063,      0.0087,      0.0155,\n            -0.0033,      0.0185,     -0.0273,      0.0026,      0.0033,\n             0.0049,      0.0060,      0.0061,     -0.0017,     -0.0012,\n            -0.0146,      0.0102,     -0.0017,     -0.0053,     -0.0006,\n             0.0028,      0.0129,     -0.0068,      0.0004,     -0.0032,\n            -0.0092,     -0.0112,      0.0175,     -0.0065,     -0.0081,\n             0.0031,      0.0090,      0.0071,     -0.0053,      0.0049,\n            -0.0057,     -0.0001,      0.0022,     -0.0024,     -0.0114,\n            -0.0056,      0.0202,     -0.0025,     -0.0061,      0.0085,\n             0.0028,     -0.0037,      0.0056,      0.0037,      0.0118,\n             0.0103,      0.0042,     -0.0351,     -0.0127,     -0.0151,\n            -0.0039,      0.0006,      0.0043,      0.0034,      0.0060,\n            -0.0001,      0.0072,     -0.0096,      0.0117,      0.0312,\n             0.0103,     -0.0027,     -0.0015,      0.0004,     -0.0036,\n            -0.0037,      0.0075,     -0.0083,     -0.0060,      0.0112,\n            -0.0030,     -0.0203,     -0.0170,      0.0025,     -0.0018,\n             0.0073,     -0.0004,     -0.0026,      0.0040,      0.0061,\n             0.0022,     -0.0096,      0.0018,     -0.0014,      0.0005,\n             0.0066,      0.0106,      0.0003,      0.0009,      0.0015,\n             0.0011,     -0.0013,      0.0038,     -0.0055,     -0.0072,\n            -0.0048,     -0.0068,      0.0078,     -0.0034,      0.0014,\n            -0.0166,     -0.0156,     -0.0073,      0.0109,     -0.0017,\n            -0.0024,      0.0117,     -0.0014,     -0.0009,     -0.0050,\n             0.0341,      0.0029,     -0.0042,      0.0044,      0.0009,\n             0.0037,      0.0128,      0.0086,     -0.0019,     -0.0069,\n             0.0061,      0.0020,     -0.0006,      0.0035,     -0.0037,\n            -0.0260,     -0.0075,     -0.0010,      0.0124,     -0.0012,\n            -0.0114,      0.0312,      0.0005,     -0.0050,      0.0004,\n            -0.0011,      0.0260,     -0.0065,      0.0050,      0.0011,\n             0.0030,      0.0005,      0.0010,     -0.0021,      0.0059,\n             0.0023,     -0.0031,     -0.0013,     -0.0082,     -0.0096,\n             0.0053,     -0.0420,     -0.0012,      0.0084,     -0.0146,\n            -0.0056,      0.0103,      0.0066,      0.0341,     -0.0011,\n            -0.0070,     -0.0007,      0.0056,      0.0039,      0.0304,\n             0.0168,     -0.0011,      0.0301,      0.0009,      0.0070,\n             0.0002,     -0.0000,     -0.0002,      0.0073,      0.0013,\n            -0.0070,      0.0250,     -0.0005,      0.0256,      0.0102,\n             0.0202,     -0.0027,      0.0106,      0.0029,      0.0260,\n            -0.0007,      0.0132,     -0.0019,     -0.0263,     -0.0240,\n             0.0186,      0.0209,      0.0053,      0.0070,     -0.0041,\n            -0.0020,      0.0115,      0.0068,     -0.0072,     -0.0040,\n             0.0053,     -0.0007,     -0.0017,      0.0020,     -0.0017,\n            -0.0025,     -0.0015,      0.0003,     -0.0042,     -0.0065,\n             0.0056,     -0.0019,      0.0038,      0.0070,      0.0029,\n             0.0034,      0.0028,      0.0126,     -0.0012,     -0.0012,\n             0.0016,     -0.0064,      0.0231,      0.0041,      0.0062,\n            -0.0069,      0.0003,     -0.0033,     -0.0063,     -0.0053,\n            -0.0061,      0.0004,      0.0009,      0.0044,      0.0050,\n             0.0039,     -0.0263,      0.0070,      0.0139,      0.0023,\n            -0.0012,     -0.0009,     -0.0011,     -0.0022,      0.0024,\n             0.0031,      0.0127,      0.0170,      0.0040,     -0.0031,\n             0.0120,      0.0127,     -0.0048,     -0.0150,     -0.0006,\n             0.0085,     -0.0036,      0.0015,      0.0009,      0.0011,\n             0.0304,     -0.0240,      0.0029,      0.0023,     -0.0011,\n             0.0001,     -0.0001,      0.0045,      0.0046,     -0.0004,\n             0.0018,      0.0092,      0.0038,      0.0088,      0.0037,\n             0.0026,      0.0039,     -0.0041,     -0.0065,      0.0028,\n             0.0028,     -0.0037,      0.0011,      0.0037,      0.0030,\n             0.0168,      0.0186,      0.0034,     -0.0012,      0.0001,\n             0.0015,     -0.0014,     -0.0018,     -0.0034,     -0.0066,\n             0.0004,      0.0027,     -0.0101,     -0.0033,      0.0000,\n            -0.0099,     -0.0027,     -0.0021,     -0.0025,      0.0129,\n            -0.0037,      0.0075,     -0.0013,      0.0128,      0.0005,\n            -0.0011,      0.0209,      0.0028,     -0.0009,     -0.0001,\n            -0.0014,      0.0012,     -0.0024,     -0.0043,     -0.0102,\n            -0.0102,      0.0093,      0.0001,     -0.0011,      0.0034,\n             0.0050,      0.0074,     -0.0025,     -0.0113,     -0.0068,\n             0.0056,     -0.0083,      0.0038,      0.0086,      0.0010,\n             0.0301,      0.0053,      0.0126,     -0.0011,      0.0045,\n            -0.0018,     -0.0024,     -0.0002,     -0.0083,      0.0036,\n             0.0022,      0.0108,      0.0098,      0.0001,      0.0012,\n             0.0074,     -0.0208,      0.0174,     -0.0067,      0.0004,\n             0.0037,     -0.0060,     -0.0055,     -0.0019,     -0.0021,\n             0.0009,      0.0070,     -0.0012,     -0.0022,      0.0046,\n            -0.0034,     -0.0043,     -0.0083,     -0.0037,      0.0046,\n             0.0005,     -0.0009,      0.0007,      0.0138,      0.0095,\n            -0.0165,     -0.0053,      0.0190,     -0.0218,     -0.0032,\n             0.0118,      0.0112,     -0.0072,     -0.0069,      0.0059,\n             0.0070,     -0.0041,     -0.0012,      0.0024,     -0.0004,\n            -0.0066,     -0.0102,      0.0036,      0.0046,     -0.0002,\n            -0.0001,      0.0001,      0.0015,      0.0067,     -0.0010,\n             0.0001,     -0.0091,      0.0236,     -0.0141,     -0.0092,\n             0.0103,     -0.0030,     -0.0048,      0.0061,      0.0023,\n             0.0002,     -0.0020,      0.0016,      0.0031,      0.0018,\n             0.0004,     -0.0102,      0.0022,      0.0005,     -0.0001,\n            -0.0007,     -0.0040,     -0.0035,     -0.0034,      0.0006,\n             0.0175,     -0.0044,      0.0123,     -0.0082,     -0.0112,\n             0.0042,     -0.0203,     -0.0068,      0.0020,     -0.0031,\n            -0.0000,      0.0115,     -0.0064,      0.0127,      0.0092,\n             0.0027,      0.0093,      0.0108,     -0.0009,      0.0001,\n            -0.0040,      0.0019,     -0.0040,     -0.0017,      0.0023,\n            -0.0043,     -0.0084,      0.0278,     -0.0063,      0.0175,\n            -0.0351,     -0.0170,      0.0078,     -0.0006,     -0.0013,\n            -0.0002,      0.0068,      0.0231,      0.0170,      0.0038,\n            -0.0101,      0.0001,      0.0098,      0.0007,      0.0015,\n            -0.0035,     -0.0040,      0.0056,     -0.0020,     -0.0025,\n             0.0034,     -0.0171,     -0.0016,      0.0087,     -0.0065,\n            -0.0127,      0.0025,     -0.0034,      0.0035,     -0.0082,\n             0.0073,     -0.0072,      0.0041,      0.0040,      0.0088,\n            -0.0033,     -0.0011,      0.0001,      0.0138,      0.0067,\n            -0.0034,     -0.0017,     -0.0020,      0.0011,     -0.0036,\n             0.0017,     -0.0095,      0.0020,      0.0155,     -0.0081,\n            -0.0151,     -0.0018,      0.0014,     -0.0037,     -0.0096,\n             0.0013,     -0.0040,      0.0062,     -0.0031,      0.0037,\n             0.0000,      0.0034,      0.0012,      0.0095,     -0.0010,\n             0.0006,      0.0023,     -0.0025,     -0.0036,      0.0034,\n            -0.0034,      0.0012,      0.0053,     -0.0082,      0.0009,\n             0.0009,      0.0073,      0.0018,      0.0012,     -0.0023,\n             0.0028,     -0.0141,     -0.0009,     -0.0009,      0.0004,\n             0.0007,      0.0046,     -0.0016,     -0.0021,      0.0007,\n            -0.0027,      0.0025,      0.0100,      0.0037,     -0.0040,\n             0.0012,      0.0068,      0.0058,      0.0184,     -0.0096,\n             0.0504,     -0.0004,      0.1401,      0.0034,      0.0025,\n            -0.0223,      0.0536,      0.0047,     -0.0030,      0.0045,\n            -0.0047,     -0.0067,     -0.0093,     -0.0070,     -0.0041,\n            -0.0037,      0.0039,     -0.0823,     -0.0313,     -0.0072,\n             0.0053,      0.0058,      0.0001,     -0.0063,     -0.0017,\n             0.0089,     -0.0026,      0.0019,      0.0058,      0.0005,\n             0.0073,     -0.0235,     -0.0012,     -0.0016,     -0.0001,\n             0.0018,     -0.0006,     -0.0012,      0.0026,      0.0028,\n             0.0053,      0.0012,      0.0103,      0.0082,     -0.0003,\n            -0.0082,      0.0184,     -0.0063,      0.0260,      0.0091,\n            -0.0112,      0.0040,     -0.1661,     -0.0029,     -0.0040,\n             0.0137,      0.1491,     -0.0078,      0.0053,     -0.0008,\n             0.0074,      0.0081,      0.0172,      0.0043,     -0.0049,\n            -0.0020,     -0.0100,      0.0109,      0.0021,      0.0099,\n             0.0009,     -0.0096,     -0.0017,      0.0091,      0.0153,\n            -0.0194,      0.0061,     -0.0049,     -0.0003,     -0.0050,\n             0.0111,      0.0156,      0.0008,     -0.0045,      0.0012,\n             0.0025,     -0.0002,     -0.0026,      0.0023,     -0.0019,\n            -0.0036,     -0.0113,      0.0013,     -0.0150,      0.0016,\n             0.0009,      0.0504,      0.0089,     -0.0112,     -0.0194,\n             0.0458,      0.0022,      0.2102,      0.0015,      0.0010,\n            -0.0171,     -0.1047,      0.0028,     -0.0054,      0.0003,\n            -0.0091,     -0.0092,     -0.0206,      0.0018,      0.0126,\n             0.0071,      0.0130,     -0.0030,     -0.1122,     -0.0095,\n             0.0073,     -0.0004,     -0.0026,      0.0040,      0.0061,\n             0.0022,     -0.0096,      0.0018,     -0.0014,      0.0005,\n             0.0066,      0.0106,      0.0003,      0.0009,      0.0015,\n             0.0011,     -0.0013,      0.0038,     -0.0055,     -0.0072,\n            -0.0048,     -0.0068,      0.0078,     -0.0034,      0.0014,\n             0.0018,      0.1401,      0.0019,     -0.1661,     -0.0049,\n             0.2102,      0.0018,      0.0131,     -0.0037,     -0.0007,\n            -0.0075,      0.0006,      0.0207,     -0.0113,     -0.0119,\n            -0.0080,     -0.0065,      0.0012,      0.0033,     -0.0116,\n            -0.0260,     -0.0094,     -0.0070,     -0.0225,     -0.0086,\n             0.0012,      0.0034,      0.0058,     -0.0029,     -0.0003,\n             0.0015,     -0.0014,     -0.0037,      0.0013,     -0.0001,\n            -0.0041,      0.0033,     -0.0010,      0.0002,     -0.0006,\n             0.0003,      0.0000,      0.0014,      0.0013,     -0.0024,\n            -0.0024,     -0.0033,     -0.0005,     -0.0022,      0.0024,\n            -0.0023,      0.0025,      0.0005,     -0.0040,     -0.0050,\n             0.0010,      0.0005,     -0.0007,     -0.0001,      0.0042,\n            -0.0029,      0.0019,     -0.0015,      0.0032,      0.0002,\n            -0.0001,      0.0013,      0.0027,      0.0014,     -0.0012,\n            -0.0045,     -0.0034,     -0.0035,     -0.0059,     -0.0014,\n             0.0028,     -0.0223,      0.0073,      0.0137,      0.0111,\n            -0.0171,      0.0066,     -0.0075,     -0.0041,     -0.0029,\n             0.0014,     -0.0226,      0.0006,     -0.0158,     -0.0007,\n             0.0027,      0.0006,      0.0055,      0.0033,      0.0006,\n             0.0011,     -0.0011,     -0.0022,     -0.0011,      0.0096,\n            -0.0141,      0.0536,     -0.0235,      0.1491,      0.0156,\n            -0.1047,      0.0106,      0.0006,      0.0033,      0.0019,\n            -0.0226,      0.0798,     -0.0191,      0.0068,     -0.0265,\n            -0.0030,     -0.0071,     -0.0339,      0.0038,     -0.0089,\n            -0.0039,     -0.0467,     -0.0176,     -0.1011,     -0.0153,\n            -0.0009,      0.0047,     -0.0012,     -0.0078,      0.0008,\n             0.0028,      0.0003,      0.0207,     -0.0010,     -0.0015,\n             0.0006,     -0.0191,     -0.0156,      0.0020,      0.0015,\n             0.0020,      0.0014,      0.0023,     -0.0008,     -0.0075,\n            -0.0038,      0.0044,      0.0496,      0.0070,      0.0005,\n            -0.0009,     -0.0030,     -0.0016,      0.0053,     -0.0045,\n            -0.0054,      0.0009,     -0.0113,      0.0002,      0.0032,\n            -0.0158,      0.0068,      0.0020,     -0.0008,     -0.0016,\n            -0.0032,     -0.0046,     -0.0004,      0.0031,      0.0007,\n             0.0299,      0.0124,      0.0062,     -0.0049,      0.0007,\n             0.0004,      0.0045,     -0.0001,     -0.0008,      0.0012,\n             0.0003,      0.0015,     -0.0119,     -0.0006,      0.0002,\n            -0.0007,     -0.0265,      0.0015,     -0.0016,     -0.0017,\n            -0.0004,     -0.0005,      0.0002,     -0.0010,     -0.0035,\n             0.0175,      0.0073,      0.0132,     -0.0005,      0.0018,\n             0.0007,     -0.0047,      0.0018,      0.0074,      0.0025,\n            -0.0091,      0.0011,     -0.0080,      0.0003,     -0.0001,\n             0.0027,     -0.0030,      0.0020,     -0.0032,     -0.0004,\n            -0.0008,     -0.0026,     -0.0007,     -0.0007,     -0.0029,\n             0.0197,      0.0057,     -0.0124,     -0.0065,      0.0005,\n             0.0046,     -0.0067,     -0.0006,      0.0081,     -0.0002,\n            -0.0092,     -0.0013,     -0.0065,      0.0000,      0.0013,\n             0.0006,     -0.0071,      0.0014,     -0.0046,     -0.0005,\n            -0.0026,     -0.0022,     -0.0000,      0.0010,     -0.0005,\n             0.0250,      0.0103,      0.0200,     -0.0129,      0.0003,\n            -0.0016,     -0.0093,     -0.0012,      0.0172,     -0.0026,\n            -0.0206,      0.0038,      0.0012,      0.0014,      0.0027,\n             0.0055,     -0.0339,      0.0023,     -0.0004,      0.0002,\n            -0.0007,     -0.0000,     -0.0029,      0.0012,     -0.0099,\n             0.0204,      0.0050,     -0.0206,      0.0010,      0.0004,\n            -0.0021,     -0.0070,      0.0026,      0.0043,      0.0023,\n             0.0018,     -0.0055,      0.0033,      0.0013,      0.0014,\n             0.0033,      0.0038,     -0.0008,      0.0031,     -0.0010,\n            -0.0007,      0.0010,      0.0012,     -0.0104,      0.0035,\n             0.0035,      0.0034,      0.0000,      0.0008,     -0.0014,\n             0.0007,     -0.0041,      0.0028,     -0.0049,     -0.0019,\n             0.0126,     -0.0072,     -0.0116,     -0.0024,     -0.0012,\n             0.0006,     -0.0089,     -0.0075,      0.0007,     -0.0035,\n            -0.0029,     -0.0005,     -0.0099,      0.0035,     -0.0027,\n            -0.0001,     -0.0006,     -0.0013,     -0.0000,      0.0019,\n            -0.0027,     -0.0037,      0.0053,     -0.0020,     -0.0036,\n             0.0071,     -0.0048,     -0.0260,     -0.0024,     -0.0045,\n             0.0011,     -0.0039,     -0.0038,      0.0299,      0.0175,\n             0.0197,      0.0250,      0.0204,      0.0035,     -0.0001,\n            -0.0013,     -0.0001,     -0.0001,      0.0007,     -0.0004,\n             0.0025,      0.0039,      0.0012,     -0.0100,     -0.0113,\n             0.0130,     -0.0068,     -0.0094,     -0.0033,     -0.0034,\n            -0.0011,     -0.0467,      0.0044,      0.0124,      0.0073,\n             0.0057,      0.0103,      0.0050,      0.0034,     -0.0006,\n            -0.0001,     -0.0002,     -0.0038,     -0.0016,     -0.0001,\n             0.0100,     -0.0823,      0.0103,      0.0109,      0.0013,\n            -0.0030,      0.0078,     -0.0070,     -0.0005,     -0.0035,\n            -0.0022,     -0.0176,      0.0496,      0.0062,      0.0132,\n            -0.0124,      0.0200,     -0.0206,      0.0000,     -0.0013,\n            -0.0001,     -0.0038,     -0.0002,     -0.0021,     -0.0133,\n             0.0037,     -0.0313,      0.0082,      0.0021,     -0.0150,\n            -0.1122,     -0.0034,     -0.0225,     -0.0022,     -0.0059,\n            -0.0011,     -0.1011,      0.0070,     -0.0049,     -0.0005,\n            -0.0065,     -0.0129,      0.0010,      0.0008,     -0.0000,\n             0.0007,     -0.0016,     -0.0021,     -0.0066,      0.0047,\n            -0.0040,     -0.0072,     -0.0003,      0.0099,      0.0016,\n            -0.0095,      0.0014,     -0.0086,      0.0024,     -0.0014,\n             0.0096,     -0.0153,      0.0005,      0.0007,      0.0018,\n             0.0005,      0.0003,      0.0004,     -0.0014,      0.0019,\n            -0.0004,     -0.0001,     -0.0133,      0.0047,     -0.0037,\n             0.0181,      0.0151,      0.0057,     -0.0132,      0.0069,\n             0.0052,     -0.0166,      0.0012,      0.0024,      0.0041,\n            -0.0038,     -0.0009,      0.0063,     -0.0071,     -0.0018,\n            -0.0063,     -0.0135,     -0.0093,      0.0006,     -0.0135,\n            -0.0006,     -0.0006,     -0.0067,     -0.0037,      0.0090,\n             0.0151,      0.0006,      0.1118,     -0.0045,     -0.0013,\n             0.0095,     -0.0156,      0.0034,      0.0035,     -0.0021,\n            -0.0021,      0.0125,     -0.0147,     -0.0069,     -0.0149,\n            -0.0098,     -0.0062,     -0.0109,      0.0025,      0.0040,\n             0.0005,     -0.0008,     -0.0208,     -0.0019,      0.0006,\n             0.0057,      0.1118,      0.0055,     -0.2000,      0.0050,\n             0.0822,     -0.0073,      0.0058,      0.0013,      0.0020,\n             0.0011,      0.0571,     -0.0113,      0.0110,     -0.0009,\n             0.0041,      0.0028,      0.0052,      0.0193,     -0.0003,\n            -0.0084,     -0.0059,     -0.0014,      0.0017,      0.0023,\n            -0.0132,     -0.0045,     -0.2000,      0.0407,     -0.0088,\n            -0.0107,      0.0109,     -0.0029,     -0.0028,      0.0056,\n             0.0015,      0.0084,      0.0232,      0.0057,      0.0194,\n             0.0127,      0.0102,      0.0317,     -0.0035,      0.0162,\n            -0.0006,      0.0079,     -0.0021,      0.0004,     -0.0158,\n             0.0069,     -0.0013,      0.0050,     -0.0088,      0.0140,\n             0.0002,     -0.0017,     -0.0003,      0.0003,     -0.0030,\n            -0.0026,      0.0051,     -0.0003,     -0.0156,     -0.0091,\n            -0.0201,     -0.0107,     -0.0268,     -0.0018,      0.0406,\n            -0.0114,     -0.0058,      0.0007,     -0.0004,     -0.0199,\n             0.0052,      0.0095,      0.0822,     -0.0107,      0.0002,\n             0.0082,     -0.0024,      0.0015,      0.0019,     -0.0034,\n            -0.0018,     -0.0185,     -0.0204,     -0.0038,     -0.0172,\n            -0.0106,     -0.0037,     -0.0127,      0.0024,     -0.0105,\n            -0.0148,     -0.0038,      0.0050,     -0.0025,      0.0125,\n            -0.0166,     -0.0156,     -0.0073,      0.0109,     -0.0017,\n            -0.0024,      0.0117,     -0.0014,     -0.0009,     -0.0050,\n             0.0341,      0.0029,     -0.0042,      0.0044,      0.0009,\n             0.0037,      0.0128,      0.0086,     -0.0019,     -0.0069,\n             0.0061,      0.0020,     -0.0006,      0.0035,     -0.0037,\n             0.0012,      0.0034,      0.0058,     -0.0029,     -0.0003,\n             0.0015,     -0.0014,     -0.0037,      0.0013,     -0.0001,\n            -0.0041,      0.0033,     -0.0010,      0.0002,     -0.0006,\n             0.0003,      0.0000,      0.0014,      0.0013,     -0.0024,\n            -0.0024,     -0.0033,     -0.0005,     -0.0022,      0.0024,\n             0.0024,      0.0035,      0.0013,     -0.0028,      0.0003,\n             0.0019,     -0.0009,      0.0013,      0.0013,      0.0008,\n            -0.0013,      0.0039,     -0.0010,      0.0005,     -0.0005,\n            -0.0007,      0.0011,      0.0021,      0.0008,     -0.0020,\n            -0.0014,     -0.0015,     -0.0007,     -0.0013,      0.0024,\n             0.0041,     -0.0021,      0.0020,      0.0056,     -0.0030,\n            -0.0034,     -0.0050,     -0.0001,      0.0008,     -0.0007,\n            -0.0245,     -0.0063,      0.0031,      0.0001,     -0.0061,\n             0.0029,      0.0011,      0.0038,     -0.0001,     -0.0000,\n            -0.0007,     -0.0075,     -0.0052,     -0.0024,      0.0129,\n            -0.0038,     -0.0021,      0.0011,      0.0015,     -0.0026,\n            -0.0018,      0.0341,     -0.0041,     -0.0013,     -0.0245,\n            -0.0013,     -0.0143,      0.0021,     -0.0036,     -0.0014,\n             0.0008,     -0.0015,     -0.0042,     -0.0017,      0.0015,\n            -0.0014,     -0.0024,     -0.0031,     -0.0035,     -0.0038,\n            -0.0009,      0.0125,      0.0571,      0.0084,      0.0051,\n            -0.0185,      0.0029,      0.0033,      0.0039,     -0.0063,\n            -0.0143,      0.0108,     -0.0013,      0.0062,     -0.0008,\n            -0.0019,     -0.0005,      0.0020,     -0.0046,      0.0001,\n            -0.0008,      0.0030,     -0.0306,     -0.0037,     -0.0034,\n             0.0063,     -0.0147,     -0.0113,      0.0232,     -0.0003,\n            -0.0204,     -0.0042,     -0.0010,     -0.0010,      0.0031,\n             0.0021,     -0.0013,     -0.0173,      0.0001,     -0.0048,\n            -0.0015,     -0.0023,      0.0023,      0.0005,      0.0013,\n            -0.0428,     -0.0080,      0.0013,      0.0010,      0.0014,\n            -0.0071,     -0.0069,      0.0110,      0.0057,     -0.0156,\n            -0.0038,      0.0044,      0.0002,      0.0005,      0.0001,\n            -0.0036,      0.0062,      0.0001,     -0.0049,      0.0030,\n             0.0039,      0.0043,      0.0019,      0.0077,      0.0101,\n             0.0363,     -0.0025,     -0.0127,     -0.0005,     -0.0045,\n            -0.0018,     -0.0149,     -0.0009,      0.0194,     -0.0091,\n            -0.0172,      0.0009,     -0.0006,     -0.0005,     -0.0061,\n            -0.0014,     -0.0008,     -0.0048,      0.0030,     -0.0050,\n             0.0049,      0.0032,      0.0016,     -0.0001,      0.0038,\n             0.0036,     -0.0029,     -0.0071,     -0.0068,     -0.0025,\n            -0.0063,     -0.0098,      0.0041,      0.0127,     -0.0201,\n            -0.0106,      0.0037,      0.0003,     -0.0007,      0.0029,\n             0.0008,     -0.0019,     -0.0015,      0.0039,      0.0049,\n            -0.0050,      0.0052,      0.0007,      0.0049,      0.0075,\n            -0.0042,     -0.0026,     -0.0055,      0.0003,      0.0076,\n            -0.0135,     -0.0062,      0.0028,      0.0102,     -0.0107,\n            -0.0037,      0.0128,      0.0000,      0.0011,      0.0011,\n            -0.0015,     -0.0005,     -0.0023,      0.0043,      0.0032,\n             0.0052,     -0.0065,     -0.0002,      0.0042,      0.0117,\n             0.0220,     -0.0038,     -0.0086,     -0.0017,     -0.0100,\n            -0.0093,     -0.0109,      0.0052,      0.0317,     -0.0268,\n            -0.0127,      0.0086,      0.0014,      0.0021,      0.0038,\n            -0.0042,      0.0020,      0.0023,      0.0019,      0.0016,\n             0.0007,     -0.0002,     -0.0045,      0.0075,      0.0251,\n             0.0278,     -0.0007,     -0.0138,     -0.0031,     -0.0092,\n             0.0006,      0.0025,      0.0193,     -0.0035,     -0.0018,\n             0.0024,     -0.0019,      0.0013,      0.0008,     -0.0001,\n            -0.0017,     -0.0046,      0.0005,      0.0077,     -0.0001,\n             0.0049,      0.0042,      0.0075,     -0.0105,      0.0060,\n             0.0008,      0.0008,     -0.0010,     -0.0010,      0.0011,\n            -0.0135,      0.0040,     -0.0003,      0.0162,      0.0406,\n            -0.0105,     -0.0069,     -0.0024,     -0.0020,     -0.0000,\n             0.0015,      0.0001,      0.0013,      0.0101,      0.0038,\n             0.0075,      0.0117,      0.0251,      0.0060,     -0.0105,\n             0.0035,      0.0013,      0.0015,      0.0015,     -0.0043,\n            -0.0006,      0.0005,     -0.0084,     -0.0006,     -0.0114,\n            -0.0148,      0.0061,     -0.0024,     -0.0014,     -0.0007,\n            -0.0014,     -0.0008,     -0.0428,      0.0363,      0.0036,\n            -0.0042,      0.0220,      0.0278,      0.0008,      0.0035,\n            -0.0101,     -0.0032,     -0.0021,     -0.0090,      0.0188,\n            -0.0006,     -0.0008,     -0.0059,      0.0079,     -0.0058,\n            -0.0038,      0.0020,     -0.0033,     -0.0015,     -0.0075,\n            -0.0024,      0.0030,     -0.0080,     -0.0025,     -0.0029,\n            -0.0026,     -0.0038,     -0.0007,      0.0008,      0.0013,\n            -0.0032,     -0.0004,     -0.0034,     -0.0078,     -0.0077,\n            -0.0067,     -0.0208,     -0.0014,     -0.0021,      0.0007,\n             0.0050,     -0.0006,     -0.0005,     -0.0007,     -0.0052,\n            -0.0031,     -0.0306,      0.0013,     -0.0127,     -0.0071,\n            -0.0055,     -0.0086,     -0.0138,     -0.0010,      0.0015,\n            -0.0021,     -0.0034,      0.0013,     -0.0128,     -0.0001,\n            -0.0037,     -0.0019,      0.0017,      0.0004,     -0.0004,\n            -0.0025,      0.0035,     -0.0022,     -0.0013,     -0.0024,\n            -0.0035,     -0.0037,      0.0010,     -0.0005,     -0.0068,\n             0.0003,     -0.0017,     -0.0031,     -0.0010,      0.0015,\n            -0.0090,     -0.0078,     -0.0128,     -0.0004,     -0.0045,\n             0.0090,      0.0006,      0.0023,     -0.0158,     -0.0199,\n             0.0125,     -0.0037,      0.0024,      0.0024,      0.0129,\n            -0.0038,     -0.0034,      0.0014,     -0.0045,     -0.0025,\n             0.0076,     -0.0100,     -0.0092,      0.0011,     -0.0043,\n             0.0188,     -0.0077,     -0.0001,     -0.0045,     -0.0055,\n             0.0250,      0.0094,      0.0034,     -0.0187,     -0.0069,\n             0.0175,     -0.0260,     -0.0023,      0.0041,     -0.0017,\n             0.0023,     -0.0287,      0.0032,     -0.0064,     -0.0009,\n            -0.0041,     -0.0007,     -0.0037,      0.0076,      0.0027,\n             0.0056,      0.0027,      0.0173,      0.0062,      0.0029,\n             0.0094,     -0.0087,      0.0026,      0.0060,      0.0044,\n             0.0044,     -0.0075,      0.0025,     -0.0021,      0.0039,\n            -0.0012,      0.0068,      0.0001,     -0.0025,     -0.0034,\n            -0.0039,      0.0054,      0.0049,      0.0006,     -0.0057,\n             0.0012,      0.0003,     -0.0011,     -0.0012,     -0.0006,\n             0.0034,      0.0026,      0.0033,     -0.0026,      0.0023,\n            -0.0006,     -0.0010,      0.0005,      0.0020,      0.0005,\n            -0.0043,     -0.0078,      0.0019,      0.0007,     -0.0003,\n             0.0014,      0.0018,      0.0056,     -0.0022,     -0.0008,\n             0.0007,      0.0002,      0.0091,      0.0013,      0.0019,\n            -0.0187,      0.0060,     -0.0026,      0.0084,     -0.0077,\n            -0.0079,      0.0124,     -0.0040,      0.0056,     -0.0047,\n             0.0019,     -0.0047,     -0.0016,     -0.0046,      0.0000,\n            -0.0026,     -0.0067,     -0.0137,     -0.0107,      0.0010,\n            -0.0049,      0.0046,     -0.0011,      0.0057,     -0.0019,\n            -0.0069,      0.0044,      0.0023,     -0.0077,     -0.0057,\n            -0.0028,     -0.0012,     -0.0050,     -0.0030,      0.0033,\n             0.0004,     -0.0052,      0.0025,     -0.0428,     -0.0200,\n            -0.0119,     -0.0279,     -0.0130,      0.0024,     -0.0132,\n             0.0027,     -0.0002,      0.0028,      0.0077,     -0.0033,\n             0.0175,      0.0044,     -0.0006,     -0.0079,     -0.0028,\n             0.0073,     -0.0114,      0.0010,     -0.0034,      0.0017,\n            -0.0004,     -0.0181,     -0.0023,      0.0025,     -0.0016,\n             0.0006,      0.0072,      0.0108,      0.0084,     -0.0003,\n             0.0046,     -0.0007,     -0.0108,     -0.0012,     -0.0013,\n            -0.0260,     -0.0075,     -0.0010,      0.0124,     -0.0012,\n            -0.0114,      0.0312,      0.0005,     -0.0050,      0.0004,\n            -0.0011,      0.0260,     -0.0065,      0.0050,      0.0011,\n             0.0030,      0.0005,      0.0010,     -0.0021,      0.0059,\n             0.0023,     -0.0031,     -0.0013,     -0.0082,     -0.0096,\n            -0.0023,      0.0025,      0.0005,     -0.0040,     -0.0050,\n             0.0010,      0.0005,     -0.0007,     -0.0001,      0.0042,\n            -0.0029,      0.0019,     -0.0015,      0.0032,      0.0002,\n            -0.0001,      0.0013,      0.0027,      0.0014,     -0.0012,\n            -0.0045,     -0.0034,     -0.0035,     -0.0059,     -0.0014,\n             0.0041,     -0.0021,      0.0020,      0.0056,     -0.0030,\n            -0.0034,     -0.0050,     -0.0001,      0.0008,     -0.0007,\n            -0.0245,     -0.0063,      0.0031,      0.0001,     -0.0061,\n             0.0029,      0.0011,      0.0038,     -0.0001,     -0.0000,\n            -0.0007,     -0.0075,     -0.0052,     -0.0024,      0.0129,\n            -0.0017,      0.0039,      0.0005,     -0.0047,      0.0033,\n             0.0017,      0.0004,      0.0042,     -0.0007,      0.0042,\n            -0.0025,      0.0050,     -0.0012,      0.0043,      0.0002,\n             0.0007,      0.0028,      0.0046,      0.0000,      0.0005,\n            -0.0047,     -0.0040,     -0.0026,     -0.0053,     -0.0009,\n             0.0023,     -0.0012,     -0.0043,      0.0019,      0.0004,\n            -0.0004,     -0.0011,     -0.0029,     -0.0245,     -0.0025,\n             0.0000,     -0.0021,      0.0014,     -0.0030,     -0.0044,\n            -0.0018,     -0.0028,     -0.0027,     -0.0002,     -0.0033,\n            -0.0078,     -0.0006,     -0.0027,     -0.0030,     -0.0060,\n            -0.0287,      0.0068,     -0.0078,     -0.0047,     -0.0052,\n            -0.0181,      0.0260,      0.0019,     -0.0063,      0.0050,\n            -0.0021,      0.0024,      0.0010,     -0.0023,      0.0075,\n             0.0019,      0.0151,      0.0147,      0.0141,      0.0026,\n             0.0253,      0.0402,     -0.0047,     -0.0027,      0.0011,\n             0.0032,      0.0001,      0.0019,     -0.0016,      0.0025,\n            -0.0023,     -0.0065,     -0.0015,      0.0031,     -0.0012,\n             0.0014,      0.0010,     -0.0168,     -0.0080,     -0.0020,\n            -0.0020,     -0.0017,     -0.0053,      0.0014,      0.0005,\n             0.0004,      0.0066,     -0.0016,      0.0042,      0.0002,\n            -0.0064,     -0.0025,      0.0007,     -0.0046,     -0.0428,\n             0.0025,      0.0050,      0.0032,      0.0001,      0.0043,\n            -0.0030,     -0.0023,     -0.0080,     -0.0134,      0.0028,\n             0.0034,      0.0037,      0.0040,      0.0027,      0.0056,\n             0.0171,      0.0073,     -0.0059,     -0.0038,     -0.0023,\n            -0.0009,     -0.0034,     -0.0003,      0.0000,     -0.0200,\n            -0.0016,      0.0011,      0.0002,     -0.0061,      0.0002,\n            -0.0044,      0.0075,     -0.0020,      0.0028,     -0.0006,\n            -0.0009,     -0.0012,      0.0011,      0.0029,      0.0062,\n             0.0140,      0.0032,     -0.0002,     -0.0076,     -0.0028,\n            -0.0041,     -0.0039,      0.0014,     -0.0026,     -0.0119,\n             0.0006,      0.0030,     -0.0001,      0.0029,      0.0007,\n            -0.0018,      0.0019,     -0.0020,      0.0034,     -0.0009,\n            -0.0040,     -0.0024,     -0.0002,      0.0005,      0.0051,\n             0.0134,      0.0011,     -0.0113,     -0.0022,     -0.0028,\n            -0.0007,      0.0054,      0.0018,     -0.0067,     -0.0279,\n             0.0072,      0.0005,      0.0013,      0.0011,      0.0028,\n            -0.0028,      0.0151,     -0.0017,      0.0037,     -0.0012,\n            -0.0024,     -0.0043,     -0.0019,      0.0027,      0.0026,\n             0.0168,      0.0028,      0.0025,     -0.0028,      0.0003,\n            -0.0037,      0.0049,      0.0056,     -0.0137,     -0.0130,\n             0.0108,      0.0010,      0.0027,      0.0038,      0.0046,\n            -0.0027,      0.0147,     -0.0053,      0.0040,      0.0011,\n            -0.0002,     -0.0019,     -0.0129,      0.0044,      0.0053,\n             0.0174,      0.0039,     -0.0023,     -0.0069,     -0.0020,\n             0.0076,      0.0006,     -0.0022,     -0.0107,      0.0024,\n             0.0084,     -0.0021,      0.0014,     -0.0001,      0.0000,\n            -0.0002,      0.0141,      0.0013,      0.0027,      0.0029,\n             0.0005,      0.0027,      0.0044,     -0.0131,      0.0053,\n            -0.0023,     -0.0010,     -0.0014,      0.0002,     -0.0012,\n             0.0027,     -0.0057,     -0.0008,      0.0010,     -0.0132,\n            -0.0003,      0.0059,     -0.0012,     -0.0000,      0.0005,\n            -0.0033,      0.0026,      0.0005,      0.0056,      0.0062,\n             0.0051,      0.0026,      0.0053,      0.0053,     -0.0060,\n            -0.0016,     -0.0023,     -0.0027,     -0.0023,      0.0003,\n             0.0056,      0.0012,      0.0007,     -0.0049,      0.0027,\n             0.0046,      0.0023,     -0.0045,     -0.0007,     -0.0047,\n            -0.0078,      0.0253,      0.0004,      0.0171,      0.0140,\n             0.0134,      0.0168,      0.0174,     -0.0023,     -0.0016,\n             0.0014,     -0.0072,     -0.0054,     -0.0052,     -0.0003,\n             0.0027,      0.0003,      0.0002,      0.0046,     -0.0002,\n            -0.0007,     -0.0031,     -0.0034,     -0.0075,     -0.0040,\n            -0.0006,      0.0402,      0.0066,      0.0073,      0.0032,\n             0.0011,      0.0028,      0.0039,     -0.0010,     -0.0023,\n            -0.0072,      0.0015,     -0.0032,     -0.0032,     -0.0007,\n             0.0173,     -0.0011,      0.0091,     -0.0011,      0.0028,\n            -0.0108,     -0.0013,     -0.0035,     -0.0052,     -0.0026,\n            -0.0027,     -0.0047,     -0.0016,     -0.0059,     -0.0002,\n            -0.0113,      0.0025,     -0.0023,     -0.0014,     -0.0027,\n            -0.0054,     -0.0032,     -0.0009,     -0.0026,      0.0014,\n             0.0062,     -0.0012,      0.0013,      0.0057,      0.0077,\n            -0.0012,     -0.0082,     -0.0059,     -0.0024,     -0.0053,\n            -0.0030,     -0.0027,      0.0042,     -0.0038,     -0.0076,\n            -0.0022,     -0.0028,     -0.0068,      0.0002,     -0.0023,\n            -0.0052,     -0.0032,     -0.0026,     -0.0008,     -0.0081,\n             0.0029,     -0.0006,      0.0019,     -0.0019,     -0.0033,\n            -0.0013,     -0.0096,     -0.0014,      0.0129,     -0.0009,\n            -0.0060,      0.0011,      0.0002,     -0.0023,     -0.0028,\n            -0.0028,      0.0003,     -0.0020,     -0.0012,      0.0003,\n            -0.0003,     -0.0007,      0.0014,     -0.0081,     -0.0010,\n             0.0127,     -0.0004,     -0.0023,      0.0012,     -0.0047,\n             0.0168,      0.0053,      0.0028,     -0.0038,      0.0023,\n             0.0042,     -0.0178,      0.0005,     -0.0039,     -0.0033,\n             0.0044,      0.0086,      0.0023,     -0.0010,      0.0118,\n             0.0026,      0.0071,      0.0001,      0.0009,     -0.0098,\n            -0.0004,     -0.0658,      0.0125,     -0.0036,     -0.0036,\n            -0.0779,     -0.0420,     -0.0223,     -0.0021,     -0.0012,\n             0.0073,     -0.0006,      0.0166,      0.0088,      0.0043,\n            -0.0340,      0.0127,      0.0229,      0.0003,     -0.0042,\n             0.0255,      0.0155,      0.0043,      0.0092,     -0.0102,\n            -0.0023,      0.0125,     -0.0063,     -0.0382,      0.0041,\n             0.0305,     -0.0012,      0.0073,      0.0011,     -0.0043,\n             0.0023,      0.0067,     -0.0032,      0.0070,      0.0009,\n             0.0248,      0.0114,      0.0129,      0.0108,      0.0396,\n            -0.0039,     -0.0024,     -0.0041,      0.0030,      0.0158,\n             0.0012,     -0.0036,     -0.0382,     -0.0508,      0.0030,\n             0.0003,      0.0084,      0.0137,      0.0015,      0.0019,\n            -0.0035,     -0.0445,     -0.0814,     -0.0078,     -0.0035,\n             0.0226,     -0.0229,     -0.0092,      0.0034,      0.0094,\n             0.0010,     -0.0016,     -0.0744,     -0.0103,      0.0046,\n            -0.0047,     -0.0036,      0.0041,      0.0030,      0.0199,\n            -0.0020,     -0.0146,      0.0111,     -0.0026,      0.0004,\n             0.0040,     -0.0084,      0.0158,     -0.0046,     -0.0001,\n             0.0026,      0.0039,     -0.0063,      0.0972,     -0.0011,\n             0.0035,      0.0009,     -0.0003,     -0.0013,      0.0001,\n             0.0168,     -0.0779,      0.0305,      0.0003,     -0.0020,\n            -0.0549,     -0.0056,     -0.0171,     -0.0018,     -0.0004,\n             0.0094,      0.1075,      0.2261,      0.0063,      0.0200,\n            -0.0022,      0.0238,      0.0008,     -0.0033,     -0.0056,\n             0.0025,     -0.0014,      0.0254,      0.0092,     -0.0085,\n             0.0053,     -0.0420,     -0.0012,      0.0084,     -0.0146,\n            -0.0056,      0.0103,      0.0066,      0.0341,     -0.0011,\n            -0.0070,     -0.0007,      0.0056,      0.0039,      0.0304,\n             0.0168,     -0.0011,      0.0301,      0.0009,      0.0070,\n             0.0002,     -0.0000,     -0.0002,      0.0073,      0.0013,\n             0.0028,     -0.0223,      0.0073,      0.0137,      0.0111,\n            -0.0171,      0.0066,     -0.0075,     -0.0041,     -0.0029,\n             0.0014,     -0.0226,      0.0006,     -0.0158,     -0.0007,\n             0.0027,      0.0006,      0.0055,      0.0033,      0.0006,\n             0.0011,     -0.0011,     -0.0022,     -0.0011,      0.0096,\n            -0.0038,     -0.0021,      0.0011,      0.0015,     -0.0026,\n            -0.0018,      0.0341,     -0.0041,     -0.0013,     -0.0245,\n            -0.0013,     -0.0143,      0.0021,     -0.0036,     -0.0014,\n             0.0008,     -0.0015,     -0.0042,     -0.0017,      0.0015,\n            -0.0014,     -0.0024,     -0.0031,     -0.0035,     -0.0038,\n             0.0023,     -0.0012,     -0.0043,      0.0019,      0.0004,\n            -0.0004,     -0.0011,     -0.0029,     -0.0245,     -0.0025,\n             0.0000,     -0.0021,      0.0014,     -0.0030,     -0.0044,\n            -0.0018,     -0.0028,     -0.0027,     -0.0002,     -0.0033,\n            -0.0078,     -0.0006,     -0.0027,     -0.0030,     -0.0060,\n             0.0042,      0.0073,      0.0023,     -0.0035,      0.0040,\n             0.0094,     -0.0070,      0.0014,     -0.0013,      0.0000,\n             0.0015,      0.0002,     -0.0000,      0.0063,      0.0016,\n            -0.0004,      0.0024,      0.0068,     -0.0026,     -0.0030,\n            -0.0102,     -0.0005,     -0.0021,     -0.0061,     -0.0034,\n            -0.0178,     -0.0006,      0.0067,     -0.0445,     -0.0084,\n             0.1075,     -0.0007,     -0.0226,     -0.0143,     -0.0021,\n             0.0002,     -0.0986,      0.0353,     -0.0051,     -0.0231,\n            -0.0017,     -0.0099,     -0.0462,     -0.0034,     -0.0031,\n            -0.0065,      0.0026,      0.0009,     -0.0003,     -0.0084,\n             0.0005,      0.0166,     -0.0032,     -0.0814,      0.0158,\n             0.2261,      0.0056,      0.0006,      0.0021,      0.0014,\n            -0.0000,      0.0353,     -0.0081,     -0.0002,      0.0030,\n            -0.0005,      0.0004,      0.0036,     -0.0040,     -0.0082,\n             0.0122,      0.0003,      0.0069,      0.0022,     -0.0037,\n            -0.0039,      0.0088,      0.0070,     -0.0078,     -0.0046,\n             0.0063,      0.0039,     -0.0158,     -0.0036,     -0.0030,\n             0.0063,     -0.0051,     -0.0002,     -0.0054,      0.0037,\n             0.0031,      0.0043,      0.0004,     -0.0053,     -0.0326,\n            -0.0109,     -0.0049,     -0.0003,     -0.0034,      0.0020,\n            -0.0033,      0.0043,      0.0009,     -0.0035,     -0.0001,\n             0.0200,      0.0304,     -0.0007,     -0.0014,     -0.0044,\n             0.0016,     -0.0231,      0.0030,      0.0037,     -0.0070,\n             0.0027,      0.0011,      0.0020,     -0.0040,     -0.0046,\n            -0.0035,     -0.0025,     -0.0018,     -0.0022,      0.0005,\n             0.0044,     -0.0340,      0.0248,      0.0226,      0.0026,\n            -0.0022,      0.0168,      0.0027,      0.0008,     -0.0018,\n            -0.0004,     -0.0017,     -0.0005,      0.0031,      0.0027,\n            -0.0075,      0.0020,     -0.0058,      0.0003,     -0.0063,\n            -0.0042,     -0.0112,     -0.0001,     -0.0013,     -0.0197,\n             0.0086,      0.0127,      0.0114,     -0.0229,      0.0039,\n             0.0238,     -0.0011,      0.0006,     -0.0015,     -0.0028,\n             0.0024,     -0.0099,      0.0004,      0.0043,      0.0011,\n             0.0020,     -0.0048,     -0.0054,     -0.0116,      0.0047,\n            -0.0645,     -0.0057,     -0.0055,     -0.0316,      0.0010,\n             0.0023,      0.0229,      0.0129,     -0.0092,     -0.0063,\n             0.0008,      0.0301,      0.0055,     -0.0042,     -0.0027,\n             0.0068,     -0.0462,      0.0036,      0.0004,      0.0020,\n            -0.0058,     -0.0054,     -0.0034,      0.0008,      0.0005,\n            -0.0006,     -0.0020,     -0.0569,     -0.0007,     -0.0004,\n            -0.0010,      0.0003,      0.0108,      0.0034,      0.0972,\n            -0.0033,      0.0009,      0.0033,     -0.0017,     -0.0002,\n            -0.0026,     -0.0034,     -0.0040,     -0.0053,     -0.0040,\n             0.0003,     -0.0116,      0.0008,     -0.0050,     -0.0040,\n            -0.0043,     -0.0016,     -0.0016,     -0.0035,     -0.1154,\n             0.0118,     -0.0042,      0.0396,      0.0094,     -0.0011,\n            -0.0056,      0.0070,      0.0006,      0.0015,     -0.0033,\n            -0.0030,     -0.0031,     -0.0082,     -0.0326,     -0.0046,\n            -0.0063,      0.0047,      0.0005,     -0.0040,     -0.0053,\n            -0.0033,     -0.0031,     -0.0043,     -0.0147,     -0.0027,\n             0.0026,      0.0255,     -0.0039,      0.0010,      0.0035,\n             0.0025,      0.0002,      0.0011,     -0.0014,     -0.0078,\n            -0.0102,     -0.0065,      0.0122,     -0.0109,     -0.0035,\n            -0.0042,     -0.0645,     -0.0006,     -0.0043,     -0.0033,\n             0.0015,     -0.0069,     -0.0040,     -0.0081,     -0.0020,\n             0.0071,      0.0155,     -0.0024,     -0.0016,      0.0009,\n            -0.0014,     -0.0000,     -0.0011,     -0.0024,     -0.0006,\n            -0.0005,      0.0026,      0.0003,     -0.0049,     -0.0025,\n            -0.0112,     -0.0057,     -0.0020,     -0.0016,     -0.0031,\n            -0.0069,      0.0013,     -0.0025,     -0.0036,     -0.0008,\n             0.0001,      0.0043,     -0.0041,     -0.0744,     -0.0003,\n             0.0254,     -0.0002,     -0.0022,     -0.0031,     -0.0027,\n            -0.0021,      0.0009,      0.0069,     -0.0003,     -0.0018,\n            -0.0001,     -0.0055,     -0.0569,     -0.0016,     -0.0043,\n            -0.0040,     -0.0025,     -0.0006,     -0.0029,     -0.0018,\n             0.0009,      0.0092,      0.0030,     -0.0103,     -0.0013,\n             0.0092,      0.0073,     -0.0011,     -0.0035,     -0.0030,\n            -0.0061,     -0.0003,      0.0022,     -0.0034,     -0.0022,\n            -0.0013,     -0.0316,     -0.0007,     -0.0035,     -0.0147,\n            -0.0081,     -0.0036,     -0.0029,     -0.0006,     -0.0020,\n            -0.0098,     -0.0102,      0.0158,      0.0046,      0.0001,\n            -0.0085,      0.0013,      0.0096,     -0.0038,     -0.0060,\n            -0.0034,     -0.0084,     -0.0037,      0.0020,      0.0005,\n            -0.0197,      0.0010,     -0.0004,     -0.1154,     -0.0027,\n            -0.0020,     -0.0008,     -0.0018,     -0.0020,     -0.0060,\n             0.0152,      0.0050,      0.0006,     -0.0483,     -0.0132,\n            -0.0128,     -0.0070,     -0.0141,     -0.0009,     -0.0287,\n            -0.0178,      0.0089,     -0.0071,      0.0152,      0.0314,\n            -0.0304,     -0.0219,      0.0030,     -0.0085,      0.0037,\n            -0.0064,     -0.0269,      0.0024,     -0.0230,      0.0123,\n             0.0050,      0.0494,      0.0094,     -0.0326,     -0.0162,\n             0.0178,      0.0250,      0.0536,      0.0125,      0.0068,\n            -0.0006,      0.0460,     -0.0053,      0.0609,      0.0540,\n             0.0082,      0.0124,     -0.0221,     -0.0172,      0.0120,\n            -0.0028,      0.0011,     -0.0018,     -0.0021,      0.0115,\n             0.0006,      0.0094,      0.0269,     -0.0134,     -0.0043,\n             0.0452,     -0.0005,     -0.0235,      0.0571,     -0.0078,\n             0.0067,      0.0016,     -0.0056,     -0.0733,      0.0037,\n            -0.0433,     -0.0334,     -0.0084,      0.0059,     -0.0039,\n             0.0031,      0.0019,      0.0029,      0.0054,      0.0140,\n            -0.0483,     -0.0326,     -0.0134,      0.0114,      0.0228,\n             0.0474,      0.0256,      0.1491,      0.0084,     -0.0047,\n            -0.0445,      0.0170,     -0.0835,     -0.0800,     -0.0625,\n            -0.0105,      0.0113,      0.1788,      0.0404,     -0.0198,\n             0.0052,     -0.0152,     -0.0366,     -0.0534,     -0.0218,\n            -0.0132,     -0.0162,     -0.0043,      0.0228,      0.0322,\n            -0.0213,      0.0102,      0.0156,      0.0051,     -0.0052,\n            -0.0084,     -0.0031,     -0.0017,      0.0086,      0.0164,\n            -0.0024,     -0.0205,     -0.0101,     -0.0658,      0.0231,\n            -0.0012,      0.0184,      0.0061,      0.0023,     -0.0316,\n            -0.0128,      0.0178,      0.0452,      0.0474,     -0.0213,\n             0.0245,      0.0202,     -0.1047,     -0.0185,     -0.0181,\n             0.1075,      0.0019,      0.0520,      0.0154,      0.0433,\n             0.0039,      0.0107,     -0.0359,      0.0001,     -0.0153,\n            -0.0137,      0.0144,      0.0345,      0.0243,      0.0544,\n            -0.0070,      0.0250,     -0.0005,      0.0256,      0.0102,\n             0.0202,     -0.0027,      0.0106,      0.0029,      0.0260,\n            -0.0007,      0.0132,     -0.0019,     -0.0263,     -0.0240,\n             0.0186,      0.0209,      0.0053,      0.0070,     -0.0041,\n            -0.0020,      0.0115,      0.0068,     -0.0072,     -0.0040,\n            -0.0141,      0.0536,     -0.0235,      0.1491,      0.0156,\n            -0.1047,      0.0106,      0.0006,      0.0033,      0.0019,\n            -0.0226,      0.0798,     -0.0191,      0.0068,     -0.0265,\n            -0.0030,     -0.0071,     -0.0339,      0.0038,     -0.0089,\n            -0.0039,     -0.0467,     -0.0176,     -0.1011,     -0.0153,\n            -0.0009,      0.0125,      0.0571,      0.0084,      0.0051,\n            -0.0185,      0.0029,      0.0033,      0.0039,     -0.0063,\n            -0.0143,      0.0108,     -0.0013,      0.0062,     -0.0008,\n            -0.0019,     -0.0005,      0.0020,     -0.0046,      0.0001,\n            -0.0008,      0.0030,     -0.0306,     -0.0037,     -0.0034,\n            -0.0287,      0.0068,     -0.0078,     -0.0047,     -0.0052,\n            -0.0181,      0.0260,      0.0019,     -0.0063,      0.0050,\n            -0.0021,      0.0024,      0.0010,     -0.0023,      0.0075,\n             0.0019,      0.0151,      0.0147,      0.0141,      0.0026,\n             0.0253,      0.0402,     -0.0047,     -0.0027,      0.0011,\n            -0.0178,     -0.0006,      0.0067,     -0.0445,     -0.0084,\n             0.1075,     -0.0007,     -0.0226,     -0.0143,     -0.0021,\n             0.0002,     -0.0986,      0.0353,     -0.0051,     -0.0231,\n            -0.0017,     -0.0099,     -0.0462,     -0.0034,     -0.0031,\n            -0.0065,      0.0026,      0.0009,     -0.0003,     -0.0084,\n             0.0089,      0.0460,      0.0016,      0.0170,     -0.0031,\n             0.0019,      0.0132,      0.0798,      0.0108,      0.0024,\n            -0.0986,      0.0469,     -0.0280,      0.0058,      0.0054,\n            -0.0068,      0.0044,      0.0431,     -0.0022,     -0.0112,\n            -0.0044,     -0.0538,     -0.0153,     -0.0301,     -0.0028,\n            -0.0071,     -0.0053,     -0.0056,     -0.0835,     -0.0017,\n             0.0520,     -0.0019,     -0.0191,     -0.0013,      0.0010,\n             0.0353,     -0.0280,     -0.0022,     -0.0030,     -0.0021,\n             0.0016,     -0.0031,     -0.0074,     -0.0043,     -0.0114,\n             0.0014,      0.0449,      0.0195,      0.0378,     -0.0015,\n             0.0152,      0.0609,     -0.0733,     -0.0800,      0.0086,\n             0.0154,     -0.0263,      0.0068,      0.0062,     -0.0023,\n            -0.0051,      0.0058,     -0.0030,      0.0003,     -0.0069,\n            -0.0026,     -0.0101,     -0.0612,     -0.0116,      0.0108,\n            -0.0538,      0.0020,     -0.0137,     -0.0105,      0.0054,\n             0.0314,      0.0540,      0.0037,     -0.0625,      0.0164,\n             0.0433,     -0.0240,     -0.0265,     -0.0008,      0.0075,\n            -0.0231,      0.0054,     -0.0021,     -0.0069,      0.0016,\n            -0.0371,     -0.0062,     -0.0274,     -0.0133,      0.0110,\n            -0.0332,     -0.0037,      0.0191,     -0.0276,     -0.0074,\n            -0.0304,      0.0082,     -0.0433,     -0.0105,     -0.0024,\n             0.0039,      0.0186,     -0.0030,     -0.0019,      0.0019,\n            -0.0017,     -0.0068,      0.0016,     -0.0026,     -0.0371,\n            -0.0018,     -0.0023,     -0.0122,     -0.0009,      0.0081,\n            -0.0417,     -0.0546,     -0.0108,     -0.0012,      0.0113,\n            -0.0219,      0.0124,     -0.0334,      0.0113,     -0.0205,\n             0.0107,      0.0209,     -0.0071,     -0.0005,      0.0151,\n            -0.0099,      0.0044,     -0.0031,     -0.0101,     -0.0062,\n            -0.0023,      0.0008,     -0.0196,      0.0185,      0.0159,\n             0.0087,      0.0259,     -0.0114,     -0.0039,     -0.0126,\n             0.0030,     -0.0221,     -0.0084,      0.1788,     -0.0101,\n            -0.0359,      0.0053,     -0.0339,      0.0020,      0.0147,\n            -0.0462,      0.0431,     -0.0074,     -0.0612,     -0.0274,\n            -0.0122,     -0.0196,      0.0039,     -0.0125,      0.0162,\n            -0.0334,     -0.0082,     -0.0166,     -0.0062,      0.0078,\n            -0.0085,     -0.0172,      0.0059,      0.0404,     -0.0658,\n             0.0001,      0.0070,      0.0038,     -0.0046,      0.0141,\n            -0.0034,     -0.0022,     -0.0043,     -0.0116,     -0.0133,\n            -0.0009,      0.0185,     -0.0125,     -0.0005,     -0.0095,\n            -0.0143,      0.0020,     -0.0019,     -0.0015,     -0.0022,\n             0.0037,      0.0120,     -0.0039,     -0.0198,      0.0231,\n            -0.0153,     -0.0041,     -0.0089,      0.0001,      0.0026,\n            -0.0031,     -0.0112,     -0.0114,      0.0108,      0.0110,\n             0.0081,      0.0159,      0.0162,     -0.0095,     -0.0015,\n            -0.0143,     -0.0005,     -0.0066,     -0.0056,     -0.0007,\n            -0.0064,     -0.0028,      0.0031,      0.0052,     -0.0012,\n            -0.0137,     -0.0020,     -0.0039,     -0.0008,      0.0253,\n            -0.0065,     -0.0044,      0.0014,     -0.0538,     -0.0332,\n            -0.0417,      0.0087,     -0.0334,     -0.0143,     -0.0143,\n             0.0076,     -0.0039,     -0.0099,     -0.0159,     -0.0014,\n            -0.0269,      0.0011,      0.0019,     -0.0152,      0.0184,\n             0.0144,      0.0115,     -0.0467,      0.0030,      0.0402,\n             0.0026,     -0.0538,      0.0449,      0.0020,     -0.0037,\n            -0.0546,      0.0259,     -0.0082,      0.0020,     -0.0005,\n            -0.0039,     -0.0008,     -0.0035,      0.0020,      0.0028,\n             0.0024,     -0.0018,      0.0029,     -0.0366,      0.0061,\n             0.0345,      0.0068,     -0.0176,     -0.0306,     -0.0047,\n             0.0009,     -0.0153,      0.0195,     -0.0137,      0.0191,\n            -0.0108,     -0.0114,     -0.0166,     -0.0019,     -0.0066,\n            -0.0099,     -0.0035,      0.0048,     -0.0002,      0.0291,\n            -0.0230,     -0.0021,      0.0054,     -0.0534,      0.0023,\n             0.0243,     -0.0072,     -0.1011,     -0.0037,     -0.0027,\n            -0.0003,     -0.0301,      0.0378,     -0.0105,     -0.0276,\n            -0.0012,     -0.0039,     -0.0062,     -0.0015,     -0.0056,\n            -0.0159,      0.0020,     -0.0002,      0.0079,     -0.0091,\n             0.0123,      0.0115,      0.0140,     -0.0218,     -0.0316,\n             0.0544,     -0.0040,     -0.0153,     -0.0034,      0.0011,\n            -0.0084,     -0.0028,     -0.0015,      0.0054,     -0.0074,\n             0.0113,     -0.0126,      0.0078,     -0.0022,     -0.0007,\n            -0.0014,      0.0028,      0.0291,     -0.0091,      0.0006,\n            -0.0058,      0.0085,      0.0027,     -0.0132,      0.0018,\n             0.0104,      0.0053,     -0.0009,      0.0063,      0.0032,\n             0.0005,     -0.0071,      0.0012,     -0.0048,     -0.0051,\n            -0.0033,     -0.0038,     -0.0152,      0.0002,     -0.0012,\n            -0.0048,      0.0069,     -0.0388,      0.0012,     -0.0066,\n             0.0085,     -0.0184,     -0.0078,     -0.0320,     -0.0052,\n             0.0018,     -0.0007,      0.0047,     -0.0147,      0.0001,\n             0.0166,     -0.0053,      0.0435,      0.0091,      0.0213,\n            -0.0129,     -0.0066,     -0.0050,      0.0095,      0.0134,\n             0.0059,      0.0182,      0.0036,      0.0507,     -0.0036,\n             0.0027,     -0.0078,     -0.0054,      0.0120,     -0.0039,\n            -0.0096,     -0.0017,     -0.0012,     -0.0113,      0.0019,\n            -0.0032,     -0.0056,      0.0060,      0.0018,      0.0036,\n            -0.0008,      0.0004,      0.0101,      0.0026,      0.0027,\n             0.0269,      0.0119,      0.0560,     -0.0055,     -0.0005,\n            -0.0132,     -0.0320,      0.0120,      0.0331,     -0.0198,\n            -0.0525,      0.0020,     -0.0078,      0.0232,     -0.0016,\n            -0.0814,     -0.0835,     -0.0279,     -0.0119,     -0.0037,\n             0.0067,      0.0060,     -0.0028,      0.0055,      0.0112,\n             0.0098,     -0.0078,     -0.0260,     -0.6972,      0.0016,\n             0.0018,     -0.0052,     -0.0039,     -0.0198,      0.0214,\n             0.0143,     -0.0017,      0.0008,     -0.0003,      0.0025,\n             0.0158,     -0.0017,     -0.0208,     -0.0080,     -0.0105,\n            -0.0098,     -0.0065,     -0.0202,     -0.0042,     -0.0161,\n            -0.0127,      0.0004,     -0.0015,      0.0071,     -0.0039,\n             0.0104,      0.0018,     -0.0096,     -0.0525,      0.0143,\n            -0.0563,     -0.0025,      0.0028,     -0.0204,     -0.0023,\n             0.2261,      0.0520,      0.0368,      0.0167,      0.0093,\n            -0.0050,     -0.0038,     -0.0008,     -0.0016,     -0.0040,\n             0.0008,      0.0191,     -0.0183,      0.0217,     -0.0020,\n             0.0053,     -0.0007,     -0.0017,      0.0020,     -0.0017,\n            -0.0025,     -0.0015,      0.0003,     -0.0042,     -0.0065,\n             0.0056,     -0.0019,      0.0038,      0.0070,      0.0029,\n             0.0034,      0.0028,      0.0126,     -0.0012,     -0.0012,\n             0.0016,     -0.0064,      0.0231,      0.0041,      0.0062,\n            -0.0009,      0.0047,     -0.0012,     -0.0078,      0.0008,\n             0.0028,      0.0003,      0.0207,     -0.0010,     -0.0015,\n             0.0006,     -0.0191,     -0.0156,      0.0020,      0.0015,\n             0.0020,      0.0014,      0.0023,     -0.0008,     -0.0075,\n            -0.0038,      0.0044,      0.0496,      0.0070,      0.0005,\n             0.0063,     -0.0147,     -0.0113,      0.0232,     -0.0003,\n            -0.0204,     -0.0042,     -0.0010,     -0.0010,      0.0031,\n             0.0021,     -0.0013,     -0.0173,      0.0001,     -0.0048,\n            -0.0015,     -0.0023,      0.0023,      0.0005,      0.0013,\n            -0.0428,     -0.0080,      0.0013,      0.0010,      0.0014,\n             0.0032,      0.0001,      0.0019,     -0.0016,      0.0025,\n            -0.0023,     -0.0065,     -0.0015,      0.0031,     -0.0012,\n             0.0014,      0.0010,     -0.0168,     -0.0080,     -0.0020,\n            -0.0020,     -0.0017,     -0.0053,      0.0014,      0.0005,\n             0.0004,      0.0066,     -0.0016,      0.0042,      0.0002,\n             0.0005,      0.0166,     -0.0032,     -0.0814,      0.0158,\n             0.2261,      0.0056,      0.0006,      0.0021,      0.0014,\n            -0.0000,      0.0353,     -0.0081,     -0.0002,      0.0030,\n            -0.0005,      0.0004,      0.0036,     -0.0040,     -0.0082,\n             0.0122,      0.0003,      0.0069,      0.0022,     -0.0037,\n            -0.0071,     -0.0053,     -0.0056,     -0.0835,     -0.0017,\n             0.0520,     -0.0019,     -0.0191,     -0.0013,      0.0010,\n             0.0353,     -0.0280,     -0.0022,     -0.0030,     -0.0021,\n             0.0016,     -0.0031,     -0.0074,     -0.0043,     -0.0114,\n             0.0014,      0.0449,      0.0195,      0.0378,     -0.0015,\n             0.0012,      0.0435,      0.0060,     -0.0279,     -0.0208,\n             0.0368,      0.0038,     -0.0156,     -0.0173,     -0.0168,\n            -0.0081,     -0.0022,      0.0161,     -0.0186,     -0.0143,\n            -0.0568,     -0.0048,     -0.0050,     -0.0151,     -0.0258,\n            -0.0226,     -0.0091,     -0.0067,     -0.1849,     -0.0104,\n            -0.0048,      0.0091,      0.0018,     -0.0119,     -0.0080,\n             0.0167,      0.0070,      0.0020,      0.0001,     -0.0080,\n            -0.0002,     -0.0030,     -0.0186,      0.0005,      0.0062,\n            -0.0078,      0.0021,      0.0134,      0.0066,      0.0041,\n            -0.0005,     -0.0012,     -0.0001,      0.0006,     -0.0017,\n            -0.0051,      0.0213,      0.0036,     -0.0037,     -0.0105,\n             0.0093,      0.0029,      0.0015,     -0.0048,     -0.0020,\n             0.0030,     -0.0021,     -0.0143,      0.0062,     -0.0025,\n             0.0022,      0.0033,      0.0083,      0.0051,      0.0088,\n             0.0044,      0.0021,      0.0145,      0.0042,     -0.0020,\n            -0.0033,     -0.0129,     -0.0008,      0.0067,     -0.0098,\n            -0.0050,      0.0034,      0.0020,     -0.0015,     -0.0020,\n            -0.0005,      0.0016,     -0.0568,     -0.0078,      0.0022,\n             0.0016,     -0.0001,      0.0021,      0.0080,      0.0077,\n             0.0117,      0.0079,      0.0017,     -0.0014,     -0.0023,\n            -0.0038,     -0.0066,      0.0004,      0.0060,     -0.0065,\n            -0.0038,      0.0028,      0.0014,     -0.0023,     -0.0017,\n             0.0004,     -0.0031,     -0.0048,      0.0021,      0.0033,\n            -0.0001,      0.0007,      0.0037,      0.0051,      0.0041,\n            -0.0003,      0.0006,      0.0175,      0.0005,     -0.0014,\n            -0.0152,     -0.0050,      0.0101,     -0.0028,     -0.0202,\n            -0.0008,      0.0126,      0.0023,      0.0023,     -0.0053,\n             0.0036,     -0.0074,     -0.0050,      0.0134,      0.0083,\n             0.0021,      0.0037,     -0.0028,      0.0106,      0.0222,\n             0.0175,      0.0066,      0.0059,      0.0178,     -0.0009,\n             0.0002,      0.0095,      0.0026,      0.0055,     -0.0042,\n            -0.0016,     -0.0012,     -0.0008,      0.0005,      0.0013,\n            -0.0040,     -0.0043,     -0.0151,      0.0066,      0.0051,\n             0.0080,      0.0051,      0.0106,      0.0006,     -0.0091,\n            -0.0030,      0.0040,      0.0014,     -0.0018,      0.0025,\n            -0.0012,      0.0134,      0.0027,      0.0112,     -0.0161,\n            -0.0040,     -0.0012,     -0.0075,      0.0013,      0.0005,\n            -0.0082,     -0.0114,     -0.0258,      0.0041,      0.0088,\n             0.0077,      0.0041,      0.0222,     -0.0091,      0.0040,\n            -0.0045,     -0.0019,     -0.0012,     -0.0056,     -0.0013,\n            -0.0048,      0.0059,      0.0269,      0.0098,     -0.0127,\n             0.0008,      0.0016,     -0.0038,     -0.0428,      0.0004,\n             0.0122,      0.0014,     -0.0226,     -0.0005,      0.0044,\n             0.0117,     -0.0003,      0.0175,     -0.0030,     -0.0045,\n             0.0001,      0.0025,      0.0086,      0.0004,      0.0020,\n             0.0069,      0.0182,      0.0119,     -0.0078,      0.0004,\n             0.0191,     -0.0064,      0.0044,     -0.0080,      0.0066,\n             0.0003,      0.0449,     -0.0091,     -0.0012,      0.0021,\n             0.0079,      0.0006,      0.0066,      0.0040,     -0.0019,\n             0.0025,     -0.0008,      0.0233,      0.0000,     -0.0001,\n            -0.0388,      0.0036,      0.0560,     -0.0260,     -0.0015,\n            -0.0183,      0.0231,      0.0496,      0.0013,     -0.0016,\n             0.0069,      0.0195,     -0.0067,     -0.0001,      0.0145,\n             0.0017,      0.0175,      0.0059,      0.0014,     -0.0012,\n             0.0086,      0.0233,     -0.0039,      0.0096,      0.0020,\n             0.0012,      0.0507,     -0.0055,     -0.6972,      0.0071,\n             0.0217,      0.0041,      0.0070,      0.0010,      0.0042,\n             0.0022,      0.0378,     -0.1849,      0.0006,      0.0042,\n            -0.0014,      0.0005,      0.0178,     -0.0018,     -0.0056,\n             0.0004,      0.0000,      0.0096,      0.0013,     -0.0034,\n            -0.0066,     -0.0036,     -0.0005,      0.0016,     -0.0039,\n            -0.0020,      0.0062,      0.0005,      0.0014,      0.0002,\n            -0.0037,     -0.0015,     -0.0104,     -0.0017,     -0.0020,\n            -0.0023,     -0.0014,     -0.0009,      0.0025,     -0.0013,\n             0.0020,     -0.0001,      0.0020,     -0.0034,      0.0017,\n             0.0183,      0.0011,      0.0025,      0.0036,      0.0063,\n             0.0079,     -0.0069,     -0.0009,     -0.0071,     -0.0064,\n            -0.0039,      0.0152,     -0.0048,     -0.0034,     -0.0088,\n             0.0055,      0.0096,      0.0021,      0.0011,     -0.0035,\n            -0.0049,     -0.0039,     -0.0048,      0.0059,      0.0007,\n             0.0011,      0.0114,     -0.0017,     -0.0023,     -0.0049,\n             0.0104,      0.0003,     -0.0030,     -0.0069,     -0.0025,\n             0.0088,      0.0609,      0.0091,      0.0116,     -0.0052,\n            -0.0325,     -0.0033,     -0.0047,      0.0043,      0.0023,\n             0.0111,      0.0035,      0.0112,      0.0054,     -0.0039,\n             0.0025,     -0.0017,     -0.0022,      0.0042,     -0.0011,\n            -0.0024,     -0.0033,     -0.0016,      0.0110,      0.0007,\n             0.0070,     -0.0733,      0.0018,      0.0006,      0.0003,\n             0.0051,      0.0003,      0.0012,     -0.0013,     -0.0052,\n             0.0018,     -0.0083,      0.0080,      0.0090,      0.0075,\n             0.0036,     -0.0023,      0.0042,      0.0025,      0.0118,\n            -0.0061,     -0.0063,      0.0053,      0.0057,     -0.0046,\n            -0.0078,     -0.0800,     -0.0119,     -0.0170,     -0.0052,\n             0.0310,      0.0004,      0.0058,     -0.0101,     -0.0072,\n            -0.0122,     -0.0053,     -0.0058,     -0.0078,     -0.0005,\n             0.0063,     -0.0049,     -0.0011,      0.0118,      0.0001,\n            -0.0026,     -0.0053,     -0.0045,     -0.0156,     -0.0428,\n            -0.0046,      0.0086,     -0.0080,     -0.0074,      0.0002,\n            -0.0026,      0.0003,     -0.0045,      0.0077,     -0.0012,\n             0.0073,      0.0007,      0.0235,     -0.0047,      0.0016,\n             0.0079,      0.0104,     -0.0024,     -0.0061,     -0.0026,\n             0.0151,     -0.0061,     -0.0054,     -0.0038,      0.0025,\n             0.0063,      0.0154,      0.0167,      0.0036,      0.0027,\n            -0.0112,      0.0001,     -0.0027,      0.0077,      0.0060,\n             0.0086,     -0.0058,     -0.0095,      0.0008,     -0.0006,\n            -0.0069,      0.0003,     -0.0033,     -0.0063,     -0.0053,\n            -0.0061,      0.0004,      0.0009,      0.0044,      0.0050,\n             0.0039,     -0.0263,      0.0070,      0.0139,      0.0023,\n            -0.0012,     -0.0009,     -0.0011,     -0.0022,      0.0024,\n             0.0031,      0.0127,      0.0170,      0.0040,     -0.0031,\n            -0.0009,     -0.0030,     -0.0016,      0.0053,     -0.0045,\n            -0.0054,      0.0009,     -0.0113,      0.0002,      0.0032,\n            -0.0158,      0.0068,      0.0020,     -0.0008,     -0.0016,\n            -0.0032,     -0.0046,     -0.0004,      0.0031,      0.0007,\n             0.0299,      0.0124,      0.0062,     -0.0049,      0.0007,\n            -0.0071,     -0.0069,      0.0110,      0.0057,     -0.0156,\n            -0.0038,      0.0044,      0.0002,      0.0005,      0.0001,\n            -0.0036,      0.0062,      0.0001,     -0.0049,      0.0030,\n             0.0039,      0.0043,      0.0019,      0.0077,      0.0101,\n             0.0363,     -0.0025,     -0.0127,     -0.0005,     -0.0045,\n            -0.0064,     -0.0025,      0.0007,     -0.0046,     -0.0428,\n             0.0025,      0.0050,      0.0032,      0.0001,      0.0043,\n            -0.0030,     -0.0023,     -0.0080,     -0.0134,      0.0028,\n             0.0034,      0.0037,      0.0040,      0.0027,      0.0056,\n             0.0171,      0.0073,     -0.0059,     -0.0038,     -0.0023,\n            -0.0039,      0.0088,      0.0070,     -0.0078,     -0.0046,\n             0.0063,      0.0039,     -0.0158,     -0.0036,     -0.0030,\n             0.0063,     -0.0051,     -0.0002,     -0.0054,      0.0037,\n             0.0031,      0.0043,      0.0004,     -0.0053,     -0.0326,\n            -0.0109,     -0.0049,     -0.0003,     -0.0034,      0.0020,\n             0.0152,      0.0609,     -0.0733,     -0.0800,      0.0086,\n             0.0154,     -0.0263,      0.0068,      0.0062,     -0.0023,\n            -0.0051,      0.0058,     -0.0030,      0.0003,     -0.0069,\n            -0.0026,     -0.0101,     -0.0612,     -0.0116,      0.0108,\n            -0.0538,      0.0020,     -0.0137,     -0.0105,      0.0054,\n            -0.0048,      0.0091,      0.0018,     -0.0119,     -0.0080,\n             0.0167,      0.0070,      0.0020,      0.0001,     -0.0080,\n            -0.0002,     -0.0030,     -0.0186,      0.0005,      0.0062,\n            -0.0078,      0.0021,      0.0134,      0.0066,      0.0041,\n            -0.0005,     -0.0012,     -0.0001,      0.0006,     -0.0017,\n            -0.0034,      0.0116,      0.0006,     -0.0170,     -0.0074,\n             0.0036,      0.0139,     -0.0008,     -0.0049,     -0.0134,\n            -0.0054,      0.0003,      0.0005,      0.0135,     -0.0218,\n            -0.0152,     -0.0335,     -0.0147,      0.0007,     -0.0060,\n            -0.0116,     -0.0070,     -0.0063,     -0.0181,     -0.0092,\n            -0.0088,     -0.0052,      0.0003,     -0.0052,      0.0002,\n             0.0027,      0.0023,     -0.0016,      0.0030,      0.0028,\n             0.0037,     -0.0069,      0.0062,     -0.0218,     -0.0052,\n             0.0056,      0.0061,      0.0075,      0.0012,     -0.0003,\n            -0.0033,      0.0023,      0.0027,      0.0052,      0.0026,\n             0.0055,     -0.0325,      0.0051,      0.0310,     -0.0026,\n            -0.0112,     -0.0012,     -0.0032,      0.0039,      0.0034,\n             0.0031,     -0.0026,     -0.0078,     -0.0152,      0.0056,\n            -0.0114,      0.0044,      0.0065,      0.0026,      0.0023,\n            -0.0042,     -0.0076,      0.0023,      0.0018,      0.0010,\n             0.0096,     -0.0033,      0.0003,      0.0004,      0.0003,\n             0.0001,     -0.0009,     -0.0046,      0.0043,      0.0037,\n             0.0043,     -0.0101,      0.0021,     -0.0335,      0.0061,\n             0.0044,     -0.0121,      0.0108,      0.0055,      0.0009,\n            -0.0023,      0.0037,      0.0036,      0.0010,      0.0094,\n             0.0021,     -0.0047,      0.0012,      0.0058,     -0.0045,\n            -0.0027,     -0.0011,     -0.0004,      0.0019,      0.0040,\n             0.0004,     -0.0612,      0.0134,     -0.0147,      0.0075,\n             0.0065,      0.0108,     -0.0197,      0.0027,      0.0058,\n             0.0010,      0.0043,      0.0031,      0.0065,      0.0087,\n             0.0011,      0.0043,     -0.0013,     -0.0101,      0.0077,\n             0.0077,     -0.0022,      0.0031,      0.0077,      0.0027,\n            -0.0053,     -0.0116,      0.0066,      0.0007,      0.0012,\n             0.0026,      0.0055,      0.0027,     -0.0107,      0.0015,\n            -0.0108,     -0.0067,     -0.0002,      0.0007,     -0.0070,\n            -0.0035,      0.0023,     -0.0052,     -0.0072,     -0.0012,\n             0.0060,      0.0024,      0.0007,      0.0101,      0.0056,\n            -0.0326,      0.0108,      0.0041,     -0.0060,     -0.0003,\n             0.0023,      0.0009,      0.0058,      0.0015,     -0.0049,\n            -0.0142,     -0.0156,     -0.0031,     -0.0033,      0.0008,\n            -0.0049,      0.0111,      0.0018,     -0.0122,      0.0073,\n             0.0086,      0.0031,      0.0299,      0.0363,      0.0171,\n            -0.0109,     -0.0538,     -0.0005,     -0.0116,     -0.0033,\n            -0.0042,     -0.0023,      0.0010,     -0.0108,     -0.0142,\n             0.0072,     -0.0204,     -0.0079,     -0.0252,     -0.0009,\n            -0.0039,      0.0035,     -0.0083,     -0.0053,      0.0007,\n            -0.0058,      0.0127,      0.0124,     -0.0025,      0.0073,\n            -0.0049,      0.0020,     -0.0012,     -0.0070,      0.0023,\n            -0.0076,      0.0037,      0.0043,     -0.0067,     -0.0156,\n            -0.0204,      0.0061,     -0.0096,     -0.0105,     -0.0047,\n            -0.0048,      0.0112,      0.0080,     -0.0058,      0.0235,\n            -0.0095,      0.0170,      0.0062,     -0.0127,     -0.0059,\n            -0.0003,     -0.0137,     -0.0001,     -0.0063,      0.0027,\n             0.0023,      0.0036,      0.0031,     -0.0002,     -0.0031,\n            -0.0079,     -0.0096,     -0.0001,     -0.0005,      0.0035,\n             0.0059,      0.0054,      0.0090,     -0.0078,     -0.0047,\n             0.0008,      0.0040,     -0.0049,     -0.0005,     -0.0038,\n            -0.0034,     -0.0105,      0.0006,     -0.0181,      0.0052,\n             0.0018,      0.0010,      0.0065,      0.0007,     -0.0033,\n            -0.0252,     -0.0105,     -0.0005,     -0.0060,     -0.0025,\n             0.0007,     -0.0039,      0.0075,     -0.0005,      0.0016,\n            -0.0006,     -0.0031,      0.0007,     -0.0045,     -0.0023,\n             0.0020,      0.0054,     -0.0017,     -0.0092,      0.0026,\n             0.0010,      0.0094,      0.0087,     -0.0070,      0.0008,\n            -0.0009,     -0.0047,      0.0035,     -0.0025,     -0.0010,\n            -0.0357,     -0.0095,      0.0053,      0.0117,      0.0125,\n            -0.0076,      0.0120,      0.0004,     -0.0018,     -0.0009,\n            -0.0033,      0.0314,     -0.0051,     -0.0088,      0.0130,\n            -0.0078,     -0.0075,     -0.0198,     -0.0140,     -0.0106,\n            -0.0113,     -0.0078,      0.0008,     -0.0086,     -0.0035,\n            -0.0095,     -0.0239,     -0.0064,      0.0034,      0.0024,\n            -0.0074,      0.0127,      0.0045,     -0.0149,     -0.0034,\n             0.0043,      0.0540,      0.0213,     -0.0052,      0.0070,\n            -0.0119,     -0.0026,     -0.0055,      0.0043,     -0.0008,\n             0.0035,      0.0055,      0.0249,     -0.0016,      0.0022,\n             0.0053,     -0.0064,      0.0021,      0.0076,      0.0069,\n            -0.0013,     -0.0048,     -0.0001,     -0.0009,     -0.0003,\n             0.0009,      0.0037,      0.0036,      0.0003,      0.0020,\n             0.0003,      0.0005,     -0.0024,     -0.0064,     -0.0073,\n            -0.0030,     -0.0089,      0.0079,      0.0012,      0.0049,\n             0.0117,      0.0034,      0.0076,     -0.0082,     -0.0064,\n            -0.0044,     -0.0150,     -0.0008,      0.0194,      0.0000,\n            -0.0035,     -0.0625,     -0.0037,     -0.0052,      0.0020,\n            -0.0008,     -0.0043,     -0.0039,     -0.0051,      0.0040,\n             0.0026,      0.0009,      0.0017,     -0.0106,     -0.0055,\n             0.0125,      0.0024,      0.0069,     -0.0064,     -0.0137,\n             0.0191,     -0.0006,      0.0012,     -0.0091,     -0.0200,\n            -0.0001,      0.0164,     -0.0105,      0.0002,     -0.0043,\n            -0.0026,      0.0020,     -0.0015,      0.0012,      0.0039,\n             0.0114,      0.0034,      0.0095,     -0.0028,      0.0040,\n            -0.0076,     -0.0074,     -0.0013,     -0.0044,      0.0191,\n             0.0155,      0.0085,      0.0003,     -0.0172,     -0.0016,\n             0.0200,      0.0433,      0.0093,      0.0027,     -0.0010,\n            -0.0043,      0.0017,      0.0025,     -0.0007,     -0.0100,\n            -0.0129,     -0.0097,     -0.0072,      0.0125,      0.0051,\n             0.0120,      0.0127,     -0.0048,     -0.0150,     -0.0006,\n             0.0085,     -0.0036,      0.0015,      0.0009,      0.0011,\n             0.0304,     -0.0240,      0.0029,      0.0023,     -0.0011,\n             0.0001,     -0.0001,      0.0045,      0.0046,     -0.0004,\n             0.0018,      0.0092,      0.0038,      0.0088,      0.0037,\n             0.0004,      0.0045,     -0.0001,     -0.0008,      0.0012,\n             0.0003,      0.0015,     -0.0119,     -0.0006,      0.0002,\n            -0.0007,     -0.0265,      0.0015,     -0.0016,     -0.0017,\n            -0.0004,     -0.0005,      0.0002,     -0.0010,     -0.0035,\n             0.0175,      0.0073,      0.0132,     -0.0005,      0.0018,\n            -0.0018,     -0.0149,     -0.0009,      0.0194,     -0.0091,\n            -0.0172,      0.0009,     -0.0006,     -0.0005,     -0.0061,\n            -0.0014,     -0.0008,     -0.0048,      0.0030,     -0.0050,\n             0.0049,      0.0032,      0.0016,     -0.0001,      0.0038,\n             0.0036,     -0.0029,     -0.0071,     -0.0068,     -0.0025,\n            -0.0009,     -0.0034,     -0.0003,      0.0000,     -0.0200,\n            -0.0016,      0.0011,      0.0002,     -0.0061,      0.0002,\n            -0.0044,      0.0075,     -0.0020,      0.0028,     -0.0006,\n            -0.0009,     -0.0012,      0.0011,      0.0029,      0.0062,\n             0.0140,      0.0032,     -0.0002,     -0.0076,     -0.0028,\n            -0.0033,      0.0043,      0.0009,     -0.0035,     -0.0001,\n             0.0200,      0.0304,     -0.0007,     -0.0014,     -0.0044,\n             0.0016,     -0.0231,      0.0030,      0.0037,     -0.0070,\n             0.0027,      0.0011,      0.0020,     -0.0040,     -0.0046,\n            -0.0035,     -0.0025,     -0.0018,     -0.0022,      0.0005,\n             0.0314,      0.0540,      0.0037,     -0.0625,      0.0164,\n             0.0433,     -0.0240,     -0.0265,     -0.0008,      0.0075,\n            -0.0231,      0.0054,     -0.0021,     -0.0069,      0.0016,\n            -0.0371,     -0.0062,     -0.0274,     -0.0133,      0.0110,\n            -0.0332,     -0.0037,      0.0191,     -0.0276,     -0.0074,\n            -0.0051,      0.0213,      0.0036,     -0.0037,     -0.0105,\n             0.0093,      0.0029,      0.0015,     -0.0048,     -0.0020,\n             0.0030,     -0.0021,     -0.0143,      0.0062,     -0.0025,\n             0.0022,      0.0033,      0.0083,      0.0051,      0.0088,\n             0.0044,      0.0021,      0.0145,      0.0042,     -0.0020,\n            -0.0088,     -0.0052,      0.0003,     -0.0052,      0.0002,\n             0.0027,      0.0023,     -0.0016,      0.0030,      0.0028,\n             0.0037,     -0.0069,      0.0062,     -0.0218,     -0.0052,\n             0.0056,      0.0061,      0.0075,      0.0012,     -0.0003,\n            -0.0033,      0.0023,      0.0027,      0.0052,      0.0026,\n             0.0130,      0.0070,      0.0020,      0.0020,     -0.0043,\n            -0.0010,     -0.0011,     -0.0017,     -0.0050,     -0.0006,\n            -0.0070,      0.0016,     -0.0025,     -0.0052,      0.0057,\n            -0.0058,     -0.0057,     -0.0059,      0.0008,     -0.0011,\n            -0.0053,     -0.0032,     -0.0039,     -0.0066,     -0.0097,\n            -0.0078,     -0.0119,      0.0003,     -0.0008,     -0.0026,\n            -0.0043,      0.0001,     -0.0004,      0.0049,     -0.0009,\n             0.0027,     -0.0371,      0.0022,      0.0056,     -0.0058,\n            -0.0070,      0.0056,      0.0066,     -0.0039,     -0.0025,\n             0.0001,     -0.0060,      0.0022,      0.0025,      0.0001,\n            -0.0075,     -0.0026,      0.0005,     -0.0043,      0.0020,\n             0.0017,     -0.0001,     -0.0005,      0.0032,     -0.0012,\n             0.0011,     -0.0062,      0.0033,      0.0061,     -0.0057,\n             0.0056,     -0.0118,      0.0068,     -0.0004,     -0.0022,\n            -0.0069,      0.0036,      0.0022,      0.0024,      0.0007,\n            -0.0198,     -0.0055,     -0.0024,     -0.0039,     -0.0015,\n             0.0025,      0.0045,      0.0002,      0.0016,      0.0011,\n             0.0020,     -0.0274,      0.0083,      0.0075,     -0.0059,\n             0.0066,      0.0068,     -0.0169,      0.0003,      0.0010,\n             0.0016,      0.0044,      0.0033,      0.0065,      0.0014,\n            -0.0140,      0.0043,     -0.0064,     -0.0051,      0.0012,\n            -0.0007,      0.0046,     -0.0010,     -0.0001,      0.0029,\n            -0.0040,     -0.0133,      0.0051,      0.0012,      0.0008,\n            -0.0039,     -0.0004,      0.0003,     -0.0109,      0.0066,\n            -0.0077,     -0.0042,     -0.0011,     -0.0016,      0.0021,\n            -0.0106,     -0.0008,     -0.0073,      0.0040,      0.0039,\n            -0.0100,     -0.0004,     -0.0035,      0.0038,      0.0062,\n            -0.0046,      0.0110,      0.0088,     -0.0003,     -0.0011,\n            -0.0025,     -0.0022,      0.0010,      0.0066,     -0.0091,\n            -0.0095,     -0.0075,     -0.0019,     -0.0049,     -0.0006,\n            -0.0113,      0.0035,     -0.0030,      0.0026,      0.0114,\n            -0.0129,      0.0018,      0.0175,      0.0036,      0.0140,\n            -0.0035,     -0.0332,      0.0044,     -0.0033,     -0.0053,\n             0.0001,     -0.0069,      0.0016,     -0.0077,     -0.0095,\n             0.0086,     -0.0142,     -0.0076,     -0.0148,     -0.0035,\n            -0.0078,      0.0055,     -0.0089,      0.0009,      0.0034,\n            -0.0097,      0.0092,      0.0073,     -0.0029,      0.0032,\n            -0.0025,     -0.0037,      0.0021,      0.0023,     -0.0032,\n            -0.0060,      0.0036,      0.0044,     -0.0042,     -0.0075,\n            -0.0142,      0.0055,     -0.0100,     -0.0086,     -0.0029,\n             0.0008,      0.0249,      0.0079,      0.0017,      0.0095,\n            -0.0072,      0.0038,      0.0132,     -0.0071,     -0.0002,\n            -0.0018,      0.0191,      0.0145,      0.0027,     -0.0039,\n             0.0022,      0.0022,      0.0033,     -0.0011,     -0.0019,\n            -0.0076,     -0.0100,     -0.0001,     -0.0037,     -0.0073,\n            -0.0086,     -0.0016,      0.0012,     -0.0106,     -0.0028,\n             0.0125,      0.0088,     -0.0005,     -0.0068,     -0.0076,\n            -0.0022,     -0.0276,      0.0042,      0.0052,     -0.0066,\n             0.0025,      0.0024,      0.0065,     -0.0016,     -0.0049,\n            -0.0148,     -0.0086,     -0.0037,     -0.0014,     -0.0069,\n            -0.0035,      0.0022,      0.0049,     -0.0055,      0.0040,\n             0.0051,      0.0037,      0.0018,     -0.0025,     -0.0028,\n             0.0005,     -0.0074,     -0.0020,      0.0026,     -0.0097,\n             0.0001,      0.0007,      0.0014,      0.0021,     -0.0006,\n            -0.0035,     -0.0029,     -0.0073,     -0.0069,     -0.0004,\n             0.0001,     -0.0044,      0.0057,      0.0024,      0.0056,\n             0.0013,      0.0026,      0.0007,     -0.0063,     -0.0041,\n             0.0044,     -0.0304,     -0.0033,      0.0055,     -0.0078,\n            -0.0013,      0.0036,      0.0007,      0.0012,      0.0010,\n            -0.0043,      0.0049,      0.0014,     -0.0001,     -0.0009,\n            -0.0044,      0.0287,     -0.0062,      0.0181,     -0.0055,\n             0.0460,      0.0039,     -0.0047,     -0.0098,     -0.0039,\n            -0.0340,      0.0082,     -0.0129,     -0.0325,     -0.0119,\n             0.0968,     -0.0073,     -0.0552,      0.0143,     -0.0001,\n            -0.0050,     -0.0014,     -0.0600,     -0.0175,      0.0045,\n             0.0057,     -0.0062,      0.0001,      0.0084,      0.0054,\n            -0.0042,     -0.0041,      0.0018,      0.0041,      0.0014,\n             0.0248,     -0.0433,     -0.0008,      0.0051,      0.0003,\n            -0.0001,      0.0021,      0.0016,     -0.0037,     -0.0067,\n            -0.0052,     -0.0146,     -0.0056,      0.0098,      0.0031,\n             0.0024,      0.0181,      0.0084,      0.0159,      0.0091,\n             0.0035,     -0.0065,      0.0074,      0.0127,     -0.0026,\n             0.0226,     -0.0105,      0.0067,      0.0310,     -0.0008,\n            -0.1437,      0.0081,      0.0072,     -0.0136,     -0.0015,\n             0.0063,     -0.0009,     -0.0017,      0.0516,     -0.0076,\n             0.0056,     -0.0055,      0.0054,      0.0091,      0.0248,\n            -0.0048,      0.0028,      0.0025,     -0.0201,     -0.0119,\n             0.0026,     -0.0024,     -0.0098,     -0.0026,     -0.0026,\n            -0.0064,      0.0020,     -0.0062,     -0.0014,     -0.0054,\n            -0.0014,     -0.0193,      0.0017,      0.0020,     -0.0094,\n             0.0013,      0.0460,     -0.0042,      0.0035,     -0.0048,\n             0.0177,      0.0028,     -0.0091,     -0.0106,      0.0006,\n            -0.0022,      0.0039,     -0.0050,     -0.0112,     -0.0043,\n             0.0933,     -0.0073,     -0.0265,      0.0115,      0.0024,\n            -0.0064,     -0.0121,     -0.0061,     -0.0488,      0.0067,\n             0.0026,      0.0039,     -0.0041,     -0.0065,      0.0028,\n             0.0028,     -0.0037,      0.0011,      0.0037,      0.0030,\n             0.0168,      0.0186,      0.0034,     -0.0012,      0.0001,\n             0.0015,     -0.0014,     -0.0018,     -0.0034,     -0.0066,\n             0.0004,      0.0027,     -0.0101,     -0.0033,      0.0000,\n             0.0007,     -0.0047,      0.0018,      0.0074,      0.0025,\n            -0.0091,      0.0011,     -0.0080,      0.0003,     -0.0001,\n             0.0027,     -0.0030,      0.0020,     -0.0032,     -0.0004,\n            -0.0008,     -0.0026,     -0.0007,     -0.0007,     -0.0029,\n             0.0197,      0.0057,     -0.0124,     -0.0065,      0.0005,\n            -0.0063,     -0.0098,      0.0041,      0.0127,     -0.0201,\n            -0.0106,      0.0037,      0.0003,     -0.0007,      0.0029,\n             0.0008,     -0.0019,     -0.0015,      0.0039,      0.0049,\n            -0.0050,      0.0052,      0.0007,      0.0049,      0.0075,\n            -0.0042,     -0.0026,     -0.0055,      0.0003,      0.0076,\n            -0.0041,     -0.0039,      0.0014,     -0.0026,     -0.0119,\n             0.0006,      0.0030,     -0.0001,      0.0029,      0.0007,\n            -0.0018,      0.0019,     -0.0020,      0.0034,     -0.0009,\n            -0.0040,     -0.0024,     -0.0002,      0.0005,      0.0051,\n             0.0134,      0.0011,     -0.0113,     -0.0022,     -0.0028,\n             0.0044,     -0.0340,      0.0248,      0.0226,      0.0026,\n            -0.0022,      0.0168,      0.0027,      0.0008,     -0.0018,\n            -0.0004,     -0.0017,     -0.0005,      0.0031,      0.0027,\n            -0.0075,      0.0020,     -0.0058,      0.0003,     -0.0063,\n            -0.0042,     -0.0112,     -0.0001,     -0.0013,     -0.0197,\n            -0.0304,      0.0082,     -0.0433,     -0.0105,     -0.0024,\n             0.0039,      0.0186,     -0.0030,     -0.0019,      0.0019,\n            -0.0017,     -0.0068,      0.0016,     -0.0026,     -0.0371,\n            -0.0018,     -0.0023,     -0.0122,     -0.0009,      0.0081,\n            -0.0417,     -0.0546,     -0.0108,     -0.0012,      0.0113,\n            -0.0033,     -0.0129,     -0.0008,      0.0067,     -0.0098,\n            -0.0050,      0.0034,      0.0020,     -0.0015,     -0.0020,\n            -0.0005,      0.0016,     -0.0568,     -0.0078,      0.0022,\n             0.0016,     -0.0001,      0.0021,      0.0080,      0.0077,\n             0.0117,      0.0079,      0.0017,     -0.0014,     -0.0023,\n             0.0055,     -0.0325,      0.0051,      0.0310,     -0.0026,\n            -0.0112,     -0.0012,     -0.0032,      0.0039,      0.0034,\n             0.0031,     -0.0026,     -0.0078,     -0.0152,      0.0056,\n            -0.0114,      0.0044,      0.0065,      0.0026,      0.0023,\n            -0.0042,     -0.0076,      0.0023,      0.0018,      0.0010,\n            -0.0078,     -0.0119,      0.0003,     -0.0008,     -0.0026,\n            -0.0043,      0.0001,     -0.0004,      0.0049,     -0.0009,\n             0.0027,     -0.0371,      0.0022,      0.0056,     -0.0058,\n            -0.0070,      0.0056,      0.0066,     -0.0039,     -0.0025,\n             0.0001,     -0.0060,      0.0022,      0.0025,      0.0001,\n            -0.0013,      0.0968,     -0.0001,     -0.1437,     -0.0064,\n             0.0933,      0.0015,     -0.0008,     -0.0050,     -0.0040,\n            -0.0075,     -0.0018,      0.0016,     -0.0114,     -0.0070,\n             0.0064,     -0.0112,     -0.0111,      0.0006,     -0.0039,\n            -0.0116,     -0.0068,     -0.0137,     -0.0114,     -0.0075,\n             0.0036,     -0.0073,      0.0021,      0.0081,      0.0020,\n            -0.0073,     -0.0014,     -0.0026,      0.0052,     -0.0024,\n             0.0020,     -0.0023,     -0.0001,      0.0044,      0.0056,\n            -0.0112,     -0.0125,      0.0105,      0.0015,     -0.0007,\n            -0.0008,     -0.0217,      0.0030,      0.0005,     -0.0046,\n             0.0007,     -0.0552,      0.0016,      0.0072,     -0.0062,\n            -0.0265,     -0.0018,     -0.0007,      0.0007,     -0.0002,\n            -0.0058,     -0.0122,      0.0021,      0.0065,      0.0066,\n            -0.0111,      0.0105,     -0.0544,      0.0014,      0.0036,\n            -0.0008,     -0.0010,      0.0029,      0.0061,      0.0023,\n             0.0012,      0.0143,     -0.0037,     -0.0136,     -0.0014,\n             0.0115,     -0.0034,     -0.0007,      0.0049,      0.0005,\n             0.0003,     -0.0009,      0.0080,      0.0026,     -0.0039,\n             0.0006,      0.0015,      0.0014,     -0.0033,      0.0013,\n            -0.0019,     -0.0034,     -0.0011,     -0.0027,     -0.0066,\n             0.0010,     -0.0001,     -0.0067,     -0.0015,     -0.0054,\n             0.0024,     -0.0066,     -0.0029,      0.0075,      0.0051,\n            -0.0063,      0.0081,      0.0077,      0.0023,     -0.0025,\n            -0.0039,     -0.0007,      0.0036,      0.0013,     -0.0021,\n            -0.0043,     -0.0030,     -0.0018,     -0.0023,     -0.0008,\n            -0.0043,     -0.0050,     -0.0052,      0.0063,     -0.0014,\n            -0.0064,      0.0004,      0.0197,     -0.0042,      0.0134,\n            -0.0042,     -0.0417,      0.0117,     -0.0042,      0.0001,\n            -0.0116,     -0.0008,     -0.0008,     -0.0019,     -0.0043,\n             0.0011,     -0.0052,     -0.0015,     -0.0091,      0.0020,\n             0.0049,     -0.0014,     -0.0146,     -0.0009,     -0.0193,\n            -0.0121,      0.0027,      0.0057,     -0.0026,      0.0011,\n            -0.0112,     -0.0546,      0.0079,     -0.0076,     -0.0060,\n            -0.0068,     -0.0217,     -0.0010,     -0.0034,     -0.0030,\n            -0.0052,      0.0030,     -0.0019,     -0.0098,     -0.0036,\n             0.0014,     -0.0600,     -0.0056,     -0.0017,      0.0017,\n            -0.0061,     -0.0101,     -0.0124,     -0.0055,     -0.0113,\n            -0.0001,     -0.0108,      0.0017,      0.0023,      0.0022,\n            -0.0137,      0.0030,      0.0029,     -0.0011,     -0.0018,\n            -0.0015,     -0.0019,     -0.0048,     -0.0001,     -0.0020,\n            -0.0001,     -0.0175,      0.0098,      0.0516,      0.0020,\n            -0.0488,     -0.0033,     -0.0065,      0.0003,     -0.0022,\n            -0.0013,     -0.0012,     -0.0014,      0.0018,      0.0025,\n            -0.0114,      0.0005,      0.0061,     -0.0027,     -0.0023,\n            -0.0091,     -0.0098,     -0.0001,     -0.0098,     -0.0018,\n            -0.0009,      0.0045,      0.0031,     -0.0076,     -0.0094,\n             0.0067,      0.0000,      0.0005,      0.0076,     -0.0028,\n            -0.0197,      0.0113,     -0.0023,      0.0010,      0.0001,\n            -0.0075,     -0.0046,      0.0023,     -0.0066,     -0.0008,\n             0.0020,     -0.0036,     -0.0020,     -0.0018,      0.0023,\n             0.0122,      0.0022,      0.0038,      0.0038,     -0.0101,\n             0.0019,     -0.0099,      0.0046,     -0.0135,     -0.0007,\n             0.0086,     -0.0219,     -0.0038,      0.0096,     -0.0075,\n             0.0036,     -0.0016,      0.0007,      0.0016,      0.0074,\n             0.0078,     -0.0003,      0.0029,      0.0031,     -0.0045,\n             0.0022,      0.0093,     -0.0144,     -0.0248,      0.0106,\n             0.0065,     -0.0027,     -0.0067,     -0.0062,      0.0054,\n             0.0127,      0.0124,     -0.0066,     -0.0033,     -0.0026,\n            -0.0073,      0.0050,     -0.0001,     -0.0022,     -0.0114,\n            -0.0039,     -0.0053,      0.0418,      0.0138,     -0.0042,\n             0.0038,     -0.0144,     -0.0026,      0.0189,     -0.0006,\n            -0.0128,     -0.0021,     -0.0006,      0.0028,      0.0018,\n             0.0114,     -0.0334,      0.0004,      0.0003,      0.0005,\n             0.0021,     -0.0006,     -0.0003,     -0.0027,     -0.0042,\n             0.0027,     -0.0051,      0.0561,      0.0060,      0.0050,\n             0.0038,     -0.0248,      0.0189,      0.0181,      0.0110,\n            -0.0206,     -0.0025,      0.0081,      0.0102,     -0.0067,\n            -0.0229,      0.0113,      0.0060,      0.0004,     -0.0043,\n             0.0081,     -0.0038,      0.0008,     -0.0058,     -0.0024,\n            -0.0059,      0.0016,     -0.0508,     -0.0234,      0.0016,\n            -0.0101,      0.0106,     -0.0006,      0.0110,      0.0056,\n            -0.0018,      0.0129,     -0.0002,     -0.0107,     -0.0279,\n             0.0039,     -0.0205,     -0.0065,      0.0003,      0.0020,\n             0.0020,     -0.0098,      0.0038,      0.0035,      0.0009,\n             0.0096,     -0.0037,      0.0015,      0.0052,     -0.0012,\n             0.0019,      0.0065,     -0.0128,     -0.0206,     -0.0018,\n             0.0067,     -0.0037,     -0.0092,     -0.0037,      0.0072,\n             0.0238,      0.0107,     -0.0038,      0.0001,      0.0017,\n            -0.0073,     -0.0012,      0.0013,      0.0009,     -0.0026,\n             0.0006,     -0.0091,      0.0376,      0.0320,     -0.0017,\n            -0.0099,     -0.0027,     -0.0021,     -0.0025,      0.0129,\n            -0.0037,      0.0075,     -0.0013,      0.0128,      0.0005,\n            -0.0011,      0.0209,      0.0028,     -0.0009,     -0.0001,\n            -0.0014,      0.0012,     -0.0024,     -0.0043,     -0.0102,\n            -0.0102,      0.0093,      0.0001,     -0.0011,      0.0034,\n             0.0046,     -0.0067,     -0.0006,      0.0081,     -0.0002,\n            -0.0092,     -0.0013,     -0.0065,      0.0000,      0.0013,\n             0.0006,     -0.0071,      0.0014,     -0.0046,     -0.0005,\n            -0.0026,     -0.0022,     -0.0000,      0.0010,     -0.0005,\n             0.0250,      0.0103,      0.0200,     -0.0129,      0.0003,\n            -0.0135,     -0.0062,      0.0028,      0.0102,     -0.0107,\n            -0.0037,      0.0128,      0.0000,      0.0011,      0.0011,\n            -0.0015,     -0.0005,     -0.0023,      0.0043,      0.0032,\n             0.0052,     -0.0065,     -0.0002,      0.0042,      0.0117,\n             0.0220,     -0.0038,     -0.0086,     -0.0017,     -0.0100,\n            -0.0007,      0.0054,      0.0018,     -0.0067,     -0.0279,\n             0.0072,      0.0005,      0.0013,      0.0011,      0.0028,\n            -0.0028,      0.0151,     -0.0017,      0.0037,     -0.0012,\n            -0.0024,     -0.0043,     -0.0019,      0.0027,      0.0026,\n             0.0168,      0.0028,      0.0025,     -0.0028,      0.0003,\n             0.0086,      0.0127,      0.0114,     -0.0229,      0.0039,\n             0.0238,     -0.0011,      0.0006,     -0.0015,     -0.0028,\n             0.0024,     -0.0099,      0.0004,      0.0043,      0.0011,\n             0.0020,     -0.0048,     -0.0054,     -0.0116,      0.0047,\n            -0.0645,     -0.0057,     -0.0055,     -0.0316,      0.0010,\n            -0.0219,      0.0124,     -0.0334,      0.0113,     -0.0205,\n             0.0107,      0.0209,     -0.0071,     -0.0005,      0.0151,\n            -0.0099,      0.0044,     -0.0031,     -0.0101,     -0.0062,\n            -0.0023,      0.0008,     -0.0196,      0.0185,      0.0159,\n             0.0087,      0.0259,     -0.0114,     -0.0039,     -0.0126,\n            -0.0038,     -0.0066,      0.0004,      0.0060,     -0.0065,\n            -0.0038,      0.0028,      0.0014,     -0.0023,     -0.0017,\n             0.0004,     -0.0031,     -0.0048,      0.0021,      0.0033,\n            -0.0001,      0.0007,      0.0037,      0.0051,      0.0041,\n            -0.0003,      0.0006,      0.0175,      0.0005,     -0.0014,\n             0.0096,     -0.0033,      0.0003,      0.0004,      0.0003,\n             0.0001,     -0.0009,     -0.0046,      0.0043,      0.0037,\n             0.0043,     -0.0101,      0.0021,     -0.0335,      0.0061,\n             0.0044,     -0.0121,      0.0108,      0.0055,      0.0009,\n            -0.0023,      0.0037,      0.0036,      0.0010,      0.0094,\n            -0.0075,     -0.0026,      0.0005,     -0.0043,      0.0020,\n             0.0017,     -0.0001,     -0.0005,      0.0032,     -0.0012,\n             0.0011,     -0.0062,      0.0033,      0.0061,     -0.0057,\n             0.0056,     -0.0118,      0.0068,     -0.0004,     -0.0022,\n            -0.0069,      0.0036,      0.0022,      0.0024,      0.0007,\n             0.0036,     -0.0073,      0.0021,      0.0081,      0.0020,\n            -0.0073,     -0.0014,     -0.0026,      0.0052,     -0.0024,\n             0.0020,     -0.0023,     -0.0001,      0.0044,      0.0056,\n            -0.0112,     -0.0125,      0.0105,      0.0015,     -0.0007,\n            -0.0008,     -0.0217,      0.0030,      0.0005,     -0.0046,\n            -0.0016,      0.0050,     -0.0006,     -0.0038,     -0.0098,\n            -0.0012,      0.0012,     -0.0022,     -0.0065,     -0.0043,\n            -0.0048,      0.0008,      0.0007,     -0.0121,     -0.0118,\n            -0.0125,      0.0079,     -0.0085,      0.0049,     -0.0028,\n            -0.0078,     -0.0057,     -0.0059,     -0.0139,     -0.0089,\n             0.0007,     -0.0001,     -0.0003,      0.0008,      0.0038,\n             0.0013,     -0.0024,     -0.0000,     -0.0002,     -0.0019,\n            -0.0054,     -0.0196,      0.0037,      0.0108,      0.0068,\n             0.0105,     -0.0085,     -0.0419,      0.0013,      0.0022,\n             0.0013,      0.0045,     -0.0058,      0.0109,      0.0089,\n             0.0016,     -0.0022,     -0.0027,     -0.0058,      0.0035,\n             0.0009,     -0.0043,      0.0010,      0.0042,      0.0027,\n            -0.0116,      0.0185,      0.0051,      0.0055,     -0.0004,\n             0.0015,      0.0049,      0.0013,     -0.0103,      0.0003,\n            -0.0102,     -0.0073,     -0.0079,     -0.0076,     -0.0052,\n             0.0074,     -0.0114,     -0.0042,     -0.0024,      0.0009,\n            -0.0026,     -0.0102,     -0.0005,      0.0117,      0.0026,\n             0.0047,      0.0159,      0.0041,      0.0009,     -0.0022,\n            -0.0007,     -0.0028,      0.0022,      0.0003,     -0.0070,\n            -0.0135,     -0.0137,     -0.0104,     -0.0037,     -0.0036,\n             0.0078,     -0.0039,      0.0027,     -0.0059,      0.0096,\n             0.0006,     -0.0102,      0.0250,      0.0220,      0.0168,\n            -0.0645,      0.0087,     -0.0003,     -0.0023,     -0.0069,\n            -0.0008,     -0.0078,      0.0013,     -0.0102,     -0.0135,\n             0.0092,     -0.0303,     -0.0688,     -0.0208,     -0.0035,\n            -0.0003,     -0.0053,     -0.0051,      0.0016,     -0.0037,\n            -0.0091,      0.0093,      0.0103,     -0.0038,      0.0028,\n            -0.0057,      0.0259,      0.0006,      0.0037,      0.0036,\n            -0.0217,     -0.0057,      0.0045,     -0.0073,     -0.0137,\n            -0.0303,      0.0066,     -0.0129,     -0.0094,     -0.0042,\n             0.0029,      0.0418,      0.0561,     -0.0508,      0.0015,\n             0.0376,      0.0001,      0.0200,     -0.0086,      0.0025,\n            -0.0055,     -0.0114,      0.0175,      0.0036,      0.0022,\n             0.0030,     -0.0059,     -0.0058,     -0.0079,     -0.0104,\n            -0.0688,     -0.0129,      0.0026,     -0.0149,      0.0182,\n             0.0031,      0.0138,      0.0060,     -0.0234,      0.0052,\n             0.0320,     -0.0011,     -0.0129,     -0.0017,     -0.0028,\n            -0.0316,     -0.0039,      0.0005,      0.0010,      0.0024,\n             0.0005,     -0.0139,      0.0109,     -0.0076,     -0.0037,\n            -0.0208,     -0.0094,     -0.0149,      0.0003,     -0.0023,\n            -0.0045,     -0.0042,      0.0050,      0.0016,     -0.0012,\n            -0.0017,      0.0034,      0.0003,     -0.0100,      0.0003,\n             0.0010,     -0.0126,     -0.0014,      0.0094,      0.0007,\n            -0.0046,     -0.0089,      0.0089,     -0.0052,     -0.0036,\n            -0.0035,     -0.0042,      0.0182,     -0.0023,      0.0016,\n             0.0034,     -0.0095,     -0.0028,      0.0152,      0.0126,\n            -0.0069,      0.0050,     -0.0016,     -0.0093,     -0.0037,\n             0.0023,      0.0030,     -0.0152,      0.0021,     -0.0198,\n             0.0007,      0.0007,      0.0002,      0.0087,     -0.0076,\n            -0.0067,     -0.0035,      0.0109,     -0.0004,     -0.0007,\n            -0.0095,      0.0202,     -0.0011,     -0.0226,     -0.0034,\n             0.0291,      0.0074,     -0.0093,     -0.0109,      0.0049,\n             0.0229,     -0.0221,     -0.0050,     -0.0047,     -0.0055,\n            -0.0552,     -0.0001,      0.0203,     -0.0037,     -0.0174,\n            -0.0037,     -0.0020,     -0.0281,      0.0309,     -0.0022,\n            -0.0028,     -0.0011,      0.0099,     -0.0015,      0.0025,\n            -0.0010,     -0.0025,     -0.0012,      0.0052,      0.0056,\n             0.0129,     -0.0084,      0.0101,      0.0012,     -0.0024,\n             0.0016,     -0.0003,      0.0020,     -0.0024,     -0.0055,\n             0.0058,     -0.0064,     -0.0298,      0.0038,      0.0067,\n             0.0152,     -0.0226,     -0.0015,      0.0223,      0.0113,\n            -0.0207,     -0.0113,      0.0172,      0.0317,     -0.0137,\n            -0.0092,      0.1788,     -0.0028,      0.0058,     -0.0039,\n             0.0072,      0.0008,     -0.0091,     -0.0057,      0.0105,\n             0.0032,      0.0014,     -0.0047,     -0.0141,     -0.0024,\n             0.0126,     -0.0034,      0.0025,      0.0113,     -0.0007,\n            -0.0109,     -0.0068,     -0.0026,     -0.0268,     -0.0130,\n            -0.0063,     -0.0101,     -0.0202,     -0.0045,     -0.0015,\n            -0.0062,      0.0038,     -0.0068,      0.0069,     -0.0048,\n             0.0094,      0.0025,     -0.0065,     -0.0013,     -0.0070,\n            -0.0069,      0.0291,     -0.0010,     -0.0207,     -0.0109,\n             0.0137,      0.0056,     -0.0206,     -0.0127,      0.0108,\n             0.0008,     -0.0359,     -0.0008,     -0.0027,      0.0025,\n            -0.0265,      0.0013,      0.0091,     -0.0003,     -0.0022,\n            -0.0002,     -0.0054,      0.0159,     -0.0010,      0.0039,\n             0.0050,      0.0074,     -0.0025,     -0.0113,     -0.0068,\n             0.0056,     -0.0083,      0.0038,      0.0086,      0.0010,\n             0.0301,      0.0053,      0.0126,     -0.0011,      0.0045,\n            -0.0018,     -0.0024,     -0.0002,     -0.0083,      0.0036,\n             0.0022,      0.0108,      0.0098,      0.0001,      0.0012,\n            -0.0016,     -0.0093,     -0.0012,      0.0172,     -0.0026,\n            -0.0206,      0.0038,      0.0012,      0.0014,      0.0027,\n             0.0055,     -0.0339,      0.0023,     -0.0004,      0.0002,\n            -0.0007,     -0.0000,     -0.0029,      0.0012,     -0.0099,\n             0.0204,      0.0050,     -0.0206,      0.0010,      0.0004,\n            -0.0093,     -0.0109,      0.0052,      0.0317,     -0.0268,\n            -0.0127,      0.0086,      0.0014,      0.0021,      0.0038,\n            -0.0042,      0.0020,      0.0023,      0.0019,      0.0016,\n             0.0007,     -0.0002,     -0.0045,      0.0075,      0.0251,\n             0.0278,     -0.0007,     -0.0138,     -0.0031,     -0.0092,\n            -0.0037,      0.0049,      0.0056,     -0.0137,     -0.0130,\n             0.0108,      0.0010,      0.0027,      0.0038,      0.0046,\n            -0.0027,      0.0147,     -0.0053,      0.0040,      0.0011,\n            -0.0002,     -0.0019,     -0.0129,      0.0044,      0.0053,\n             0.0174,      0.0039,     -0.0023,     -0.0069,     -0.0020,\n             0.0023,      0.0229,      0.0129,     -0.0092,     -0.0063,\n             0.0008,      0.0301,      0.0055,     -0.0042,     -0.0027,\n             0.0068,     -0.0462,      0.0036,      0.0004,      0.0020,\n            -0.0058,     -0.0054,     -0.0034,      0.0008,      0.0005,\n            -0.0006,     -0.0020,     -0.0569,     -0.0007,     -0.0004,\n             0.0030,     -0.0221,     -0.0084,      0.1788,     -0.0101,\n            -0.0359,      0.0053,     -0.0339,      0.0020,      0.0147,\n            -0.0462,      0.0431,     -0.0074,     -0.0612,     -0.0274,\n            -0.0122,     -0.0196,      0.0039,     -0.0125,      0.0162,\n            -0.0334,     -0.0082,     -0.0166,     -0.0062,      0.0078,\n            -0.0152,     -0.0050,      0.0101,     -0.0028,     -0.0202,\n            -0.0008,      0.0126,      0.0023,      0.0023,     -0.0053,\n             0.0036,     -0.0074,     -0.0050,      0.0134,      0.0083,\n             0.0021,      0.0037,     -0.0028,      0.0106,      0.0222,\n             0.0175,      0.0066,      0.0059,      0.0178,     -0.0009,\n             0.0021,     -0.0047,      0.0012,      0.0058,     -0.0045,\n            -0.0027,     -0.0011,     -0.0004,      0.0019,      0.0040,\n             0.0004,     -0.0612,      0.0134,     -0.0147,      0.0075,\n             0.0065,      0.0108,     -0.0197,      0.0027,      0.0058,\n             0.0010,      0.0043,      0.0031,      0.0065,      0.0087,\n            -0.0198,     -0.0055,     -0.0024,     -0.0039,     -0.0015,\n             0.0025,      0.0045,      0.0002,      0.0016,      0.0011,\n             0.0020,     -0.0274,      0.0083,      0.0075,     -0.0059,\n             0.0066,      0.0068,     -0.0169,      0.0003,      0.0010,\n             0.0016,      0.0044,      0.0033,      0.0065,      0.0014,\n             0.0007,     -0.0552,      0.0016,      0.0072,     -0.0062,\n            -0.0265,     -0.0018,     -0.0007,      0.0007,     -0.0002,\n            -0.0058,     -0.0122,      0.0021,      0.0065,      0.0066,\n            -0.0111,      0.0105,     -0.0544,      0.0014,      0.0036,\n            -0.0008,     -0.0010,      0.0029,      0.0061,      0.0023,\n             0.0007,     -0.0001,     -0.0003,      0.0008,      0.0038,\n             0.0013,     -0.0024,     -0.0000,     -0.0002,     -0.0019,\n            -0.0054,     -0.0196,      0.0037,      0.0108,      0.0068,\n             0.0105,     -0.0085,     -0.0419,      0.0013,      0.0022,\n             0.0013,      0.0045,     -0.0058,      0.0109,      0.0089,\n             0.0002,      0.0203,      0.0020,     -0.0091,     -0.0068,\n             0.0091,     -0.0002,     -0.0029,     -0.0045,     -0.0129,\n            -0.0034,      0.0039,     -0.0028,     -0.0197,     -0.0169,\n            -0.0544,     -0.0419,      0.0130,     -0.0058,     -0.0162,\n            -0.0123,     -0.0059,     -0.0057,     -0.0148,     -0.0085,\n             0.0087,     -0.0037,     -0.0024,     -0.0057,      0.0069,\n            -0.0003,     -0.0083,      0.0012,      0.0075,      0.0044,\n             0.0008,     -0.0125,      0.0106,      0.0027,      0.0003,\n             0.0014,      0.0013,     -0.0058,     -0.0020,      0.0030,\n            -0.0076,     -0.0045,     -0.0021,      0.0007,     -0.0116,\n            -0.0076,     -0.0174,     -0.0055,      0.0105,     -0.0048,\n            -0.0022,      0.0036,     -0.0099,      0.0251,      0.0053,\n             0.0005,      0.0162,      0.0222,      0.0058,      0.0010,\n             0.0036,      0.0022,     -0.0162,      0.0030,     -0.0024,\n            -0.0086,     -0.0056,     -0.0034,      0.0003,      0.0041,\n            -0.0067,     -0.0037,      0.0058,      0.0032,      0.0094,\n            -0.0002,      0.0022,      0.0204,      0.0278,      0.0174,\n            -0.0006,     -0.0334,      0.0175,      0.0010,      0.0016,\n            -0.0008,      0.0013,     -0.0123,     -0.0076,     -0.0086,\n             0.0073,     -0.0078,     -0.0280,     -0.0077,      0.0017,\n            -0.0035,     -0.0020,     -0.0064,      0.0014,      0.0025,\n            -0.0054,      0.0108,      0.0050,     -0.0007,      0.0039,\n            -0.0020,     -0.0082,      0.0066,      0.0043,      0.0044,\n            -0.0010,      0.0045,     -0.0059,     -0.0045,     -0.0056,\n            -0.0078,      0.0058,     -0.0301,     -0.0064,     -0.0018,\n             0.0109,     -0.0281,     -0.0298,     -0.0047,     -0.0065,\n             0.0159,      0.0098,     -0.0206,     -0.0138,     -0.0023,\n            -0.0569,     -0.0166,      0.0059,      0.0031,      0.0033,\n             0.0029,     -0.0058,     -0.0057,     -0.0021,     -0.0034,\n            -0.0280,     -0.0301,      0.0124,     -0.0526,      0.0069,\n            -0.0004,      0.0309,      0.0038,     -0.0141,     -0.0013,\n            -0.0010,      0.0001,      0.0010,     -0.0031,     -0.0068,\n            -0.0007,     -0.0062,      0.0178,      0.0065,      0.0065,\n             0.0061,      0.0109,     -0.0148,      0.0007,      0.0003,\n            -0.0077,     -0.0064,     -0.0526,      0.0082,      0.0019,\n            -0.0007,     -0.0022,      0.0067,     -0.0024,     -0.0070,\n             0.0039,      0.0012,      0.0004,     -0.0092,     -0.0020,\n            -0.0004,      0.0078,     -0.0009,      0.0087,      0.0014,\n             0.0023,      0.0089,     -0.0085,     -0.0116,      0.0041,\n             0.0017,     -0.0018,      0.0069,      0.0019,      0.0061,\n            -0.0037,      0.0155,     -0.0169,      0.0120,     -0.0116,\n            -0.0059,      0.0074,     -0.0021,      0.0006,      0.0076,\n            -0.0010,     -0.0085,      0.0002,      0.0011,     -0.0140,\n             0.0012,      0.0016,      0.0087,      0.0052,      0.0011,\n             0.0014,      0.0039,      0.0045,     -0.0472,     -0.0003,\n             0.0155,      0.0316,      0.0159,     -0.0340,      0.0090,\n             0.0355,     -0.0208,     -0.0070,      0.0025,      0.0006,\n             0.0003,     -0.0172,      0.0095,      0.0043,      0.0043,\n             0.0143,     -0.0022,     -0.0037,      0.0175,     -0.0330,\n            -0.0002,     -0.0005,     -0.0027,     -0.0001,     -0.0074,\n            -0.0169,      0.0159,      0.0033,     -0.0235,     -0.0053,\n             0.0188,      0.0174,      0.0026,      0.0193,     -0.0022,\n             0.0108,      0.0059,      0.0026,     -0.0013,     -0.0064,\n            -0.0037,     -0.0027,     -0.0024,      0.0021,      0.0001,\n            -0.0076,     -0.0034,     -0.0080,      0.0019,      0.0021,\n             0.0120,     -0.0340,     -0.0235,      0.0414,      0.0038,\n            -0.0394,     -0.0067,      0.0043,     -0.0035,     -0.0107,\n             0.0034,      0.0404,      0.0055,     -0.0101,     -0.0051,\n            -0.0136,     -0.0058,     -0.0057,     -0.0135,      0.0173,\n             0.0003,      0.0012,     -0.0009,     -0.0003,      0.0314,\n            -0.0116,      0.0090,     -0.0053,      0.0038,      0.0041,\n            -0.0143,      0.0004,      0.0023,     -0.0018,      0.0024,\n             0.0972,     -0.0658,     -0.0042,      0.0077,      0.0012,\n            -0.0014,      0.0035,      0.0069,     -0.0339,     -0.0123,\n             0.0008,     -0.0003,      0.0041,      0.0085,      0.0257,\n            -0.0059,      0.0355,      0.0188,     -0.0394,     -0.0143,\n             0.0388,      0.0037,      0.0018,      0.0024,      0.0084,\n            -0.0033,      0.0001,     -0.0016,      0.0077,     -0.0007,\n             0.0115,      0.0009,     -0.0003,      0.0167,     -0.0120,\n             0.0005,     -0.0008,     -0.0023,     -0.0039,     -0.0245,\n             0.0074,     -0.0208,      0.0174,     -0.0067,      0.0004,\n             0.0037,     -0.0060,     -0.0055,     -0.0019,     -0.0021,\n             0.0009,      0.0070,     -0.0012,     -0.0022,      0.0046,\n            -0.0034,     -0.0043,     -0.0083,     -0.0037,      0.0046,\n             0.0005,     -0.0009,      0.0007,      0.0138,      0.0095,\n            -0.0021,     -0.0070,      0.0026,      0.0043,      0.0023,\n             0.0018,     -0.0055,      0.0033,      0.0013,      0.0014,\n             0.0033,      0.0038,     -0.0008,      0.0031,     -0.0010,\n            -0.0007,      0.0010,      0.0012,     -0.0104,      0.0035,\n             0.0035,      0.0034,      0.0000,      0.0008,     -0.0014,\n             0.0006,      0.0025,      0.0193,     -0.0035,     -0.0018,\n             0.0024,     -0.0019,      0.0013,      0.0008,     -0.0001,\n            -0.0017,     -0.0046,      0.0005,      0.0077,     -0.0001,\n             0.0049,      0.0042,      0.0075,     -0.0105,      0.0060,\n             0.0008,      0.0008,     -0.0010,     -0.0010,      0.0011,\n             0.0076,      0.0006,     -0.0022,     -0.0107,      0.0024,\n             0.0084,     -0.0021,      0.0014,     -0.0001,      0.0000,\n            -0.0002,      0.0141,      0.0013,      0.0027,      0.0029,\n             0.0005,      0.0027,      0.0044,     -0.0131,      0.0053,\n            -0.0023,     -0.0010,     -0.0014,      0.0002,     -0.0012,\n            -0.0010,      0.0003,      0.0108,      0.0034,      0.0972,\n            -0.0033,      0.0009,      0.0033,     -0.0017,     -0.0002,\n            -0.0026,     -0.0034,     -0.0040,     -0.0053,     -0.0040,\n             0.0003,     -0.0116,      0.0008,     -0.0050,     -0.0040,\n            -0.0043,     -0.0016,     -0.0016,     -0.0035,     -0.1154,\n            -0.0085,     -0.0172,      0.0059,      0.0404,     -0.0658,\n             0.0001,      0.0070,      0.0038,     -0.0046,      0.0141,\n            -0.0034,     -0.0022,     -0.0043,     -0.0116,     -0.0133,\n            -0.0009,      0.0185,     -0.0125,     -0.0005,     -0.0095,\n            -0.0143,      0.0020,     -0.0019,     -0.0015,     -0.0022,\n             0.0002,      0.0095,      0.0026,      0.0055,     -0.0042,\n            -0.0016,     -0.0012,     -0.0008,      0.0005,      0.0013,\n            -0.0040,     -0.0043,     -0.0151,      0.0066,      0.0051,\n             0.0080,      0.0051,      0.0106,      0.0006,     -0.0091,\n            -0.0030,      0.0040,      0.0014,     -0.0018,      0.0025,\n             0.0011,      0.0043,     -0.0013,     -0.0101,      0.0077,\n             0.0077,     -0.0022,      0.0031,      0.0077,      0.0027,\n            -0.0053,     -0.0116,      0.0066,      0.0007,      0.0012,\n             0.0026,      0.0055,      0.0027,     -0.0107,      0.0015,\n            -0.0108,     -0.0067,     -0.0002,      0.0007,     -0.0070,\n            -0.0140,      0.0043,     -0.0064,     -0.0051,      0.0012,\n            -0.0007,      0.0046,     -0.0010,     -0.0001,      0.0029,\n            -0.0040,     -0.0133,      0.0051,      0.0012,      0.0008,\n            -0.0039,     -0.0004,      0.0003,     -0.0109,      0.0066,\n            -0.0077,     -0.0042,     -0.0011,     -0.0016,      0.0021,\n             0.0012,      0.0143,     -0.0037,     -0.0136,     -0.0014,\n             0.0115,     -0.0034,     -0.0007,      0.0049,      0.0005,\n             0.0003,     -0.0009,      0.0080,      0.0026,     -0.0039,\n             0.0006,      0.0015,      0.0014,     -0.0033,      0.0013,\n            -0.0019,     -0.0034,     -0.0011,     -0.0027,     -0.0066,\n             0.0016,     -0.0022,     -0.0027,     -0.0058,      0.0035,\n             0.0009,     -0.0043,      0.0010,      0.0042,      0.0027,\n            -0.0116,      0.0185,      0.0051,      0.0055,     -0.0004,\n             0.0015,      0.0049,      0.0013,     -0.0103,      0.0003,\n            -0.0102,     -0.0073,     -0.0079,     -0.0076,     -0.0052,\n             0.0087,     -0.0037,     -0.0024,     -0.0057,      0.0069,\n            -0.0003,     -0.0083,      0.0012,      0.0075,      0.0044,\n             0.0008,     -0.0125,      0.0106,      0.0027,      0.0003,\n             0.0014,      0.0013,     -0.0058,     -0.0020,      0.0030,\n            -0.0076,     -0.0045,     -0.0021,      0.0007,     -0.0116,\n             0.0052,      0.0175,      0.0021,     -0.0135,     -0.0339,\n             0.0167,     -0.0037,     -0.0104,     -0.0105,     -0.0131,\n            -0.0050,     -0.0005,      0.0006,     -0.0107,     -0.0109,\n            -0.0033,     -0.0103,     -0.0020,      0.0122,     -0.0267,\n            -0.0373,     -0.0342,     -0.0286,     -0.0190,     -0.0041,\n             0.0011,     -0.0330,      0.0001,      0.0173,     -0.0123,\n            -0.0120,      0.0046,      0.0035,      0.0060,      0.0053,\n            -0.0040,     -0.0095,     -0.0091,      0.0015,      0.0066,\n             0.0013,      0.0003,      0.0030,     -0.0267,     -0.0095,\n             0.0015,      0.0033,      0.0006,     -0.0002,     -0.0025,\n             0.0014,     -0.0002,     -0.0076,      0.0003,      0.0008,\n             0.0005,      0.0005,      0.0035,      0.0008,     -0.0023,\n            -0.0043,     -0.0143,     -0.0030,     -0.0108,     -0.0077,\n            -0.0019,     -0.0102,     -0.0076,     -0.0373,      0.0015,\n             0.0004,     -0.0008,     -0.0012,     -0.0029,     -0.0044,\n             0.0039,     -0.0005,     -0.0034,      0.0012,     -0.0003,\n            -0.0008,     -0.0009,      0.0034,      0.0008,     -0.0010,\n            -0.0016,      0.0020,      0.0040,     -0.0067,     -0.0042,\n            -0.0034,     -0.0073,     -0.0045,     -0.0342,      0.0033,\n            -0.0008,     -0.0002,      0.0002,     -0.0001,      0.0019,\n             0.0045,     -0.0027,     -0.0080,     -0.0009,      0.0041,\n            -0.0023,      0.0007,      0.0000,     -0.0010,     -0.0014,\n            -0.0016,     -0.0019,      0.0014,     -0.0002,     -0.0011,\n            -0.0011,     -0.0079,     -0.0021,     -0.0286,      0.0006,\n            -0.0012,      0.0002,     -0.0008,     -0.0003,      0.0027,\n            -0.0472,     -0.0001,      0.0019,     -0.0003,      0.0085,\n            -0.0039,      0.0138,      0.0008,     -0.0010,      0.0002,\n            -0.0035,     -0.0015,     -0.0018,      0.0007,     -0.0016,\n            -0.0027,     -0.0076,      0.0007,     -0.0190,     -0.0002,\n            -0.0029,     -0.0001,     -0.0003,     -0.0018,     -0.0010,\n            -0.0003,     -0.0074,      0.0021,      0.0314,      0.0257,\n            -0.0245,      0.0095,     -0.0014,      0.0011,     -0.0012,\n            -0.1154,     -0.0022,      0.0025,     -0.0070,      0.0021,\n            -0.0066,     -0.0052,     -0.0116,     -0.0041,     -0.0025,\n            -0.0044,      0.0019,      0.0027,     -0.0010,     -0.0039,\n             0.0272,      0.0107,     -0.0210,      0.0114,     -0.0107,\n             0.0036,     -0.0165,      0.0007,     -0.0135,      0.0027,\n             0.0118,      0.0037,     -0.0012,     -0.0035,     -0.0106,\n             0.0010,      0.0074,     -0.0076,      0.0011,     -0.0009,\n             0.0036,      0.0022,      0.0003,      0.0001,      0.0016,\n             0.0107,      0.0187,      0.0133,     -0.0012,      0.0088,\n             0.0091,     -0.0053,     -0.0041,      0.0040,     -0.0057,\n            -0.0042,      0.0120,      0.0134,      0.0023,     -0.0008,\n            -0.0001,     -0.0114,     -0.0174,     -0.0330,      0.0117,\n            -0.0008,     -0.0010,     -0.0307,     -0.0055,      0.0052,\n            -0.0210,      0.0133,      0.0080,     -0.0145,     -0.0087,\n             0.0138,      0.0190,      0.0028,     -0.0003,     -0.0008,\n             0.0396,     -0.0039,      0.0027,     -0.0052,     -0.0073,\n            -0.0067,     -0.0042,     -0.0055,      0.0001,      0.0008,\n            -0.0093,     -0.0108,     -0.0068,      0.0013,     -0.0012,\n             0.0114,     -0.0012,     -0.0145,      0.0074,     -0.0073,\n            -0.0077,     -0.0218,     -0.0049,      0.0162,      0.0010,\n             0.0094,     -0.0198,      0.0112,     -0.0072,      0.0040,\n            -0.0015,     -0.0024,      0.0105,      0.0173,     -0.0213,\n             0.0013,      0.0086,      0.0028,      0.0427,     -0.0087,\n            -0.0107,      0.0088,     -0.0087,     -0.0073,     -0.0361,\n             0.0010,     -0.0032,     -0.0019,      0.0406,     -0.0132,\n            -0.0011,      0.0231,     -0.0161,     -0.0012,      0.0039,\n            -0.0054,      0.0009,     -0.0044,     -0.0123,      0.0221,\n            -0.0057,     -0.0061,     -0.0090,     -0.0039,      0.0028,\n             0.0036,      0.0091,      0.0138,     -0.0077,      0.0010,\n             0.0114,      0.0118,      0.0126,     -0.0105,     -0.0003,\n            -0.0056,     -0.0153,     -0.0040,      0.0060,     -0.0100,\n             0.0024,     -0.0026,     -0.0022,     -0.0120,      0.0069,\n            -0.0014,     -0.0028,     -0.0038,     -0.0056,      0.0071,\n            -0.0165,     -0.0053,      0.0190,     -0.0218,     -0.0032,\n             0.0118,      0.0112,     -0.0072,     -0.0069,      0.0059,\n             0.0070,     -0.0041,     -0.0012,      0.0024,     -0.0004,\n            -0.0066,     -0.0102,      0.0036,      0.0046,     -0.0002,\n            -0.0001,      0.0001,      0.0015,      0.0067,     -0.0010,\n             0.0007,     -0.0041,      0.0028,     -0.0049,     -0.0019,\n             0.0126,     -0.0072,     -0.0116,     -0.0024,     -0.0012,\n             0.0006,     -0.0089,     -0.0075,      0.0007,     -0.0035,\n            -0.0029,     -0.0005,     -0.0099,      0.0035,     -0.0027,\n            -0.0001,     -0.0006,     -0.0013,     -0.0000,      0.0019,\n            -0.0135,      0.0040,     -0.0003,      0.0162,      0.0406,\n            -0.0105,     -0.0069,     -0.0024,     -0.0020,     -0.0000,\n             0.0015,      0.0001,      0.0013,      0.0101,      0.0038,\n             0.0075,      0.0117,      0.0251,      0.0060,     -0.0105,\n             0.0035,      0.0013,      0.0015,      0.0015,     -0.0043,\n             0.0027,     -0.0057,     -0.0008,      0.0010,     -0.0132,\n            -0.0003,      0.0059,     -0.0012,     -0.0000,      0.0005,\n            -0.0033,      0.0026,      0.0005,      0.0056,      0.0062,\n             0.0051,      0.0026,      0.0053,      0.0053,     -0.0060,\n            -0.0016,     -0.0023,     -0.0027,     -0.0023,      0.0003,\n             0.0118,     -0.0042,      0.0396,      0.0094,     -0.0011,\n            -0.0056,      0.0070,      0.0006,      0.0015,     -0.0033,\n            -0.0030,     -0.0031,     -0.0082,     -0.0326,     -0.0046,\n            -0.0063,      0.0047,      0.0005,     -0.0040,     -0.0053,\n            -0.0033,     -0.0031,     -0.0043,     -0.0147,     -0.0027,\n             0.0037,      0.0120,     -0.0039,     -0.0198,      0.0231,\n            -0.0153,     -0.0041,     -0.0089,      0.0001,      0.0026,\n            -0.0031,     -0.0112,     -0.0114,      0.0108,      0.0110,\n             0.0081,      0.0159,      0.0162,     -0.0095,     -0.0015,\n            -0.0143,     -0.0005,     -0.0066,     -0.0056,     -0.0007,\n            -0.0012,      0.0134,      0.0027,      0.0112,     -0.0161,\n            -0.0040,     -0.0012,     -0.0075,      0.0013,      0.0005,\n            -0.0082,     -0.0114,     -0.0258,      0.0041,      0.0088,\n             0.0077,      0.0041,      0.0222,     -0.0091,      0.0040,\n            -0.0045,     -0.0019,     -0.0012,     -0.0056,     -0.0013,\n            -0.0035,      0.0023,     -0.0052,     -0.0072,     -0.0012,\n             0.0060,      0.0024,      0.0007,      0.0101,      0.0056,\n            -0.0326,      0.0108,      0.0041,     -0.0060,     -0.0003,\n             0.0023,      0.0009,      0.0058,      0.0015,     -0.0049,\n            -0.0142,     -0.0156,     -0.0031,     -0.0033,      0.0008,\n            -0.0106,     -0.0008,     -0.0073,      0.0040,      0.0039,\n            -0.0100,     -0.0004,     -0.0035,      0.0038,      0.0062,\n            -0.0046,      0.0110,      0.0088,     -0.0003,     -0.0011,\n            -0.0025,     -0.0022,      0.0010,      0.0066,     -0.0091,\n            -0.0095,     -0.0075,     -0.0019,     -0.0049,     -0.0006,\n             0.0010,     -0.0001,     -0.0067,     -0.0015,     -0.0054,\n             0.0024,     -0.0066,     -0.0029,      0.0075,      0.0051,\n            -0.0063,      0.0081,      0.0077,      0.0023,     -0.0025,\n            -0.0039,     -0.0007,      0.0036,      0.0013,     -0.0021,\n            -0.0043,     -0.0030,     -0.0018,     -0.0023,     -0.0008,\n             0.0074,     -0.0114,     -0.0042,     -0.0024,      0.0009,\n            -0.0026,     -0.0102,     -0.0005,      0.0117,      0.0026,\n             0.0047,      0.0159,      0.0041,      0.0009,     -0.0022,\n            -0.0007,     -0.0028,      0.0022,      0.0003,     -0.0070,\n            -0.0135,     -0.0137,     -0.0104,     -0.0037,     -0.0036,\n            -0.0076,     -0.0174,     -0.0055,      0.0105,     -0.0048,\n            -0.0022,      0.0036,     -0.0099,      0.0251,      0.0053,\n             0.0005,      0.0162,      0.0222,      0.0058,      0.0010,\n             0.0036,      0.0022,     -0.0162,      0.0030,     -0.0024,\n            -0.0086,     -0.0056,     -0.0034,      0.0003,      0.0041,\n             0.0011,     -0.0330,      0.0001,      0.0173,     -0.0123,\n            -0.0120,      0.0046,      0.0035,      0.0060,      0.0053,\n            -0.0040,     -0.0095,     -0.0091,      0.0015,      0.0066,\n             0.0013,      0.0003,      0.0030,     -0.0267,     -0.0095,\n             0.0015,      0.0033,      0.0006,     -0.0002,     -0.0025,\n            -0.0009,      0.0117,      0.0008,     -0.0213,      0.0221,\n             0.0069,     -0.0002,     -0.0027,     -0.0105,     -0.0060,\n            -0.0053,     -0.0015,      0.0040,     -0.0049,     -0.0091,\n            -0.0021,     -0.0070,     -0.0024,     -0.0095,      0.0091,\n            -0.0127,     -0.0128,     -0.0104,     -0.0094,      0.0015,\n             0.0036,     -0.0008,     -0.0093,      0.0013,     -0.0057,\n            -0.0014,     -0.0001,     -0.0001,      0.0035,     -0.0016,\n            -0.0033,     -0.0143,     -0.0045,     -0.0142,     -0.0095,\n            -0.0043,     -0.0135,     -0.0086,      0.0015,     -0.0127,\n            -0.0004,     -0.0019,     -0.0020,     -0.0030,     -0.0078,\n             0.0022,     -0.0010,     -0.0108,      0.0086,     -0.0061,\n            -0.0028,      0.0001,     -0.0006,      0.0013,     -0.0023,\n            -0.0031,     -0.0005,     -0.0019,     -0.0156,     -0.0075,\n            -0.0030,     -0.0137,     -0.0056,      0.0033,     -0.0128,\n            -0.0019,     -0.0002,     -0.0012,     -0.0025,     -0.0058,\n             0.0003,     -0.0307,     -0.0068,      0.0028,     -0.0090,\n            -0.0038,      0.0015,     -0.0013,      0.0015,     -0.0027,\n            -0.0043,     -0.0066,     -0.0012,     -0.0031,     -0.0019,\n            -0.0018,     -0.0104,     -0.0034,      0.0006,     -0.0104,\n            -0.0020,     -0.0012,     -0.0009,     -0.0038,     -0.0030,\n             0.0001,     -0.0055,      0.0013,      0.0427,     -0.0039,\n            -0.0056,      0.0067,     -0.0000,      0.0015,     -0.0023,\n            -0.0147,     -0.0056,     -0.0056,     -0.0033,     -0.0049,\n            -0.0023,     -0.0037,      0.0003,     -0.0002,     -0.0094,\n            -0.0030,     -0.0025,     -0.0038,     -0.0043,     -0.0016,\n             0.0016,      0.0052,     -0.0012,     -0.0087,      0.0028,\n             0.0071,     -0.0010,      0.0019,     -0.0043,      0.0003,\n            -0.0027,     -0.0007,     -0.0013,      0.0008,     -0.0006,\n            -0.0008,     -0.0036,      0.0041,     -0.0025,      0.0015,\n            -0.0078,     -0.0058,     -0.0030,     -0.0016,      0.0026,\n             0.0018,      0.0132,     -0.0258,      0.0044,     -0.0008,\n             0.0043,      0.0001,     -0.0027,     -0.0006,      0.0056,\n             0.0026,     -0.0064,     -0.0048,     -0.0049,     -0.0113,\n            -0.0043,      0.0078,     -0.0067,      0.0014,      0.0036,\n            -0.0010,     -0.0004,      0.0027,      0.0032,      0.0014,\n             0.0132,     -0.0069,      0.0140,      0.0168,      0.0140,\n            -0.0158,     -0.0091,     -0.0037,      0.0005,      0.0012,\n             0.0255,     -0.0028,      0.0059,      0.0111,      0.0035,\n            -0.0050,     -0.0039,     -0.0037,     -0.0002,     -0.0008,\n            -0.0001,      0.0056,      0.0039,      0.0143,      0.0079,\n            -0.0258,      0.0140,     -0.0069,     -0.0171,     -0.0096,\n             0.0150,      0.0236,      0.0053,     -0.0084,      0.0007,\n            -0.0039,      0.0031,      0.0269,      0.0018,     -0.0030,\n            -0.0052,      0.0027,      0.0058,     -0.0076,     -0.0093,\n             0.0078,     -0.0097,     -0.0585,     -0.0046,     -0.0026,\n             0.0044,      0.0168,     -0.0171,     -0.0210,     -0.0042,\n             0.0194,     -0.0141,     -0.0020,     -0.0006,     -0.0049,\n             0.0010,      0.0052,      0.0098,     -0.0122,      0.0026,\n             0.0063,     -0.0059,      0.0032,      0.0003,      0.0013,\n            -0.0013,      0.0009,     -0.0015,     -0.0006,     -0.0059,\n            -0.0008,      0.0140,     -0.0096,     -0.0042,      0.0122,\n             0.0039,     -0.0092,     -0.0036,     -0.0114,      0.0027,\n             0.0035,     -0.0012,     -0.0127,      0.0073,      0.0114,\n            -0.0014,      0.0096,      0.0094,      0.0008,     -0.0057,\n            -0.0001,      0.0001,      0.0059,      0.0065,     -0.0025,\n             0.0043,     -0.0158,      0.0150,      0.0194,      0.0039,\n            -0.0147,      0.0103,      0.0071,     -0.0148,      0.0046,\n             0.0025,     -0.0137,      0.0008,      0.0086,     -0.0129,\n            -0.0064,      0.0006,     -0.0002,      0.0005,     -0.0014,\n            -0.0001,     -0.0006,      0.0053,      0.0068,      0.0047,\n             0.0001,     -0.0091,      0.0236,     -0.0141,     -0.0092,\n             0.0103,     -0.0030,     -0.0048,      0.0061,      0.0023,\n             0.0002,     -0.0020,      0.0016,      0.0031,      0.0018,\n             0.0004,     -0.0102,      0.0022,      0.0005,     -0.0001,\n            -0.0007,     -0.0040,     -0.0035,     -0.0034,      0.0006,\n            -0.0027,     -0.0037,      0.0053,     -0.0020,     -0.0036,\n             0.0071,     -0.0048,     -0.0260,     -0.0024,     -0.0045,\n             0.0011,     -0.0039,     -0.0038,      0.0299,      0.0175,\n             0.0197,      0.0250,      0.0204,      0.0035,     -0.0001,\n            -0.0013,     -0.0001,     -0.0001,      0.0007,     -0.0004,\n            -0.0006,      0.0005,     -0.0084,     -0.0006,     -0.0114,\n            -0.0148,      0.0061,     -0.0024,     -0.0014,     -0.0007,\n            -0.0014,     -0.0008,     -0.0428,      0.0363,      0.0036,\n            -0.0042,      0.0220,      0.0278,      0.0008,      0.0035,\n            -0.0101,     -0.0032,     -0.0021,     -0.0090,      0.0188,\n             0.0056,      0.0012,      0.0007,     -0.0049,      0.0027,\n             0.0046,      0.0023,     -0.0045,     -0.0007,     -0.0047,\n            -0.0078,      0.0253,      0.0004,      0.0171,      0.0140,\n             0.0134,      0.0168,      0.0174,     -0.0023,     -0.0016,\n             0.0014,     -0.0072,     -0.0054,     -0.0052,     -0.0003,\n             0.0026,      0.0255,     -0.0039,      0.0010,      0.0035,\n             0.0025,      0.0002,      0.0011,     -0.0014,     -0.0078,\n            -0.0102,     -0.0065,      0.0122,     -0.0109,     -0.0035,\n            -0.0042,     -0.0645,     -0.0006,     -0.0043,     -0.0033,\n             0.0015,     -0.0069,     -0.0040,     -0.0081,     -0.0020,\n            -0.0064,     -0.0028,      0.0031,      0.0052,     -0.0012,\n            -0.0137,     -0.0020,     -0.0039,     -0.0008,      0.0253,\n            -0.0065,     -0.0044,      0.0014,     -0.0538,     -0.0332,\n            -0.0417,      0.0087,     -0.0334,     -0.0143,     -0.0143,\n             0.0076,     -0.0039,     -0.0099,     -0.0159,     -0.0014,\n            -0.0048,      0.0059,      0.0269,      0.0098,     -0.0127,\n             0.0008,      0.0016,     -0.0038,     -0.0428,      0.0004,\n             0.0122,      0.0014,     -0.0226,     -0.0005,      0.0044,\n             0.0117,     -0.0003,      0.0175,     -0.0030,     -0.0045,\n             0.0001,      0.0025,      0.0086,      0.0004,      0.0020,\n            -0.0049,      0.0111,      0.0018,     -0.0122,      0.0073,\n             0.0086,      0.0031,      0.0299,      0.0363,      0.0171,\n            -0.0109,     -0.0538,     -0.0005,     -0.0116,     -0.0033,\n            -0.0042,     -0.0023,      0.0010,     -0.0108,     -0.0142,\n             0.0072,     -0.0204,     -0.0079,     -0.0252,     -0.0009,\n            -0.0113,      0.0035,     -0.0030,      0.0026,      0.0114,\n            -0.0129,      0.0018,      0.0175,      0.0036,      0.0140,\n            -0.0035,     -0.0332,      0.0044,     -0.0033,     -0.0053,\n             0.0001,     -0.0069,      0.0016,     -0.0077,     -0.0095,\n             0.0086,     -0.0142,     -0.0076,     -0.0148,     -0.0035,\n            -0.0043,     -0.0050,     -0.0052,      0.0063,     -0.0014,\n            -0.0064,      0.0004,      0.0197,     -0.0042,      0.0134,\n            -0.0042,     -0.0417,      0.0117,     -0.0042,      0.0001,\n            -0.0116,     -0.0008,     -0.0008,     -0.0019,     -0.0043,\n             0.0011,     -0.0052,     -0.0015,     -0.0091,      0.0020,\n             0.0078,     -0.0039,      0.0027,     -0.0059,      0.0096,\n             0.0006,     -0.0102,      0.0250,      0.0220,      0.0168,\n            -0.0645,      0.0087,     -0.0003,     -0.0023,     -0.0069,\n            -0.0008,     -0.0078,      0.0013,     -0.0102,     -0.0135,\n             0.0092,     -0.0303,     -0.0688,     -0.0208,     -0.0035,\n            -0.0067,     -0.0037,      0.0058,      0.0032,      0.0094,\n            -0.0002,      0.0022,      0.0204,      0.0278,      0.0174,\n            -0.0006,     -0.0334,      0.0175,      0.0010,      0.0016,\n            -0.0008,      0.0013,     -0.0123,     -0.0076,     -0.0086,\n             0.0073,     -0.0078,     -0.0280,     -0.0077,      0.0017,\n             0.0014,     -0.0002,     -0.0076,      0.0003,      0.0008,\n             0.0005,      0.0005,      0.0035,      0.0008,     -0.0023,\n            -0.0043,     -0.0143,     -0.0030,     -0.0108,     -0.0077,\n            -0.0019,     -0.0102,     -0.0076,     -0.0373,      0.0015,\n             0.0004,     -0.0008,     -0.0012,     -0.0029,     -0.0044,\n             0.0036,     -0.0008,     -0.0093,      0.0013,     -0.0057,\n            -0.0014,     -0.0001,     -0.0001,      0.0035,     -0.0016,\n            -0.0033,     -0.0143,     -0.0045,     -0.0142,     -0.0095,\n            -0.0043,     -0.0135,     -0.0086,      0.0015,     -0.0127,\n            -0.0004,     -0.0019,     -0.0020,     -0.0030,     -0.0078,\n            -0.0010,     -0.0001,      0.0078,     -0.0013,     -0.0001,\n            -0.0001,     -0.0007,     -0.0013,     -0.0101,      0.0014,\n             0.0015,      0.0076,      0.0001,      0.0072,      0.0086,\n             0.0011,      0.0092,      0.0073,      0.0004,     -0.0004,\n            -0.0006,     -0.0007,     -0.0006,      0.0002,      0.0028,\n            -0.0004,      0.0056,     -0.0097,      0.0009,      0.0001,\n            -0.0006,     -0.0040,     -0.0001,     -0.0032,     -0.0072,\n            -0.0069,     -0.0039,      0.0025,     -0.0204,     -0.0142,\n            -0.0052,     -0.0303,     -0.0078,     -0.0008,     -0.0019,\n            -0.0007,     -0.0002,     -0.0079,     -0.0082,     -0.0029,\n             0.0027,      0.0039,     -0.0585,     -0.0015,      0.0059,\n             0.0053,     -0.0035,     -0.0001,     -0.0021,     -0.0054,\n            -0.0040,     -0.0099,      0.0086,     -0.0079,     -0.0076,\n            -0.0015,     -0.0688,     -0.0280,     -0.0012,     -0.0020,\n            -0.0006,     -0.0079,     -0.0010,     -0.0087,     -0.0024,\n             0.0032,      0.0143,     -0.0046,     -0.0006,      0.0065,\n             0.0068,     -0.0034,      0.0007,     -0.0090,     -0.0052,\n            -0.0081,     -0.0159,      0.0004,     -0.0252,     -0.0148,\n            -0.0091,     -0.0208,     -0.0077,     -0.0029,     -0.0030,\n             0.0002,     -0.0082,     -0.0087,     -0.0039,     -0.0041,\n             0.0014,      0.0079,     -0.0026,     -0.0059,     -0.0025,\n             0.0047,      0.0006,     -0.0004,      0.0188,     -0.0003,\n            -0.0020,     -0.0014,      0.0020,     -0.0009,     -0.0035,\n             0.0020,     -0.0035,      0.0017,     -0.0044,     -0.0078,\n             0.0028,     -0.0029,     -0.0024,     -0.0041,      0.0007,\n            -0.0096,      0.0011,     -0.0094,      0.0106,      0.0007,\n            -0.0036,      0.0175,      0.0025,     -0.0006,      0.0027,\n             0.0071,     -0.0269,      0.0069,     -0.0039,     -0.0078,\n             0.0049,     -0.0003,     -0.0035,      0.0039,      0.0022,\n            -0.0004,     -0.0022,      0.0031,      0.0022,     -0.0014,\n             0.0011,     -0.0053,     -0.0096,      0.0088,      0.0112,\n            -0.0036,     -0.0044,      0.0039,     -0.0008,      0.0003,\n             0.0155,      0.0011,      0.0182,      0.0035,      0.0055,\n            -0.0014,     -0.0053,     -0.0020,     -0.0005,     -0.0010,\n             0.0056,      0.0004,      0.0020,      0.0082,      0.0027,\n            -0.0094,     -0.0096,     -0.0072,      0.0052,     -0.0172,\n            -0.0057,      0.0123,      0.0012,     -0.0059,      0.0002,\n            -0.0024,      0.0019,      0.0119,     -0.0083,     -0.0089,\n            -0.0146,     -0.0051,     -0.0064,     -0.0034,     -0.0108,\n            -0.0097,      0.0072,     -0.0557,     -0.0139,     -0.0016,\n             0.0106,      0.0088,      0.0052,     -0.0104,      0.0012,\n             0.0007,     -0.0082,     -0.0100,      0.0079,      0.0046,\n            -0.0016,     -0.0152,     -0.0078,     -0.0053,      0.0009,\n            -0.0009,      0.0016,      0.0014,      0.0012,      0.0086,\n             0.0009,     -0.0021,     -0.0003,      0.0008,     -0.0012,\n             0.0007,      0.0112,     -0.0172,      0.0012,      0.0045,\n            -0.0015,     -0.0112,     -0.0113,     -0.0058,     -0.0002,\n             0.0009,      0.0184,      0.0004,      0.0007,      0.0034,\n            -0.0193,     -0.0037,      0.0025,     -0.0003,     -0.0061,\n             0.0001,      0.0004,     -0.0003,     -0.0002,     -0.0031,\n            -0.0037,     -0.0036,     -0.0057,      0.0007,     -0.0015,\n             0.0082,      0.0042,      0.0130,     -0.0038,     -0.0007,\n            -0.0014,      0.0144,      0.0191,     -0.0058,     -0.0097,\n            -0.0121,     -0.0091,     -0.0054,     -0.0008,     -0.0028,\n            -0.0006,      0.0006,      0.0008,      0.0007,      0.0011,\n             0.0175,     -0.0044,      0.0123,     -0.0082,     -0.0112,\n             0.0042,     -0.0203,     -0.0068,      0.0020,     -0.0031,\n            -0.0000,      0.0115,     -0.0064,      0.0127,      0.0092,\n             0.0027,      0.0093,      0.0108,     -0.0009,      0.0001,\n            -0.0040,      0.0019,     -0.0040,     -0.0017,      0.0023,\n             0.0025,      0.0039,      0.0012,     -0.0100,     -0.0113,\n             0.0130,     -0.0068,     -0.0094,     -0.0033,     -0.0034,\n            -0.0011,     -0.0467,      0.0044,      0.0124,      0.0073,\n             0.0057,      0.0103,      0.0050,      0.0034,     -0.0006,\n            -0.0001,     -0.0002,     -0.0038,     -0.0016,     -0.0001,\n            -0.0006,     -0.0008,     -0.0059,      0.0079,     -0.0058,\n            -0.0038,      0.0020,     -0.0033,     -0.0015,     -0.0075,\n            -0.0024,      0.0030,     -0.0080,     -0.0025,     -0.0029,\n            -0.0026,     -0.0038,     -0.0007,      0.0008,      0.0013,\n            -0.0032,     -0.0004,     -0.0034,     -0.0078,     -0.0077,\n             0.0027,      0.0003,      0.0002,      0.0046,     -0.0002,\n            -0.0007,     -0.0031,     -0.0034,     -0.0075,     -0.0040,\n            -0.0006,      0.0402,      0.0066,      0.0073,      0.0032,\n             0.0011,      0.0028,      0.0039,     -0.0010,     -0.0023,\n            -0.0072,      0.0015,     -0.0032,     -0.0032,     -0.0007,\n             0.0071,      0.0155,     -0.0024,     -0.0016,      0.0009,\n            -0.0014,     -0.0000,     -0.0011,     -0.0024,     -0.0006,\n            -0.0005,      0.0026,      0.0003,     -0.0049,     -0.0025,\n            -0.0112,     -0.0057,     -0.0020,     -0.0016,     -0.0031,\n            -0.0069,      0.0013,     -0.0025,     -0.0036,     -0.0008,\n            -0.0269,      0.0011,      0.0019,     -0.0152,      0.0184,\n             0.0144,      0.0115,     -0.0467,      0.0030,      0.0402,\n             0.0026,     -0.0538,      0.0449,      0.0020,     -0.0037,\n            -0.0546,      0.0259,     -0.0082,      0.0020,     -0.0005,\n            -0.0039,     -0.0008,     -0.0035,      0.0020,      0.0028,\n             0.0069,      0.0182,      0.0119,     -0.0078,      0.0004,\n             0.0191,     -0.0064,      0.0044,     -0.0080,      0.0066,\n             0.0003,      0.0449,     -0.0091,     -0.0012,      0.0021,\n             0.0079,      0.0006,      0.0066,      0.0040,     -0.0019,\n             0.0025,     -0.0008,      0.0233,      0.0000,     -0.0001,\n            -0.0039,      0.0035,     -0.0083,     -0.0053,      0.0007,\n            -0.0058,      0.0127,      0.0124,     -0.0025,      0.0073,\n            -0.0049,      0.0020,     -0.0012,     -0.0070,      0.0023,\n            -0.0076,      0.0037,      0.0043,     -0.0067,     -0.0156,\n            -0.0204,      0.0061,     -0.0096,     -0.0105,     -0.0047,\n            -0.0078,      0.0055,     -0.0089,      0.0009,      0.0034,\n            -0.0097,      0.0092,      0.0073,     -0.0029,      0.0032,\n            -0.0025,     -0.0037,      0.0021,      0.0023,     -0.0032,\n            -0.0060,      0.0036,      0.0044,     -0.0042,     -0.0075,\n            -0.0142,      0.0055,     -0.0100,     -0.0086,     -0.0029,\n             0.0049,     -0.0014,     -0.0146,     -0.0009,     -0.0193,\n            -0.0121,      0.0027,      0.0057,     -0.0026,      0.0011,\n            -0.0112,     -0.0546,      0.0079,     -0.0076,     -0.0060,\n            -0.0068,     -0.0217,     -0.0010,     -0.0034,     -0.0030,\n            -0.0052,      0.0030,     -0.0019,     -0.0098,     -0.0036,\n            -0.0003,     -0.0053,     -0.0051,      0.0016,     -0.0037,\n            -0.0091,      0.0093,      0.0103,     -0.0038,      0.0028,\n            -0.0057,      0.0259,      0.0006,      0.0037,      0.0036,\n            -0.0217,     -0.0057,      0.0045,     -0.0073,     -0.0137,\n            -0.0303,      0.0066,     -0.0129,     -0.0094,     -0.0042,\n            -0.0035,     -0.0020,     -0.0064,      0.0014,      0.0025,\n            -0.0054,      0.0108,      0.0050,     -0.0007,      0.0039,\n            -0.0020,     -0.0082,      0.0066,      0.0043,      0.0044,\n            -0.0010,      0.0045,     -0.0059,     -0.0045,     -0.0056,\n            -0.0078,      0.0058,     -0.0301,     -0.0064,     -0.0018,\n             0.0039,     -0.0005,     -0.0034,      0.0012,     -0.0003,\n            -0.0008,     -0.0009,      0.0034,      0.0008,     -0.0010,\n            -0.0016,      0.0020,      0.0040,     -0.0067,     -0.0042,\n            -0.0034,     -0.0073,     -0.0045,     -0.0342,      0.0033,\n            -0.0008,     -0.0002,      0.0002,     -0.0001,      0.0019,\n             0.0022,     -0.0010,     -0.0108,      0.0086,     -0.0061,\n            -0.0028,      0.0001,     -0.0006,      0.0013,     -0.0023,\n            -0.0031,     -0.0005,     -0.0019,     -0.0156,     -0.0075,\n            -0.0030,     -0.0137,     -0.0056,      0.0033,     -0.0128,\n            -0.0019,     -0.0002,     -0.0012,     -0.0025,     -0.0058,\n            -0.0004,      0.0056,     -0.0097,      0.0009,      0.0001,\n            -0.0006,     -0.0040,     -0.0001,     -0.0032,     -0.0072,\n            -0.0069,     -0.0039,      0.0025,     -0.0204,     -0.0142,\n            -0.0052,     -0.0303,     -0.0078,     -0.0008,     -0.0019,\n            -0.0007,     -0.0002,     -0.0079,     -0.0082,     -0.0029,\n            -0.0022,      0.0004,      0.0072,     -0.0021,      0.0004,\n             0.0006,      0.0019,     -0.0002,     -0.0004,      0.0015,\n             0.0013,     -0.0008,     -0.0008,      0.0061,      0.0055,\n             0.0030,      0.0066,      0.0058,     -0.0002,     -0.0002,\n            -0.0002,      0.0002,     -0.0002,     -0.0002,      0.0028,\n             0.0031,      0.0020,     -0.0557,     -0.0003,     -0.0003,\n             0.0008,     -0.0040,     -0.0038,     -0.0034,     -0.0032,\n            -0.0025,     -0.0035,      0.0233,     -0.0096,     -0.0100,\n            -0.0019,     -0.0129,     -0.0301,      0.0002,     -0.0012,\n            -0.0079,     -0.0002,     -0.0011,     -0.0072,     -0.0026,\n             0.0022,      0.0082,     -0.0139,      0.0008,     -0.0002,\n             0.0007,     -0.0017,     -0.0016,     -0.0078,     -0.0032,\n            -0.0036,      0.0020,      0.0000,     -0.0105,     -0.0086,\n            -0.0098,     -0.0094,     -0.0064,     -0.0001,     -0.0025,\n            -0.0082,     -0.0002,     -0.0072,     -0.0013,     -0.0047,\n            -0.0014,      0.0027,     -0.0016,     -0.0012,     -0.0031,\n             0.0011,      0.0023,     -0.0001,     -0.0077,     -0.0007,\n            -0.0008,      0.0028,     -0.0001,     -0.0047,     -0.0029,\n            -0.0036,     -0.0042,     -0.0018,      0.0019,     -0.0058,\n            -0.0029,      0.0028,     -0.0026,     -0.0047,     -0.0037,\n            -0.0016,     -0.0103,     -0.0069,     -0.0179,      0.0015,\n             0.0177,     -0.0043,      0.0100,     -0.0067,      0.0173,\n             0.0001,      0.0024,     -0.0388,     -0.0048,      0.0008,\n             0.0014,      0.0029,      0.0109,      0.0045,      0.0003,\n             0.0027,      0.0031,     -0.0024,      0.0010,     -0.0290,\n            -0.0103,     -0.0929,     -0.0432,     -0.0071,      0.0029,\n            -0.0372,     -0.0084,     -0.0823,     -0.0208,     -0.0011,\n             0.0043,     -0.0018,      0.0036,      0.0112,      0.0249,\n            -0.0600,      0.0418,     -0.0281,     -0.0027,     -0.0307,\n             0.0039,      0.0020,      0.0047,      0.0039,     -0.0001,\n            -0.0069,     -0.0432,     -0.0226,     -0.0342,      0.0013,\n             0.0405,      0.0278,      0.0103,     -0.0014,      0.0091,\n            -0.0041,      0.0029,      0.0560,      0.0080,      0.0079,\n            -0.0056,      0.0561,     -0.0298,     -0.0080,     -0.0068,\n            -0.0585,     -0.0557,      0.0119,     -0.0136,     -0.0097,\n            -0.0179,     -0.0071,     -0.0342,     -0.0039,     -0.0023,\n             0.0220,     -0.0063,      0.0109,     -0.0021,     -0.0011,\n            -0.0744,     -0.0366,     -0.0260,     -0.0058,      0.0017,\n            -0.0017,     -0.0508,     -0.0047,     -0.0009,      0.0028,\n            -0.0015,     -0.0003,      0.0012,     -0.0427,      0.0101,\n             0.0015,      0.0029,      0.0013,     -0.0023,     -0.0054,\n            -0.0439,      0.0175,      0.0013,      0.0007,      0.0028,\n            -0.0003,      0.0061,     -0.0015,      0.0235,      0.0095,\n             0.0017,      0.0015,     -0.0065,      0.0041,     -0.0090,\n             0.0059,     -0.0003,      0.0007,      0.0009,      0.0039,\n             0.0177,     -0.0372,      0.0405,      0.0220,     -0.0439,\n            -0.0092,     -0.0351,     -0.0030,      0.0050,     -0.0108,\n             0.0254,      0.0345,     -0.0183,     -0.0095,     -0.0072,\n            -0.0061,      0.0376,      0.0159,     -0.0023,     -0.0038,\n             0.0053,      0.0008,     -0.0026,      0.0269,      0.0040,\n            -0.0043,     -0.0084,      0.0278,     -0.0063,      0.0175,\n            -0.0351,     -0.0170,      0.0078,     -0.0006,     -0.0013,\n            -0.0002,      0.0068,      0.0231,      0.0170,      0.0038,\n            -0.0101,      0.0001,      0.0098,      0.0007,      0.0015,\n            -0.0035,     -0.0040,      0.0056,     -0.0020,     -0.0025,\n             0.0100,     -0.0823,      0.0103,      0.0109,      0.0013,\n            -0.0030,      0.0078,     -0.0070,     -0.0005,     -0.0035,\n            -0.0022,     -0.0176,      0.0496,      0.0062,      0.0132,\n            -0.0124,      0.0200,     -0.0206,      0.0000,     -0.0013,\n            -0.0001,     -0.0038,     -0.0002,     -0.0021,     -0.0133,\n            -0.0067,     -0.0208,     -0.0014,     -0.0021,      0.0007,\n             0.0050,     -0.0006,     -0.0005,     -0.0007,     -0.0052,\n            -0.0031,     -0.0306,      0.0013,     -0.0127,     -0.0071,\n            -0.0055,     -0.0086,     -0.0138,     -0.0010,      0.0015,\n            -0.0021,     -0.0034,      0.0013,     -0.0128,     -0.0001,\n             0.0173,     -0.0011,      0.0091,     -0.0011,      0.0028,\n            -0.0108,     -0.0013,     -0.0035,     -0.0052,     -0.0026,\n            -0.0027,     -0.0047,     -0.0016,     -0.0059,     -0.0002,\n            -0.0113,      0.0025,     -0.0023,     -0.0014,     -0.0027,\n            -0.0054,     -0.0032,     -0.0009,     -0.0026,      0.0014,\n             0.0001,      0.0043,     -0.0041,     -0.0744,     -0.0003,\n             0.0254,     -0.0002,     -0.0022,     -0.0031,     -0.0027,\n            -0.0021,      0.0009,      0.0069,     -0.0003,     -0.0018,\n            -0.0001,     -0.0055,     -0.0569,     -0.0016,     -0.0043,\n            -0.0040,     -0.0025,     -0.0006,     -0.0029,     -0.0018,\n             0.0024,     -0.0018,      0.0029,     -0.0366,      0.0061,\n             0.0345,      0.0068,     -0.0176,     -0.0306,     -0.0047,\n             0.0009,     -0.0153,      0.0195,     -0.0137,      0.0191,\n            -0.0108,     -0.0114,     -0.0166,     -0.0019,     -0.0066,\n            -0.0099,     -0.0035,      0.0048,     -0.0002,      0.0291,\n            -0.0388,      0.0036,      0.0560,     -0.0260,     -0.0015,\n            -0.0183,      0.0231,      0.0496,      0.0013,     -0.0016,\n             0.0069,      0.0195,     -0.0067,     -0.0001,      0.0145,\n             0.0017,      0.0175,      0.0059,      0.0014,     -0.0012,\n             0.0086,      0.0233,     -0.0039,      0.0096,      0.0020,\n            -0.0048,      0.0112,      0.0080,     -0.0058,      0.0235,\n            -0.0095,      0.0170,      0.0062,     -0.0127,     -0.0059,\n            -0.0003,     -0.0137,     -0.0001,     -0.0063,      0.0027,\n             0.0023,      0.0036,      0.0031,     -0.0002,     -0.0031,\n            -0.0079,     -0.0096,     -0.0001,     -0.0005,      0.0035,\n             0.0008,      0.0249,      0.0079,      0.0017,      0.0095,\n            -0.0072,      0.0038,      0.0132,     -0.0071,     -0.0002,\n            -0.0018,      0.0191,      0.0145,      0.0027,     -0.0039,\n             0.0022,      0.0022,      0.0033,     -0.0011,     -0.0019,\n            -0.0076,     -0.0100,     -0.0001,     -0.0037,     -0.0073,\n             0.0014,     -0.0600,     -0.0056,     -0.0017,      0.0017,\n            -0.0061,     -0.0101,     -0.0124,     -0.0055,     -0.0113,\n            -0.0001,     -0.0108,      0.0017,      0.0023,      0.0022,\n            -0.0137,      0.0030,      0.0029,     -0.0011,     -0.0018,\n            -0.0015,     -0.0019,     -0.0048,     -0.0001,     -0.0020,\n             0.0029,      0.0418,      0.0561,     -0.0508,      0.0015,\n             0.0376,      0.0001,      0.0200,     -0.0086,      0.0025,\n            -0.0055,     -0.0114,      0.0175,      0.0036,      0.0022,\n             0.0030,     -0.0059,     -0.0058,     -0.0079,     -0.0104,\n            -0.0688,     -0.0129,      0.0026,     -0.0149,      0.0182,\n             0.0109,     -0.0281,     -0.0298,     -0.0047,     -0.0065,\n             0.0159,      0.0098,     -0.0206,     -0.0138,     -0.0023,\n            -0.0569,     -0.0166,      0.0059,      0.0031,      0.0033,\n             0.0029,     -0.0058,     -0.0057,     -0.0021,     -0.0034,\n            -0.0280,     -0.0301,      0.0124,     -0.0526,      0.0069,\n             0.0045,     -0.0027,     -0.0080,     -0.0009,      0.0041,\n            -0.0023,      0.0007,      0.0000,     -0.0010,     -0.0014,\n            -0.0016,     -0.0019,      0.0014,     -0.0002,     -0.0011,\n            -0.0011,     -0.0079,     -0.0021,     -0.0286,      0.0006,\n            -0.0012,      0.0002,     -0.0008,     -0.0003,      0.0027,\n             0.0003,     -0.0307,     -0.0068,      0.0028,     -0.0090,\n            -0.0038,      0.0015,     -0.0013,      0.0015,     -0.0027,\n            -0.0043,     -0.0066,     -0.0012,     -0.0031,     -0.0019,\n            -0.0018,     -0.0104,     -0.0034,      0.0006,     -0.0104,\n            -0.0020,     -0.0012,     -0.0009,     -0.0038,     -0.0030,\n             0.0027,      0.0039,     -0.0585,     -0.0015,      0.0059,\n             0.0053,     -0.0035,     -0.0001,     -0.0021,     -0.0054,\n            -0.0040,     -0.0099,      0.0086,     -0.0079,     -0.0076,\n            -0.0015,     -0.0688,     -0.0280,     -0.0012,     -0.0020,\n            -0.0006,     -0.0079,     -0.0010,     -0.0087,     -0.0024,\n             0.0031,      0.0020,     -0.0557,     -0.0003,     -0.0003,\n             0.0008,     -0.0040,     -0.0038,     -0.0034,     -0.0032,\n            -0.0025,     -0.0035,      0.0233,     -0.0096,     -0.0100,\n            -0.0019,     -0.0129,     -0.0301,      0.0002,     -0.0012,\n            -0.0079,     -0.0002,     -0.0011,     -0.0072,     -0.0026,\n            -0.0024,      0.0047,      0.0119,      0.0012,      0.0007,\n            -0.0026,      0.0056,     -0.0002,      0.0013,     -0.0009,\n            -0.0006,      0.0048,     -0.0039,     -0.0001,     -0.0001,\n            -0.0048,      0.0026,      0.0124,     -0.0008,     -0.0009,\n            -0.0010,     -0.0011,      0.0008,     -0.0008,     -0.0015,\n             0.0010,      0.0039,     -0.0136,     -0.0427,      0.0009,\n             0.0269,     -0.0020,     -0.0021,     -0.0128,     -0.0026,\n            -0.0029,     -0.0002,      0.0096,     -0.0005,     -0.0037,\n            -0.0001,     -0.0149,     -0.0526,     -0.0003,     -0.0038,\n            -0.0087,     -0.0072,     -0.0008,     -0.0005,     -0.0002,\n            -0.0290,     -0.0001,     -0.0097,      0.0101,      0.0039,\n             0.0040,     -0.0025,     -0.0133,     -0.0001,      0.0014,\n            -0.0018,      0.0291,      0.0020,      0.0035,     -0.0073,\n            -0.0020,      0.0182,      0.0069,      0.0027,     -0.0030,\n            -0.0024,     -0.0026,     -0.0015,     -0.0002,     -0.0053,\n             0.0048,      0.0050,     -0.0021,      0.0082,     -0.0010,\n             0.0031,      0.0034,      0.0037,     -0.0037,      0.0062,\n             0.0009,     -0.0230,      0.0012,      0.0059,     -0.0086,\n            -0.0001,      0.0031,     -0.0004,     -0.0472,      0.0001,\n             0.0032,      0.0022,      0.0010,     -0.0003,     -0.0036,\n             0.0050,     -0.0817,      0.0105,     -0.0075,     -0.0048,\n            -0.0212,     -0.0171,     -0.0313,     -0.0019,     -0.0012,\n             0.0092,     -0.0021,      0.0507,      0.0054,     -0.0016,\n            -0.0175,      0.0138,      0.0309,     -0.0001,     -0.0055,\n             0.0143,      0.0082,      0.0039,      0.0151,     -0.0028,\n            -0.0021,      0.0105,     -0.0094,     -0.0093,      0.0034,\n             0.0121,     -0.0016,      0.0082,      0.0017,      0.0013,\n             0.0030,      0.0054,     -0.0055,      0.0090,      0.0012,\n             0.0098,      0.0060,      0.0038,      0.0019,      0.0013,\n            -0.0046,     -0.0139,     -0.0136,      0.0063,      0.0124,\n             0.0082,     -0.0075,     -0.0093,     -0.0536,     -0.0008,\n             0.0117,      0.0087,      0.0021,      0.0004,      0.0057,\n            -0.0103,     -0.0534,     -0.6972,     -0.0078,     -0.0106,\n             0.0516,     -0.0234,     -0.0141,     -0.0003,      0.0427,\n            -0.0006,      0.0008,     -0.0427,     -0.0251,      0.0049,\n            -0.0010,     -0.0048,      0.0034,     -0.0008,      0.0020,\n            -0.0046,     -0.0065,     -0.0150,     -0.0004,      0.0077,\n            -0.0013,      0.0023,      0.0071,     -0.0047,     -0.0028,\n             0.0020,      0.0052,     -0.0013,      0.0085,     -0.0039,\n             0.0065,     -0.0002,      0.0009,      0.0020,      0.0005,\n             0.0031,     -0.0212,      0.0121,      0.0117,     -0.0046,\n            -0.0350,     -0.0127,     -0.1122,     -0.0025,     -0.0012,\n             0.0092,      0.0243,      0.0217,      0.0008,      0.0125,\n            -0.0488,      0.0320,     -0.0010,     -0.0039,     -0.0056,\n             0.0068,      0.0007,      0.0269,      0.0043,     -0.0052,\n             0.0034,     -0.0171,     -0.0016,      0.0087,     -0.0065,\n            -0.0127,      0.0025,     -0.0034,      0.0035,     -0.0082,\n             0.0073,     -0.0072,      0.0041,      0.0040,      0.0088,\n            -0.0033,     -0.0011,      0.0001,      0.0138,      0.0067,\n            -0.0034,     -0.0017,     -0.0020,      0.0011,     -0.0036,\n             0.0037,     -0.0313,      0.0082,      0.0021,     -0.0150,\n            -0.1122,     -0.0034,     -0.0225,     -0.0022,     -0.0059,\n            -0.0011,     -0.1011,      0.0070,     -0.0049,     -0.0005,\n            -0.0065,     -0.0129,      0.0010,      0.0008,     -0.0000,\n             0.0007,     -0.0016,     -0.0021,     -0.0066,      0.0047,\n            -0.0037,     -0.0019,      0.0017,      0.0004,     -0.0004,\n            -0.0025,      0.0035,     -0.0022,     -0.0013,     -0.0024,\n            -0.0035,     -0.0037,      0.0010,     -0.0005,     -0.0068,\n             0.0003,     -0.0017,     -0.0031,     -0.0010,      0.0015,\n            -0.0090,     -0.0078,     -0.0128,     -0.0004,     -0.0045,\n             0.0062,     -0.0012,      0.0013,      0.0057,      0.0077,\n            -0.0012,     -0.0082,     -0.0059,     -0.0024,     -0.0053,\n            -0.0030,     -0.0027,      0.0042,     -0.0038,     -0.0076,\n            -0.0022,     -0.0028,     -0.0068,      0.0002,     -0.0023,\n            -0.0052,     -0.0032,     -0.0026,     -0.0008,     -0.0081,\n             0.0009,      0.0092,      0.0030,     -0.0103,     -0.0013,\n             0.0092,      0.0073,     -0.0011,     -0.0035,     -0.0030,\n            -0.0061,     -0.0003,      0.0022,     -0.0034,     -0.0022,\n            -0.0013,     -0.0316,     -0.0007,     -0.0035,     -0.0147,\n            -0.0081,     -0.0036,     -0.0029,     -0.0006,     -0.0020,\n            -0.0230,     -0.0021,      0.0054,     -0.0534,      0.0023,\n             0.0243,     -0.0072,     -0.1011,     -0.0037,     -0.0027,\n            -0.0003,     -0.0301,      0.0378,     -0.0105,     -0.0276,\n            -0.0012,     -0.0039,     -0.0062,     -0.0015,     -0.0056,\n            -0.0159,      0.0020,     -0.0002,      0.0079,     -0.0091,\n             0.0012,      0.0507,     -0.0055,     -0.6972,      0.0071,\n             0.0217,      0.0041,      0.0070,      0.0010,      0.0042,\n             0.0022,      0.0378,     -0.1849,      0.0006,      0.0042,\n            -0.0014,      0.0005,      0.0178,     -0.0018,     -0.0056,\n             0.0004,      0.0000,      0.0096,      0.0013,     -0.0034,\n             0.0059,      0.0054,      0.0090,     -0.0078,     -0.0047,\n             0.0008,      0.0040,     -0.0049,     -0.0005,     -0.0038,\n            -0.0034,     -0.0105,      0.0006,     -0.0181,      0.0052,\n             0.0018,      0.0010,      0.0065,      0.0007,     -0.0033,\n            -0.0252,     -0.0105,     -0.0005,     -0.0060,     -0.0025,\n            -0.0086,     -0.0016,      0.0012,     -0.0106,     -0.0028,\n             0.0125,      0.0088,     -0.0005,     -0.0068,     -0.0076,\n            -0.0022,     -0.0276,      0.0042,      0.0052,     -0.0066,\n             0.0025,      0.0024,      0.0065,     -0.0016,     -0.0049,\n            -0.0148,     -0.0086,     -0.0037,     -0.0014,     -0.0069,\n            -0.0001,     -0.0175,      0.0098,      0.0516,      0.0020,\n            -0.0488,     -0.0033,     -0.0065,      0.0003,     -0.0022,\n            -0.0013,     -0.0012,     -0.0014,      0.0018,      0.0025,\n            -0.0114,      0.0005,      0.0061,     -0.0027,     -0.0023,\n            -0.0091,     -0.0098,     -0.0001,     -0.0098,     -0.0018,\n             0.0031,      0.0138,      0.0060,     -0.0234,      0.0052,\n             0.0320,     -0.0011,     -0.0129,     -0.0017,     -0.0028,\n            -0.0316,     -0.0039,      0.0005,      0.0010,      0.0024,\n             0.0005,     -0.0139,      0.0109,     -0.0076,     -0.0037,\n            -0.0208,     -0.0094,     -0.0149,      0.0003,     -0.0023,\n            -0.0004,      0.0309,      0.0038,     -0.0141,     -0.0013,\n            -0.0010,      0.0001,      0.0010,     -0.0031,     -0.0068,\n            -0.0007,     -0.0062,      0.0178,      0.0065,      0.0065,\n             0.0061,      0.0109,     -0.0148,      0.0007,      0.0003,\n            -0.0077,     -0.0064,     -0.0526,      0.0082,      0.0019,\n            -0.0472,     -0.0001,      0.0019,     -0.0003,      0.0085,\n            -0.0039,      0.0138,      0.0008,     -0.0010,      0.0002,\n            -0.0035,     -0.0015,     -0.0018,      0.0007,     -0.0016,\n            -0.0027,     -0.0076,      0.0007,     -0.0190,     -0.0002,\n            -0.0029,     -0.0001,     -0.0003,     -0.0018,     -0.0010,\n             0.0001,     -0.0055,      0.0013,      0.0427,     -0.0039,\n            -0.0056,      0.0067,     -0.0000,      0.0015,     -0.0023,\n            -0.0147,     -0.0056,     -0.0056,     -0.0033,     -0.0049,\n            -0.0023,     -0.0037,      0.0003,     -0.0002,     -0.0094,\n            -0.0030,     -0.0025,     -0.0038,     -0.0043,     -0.0016,\n             0.0032,      0.0143,     -0.0046,     -0.0006,      0.0065,\n             0.0068,     -0.0034,      0.0007,     -0.0090,     -0.0052,\n            -0.0081,     -0.0159,      0.0004,     -0.0252,     -0.0148,\n            -0.0091,     -0.0208,     -0.0077,     -0.0029,     -0.0030,\n             0.0002,     -0.0082,     -0.0087,     -0.0039,     -0.0041,\n             0.0022,      0.0082,     -0.0139,      0.0008,     -0.0002,\n             0.0007,     -0.0017,     -0.0016,     -0.0078,     -0.0032,\n            -0.0036,      0.0020,      0.0000,     -0.0105,     -0.0086,\n            -0.0098,     -0.0094,     -0.0064,     -0.0001,     -0.0025,\n            -0.0082,     -0.0002,     -0.0072,     -0.0013,     -0.0047,\n             0.0010,      0.0039,     -0.0136,     -0.0427,      0.0009,\n             0.0269,     -0.0020,     -0.0021,     -0.0128,     -0.0026,\n            -0.0029,     -0.0002,      0.0096,     -0.0005,     -0.0037,\n            -0.0001,     -0.0149,     -0.0526,     -0.0003,     -0.0038,\n            -0.0087,     -0.0072,     -0.0008,     -0.0005,     -0.0002,\n            -0.0003,      0.0151,      0.0063,     -0.0251,      0.0020,\n             0.0043,      0.0011,     -0.0066,     -0.0004,     -0.0008,\n            -0.0006,      0.0079,      0.0013,     -0.0060,     -0.0014,\n            -0.0098,      0.0003,      0.0082,     -0.0018,     -0.0043,\n            -0.0039,     -0.0013,     -0.0005,     -0.0040,     -0.0013,\n            -0.0036,     -0.0028,      0.0124,      0.0049,      0.0005,\n            -0.0052,     -0.0036,      0.0047,     -0.0045,     -0.0081,\n            -0.0020,     -0.0091,     -0.0034,     -0.0025,     -0.0069,\n            -0.0018,     -0.0023,      0.0019,     -0.0010,     -0.0016,\n            -0.0041,     -0.0047,     -0.0002,     -0.0013,     -0.0087,\n             0.0039,     -0.0033,     -0.0017,     -0.0017,     -0.0008,\n            -0.0009,      0.0017,     -0.0040,      0.0090,      0.0029,\n            -0.0098,      0.0123,     -0.0066,      0.0007,     -0.0035,\n            -0.0009,     -0.0045,     -0.0007,     -0.0003,      0.0016,\n             0.0014,     -0.0014,     -0.0290,     -0.0036,      0.0019,\n            -0.0033,      0.0172,      0.0006,     -0.0081,     -0.0034,\n             0.0236,     -0.0095,     -0.0072,      0.0006,     -0.0006,\n            -0.0102,      0.0115,     -0.0036,     -0.0039,      0.0022,\n             0.0045,     -0.0042,     -0.0022,     -0.0074,      0.0052,\n             0.0079,      0.0027,     -0.0001,     -0.0028,      0.0245,\n            -0.0017,      0.0006,     -0.0040,     -0.0057,      0.0048,\n             0.0049,      0.0020,     -0.0003,      0.0023,      0.0019,\n             0.0158,      0.0140,     -0.0005,      0.0075,      0.0049,\n             0.0031,      0.0050,      0.0067,      0.0021,     -0.0012,\n            -0.0026,     -0.0016,     -0.0097,      0.0124,      0.0015,\n            -0.0017,     -0.0081,     -0.0057,      0.0124,     -0.0041,\n            -0.0058,      0.0155,      0.0099,     -0.0158,     -0.0019,\n             0.0046,     -0.0218,      0.0016,     -0.0005,     -0.0055,\n            -0.0076,      0.0016,     -0.0024,      0.0314,     -0.0087,\n            -0.0059,     -0.0012,      0.0101,      0.0049,     -0.0070,\n            -0.0008,     -0.0034,      0.0048,     -0.0041,      0.0081,\n             0.0021,     -0.0081,      0.0016,     -0.0199,     -0.0033,\n             0.0001,     -0.0316,     -0.0039,      0.0016,      0.0040,\n            -0.0094,     -0.0012,     -0.0070,      0.0257,      0.0028,\n            -0.0025,     -0.0031,      0.0039,      0.0005,      0.0038,\n            -0.0009,      0.0236,      0.0049,     -0.0058,      0.0021,\n             0.0144,     -0.0151,     -0.0095,      0.0125,     -0.0013,\n            -0.0085,      0.0544,     -0.0020,     -0.0006,      0.0051,\n             0.0067,     -0.0017,      0.0039,     -0.0245,      0.0071,\n             0.0047,      0.0011,      0.0040,     -0.0052,      0.0155,\n             0.0017,     -0.0095,      0.0020,      0.0155,     -0.0081,\n            -0.0151,     -0.0018,      0.0014,     -0.0037,     -0.0096,\n             0.0013,     -0.0040,      0.0062,     -0.0031,      0.0037,\n             0.0000,      0.0034,      0.0012,      0.0095,     -0.0010,\n             0.0006,      0.0023,     -0.0025,     -0.0036,      0.0034,\n            -0.0040,     -0.0072,     -0.0003,      0.0099,      0.0016,\n            -0.0095,      0.0014,     -0.0086,      0.0024,     -0.0014,\n             0.0096,     -0.0153,      0.0005,      0.0007,      0.0018,\n             0.0005,      0.0003,      0.0004,     -0.0014,      0.0019,\n            -0.0004,     -0.0001,     -0.0133,      0.0047,     -0.0037,\n             0.0090,      0.0006,      0.0023,     -0.0158,     -0.0199,\n             0.0125,     -0.0037,      0.0024,      0.0024,      0.0129,\n            -0.0038,     -0.0034,      0.0014,     -0.0045,     -0.0025,\n             0.0076,     -0.0100,     -0.0092,      0.0011,     -0.0043,\n             0.0188,     -0.0077,     -0.0001,     -0.0045,     -0.0055,\n             0.0029,     -0.0006,      0.0019,     -0.0019,     -0.0033,\n            -0.0013,     -0.0096,     -0.0014,      0.0129,     -0.0009,\n            -0.0060,      0.0011,      0.0002,     -0.0023,     -0.0028,\n            -0.0028,      0.0003,     -0.0020,     -0.0012,      0.0003,\n            -0.0003,     -0.0007,      0.0014,     -0.0081,     -0.0010,\n            -0.0098,     -0.0102,      0.0158,      0.0046,      0.0001,\n            -0.0085,      0.0013,      0.0096,     -0.0038,     -0.0060,\n            -0.0034,     -0.0084,     -0.0037,      0.0020,      0.0005,\n            -0.0197,      0.0010,     -0.0004,     -0.1154,     -0.0027,\n            -0.0020,     -0.0008,     -0.0018,     -0.0020,     -0.0060,\n             0.0123,      0.0115,      0.0140,     -0.0218,     -0.0316,\n             0.0544,     -0.0040,     -0.0153,     -0.0034,      0.0011,\n            -0.0084,     -0.0028,     -0.0015,      0.0054,     -0.0074,\n             0.0113,     -0.0126,      0.0078,     -0.0022,     -0.0007,\n            -0.0014,      0.0028,      0.0291,     -0.0091,      0.0006,\n            -0.0066,     -0.0036,     -0.0005,      0.0016,     -0.0039,\n            -0.0020,      0.0062,      0.0005,      0.0014,      0.0002,\n            -0.0037,     -0.0015,     -0.0104,     -0.0017,     -0.0020,\n            -0.0023,     -0.0014,     -0.0009,      0.0025,     -0.0013,\n             0.0020,     -0.0001,      0.0020,     -0.0034,      0.0017,\n             0.0007,     -0.0039,      0.0075,     -0.0005,      0.0016,\n            -0.0006,     -0.0031,      0.0007,     -0.0045,     -0.0023,\n             0.0020,      0.0054,     -0.0017,     -0.0092,      0.0026,\n             0.0010,      0.0094,      0.0087,     -0.0070,      0.0008,\n            -0.0009,     -0.0047,      0.0035,     -0.0025,     -0.0010,\n            -0.0035,      0.0022,      0.0049,     -0.0055,      0.0040,\n             0.0051,      0.0037,      0.0018,     -0.0025,     -0.0028,\n             0.0005,     -0.0074,     -0.0020,      0.0026,     -0.0097,\n             0.0001,      0.0007,      0.0014,      0.0021,     -0.0006,\n            -0.0035,     -0.0029,     -0.0073,     -0.0069,     -0.0004,\n            -0.0009,      0.0045,      0.0031,     -0.0076,     -0.0094,\n             0.0067,      0.0000,      0.0005,      0.0076,     -0.0028,\n            -0.0197,      0.0113,     -0.0023,      0.0010,      0.0001,\n            -0.0075,     -0.0046,      0.0023,     -0.0066,     -0.0008,\n             0.0020,     -0.0036,     -0.0020,     -0.0018,      0.0023,\n            -0.0045,     -0.0042,      0.0050,      0.0016,     -0.0012,\n            -0.0017,      0.0034,      0.0003,     -0.0100,      0.0003,\n             0.0010,     -0.0126,     -0.0014,      0.0094,      0.0007,\n            -0.0046,     -0.0089,      0.0089,     -0.0052,     -0.0036,\n            -0.0035,     -0.0042,      0.0182,     -0.0023,      0.0016,\n            -0.0007,     -0.0022,      0.0067,     -0.0024,     -0.0070,\n             0.0039,      0.0012,      0.0004,     -0.0092,     -0.0020,\n            -0.0004,      0.0078,     -0.0009,      0.0087,      0.0014,\n             0.0023,      0.0089,     -0.0085,     -0.0116,      0.0041,\n             0.0017,     -0.0018,      0.0069,      0.0019,      0.0061,\n            -0.0003,     -0.0074,      0.0021,      0.0314,      0.0257,\n            -0.0245,      0.0095,     -0.0014,      0.0011,     -0.0012,\n            -0.1154,     -0.0022,      0.0025,     -0.0070,      0.0021,\n            -0.0066,     -0.0052,     -0.0116,     -0.0041,     -0.0025,\n            -0.0044,      0.0019,      0.0027,     -0.0010,     -0.0039,\n             0.0016,      0.0052,     -0.0012,     -0.0087,      0.0028,\n             0.0071,     -0.0010,      0.0019,     -0.0043,      0.0003,\n            -0.0027,     -0.0007,     -0.0013,      0.0008,     -0.0006,\n            -0.0008,     -0.0036,      0.0041,     -0.0025,      0.0015,\n            -0.0078,     -0.0058,     -0.0030,     -0.0016,      0.0026,\n             0.0014,      0.0079,     -0.0026,     -0.0059,     -0.0025,\n             0.0047,      0.0006,     -0.0004,      0.0188,     -0.0003,\n            -0.0020,     -0.0014,      0.0020,     -0.0009,     -0.0035,\n             0.0020,     -0.0035,      0.0017,     -0.0044,     -0.0078,\n             0.0028,     -0.0029,     -0.0024,     -0.0041,      0.0007,\n            -0.0014,      0.0027,     -0.0016,     -0.0012,     -0.0031,\n             0.0011,      0.0023,     -0.0001,     -0.0077,     -0.0007,\n            -0.0008,      0.0028,     -0.0001,     -0.0047,     -0.0029,\n            -0.0036,     -0.0042,     -0.0018,      0.0019,     -0.0058,\n            -0.0029,      0.0028,     -0.0026,     -0.0047,     -0.0037,\n            -0.0290,     -0.0001,     -0.0097,      0.0101,      0.0039,\n             0.0040,     -0.0025,     -0.0133,     -0.0001,      0.0014,\n            -0.0018,      0.0291,      0.0020,      0.0035,     -0.0073,\n            -0.0020,      0.0182,      0.0069,      0.0027,     -0.0030,\n            -0.0024,     -0.0026,     -0.0015,     -0.0002,     -0.0053,\n            -0.0036,     -0.0028,      0.0124,      0.0049,      0.0005,\n            -0.0052,     -0.0036,      0.0047,     -0.0045,     -0.0081,\n            -0.0020,     -0.0091,     -0.0034,     -0.0025,     -0.0069,\n            -0.0018,     -0.0023,      0.0019,     -0.0010,     -0.0016,\n            -0.0041,     -0.0047,     -0.0002,     -0.0013,     -0.0087,\n             0.0019,      0.0245,      0.0015,     -0.0070,      0.0038,\n             0.0155,      0.0034,     -0.0037,     -0.0055,     -0.0010,\n            -0.0060,      0.0006,      0.0017,     -0.0010,     -0.0004,\n             0.0023,      0.0016,      0.0061,     -0.0039,      0.0026,\n             0.0007,     -0.0037,     -0.0053,     -0.0087,      0.0061],\n       device='cuda:0', requires_grad=True)\nbias\nParameter containing:\ntensor(0.0683, device='cuda:0', requires_grad=True)\n","output_type":"stream"}],"execution_count":56},{"cell_type":"code","source":"val_loss = 0.0\nwith torch.no_grad():\n    for start_idx in range(0, len(custom_train_loader.train_data_tensor), batch_size):\n        end_idx = min(start_idx + batch_size, len(custom_train_loader.train_data_tensor))\n        inputs = custom_train_loader.train_data_tensor[start_idx:end_idx].view(-1, num_features)\n        labels = custom_train_loader.train_labels_tensor[start_idx:end_idx]\n\n        outputs = model(inputs)\n\n    for start_idx in range(0, len(custom_train_loader.val_data_tensor), batch_size):\n        end_idx = min(start_idx + batch_size, len(custom_train_loader.val_data_tensor))\n        val_inputs = custom_train_loader.val_data_tensor[start_idx:end_idx].view(-1, num_features)\n        val_labels = custom_train_loader.val_labels_tensor[start_idx:end_idx]\n\n        val_outputs = model(val_inputs)\n        val_loss += criterion.regular_loss(val_outputs, val_labels).item() * len(val_labels)\n\navg_val_loss = val_loss / len(custom_train_loader.val_data_tensor)\nval_accuracy, val_f1 = calculate_metrics(model, custom_train_loader.val_data_tensor, custom_train_loader.val_labels_tensor, batch_size, num_features)\n\nprint(avg_val_loss)\nprint(val_accuracy)\nprint(val_f1)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-27T23:19:11.311347Z","iopub.execute_input":"2025-01-27T23:19:11.312195Z","iopub.status.idle":"2025-01-27T23:19:11.390204Z","shell.execute_reply.started":"2025-01-27T23:19:11.312162Z","shell.execute_reply":"2025-01-27T23:19:11.389197Z"},"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"max_magnitude = -float(\"inf\")\nmax_param_name = None\nmax_param_index = None\n\nfor name, param in model.named_parameters():\n    if param.requires_grad:\n        weights = param.data\n        max_val, flat_index = torch.max(torch.abs(weights).view(-1), 0)\n        if max_val > max_magnitude:\n            max_magnitude = max_val\n            max_param_name = name\n            max_param_index = flat_index\n\nif max_param_name is not None:\n    for name, param in model.named_parameters():\n        if name == max_param_name:\n            max_param_coords = torch.unravel_index(max_param_index, param.data.shape)\n            param.data[max_param_coords] = 0\n            print(f\"Modified parameter: {name}\")\n            print(f\"Set the weight at {max_param_coords} (value {max_magnitude}) to 0.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-27T23:24:20.37394Z","iopub.execute_input":"2025-01-27T23:24:20.374321Z","iopub.status.idle":"2025-01-27T23:24:20.448619Z","shell.execute_reply.started":"2025-01-27T23:24:20.374289Z","shell.execute_reply":"2025-01-27T23:24:20.447612Z"},"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"val_loss = 0.0\nwith torch.no_grad():\n    for start_idx in range(0, len(custom_train_loader.train_data_tensor), batch_size):\n        end_idx = min(start_idx + batch_size, len(custom_train_loader.train_data_tensor))\n        inputs = custom_train_loader.train_data_tensor[start_idx:end_idx].view(-1, num_features)\n        labels = custom_train_loader.train_labels_tensor[start_idx:end_idx]\n\n        outputs = model(inputs)\n\n    for start_idx in range(0, len(custom_train_loader.val_data_tensor), batch_size):\n        end_idx = min(start_idx + batch_size, len(custom_train_loader.val_data_tensor))\n        val_inputs = custom_train_loader.val_data_tensor[start_idx:end_idx].view(-1, num_features)\n        val_labels = custom_train_loader.val_labels_tensor[start_idx:end_idx]\n\n        val_outputs = model(val_inputs)\n        val_loss += criterion.regular_loss(val_outputs, val_labels).item() * len(val_labels)\n\navg_val_loss = val_loss / len(custom_train_loader.val_data_tensor)\nval_accuracy, val_f1 = calculate_metrics(model, custom_train_loader.val_data_tensor, custom_train_loader.val_labels_tensor, batch_size, num_features)\n\nprint(avg_val_loss)\nprint(val_accuracy)\nprint(val_f1)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-27T21:06:20.059092Z","iopub.execute_input":"2025-01-27T21:06:20.059683Z","iopub.status.idle":"2025-01-27T21:06:20.133978Z","shell.execute_reply.started":"2025-01-27T21:06:20.059649Z","shell.execute_reply":"2025-01-27T21:06:20.13323Z"},"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"evaluate_model(model, custom_train_loader, criterion, optimizer, 1, scheduler, batch_size, num_features, early_stopping_patience=10000)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-27T23:22:58.720528Z","iopub.execute_input":"2025-01-27T23:22:58.721375Z","iopub.status.idle":"2025-01-27T23:22:58.971951Z","shell.execute_reply.started":"2025-01-27T23:22:58.721339Z","shell.execute_reply":"2025-01-27T23:22:58.971024Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"evaluate_model(model, custom_train_loader, criterion, optimizer, 1, scheduler, batch_size, num_features, early_stopping_patience=10000)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-27T23:24:34.92678Z","iopub.execute_input":"2025-01-27T23:24:34.927628Z","iopub.status.idle":"2025-01-27T23:24:35.188502Z","shell.execute_reply.started":"2025-01-27T23:24:34.927596Z","shell.execute_reply":"2025-01-27T23:24:35.187539Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"max_magnitude = -float(\"inf\")\nmax_param_name = None\nmax_param_index = None\n\nfor name, param in model.named_parameters():\n    if param.requires_grad:\n        weights = param.data\n        max_val, flat_index = torch.max(torch.abs(weights).view(-1), 0)\n        if max_val > max_magnitude:\n            max_magnitude = max_val\n            max_param_name = name\n            max_param_index = flat_index\n\nif max_param_name is not None:\n    for name, param in model.named_parameters():\n        if name == max_param_name:\n            max_param_coords = torch.unravel_index(max_param_index, param.data.shape)\n            param.data[max_param_coords] = 0\n            print(f\"Modified parameter: {name}\")\n            print(f\"Set the weight at {max_param_coords} (value {max_magnitude}) to 0.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-27T23:24:55.356285Z","iopub.execute_input":"2025-01-27T23:24:55.357153Z","iopub.status.idle":"2025-01-27T23:24:55.36595Z","shell.execute_reply.started":"2025-01-27T23:24:55.357115Z","shell.execute_reply":"2025-01-27T23:24:55.365045Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"evaluate_model(model, custom_train_loader, criterion, optimizer, 1, scheduler, batch_size, num_features, early_stopping_patience=10000)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-27T23:25:03.234799Z","iopub.execute_input":"2025-01-27T23:25:03.235643Z","iopub.status.idle":"2025-01-27T23:25:03.483454Z","shell.execute_reply.started":"2025-01-27T23:25:03.235608Z","shell.execute_reply":"2025-01-27T23:25:03.482594Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"max_magnitude = -float(\"inf\")\nmax_param_name = None\nmax_param_index = None\n\nfor name, param in model.named_parameters():\n    if param.requires_grad:\n        weights = param.data\n        max_val, flat_index = torch.max(torch.abs(weights).view(-1), 0)\n        if max_val > max_magnitude:\n            max_magnitude = max_val\n            max_param_name = name\n            max_param_index = flat_index\n\nif max_param_name is not None:\n    for name, param in model.named_parameters():\n        if name == max_param_name:\n            max_param_coords = torch.unravel_index(max_param_index, param.data.shape)\n            param.data[max_param_coords] = 0\n            print(f\"Modified parameter: {name}\")\n            print(f\"Set the weight at {max_param_coords} (value {max_magnitude}) to 0.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-27T23:25:28.300438Z","iopub.execute_input":"2025-01-27T23:25:28.30081Z","iopub.status.idle":"2025-01-27T23:25:28.310087Z","shell.execute_reply.started":"2025-01-27T23:25:28.300782Z","shell.execute_reply":"2025-01-27T23:25:28.309159Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"evaluate_model(model, custom_train_loader, criterion, optimizer, 1, scheduler, batch_size, num_features, early_stopping_patience=10000)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-27T23:25:38.673609Z","iopub.execute_input":"2025-01-27T23:25:38.673926Z","iopub.status.idle":"2025-01-27T23:25:38.925598Z","shell.execute_reply.started":"2025-01-27T23:25:38.6739Z","shell.execute_reply":"2025-01-27T23:25:38.924748Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"max_magnitude = -float(\"inf\")\nmax_param_name = None\nmax_param_index = None\n\nfor name, param in model.named_parameters():\n    if param.requires_grad:\n        weights = param.data\n        max_val, flat_index = torch.max(torch.abs(weights).view(-1), 0)\n        if max_val > max_magnitude:\n            max_magnitude = max_val\n            max_param_name = name\n            max_param_index = flat_index\n\nif max_param_name is not None:\n    for name, param in model.named_parameters():\n        if name == max_param_name:\n            max_param_coords = torch.unravel_index(max_param_index, param.data.shape)\n            param.data[max_param_coords] = 0\n            print(f\"Modified parameter: {name}\")\n            print(f\"Set the weight at {max_param_coords} (value {max_magnitude}) to 0.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-30T22:57:24.768779Z","iopub.execute_input":"2025-01-30T22:57:24.769565Z","iopub.status.idle":"2025-01-30T22:57:24.777363Z","shell.execute_reply.started":"2025-01-30T22:57:24.769533Z","shell.execute_reply":"2025-01-30T22:57:24.776506Z"}},"outputs":[{"name":"stdout","text":"Modified parameter: final_weight\nSet the weight at (tensor(15953, device='cuda:0'),) (value 0.6976040601730347) to 0.\n","output_type":"stream"}],"execution_count":70},{"cell_type":"code","source":"evaluate_model(model, custom_train_loader, criterion, optimizer, 1, scheduler, batch_size, num_features, early_stopping_patience=10000)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-30T22:57:25.010156Z","iopub.execute_input":"2025-01-30T22:57:25.010582Z","iopub.status.idle":"2025-01-30T22:57:25.494066Z","shell.execute_reply.started":"2025-01-30T22:57:25.010549Z","shell.execute_reply":"2025-01-30T22:57:25.493247Z"}},"outputs":[{"name":"stdout","text":"Epoch 1, Training Loss: 0.2164945900440216, Validation Loss: 0.30465009808540344\nEpoch 1, Training Loss: 0.21637515723705292, Validation Loss: 0.30465009808540344\nTraining Accuracy: 0.9402302000548095, Training F1 Score: 0.9374103602656251\nValidation Accuracy: 0.931711059958347, Validation F1 Score: 0.9289306248027878\nLearning Rate: 9.811203997458424e-19\n\nBest Validation Loss after 1 epochs: 0.30465009808540344 from Epoch 1\n","output_type":"stream"}],"execution_count":71},{"cell_type":"code","source":"for i in range(20):\n    max_magnitude = -float(\"inf\")\n    max_param_name = None\n    max_param_index = None\n    \n    for name, param in model.named_parameters():\n        if param.requires_grad:\n            weights = param.data\n            max_val, flat_index = torch.max(torch.abs(weights).view(-1), 0)\n            if max_val > max_magnitude:\n                max_magnitude = max_val\n                max_param_name = name\n                max_param_index = flat_index\n    \n    if max_param_name is not None:\n        for name, param in model.named_parameters():\n            if name == max_param_name:\n                max_param_coords = torch.unravel_index(max_param_index, param.data.shape)\n                param.data[max_param_coords] = 0\n                print(f\"Modified parameter: {name}\")\n                print(f\"Set the weight at {max_param_coords} (value {max_magnitude}) to 0.\")\n\n    evaluate_model(model, custom_train_loader, criterion, optimizer, 1, scheduler, batch_size, num_features, early_stopping_patience=10000)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-27T23:26:29.187546Z","iopub.execute_input":"2025-01-27T23:26:29.187891Z","iopub.status.idle":"2025-01-27T23:26:34.191636Z","shell.execute_reply.started":"2025-01-27T23:26:29.187861Z","shell.execute_reply":"2025-01-27T23:26:34.190609Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nweights = model.final_layer.a\n\nprint(torch.isinf(weights).any())\nprint(torch.isnan(weights).any())\n\nweights_numpy = weights.cpu().detach().numpy()\nsorted_weights = torch.sort(weights).values\n\nprint(sorted_weights)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-30T22:56:14.532420Z","iopub.execute_input":"2025-01-30T22:56:14.532749Z","iopub.status.idle":"2025-01-30T22:56:14.817372Z","shell.execute_reply.started":"2025-01-30T22:56:14.532723Z","shell.execute_reply":"2025-01-30T22:56:14.816253Z"}},"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","Cell \u001b[0;32mIn[59], line 5\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mplt\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mseaborn\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01msns\u001b[39;00m\n\u001b[0;32m----> 5\u001b[0m weights \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfinal_layer\u001b[49m\u001b[38;5;241m.\u001b[39ma\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28mprint\u001b[39m(torch\u001b[38;5;241m.\u001b[39misinf(weights)\u001b[38;5;241m.\u001b[39many())\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28mprint\u001b[39m(torch\u001b[38;5;241m.\u001b[39misnan(weights)\u001b[38;5;241m.\u001b[39many())\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1729\u001b[0m, in \u001b[0;36mModule.__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1727\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m modules:\n\u001b[1;32m   1728\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m modules[name]\n\u001b[0;32m-> 1729\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m object has no attribute \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n","\u001b[0;31mAttributeError\u001b[0m: 'TabularDenseNet' object has no attribute 'final_layer'"],"ename":"AttributeError","evalue":"'TabularDenseNet' object has no attribute 'final_layer'","output_type":"error"}],"execution_count":59},{"cell_type":"code","source":"optimizer = optim.Adam(model.parameters(), lr=0.001 * 0.1)\nscheduler = LambdaLR(optimizer, lr_lambda=custom_lr_lambda)\n\ncriterion = CustomLoss(nn.CrossEntropyLoss(), 0.0, 0.0, 0.0, 0.0, 0.0)\nevaluate_model(model, custom_train_loader, criterion, optimizer, 10000, scheduler, batch_size, num_features, early_stopping_patience=10000)\n\nprint(f\"Execution time: {(time.time() - start_time):.6f} seconds\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-27T21:10:41.999847Z","iopub.execute_input":"2025-01-27T21:10:42.000687Z","iopub.status.idle":"2025-01-27T21:22:06.141388Z","shell.execute_reply.started":"2025-01-27T21:10:42.000636Z","shell.execute_reply":"2025-01-27T21:22:06.140519Z"},"collapsed":true,"jupyter":{"outputs_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def custom_lr_lambda(step):\n    num_step_threshold = 100\n\n    if step < num_step_threshold:\n        return step / num_step_threshold\n    if step == num_step_threshold:\n        print(\"here\")\n    return 0.99999 ** (step - num_step_threshold)\n\nstart_time = time.time()\nmodel = TabularDenseNet(num_features, num_classes, 1, 1).to(device)\n\noptimizer = optim.Adam(model.parameters(), lr=0.001 * 1.0)\nscheduler = LambdaLR(optimizer, lr_lambda=custom_lr_lambda)\n\ncriterion = CustomLoss(nn.CrossEntropyLoss(), 0.0, 0.0, 0.0, 0.0, 0.0)\nevaluate_model(model, custom_train_loader, criterion, optimizer, 10000, scheduler, batch_size, num_features, early_stopping_patience=10000)\n\nprint(f\"Execution time: {(time.time() - start_time):.6f} seconds\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-27T08:22:12.804765Z","iopub.status.idle":"2025-01-27T08:22:12.805748Z","shell.execute_reply.started":"2025-01-27T08:22:12.805538Z","shell.execute_reply":"2025-01-27T08:22:12.805561Z"},"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"evaluate_model(model, custom_train_loader, criterion, optimizer, 100000, scheduler, batch_size, num_features, early_stopping_patience=100000000000)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-27T08:22:12.806975Z","iopub.status.idle":"2025-01-27T08:22:12.807319Z","shell.execute_reply.started":"2025-01-27T08:22:12.80715Z","shell.execute_reply":"2025-01-27T08:22:12.807168Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def custom_lr_lambda(step):\n    num_step_threshold = 100\n\n    if step < num_step_threshold:\n        return step / num_step_threshold\n    if step == num_step_threshold:\n        print(\"here\")\n    return 0.99999 ** (step - num_step_threshold)\n\nstart_time = time.time()\nmodel = TabularDenseNet(num_features, num_classes, 1, 1).to(device)\n\noptimizer = optim.Adam(model.parameters(), lr=0.001 * 1.0)\nscheduler = LambdaLR(optimizer, lr_lambda=custom_lr_lambda)\n\ncriterion = CustomLoss(nn.CrossEntropyLoss(), 0.0, 0.0, 0.0, 0.0, 0.0)\nevaluate_model(model, custom_train_loader, criterion, optimizer, 10000, scheduler, batch_size, num_features, early_stopping_patience=10000)\n\nprint(f\"Execution time: {(time.time() - start_time):.6f} seconds\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-27T08:22:12.808896Z","iopub.status.idle":"2025-01-27T08:22:12.809373Z","shell.execute_reply.started":"2025-01-27T08:22:12.809127Z","shell.execute_reply":"2025-01-27T08:22:12.80915Z"},"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def custom_lr_lambda(step):\n    num_step_threshold = 100\n\n    if step < num_step_threshold:\n        return step / num_step_threshold\n    if step == num_step_threshold:\n        print(\"here\")\n    return 0.99999 ** (step - num_step_threshold)\n\nstart_time = time.time()\nmodel = TabularDenseNet(num_features, num_classes, 1, 1).to(device)\n\noptimizer = optim.Adam(model.parameters(), lr=0.001 * 1.0)\nscheduler = LambdaLR(optimizer, lr_lambda=custom_lr_lambda)\n\ncriterion = CustomLoss(nn.CrossEntropyLoss(), 0.0, 0.0, 0.0, 0.0, 0.0)\nevaluate_model(model, custom_train_loader, criterion, optimizer, 10000, scheduler, batch_size, num_features, early_stopping_patience=10000)\n\nprint(f\"Execution time: {(time.time() - start_time):.6f} seconds\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-27T08:22:12.810553Z","iopub.status.idle":"2025-01-27T08:22:12.81102Z","shell.execute_reply.started":"2025-01-27T08:22:12.810761Z","shell.execute_reply":"2025-01-27T08:22:12.810785Z"},"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"for name, param in model.named_parameters():\n    print(name)\n    print(param)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-27T08:22:12.812322Z","iopub.status.idle":"2025-01-27T08:22:12.812765Z","shell.execute_reply.started":"2025-01-27T08:22:12.812538Z","shell.execute_reply":"2025-01-27T08:22:12.812563Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def custom_lr_lambda(step):\n    num_step_threshold = 100\n\n    if step < num_step_threshold:\n        return step / num_step_threshold\n    if step == num_step_threshold:\n        print(\"here\")\n    return 0.99999 ** (step - num_step_threshold)\n\nstart_time = time.time()\nmodel = TabularDenseNet(num_features, num_classes, 1, 2).to(device)\n\noptimizer = optim.Adam(model.parameters(), lr=0.001 * 0.1)\nscheduler = LambdaLR(optimizer, lr_lambda=custom_lr_lambda)\n\ncriterion = CustomLoss(nn.CrossEntropyLoss(), 0.0, 0.0, 0.0, 0.0, 0.0)\nevaluate_model(model, custom_train_loader, criterion, optimizer, 10000, scheduler, batch_size, num_features, early_stopping_patience=10000)\n\nprint(f\"Execution time: {(time.time() - start_time):.6f} seconds\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-27T08:22:12.814209Z","iopub.status.idle":"2025-01-27T08:22:12.814551Z","shell.execute_reply.started":"2025-01-27T08:22:12.814379Z","shell.execute_reply":"2025-01-27T08:22:12.814398Z"},"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def custom_lr_lambda(step):\n    num_step_threshold = 100\n\n    if step < num_step_threshold:\n        return step / num_step_threshold\n    if step == num_step_threshold:\n        print(\"here\")\n    return 0.99995 ** (step - num_step_threshold)\n\nstart_time = time.time()\nmodel = TabularDenseNet(num_features, num_classes, 1, 2).to(device)\n\noptimizer = optim.Adam(model.parameters(), lr=0.001 * 0.1)\nscheduler = LambdaLR(optimizer, lr_lambda=custom_lr_lambda)\n\ncriterion = CustomLoss(nn.CrossEntropyLoss(), 0.0, 0.0, 0.0, 0.0, 0.0)\nevaluate_model(model, custom_train_loader, criterion, optimizer, 10000, scheduler, batch_size, num_features, early_stopping_patience=10000)\n\nprint(f\"Execution time: {(time.time() - start_time):.6f} seconds\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-27T08:22:12.81574Z","iopub.status.idle":"2025-01-27T08:22:12.816096Z","shell.execute_reply.started":"2025-01-27T08:22:12.815922Z","shell.execute_reply":"2025-01-27T08:22:12.815941Z"},"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def custom_lr_lambda(step):\n    num_step_threshold = 100\n\n    if step < num_step_threshold:\n        return step / num_step_threshold\n    if step == num_step_threshold:\n        print(\"here\")\n    return 0.99995 ** (step - num_step_threshold)\n\nstart_time = time.time()\nmodel = TabularDenseNet(num_features, num_classes, 1, 1).to(device)\n\noptimizer = optim.Adam(model.parameters(), lr=0.001 * 1.0)\nscheduler = LambdaLR(optimizer, lr_lambda=custom_lr_lambda)\n\ncriterion = CustomLoss(nn.CrossEntropyLoss(), 0.0, 0.0, 0.0, 0.0, 0.0)\nevaluate_model(model, custom_train_loader, criterion, optimizer, 1000, scheduler, batch_size, num_features, early_stopping_patience=10000)\n\nprint(f\"Execution time: {(time.time() - start_time):.6f} seconds\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-27T08:22:12.817431Z","iopub.status.idle":"2025-01-27T08:22:12.817873Z","shell.execute_reply.started":"2025-01-27T08:22:12.817636Z","shell.execute_reply":"2025-01-27T08:22:12.81766Z"},"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def custom_lr_lambda(step):\n    num_step_threshold = 100\n\n    if step < num_step_threshold:\n        return step / num_step_threshold\n    if step == num_step_threshold:\n        print(\"here\")\n    return 0.99995 ** (step - num_step_threshold)\n\nstart_time = time.time()\nmodel = TabularDenseNet(num_features, num_classes, 1, 2).to(device)\n\noptimizer = optim.Adam(model.parameters(), lr=0.001 * 0.1)\nscheduler = LambdaLR(optimizer, lr_lambda=custom_lr_lambda)\n\ncriterion = CustomLoss(nn.CrossEntropyLoss(), 0.0, 0.0, 0.0, 0.0, 0.0)\nevaluate_model(model, custom_train_loader, criterion, optimizer, 100, scheduler, batch_size, num_features, early_stopping_patience=10000)\n\nprint(f\"Execution time: {(time.time() - start_time):.6f} seconds\")","metadata":{"trusted":true,"jupyter":{"source_hidden":true},"execution":{"iopub.status.busy":"2025-01-27T08:22:12.819283Z","iopub.status.idle":"2025-01-27T08:22:12.819639Z","shell.execute_reply.started":"2025-01-27T08:22:12.819464Z","shell.execute_reply":"2025-01-27T08:22:12.819482Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def custom_lr_lambda(step):\n    num_step_threshold = 100\n\n    if step < num_step_threshold:\n        return step / num_step_threshold\n    if step == num_step_threshold:\n        print(\"here\")\n    return 0.99995 ** (step - num_step_threshold)\n\nstart_time = time.time()\nmodel = TabularDenseNet(num_features, num_classes, 1, 2).to(device)\n\noptimizer = optim.Adam(model.parameters(), lr=0.001 * 0.1)\nscheduler = LambdaLR(optimizer, lr_lambda=custom_lr_lambda)\n\ncriterion = CustomLoss(nn.CrossEntropyLoss(), 0.0, 0.0, 0.0, 0.0, 0.0)\nevaluate_model(model, custom_train_loader, criterion, optimizer, 100, scheduler, batch_size, num_features, early_stopping_patience=10000)\n\nprint(f\"Execution time: {(time.time() - start_time):.6f} seconds\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-27T08:22:12.824535Z","iopub.execute_input":"2025-01-27T08:22:12.824862Z","iopub.status.idle":"2025-01-27T08:22:12.864897Z","shell.execute_reply.started":"2025-01-27T08:22:12.824825Z","shell.execute_reply":"2025-01-27T08:22:12.86365Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def custom_lr_lambda(step):\n    num_step_threshold = 100\n\n    if step < num_step_threshold:\n        return step / num_step_threshold\n    if step == num_step_threshold:\n        print(\"here\")\n    return 0.99995 ** (step - num_step_threshold)\n\nstart_time = time.time()\nmodel = TabularDenseNet(num_features, num_classes, 1, 2).to(device)\n\noptimizer = optim.Adam(model.parameters(), lr=0.001 * 0.1)\nscheduler = LambdaLR(optimizer, lr_lambda=custom_lr_lambda)\n\ncriterion = CustomLoss(nn.CrossEntropyLoss(), 0.0, 0.0, 0.0, 0.0, 0.0)\nevaluate_model(model, custom_train_loader, criterion, optimizer, 100, scheduler, batch_size, num_features, early_stopping_patience=10000)\n\nprint(f\"Execution time: {(time.time() - start_time):.6f} seconds\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-27T08:22:12.865666Z","iopub.status.idle":"2025-01-27T08:22:12.866031Z","shell.execute_reply.started":"2025-01-27T08:22:12.865834Z","shell.execute_reply":"2025-01-27T08:22:12.865852Z"},"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def custom_lr_lambda(step):\n    num_step_threshold = 100\n\n    if step < num_step_threshold:\n        return step / num_step_threshold\n    if step == num_step_threshold:\n        print(\"here\")\n    return 0.99995 ** (step - num_step_threshold)\n\nstart_time = time.time()\nmodel = TabularDenseNet(num_features, num_classes, 1, 2).to(device)\n\noptimizer = optim.Adam(model.parameters(), lr=0.001 * 0.1)\nscheduler = LambdaLR(optimizer, lr_lambda=custom_lr_lambda)\n\ncriterion = CustomLoss(nn.CrossEntropyLoss(), 0.0, 0.0, 0.0, 0.0, 0.0)\nevaluate_model(model, custom_train_loader, criterion, optimizer, 100, scheduler, batch_size, num_features, early_stopping_patience=10000)\n\nprint(f\"Execution time: {(time.time() - start_time):.6f} seconds\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-27T08:22:12.867868Z","iopub.status.idle":"2025-01-27T08:22:12.868331Z","shell.execute_reply.started":"2025-01-27T08:22:12.868098Z","shell.execute_reply":"2025-01-27T08:22:12.868121Z"},"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def custom_lr_lambda(step):\n    num_step_threshold = 100\n\n    if step < num_step_threshold:\n        return step / num_step_threshold\n    if step == num_step_threshold:\n        print(\"here\")\n    return 0.99995 ** (step - num_step_threshold)\n\nstart_time = time.time()\nmodel = TabularDenseNet(num_features, num_classes, 1, 2).to(device)\n\noptimizer = optim.Adam(model.parameters(), lr=0.001 * 1.0)\nscheduler = LambdaLR(optimizer, lr_lambda=custom_lr_lambda)\n\ncriterion = CustomLoss(nn.CrossEntropyLoss(), 0.0, 0.0, 0.0, 0.0, 0.0)\nevaluate_model(model, custom_train_loader, criterion, optimizer, 1000, scheduler, batch_size, num_features, early_stopping_patience=10000)\n\nprint(f\"Execution time: {(time.time() - start_time):.6f} seconds\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-27T08:22:12.86912Z","iopub.status.idle":"2025-01-27T08:22:12.869556Z","shell.execute_reply.started":"2025-01-27T08:22:12.869323Z","shell.execute_reply":"2025-01-27T08:22:12.869346Z"},"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def custom_lr_lambda(step):\n    num_step_threshold = 100\n\n    if step < num_step_threshold:\n        return step / num_step_threshold\n    if step == num_step_threshold:\n        print(\"here\")\n    return 0.99995 ** (step - num_step_threshold)\n\nstart_time = time.time()\nmodel = TabularDenseNet(num_features, num_classes, 1, 8).to(device)\n\noptimizer = optim.Adam(model.parameters(), lr=0.001 * 0.01)\nscheduler = LambdaLR(optimizer, lr_lambda=custom_lr_lambda)\n\ncriterion = CustomLoss(nn.CrossEntropyLoss(), 0.0, 0.0, 0.0, 0.0, 0.0)\nevaluate_model(model, custom_train_loader, criterion, optimizer, 1000, scheduler, batch_size, num_features, early_stopping_patience=10000)\n\nprint(f\"Execution time: {(time.time() - start_time):.6f} seconds\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-27T08:22:12.890115Z","iopub.execute_input":"2025-01-27T08:22:12.890393Z","iopub.status.idle":"2025-01-27T08:22:12.923902Z","shell.execute_reply.started":"2025-01-27T08:22:12.890364Z","shell.execute_reply":"2025-01-27T08:22:12.922477Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def custom_lr_lambda(step):\n    num_step_threshold = 100\n\n    if step < num_step_threshold:\n        return step / num_step_threshold\n    if step == num_step_threshold:\n        print(\"here\")\n    return 0.99995 ** (step - num_step_threshold)\n\nstart_time = time.time()\nmodel = TabularDenseNet(num_features, num_classes, 1, 10).to(device)\n\noptimizer = optim.Adam(model.parameters(), lr=0.001 * 0.01)\nscheduler = LambdaLR(optimizer, lr_lambda=custom_lr_lambda)\n\ncriterion = CustomLoss(nn.CrossEntropyLoss(), 0.0, 0.0, 0.0, 0.0, 0.0)\nevaluate_model(model, custom_train_loader, criterion, optimizer, 1000, scheduler, batch_size, num_features, early_stopping_patience=10000)\n\nprint(f\"Execution time: {(time.time() - start_time):.6f} seconds\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-27T08:22:12.924735Z","iopub.status.idle":"2025-01-27T08:22:12.925192Z","shell.execute_reply.started":"2025-01-27T08:22:12.92496Z","shell.execute_reply":"2025-01-27T08:22:12.924985Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"total_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\nprint(f\"Total number of trainable parameters: {total_params}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-30T22:55:59.899446Z","iopub.execute_input":"2025-01-30T22:55:59.900224Z","iopub.status.idle":"2025-01-30T22:55:59.904761Z","shell.execute_reply.started":"2025-01-30T22:55:59.900165Z","shell.execute_reply":"2025-01-30T22:55:59.903865Z"}},"outputs":[{"name":"stdout","text":"Total number of trainable parameters: 16901\n","output_type":"stream"}],"execution_count":57},{"cell_type":"code","source":"total_params = sum(p.numel() for p in model.parameters())\nprint(f\"Total number of parameters (trainable + non-trainable): {total_params}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-30T22:56:00.657132Z","iopub.execute_input":"2025-01-30T22:56:00.657523Z","iopub.status.idle":"2025-01-30T22:56:00.662859Z","shell.execute_reply.started":"2025-01-30T22:56:00.657492Z","shell.execute_reply":"2025-01-30T22:56:00.661719Z"}},"outputs":[{"name":"stdout","text":"Total number of parameters (trainable + non-trainable): 16901\n","output_type":"stream"}],"execution_count":58},{"cell_type":"code","source":"for name, param in model.named_parameters():\n    print(name)\n    print(param)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-27T08:22:12.956015Z","iopub.execute_input":"2025-01-27T08:22:12.95644Z","iopub.status.idle":"2025-01-27T08:22:13.005797Z","shell.execute_reply.started":"2025-01-27T08:22:12.956414Z","shell.execute_reply":"2025-01-27T08:22:13.005026Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"data = pd.read_csv('/kaggle/input/playground-series-s4e10/test.csv')\n\ndata = data.drop([\"id\"], axis=1)\ndata['source'] = 0\n\ndata['person_emp_length_missing'] = data['person_emp_length'].isna().astype(int)\ndata['loan_int_rate_missing'] = data['loan_int_rate'].isna().astype(int)\n\ndata['person_emp_length'] = data['person_emp_length'].fillna(median_emp_length)\ndata['loan_int_rate'] = data['loan_int_rate'].fillna(median_int_rate)\n\n# grade_mapping = {'A': 7, 'B': 6, 'C': 5, 'D': 4, 'E': 3, 'F': 2, 'G': 1}\n# data['loan_grade'] = data['loan_grade'].map(grade_mapping)\n\n# purpose_mapping = {\n#     'DEBTCONSOLIDATION': 1,\n#     'HOMEIMPROVEMENT': 2,\n#     'MEDICAL': 3,\n#     'PERSONAL': 4,\n#     'EDUCATION': 5,\n#     'VENTURE': 6\n# }\n# data['loan_intent'] = data['loan_intent'].map(purpose_mapping)\n\n# home_ownership_mapping = {\n#     'OWN': 1,\n#     'MORTGAGE': 2,\n#     'OTHER': 3,\n#     'RENT': 4\n# }\n# data['person_home_ownership'] = data['person_home_ownership'].map(home_ownership_mapping)\n\nX = data.drop([], axis=1)\nX = pd.get_dummies(X, drop_first=True)\n\ncolumn_to_log = [\n    'person_age',\n    'person_income',\n]\n\ncolumn_to_sqrt = [\n    'person_emp_length',\n    'loan_percent_income',\n]\n\nfor col in column_to_log:\n    if (X[col] <= 0).any():\n        print(f\"Column '{col}' contains non-positive values. Adding 1 to avoid log of non-positive numbers.\")\n        X[col] = np.log(X[col] + 1)\n    else:\n        X[col] = np.log(X[col])\n\nfor col in column_to_sqrt:\n    if (X[col] < 0).any():\n        print(f\"Column '{col}' contains negative values. Setting negative values to NaN before applying sqrt.\")\n        X[col] = np.sqrt(X[col].clip(lower=0))\n    else:\n        X[col] = np.sqrt(X[col])\n\nprint(data.isnull().sum())\nprint(X.columns)\nprint(X.columns.get_loc('source'))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-27T08:22:13.007131Z","iopub.execute_input":"2025-01-27T08:22:13.007393Z","iopub.status.idle":"2025-01-27T08:22:13.102256Z","shell.execute_reply.started":"2025-01-27T08:22:13.007363Z","shell.execute_reply":"2025-01-27T08:22:13.101387Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(X)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-27T08:22:13.103433Z","iopub.execute_input":"2025-01-27T08:22:13.104109Z","iopub.status.idle":"2025-01-27T08:22:13.118681Z","shell.execute_reply.started":"2025-01-27T08:22:13.104067Z","shell.execute_reply":"2025-01-27T08:22:13.117927Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(X.shape)\nX_scaled_test = x_scaler.transform(X)\nprint(X_scaled_test.shape)\nprint(X_scaled_test)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-27T08:22:13.120609Z","iopub.execute_input":"2025-01-27T08:22:13.121193Z","iopub.status.idle":"2025-01-27T08:22:13.151321Z","shell.execute_reply.started":"2025-01-27T08:22:13.121151Z","shell.execute_reply":"2025-01-27T08:22:13.150563Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"X_scaled_test_tensor = torch.tensor(X_scaled_test).float().to(device)\noutputs = model(X_scaled_test_tensor)\nprint(outputs)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-27T08:22:13.152179Z","iopub.execute_input":"2025-01-27T08:22:13.15241Z","iopub.status.idle":"2025-01-27T08:22:13.172282Z","shell.execute_reply.started":"2025-01-27T08:22:13.152386Z","shell.execute_reply":"2025-01-27T08:22:13.17145Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"probabilities = F.softmax(outputs, dim=1)\nprint(probabilities)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-27T08:22:13.173281Z","iopub.execute_input":"2025-01-27T08:22:13.173543Z","iopub.status.idle":"2025-01-27T08:22:13.179639Z","shell.execute_reply.started":"2025-01-27T08:22:13.173518Z","shell.execute_reply":"2025-01-27T08:22:13.178764Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"positive_class_probs = probabilities[:, 1]\nprint(positive_class_probs)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-27T08:22:13.180742Z","iopub.execute_input":"2025-01-27T08:22:13.181081Z","iopub.status.idle":"2025-01-27T08:22:13.193244Z","shell.execute_reply.started":"2025-01-27T08:22:13.181044Z","shell.execute_reply":"2025-01-27T08:22:13.192326Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import pandas as pd\n\ntest_df = pd.read_csv('/kaggle/input/playground-series-s4e10/test.csv')\nids = test_df['id']\n\npositive_class_probs = positive_class_probs.cpu().detach().numpy()\n\nsubmission_df = pd.DataFrame({\n    'id': ids,\n    'loan_status': positive_class_probs\n})\n\nsubmission_df.to_csv('submission.csv', index=False)\nprint(\"Submission file created successfully.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-27T08:22:13.194238Z","iopub.execute_input":"2025-01-27T08:22:13.194502Z","iopub.status.idle":"2025-01-27T08:22:13.290244Z","shell.execute_reply.started":"2025-01-27T08:22:13.194474Z","shell.execute_reply":"2025-01-27T08:22:13.289354Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}