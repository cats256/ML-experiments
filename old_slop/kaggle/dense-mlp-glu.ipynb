{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":30732,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import torch\nimport numpy as np\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torchinfo import summary\nimport torch.nn.functional as F\nfrom sklearn.model_selection import train_test_split\nfrom torch.utils.data import DataLoader, TensorDataset\nfrom torch.profiler import profile, record_function, ProfilerActivity\nimport numpy as np\nfrom sklearn.preprocessing import StandardScaler\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport torch","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-06-15T23:57:05.426506Z","iopub.execute_input":"2024-06-15T23:57:05.426944Z","iopub.status.idle":"2024-06-15T23:57:05.432955Z","shell.execute_reply.started":"2024-06-15T23:57:05.426918Z","shell.execute_reply":"2024-06-15T23:57:05.432050Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import DataLoader, TensorDataset\nfrom sklearn.model_selection import train_test_split\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport imageio\nimport os\n\nclass CustomDataLoader:\n    def __init__(self, features, labels, batch_size=1, validation_size=0.0, shuffle=False):\n        if validation_size > 0:\n            train_data, val_data, train_labels, val_labels = train_test_split(\n                features, labels, test_size=validation_size, random_state=42\n            )\n            self.train_loader = DataLoader(\n                TensorDataset(torch.tensor(train_data).float(), torch.tensor(train_labels).float()),\n                batch_size=batch_size,\n                shuffle=shuffle,\n            )\n            self.val_loader = DataLoader(\n                TensorDataset(torch.tensor(val_data).float(), torch.tensor(val_labels).float()), batch_size=batch_size, shuffle=shuffle\n            )\n        else:\n            self.train_loader = DataLoader(\n                TensorDataset(torch.tensor(features).float(), torch.tensor(labels).float()), batch_size=batch_size, shuffle=shuffle\n            )\n            self.val_loader = None\n\n    def get_train_loader(self):\n        return self.train_loader\n\n    def get_val_loader(self):\n        return self.val_loader\n\ndef evaluate_model(model, custom_train_loader, criterion, optimizer):\n    num_epochs = 1200\n    parameters = []\n    image_folder = 'training_images'\n    os.makedirs(image_folder, exist_ok=True)\n    \n#     num = 10000\n#     x = np.linspace(-6, 6, num)\n#     y = np.linspace(-0, 0, num)\n#     random_feature1 = np.linspace(-0, 0, num)\n#     random_feature2 = np.linspace(-0, 0, num)\n\n#     inputs = np.stack([x], axis=1)\n#     inputs_tensor = torch.from_numpy(inputs).float().to(device)\n\n#     model.eval()\n#     with torch.no_grad():\n#         y_pred_model = model(inputs_tensor).cpu().numpy()\n\n#     a = x\n#     y_pred_manual = np.sin(a) + 2 * np.cos(a + 3 * np.sin(a)) + 3 * np.cos(a) ** 2 * np.sin(a) ** 2 + 0.5 * np.cos(a)\n\n#     plt.figure(figsize=(10, 5))\n#     plt.scatter(x, y_pred_model.flatten(), label='Model Output', s=1, alpha=0.1)\n#     plt.scatter(x, y_pred_manual, label='Manual Calculation', s=1, alpha=0.1)\n#     plt.xlabel('Input Feature 1')\n#     plt.ylabel('Output')\n#     plt.legend()\n#     plt.grid(True)\n#     plt.savefig(f\"{image_folder}/epoch_0000.png\")\n#     plt.close()\n\n    for epoch in range(num_epochs):\n        running_loss = 0.0\n        for inputs, labels in custom_train_loader.get_train_loader():\n            inputs, labels = inputs.to(device), labels.to(device)\n            optimizer.zero_grad()\n            outputs = model(inputs.view(-1, 2))\n            loss = criterion(outputs, labels.view(-1, 1))\n\n            if torch.isnan(loss):\n                print(\"Loss is NaN or Inf\")\n                print(parameters)\n\n                for name, param in model.named_parameters():\n                    print(f\"{name}: {param}\")\n                break\n\n            parameters = []\n            for name, param in model.named_parameters():\n                parameters.append(f\"{name}: {param}\")\n\n            loss.backward()\n            optimizer.step()\n\n            running_loss += loss.item()\n        avg_train_loss = running_loss / len(custom_train_loader.get_train_loader())\n            \n        model.eval()\n        running_val_loss = 0.0\n        with torch.no_grad():\n            for inputs, labels in custom_train_loader.get_val_loader():\n                inputs, labels = inputs.to(device), labels.to(device)\n                outputs = model(inputs.view(-1, 2))\n                val_loss = criterion(outputs, labels.view(-1, 1))\n                running_val_loss += val_loss.item()\n\n        avg_val_loss = running_val_loss / len(custom_train_loader.get_val_loader())\n        print(f\"Epoch {epoch+1:4d} | Train Loss: {avg_train_loss:10.4f} | Validation Loss: {avg_val_loss:10.4f}\")\n\n        num = 10000\n        \n        columns = []\n        \n        x = np.linspace(-6, 6, num).reshape(-1, 1)\n        y = np.linspace(-0, 0, num)\n        random_feature1 = np.linspace(-0, 0, num)\n        random_feature2 = np.linspace(-0, 0, num)\n\n        const_array = np.full((num, 1), const)\n\n#         inputs = np.hstack([x, const_array])\n#         inputs_tensor = torch.from_numpy(inputs).float().to(device)\n\n        for bias in biases:\n            column = x + bias\n            columns.append(column)\n        columns.append(const_array)\n        \n        inputs = np.column_stack(columns)\n        inputs_tensor = torch.from_numpy(inputs).float().to(device)\n\n        model.eval()\n        with torch.no_grad():\n            y_pred_model = model(inputs_tensor).cpu().numpy()\n\n        a = x\n        y_pred_manual = np.sin(a) + 2 * np.cos(a + 3 * np.sin(a)) + 3 * np.cos(a) ** 2 * np.sin(a) ** 2 + 0.5 * np.cos(a)\n        \n        plt.figure(figsize=(10, 5))\n        plt.scatter(x, y_pred_model.flatten(), label='Model Output', s=1, alpha=0.1)\n        plt.scatter(x, y_pred_manual, label='Manual Calculation', s=1, alpha=0.1)\n        plt.xlabel('Input Feature 1')\n        plt.ylabel('Output')\n        plt.legend()\n        plt.grid(True)\n        plt.savefig(f\"{image_folder}/epoch_{epoch+1:04d}.png\")\n        plt.close()\n\n    # Create a GIF from the saved images\n    images = []\n    for epoch in range(num_epochs):\n        filename = f\"{image_folder}/epoch_{epoch+1:04d}.png\"\n        images.append(imageio.imread(filename))\n    imageio.mimsave('training_progress.gif', images, duration=0.2)","metadata":{"execution":{"iopub.status.busy":"2024-06-15T23:57:05.874231Z","iopub.execute_input":"2024-06-15T23:57:05.874877Z","iopub.status.idle":"2024-06-15T23:57:05.955138Z","shell.execute_reply.started":"2024-06-15T23:57:05.874833Z","shell.execute_reply":"2024-06-15T23:57:05.954374Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"import numpy as np\nnum_samples = 128 * 100\nimport math\n\na = (np.random.rand(num_samples, 1) - 0.5) * 12\nb = (np.random.rand(num_samples, 1) - 0.5) * 8\nc = (np.random.rand(num_samples, 1) - 0.5) * 4\nd = (np.random.rand(num_samples, 1) - 0.5) * 4\ne = (np.random.rand(num_samples, 1) - 0.5) * 4\n\nconst = 1\nconst_array = np.full((num_samples, 1), const)\n\n# x_train = np.hstack([a, const_array])\ncolumns = []\nbiases = []\n\nfor _ in range(1):\n    bias = np.random.uniform(-1, 1) \n    biases.append(bias)\n    column = a + 0\n    columns.append(column)\n\ncolumns.append(const_array)\nbiases = np.array(biases)\n\nx_train = np.column_stack(columns)\ny_train = np.abs(a)\ny_train = np.sin(a) + 2 * np.cos(a + 3 * np.sin(a)) + 3 * np.cos(a) ** 2 * np.sin(a) ** 2 + 0.5 * np.cos(a)\n\ncustom_train_loader = CustomDataLoader(x_train, y_train, batch_size=128, validation_size=0.2, shuffle=True)","metadata":{"execution":{"iopub.status.busy":"2024-06-15T23:57:06.801826Z","iopub.execute_input":"2024-06-15T23:57:06.802557Z","iopub.status.idle":"2024-06-15T23:57:06.847842Z","shell.execute_reply.started":"2024-06-15T23:57:06.802525Z","shell.execute_reply":"2024-06-15T23:57:06.847083Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")","metadata":{"execution":{"iopub.status.busy":"2024-06-15T23:57:07.578128Z","iopub.execute_input":"2024-06-15T23:57:07.579223Z","iopub.status.idle":"2024-06-15T23:57:07.609336Z","shell.execute_reply.started":"2024-06-15T23:57:07.579178Z","shell.execute_reply":"2024-06-15T23:57:07.608355Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nfrom numpy import pi\n\nclass CustomLayer1(nn.Module):\n    def __init__(self, input_dim, output_dim):\n        super(CustomLayer1, self).__init__()\n        self.sigmoid = nn.Sigmoid()\n        self.hard_sigmoid = nn.Hardsigmoid()\n        self.linear_sigmoid = nn.Linear(input_dim, output_dim, bias=False)\n        self.relu = nn.ReLU()\n        self.softplus = nn.Softplus()\n        self.tanh = nn.Tanh()\n\n        nn.init.zeros_(self.linear_sigmoid.weight)\n#         nn.init.zeros_(self.linear_sigmoid.bias)\n\n    def forward(self, x, prev_x):\n        sigmoid_gate = self.sigmoid(2 * self.linear_sigmoid(prev_x))\n        return x * sigmoid_gate * 2","metadata":{"execution":{"iopub.status.busy":"2024-06-15T23:57:08.370311Z","iopub.execute_input":"2024-06-15T23:57:08.370744Z","iopub.status.idle":"2024-06-15T23:57:08.380789Z","shell.execute_reply.started":"2024-06-15T23:57:08.370712Z","shell.execute_reply":"2024-06-15T23:57:08.379843Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"class CustomActivationLayerTwo(nn.Module):\n    def __init__(self, num_features):\n        super(CustomActivationLayerTwo, self).__init__()\n        self.beta = nn.Parameter(torch.zeros(num_features))\n\n    def forward(self, inputs):\n        return torch.where(inputs < 0, inputs * self.beta, inputs)","metadata":{"execution":{"iopub.status.busy":"2024-06-15T23:57:09.085753Z","iopub.execute_input":"2024-06-15T23:57:09.086387Z","iopub.status.idle":"2024-06-15T23:57:09.091706Z","shell.execute_reply.started":"2024-06-15T23:57:09.086354Z","shell.execute_reply":"2024-06-15T23:57:09.090782Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"class PairwiseCustomActivationNetwork(nn.Module):\n    def __init__(self, input_size, num_layers, output_size):\n        super(PairwiseCustomActivationNetwork, self).__init__()\n\n        self.num_layers = num_layers\n        self.layers = nn.ModuleList()\n        self.custom_layers = nn.ModuleList()\n        self.softplus = nn.Softplus()\n        self.relu = nn.ReLU()\n   \n        layer_size = input_size\n        for i in range(1, num_layers):\n            self.layers.append(nn.Linear(layer_size, layer_size, bias=True))\n#             self.custom_layers.append(CustomActivationLayerTwo(layer_size))\n            self.custom_layers.append(CustomLayer1(layer_size, layer_size))\n            layer_size *= 2\n    \n        self.layers.append(nn.Linear(layer_size, output_size))\n#         self.custom_layers.append(CustomActivationLayerTwo(output_size))\n        self.custom_layers.append(CustomLayer1(layer_size, output_size))\n        self._initialize_weights()\n\n    def forward(self, x):\n        outputs = [x]\n        \n        for layer, custom_layer in zip(self.layers, self.custom_layers):\n            concatenated_outputs = torch.cat(outputs, dim=1)\n            out = (layer(concatenated_outputs))\n            out = custom_layer(out, concatenated_outputs)\n            outputs.append(out)\n\n        return outputs[-1]\n    \n    def _initialize_weights(self):\n        for i, layer in enumerate(self.layers):\n            if isinstance(layer, nn.Linear):\n                if i == len(self.layers) - 1:\n                    layer.weight.data.fill_(0)\n                else:\n                    in_features = layer.weight.size(1)\n                    eye_matrix = torch.eye(in_features)\n                    layer.weight.data = eye_matrix\n                if layer.bias is not None:\n                    nn.init.normal_(layer.bias, mean=0.0, std=0.1)\n                    layer.bias.data.zero_()\n\nmodel_prepu = PairwiseCustomActivationNetwork(2, 12, 1).to(device)\nprint(summary(model_prepu, input_size=(1, 2)))\ncriterion = nn.MSELoss()\noptimizer = optim.SGD(model_prepu.parameters(), lr=0.0000001)\noptimizer = optim.Adam(model_prepu.parameters(), lr=0.000001)","metadata":{"execution":{"iopub.status.busy":"2024-06-16T00:37:52.688717Z","iopub.execute_input":"2024-06-16T00:37:52.689084Z","iopub.status.idle":"2024-06-16T00:37:52.833815Z","shell.execute_reply.started":"2024-06-16T00:37:52.689055Z","shell.execute_reply":"2024-06-16T00:37:52.832922Z"},"trusted":true},"execution_count":35,"outputs":[{"name":"stdout","text":"==========================================================================================\nLayer (type:depth-idx)                   Output Shape              Param #\n==========================================================================================\nPairwiseCustomActivationNetwork          [1, 1]                    --\n├─ModuleList: 1-23                       --                        (recursive)\n│    └─Linear: 2-1                       [1, 2]                    6\n├─ModuleList: 1-24                       --                        (recursive)\n│    └─CustomLayer1: 2-2                 [1, 2]                    --\n│    │    └─Linear: 3-1                  [1, 2]                    4\n│    │    └─Sigmoid: 3-2                 [1, 2]                    --\n├─ModuleList: 1-23                       --                        (recursive)\n│    └─Linear: 2-3                       [1, 4]                    20\n├─ModuleList: 1-24                       --                        (recursive)\n│    └─CustomLayer1: 2-4                 [1, 4]                    --\n│    │    └─Linear: 3-3                  [1, 4]                    16\n│    │    └─Sigmoid: 3-4                 [1, 4]                    --\n├─ModuleList: 1-23                       --                        (recursive)\n│    └─Linear: 2-5                       [1, 8]                    72\n├─ModuleList: 1-24                       --                        (recursive)\n│    └─CustomLayer1: 2-6                 [1, 8]                    --\n│    │    └─Linear: 3-5                  [1, 8]                    64\n│    │    └─Sigmoid: 3-6                 [1, 8]                    --\n├─ModuleList: 1-23                       --                        (recursive)\n│    └─Linear: 2-7                       [1, 16]                   272\n├─ModuleList: 1-24                       --                        (recursive)\n│    └─CustomLayer1: 2-8                 [1, 16]                   --\n│    │    └─Linear: 3-7                  [1, 16]                   256\n│    │    └─Sigmoid: 3-8                 [1, 16]                   --\n├─ModuleList: 1-23                       --                        (recursive)\n│    └─Linear: 2-9                       [1, 32]                   1,056\n├─ModuleList: 1-24                       --                        (recursive)\n│    └─CustomLayer1: 2-10                [1, 32]                   --\n│    │    └─Linear: 3-9                  [1, 32]                   1,024\n│    │    └─Sigmoid: 3-10                [1, 32]                   --\n├─ModuleList: 1-23                       --                        (recursive)\n│    └─Linear: 2-11                      [1, 64]                   4,160\n├─ModuleList: 1-24                       --                        (recursive)\n│    └─CustomLayer1: 2-12                [1, 64]                   --\n│    │    └─Linear: 3-11                 [1, 64]                   4,096\n│    │    └─Sigmoid: 3-12                [1, 64]                   --\n├─ModuleList: 1-23                       --                        (recursive)\n│    └─Linear: 2-13                      [1, 128]                  16,512\n├─ModuleList: 1-24                       --                        (recursive)\n│    └─CustomLayer1: 2-14                [1, 128]                  --\n│    │    └─Linear: 3-13                 [1, 128]                  16,384\n│    │    └─Sigmoid: 3-14                [1, 128]                  --\n├─ModuleList: 1-23                       --                        (recursive)\n│    └─Linear: 2-15                      [1, 256]                  65,792\n├─ModuleList: 1-24                       --                        (recursive)\n│    └─CustomLayer1: 2-16                [1, 256]                  --\n│    │    └─Linear: 3-15                 [1, 256]                  65,536\n│    │    └─Sigmoid: 3-16                [1, 256]                  --\n├─ModuleList: 1-23                       --                        (recursive)\n│    └─Linear: 2-17                      [1, 512]                  262,656\n├─ModuleList: 1-24                       --                        (recursive)\n│    └─CustomLayer1: 2-18                [1, 512]                  --\n│    │    └─Linear: 3-17                 [1, 512]                  262,144\n│    │    └─Sigmoid: 3-18                [1, 512]                  --\n├─ModuleList: 1-23                       --                        (recursive)\n│    └─Linear: 2-19                      [1, 1024]                 1,049,600\n├─ModuleList: 1-24                       --                        (recursive)\n│    └─CustomLayer1: 2-20                [1, 1024]                 --\n│    │    └─Linear: 3-19                 [1, 1024]                 1,048,576\n│    │    └─Sigmoid: 3-20                [1, 1024]                 --\n├─ModuleList: 1-23                       --                        (recursive)\n│    └─Linear: 2-21                      [1, 2048]                 4,196,352\n├─ModuleList: 1-24                       --                        (recursive)\n│    └─CustomLayer1: 2-22                [1, 2048]                 --\n│    │    └─Linear: 3-21                 [1, 2048]                 4,194,304\n│    │    └─Sigmoid: 3-22                [1, 2048]                 --\n├─ModuleList: 1-23                       --                        (recursive)\n│    └─Linear: 2-23                      [1, 1]                    4,097\n├─ModuleList: 1-24                       --                        (recursive)\n│    └─CustomLayer1: 2-24                [1, 1]                    --\n│    │    └─Linear: 3-23                 [1, 1]                    4,096\n│    │    └─Sigmoid: 3-24                [1, 1]                    --\n==========================================================================================\nTotal params: 11,197,095\nTrainable params: 11,197,095\nNon-trainable params: 0\nTotal mult-adds (M): 11.20\n==========================================================================================\nInput size (MB): 0.00\nForward/backward pass size (MB): 0.07\nParams size (MB): 44.79\nEstimated Total Size (MB): 44.85\n==========================================================================================\n","output_type":"stream"}]},{"cell_type":"code","source":"optimizer = optim.SGD(model_prepu.parameters(), lr=0.000001 * 10)\noptimizer = optim.Adam(model_prepu.parameters(), lr=0.000001)\nevaluate_model(model_prepu, custom_train_loader, criterion, optimizer)","metadata":{"execution":{"iopub.status.busy":"2024-06-16T00:37:54.903361Z","iopub.execute_input":"2024-06-16T00:37:54.903728Z"},"trusted":true},"execution_count":null,"outputs":[{"name":"stdout","text":"Epoch    1 | Train Loss:     1.5332 | Validation Loss:     1.4640\nEpoch    2 | Train Loss:     1.4550 | Validation Loss:     1.4441\nEpoch    3 | Train Loss:     1.4422 | Validation Loss:     1.4411\nEpoch    4 | Train Loss:     1.4317 | Validation Loss:     1.4255\nEpoch    5 | Train Loss:     1.4221 | Validation Loss:     1.4141\nEpoch    6 | Train Loss:     1.4106 | Validation Loss:     1.4045\nEpoch    7 | Train Loss:     1.4010 | Validation Loss:     1.3888\nEpoch    8 | Train Loss:     1.3868 | Validation Loss:     1.3796\nEpoch    9 | Train Loss:     1.3665 | Validation Loss:     1.3470\nEpoch   10 | Train Loss:     1.3333 | Validation Loss:     1.2943\nEpoch   11 | Train Loss:     1.2638 | Validation Loss:     1.2111\nEpoch   12 | Train Loss:     1.1964 | Validation Loss:     1.1562\nEpoch   13 | Train Loss:     1.1539 | Validation Loss:     1.1047\nEpoch   14 | Train Loss:     1.1022 | Validation Loss:     1.0414\nEpoch   15 | Train Loss:     1.0290 | Validation Loss:     0.9500\nEpoch   16 | Train Loss:     0.9278 | Validation Loss:     0.8386\nEpoch   17 | Train Loss:     0.8242 | Validation Loss:     0.7627\nEpoch   18 | Train Loss:     0.7702 | Validation Loss:     0.7156\nEpoch   19 | Train Loss:     0.7554 | Validation Loss:     0.7130\nEpoch   20 | Train Loss:     0.7459 | Validation Loss:     0.7069\nEpoch   21 | Train Loss:     0.7437 | Validation Loss:     0.7007\nEpoch   22 | Train Loss:     0.7395 | Validation Loss:     0.6967\nEpoch   23 | Train Loss:     0.7350 | Validation Loss:     0.6991\nEpoch   24 | Train Loss:     0.7267 | Validation Loss:     0.6871\nEpoch   25 | Train Loss:     0.7225 | Validation Loss:     0.6819\nEpoch   26 | Train Loss:     0.7185 | Validation Loss:     0.6799\nEpoch   27 | Train Loss:     0.7153 | Validation Loss:     0.6738\nEpoch   28 | Train Loss:     0.7086 | Validation Loss:     0.6884\nEpoch   29 | Train Loss:     0.7050 | Validation Loss:     0.6744\nEpoch   30 | Train Loss:     0.6973 | Validation Loss:     0.6577\nEpoch   31 | Train Loss:     0.6874 | Validation Loss:     0.6499\nEpoch   32 | Train Loss:     0.6793 | Validation Loss:     0.6444\nEpoch   33 | Train Loss:     0.6731 | Validation Loss:     0.6390\nEpoch   34 | Train Loss:     0.6633 | Validation Loss:     0.6293\nEpoch   35 | Train Loss:     0.6558 | Validation Loss:     0.6334\nEpoch   36 | Train Loss:     0.6485 | Validation Loss:     0.6172\nEpoch   37 | Train Loss:     0.6387 | Validation Loss:     0.6057\nEpoch   38 | Train Loss:     0.6301 | Validation Loss:     0.5994\nEpoch   39 | Train Loss:     0.6301 | Validation Loss:     0.5965\nEpoch   40 | Train Loss:     0.6183 | Validation Loss:     0.5855\nEpoch   41 | Train Loss:     0.6123 | Validation Loss:     0.5815\nEpoch   42 | Train Loss:     0.6078 | Validation Loss:     0.5778\nEpoch   43 | Train Loss:     0.6012 | Validation Loss:     0.5717\nEpoch   44 | Train Loss:     0.5970 | Validation Loss:     0.5663\nEpoch   45 | Train Loss:     0.5923 | Validation Loss:     0.5601\nEpoch   46 | Train Loss:     0.5876 | Validation Loss:     0.5575\nEpoch   47 | Train Loss:     0.5819 | Validation Loss:     0.5502\nEpoch   48 | Train Loss:     0.5772 | Validation Loss:     0.5453\nEpoch   49 | Train Loss:     0.5716 | Validation Loss:     0.5395\nEpoch   50 | Train Loss:     0.5654 | Validation Loss:     0.5324\nEpoch   51 | Train Loss:     0.5592 | Validation Loss:     0.5293\nEpoch   52 | Train Loss:     0.5533 | Validation Loss:     0.5207\nEpoch   53 | Train Loss:     0.5457 | Validation Loss:     0.5127\nEpoch   54 | Train Loss:     0.5397 | Validation Loss:     0.5169\nEpoch   55 | Train Loss:     0.5304 | Validation Loss:     0.4970\nEpoch   56 | Train Loss:     0.5221 | Validation Loss:     0.4920\nEpoch   57 | Train Loss:     0.5144 | Validation Loss:     0.4817\nEpoch   58 | Train Loss:     0.5040 | Validation Loss:     0.4698\nEpoch   59 | Train Loss:     0.4953 | Validation Loss:     0.4628\nEpoch   60 | Train Loss:     0.4874 | Validation Loss:     0.4534\nEpoch   61 | Train Loss:     0.4810 | Validation Loss:     0.4467\nEpoch   62 | Train Loss:     0.4716 | Validation Loss:     0.4379\nEpoch   63 | Train Loss:     0.4655 | Validation Loss:     0.4317\nEpoch   64 | Train Loss:     0.4588 | Validation Loss:     0.4227\nEpoch   65 | Train Loss:     0.4532 | Validation Loss:     0.4196\nEpoch   66 | Train Loss:     0.4479 | Validation Loss:     0.4101\nEpoch   67 | Train Loss:     0.4434 | Validation Loss:     0.4055\nEpoch   68 | Train Loss:     0.4356 | Validation Loss:     0.3994\nEpoch   69 | Train Loss:     0.4301 | Validation Loss:     0.3938\nEpoch   70 | Train Loss:     0.4249 | Validation Loss:     0.3882\nEpoch   71 | Train Loss:     0.4201 | Validation Loss:     0.3823\nEpoch   72 | Train Loss:     0.4154 | Validation Loss:     0.3768\nEpoch   73 | Train Loss:     0.4104 | Validation Loss:     0.3728\nEpoch   74 | Train Loss:     0.4061 | Validation Loss:     0.3679\nEpoch   75 | Train Loss:     0.4024 | Validation Loss:     0.3647\nEpoch   76 | Train Loss:     0.3990 | Validation Loss:     0.3615\nEpoch   77 | Train Loss:     0.3964 | Validation Loss:     0.3585\nEpoch   78 | Train Loss:     0.3938 | Validation Loss:     0.3561\nEpoch   79 | Train Loss:     0.3922 | Validation Loss:     0.3545\nEpoch   80 | Train Loss:     0.3908 | Validation Loss:     0.3532\nEpoch   81 | Train Loss:     0.3898 | Validation Loss:     0.3524\nEpoch   82 | Train Loss:     0.3891 | Validation Loss:     0.3516\nEpoch   83 | Train Loss:     0.3886 | Validation Loss:     0.3512\nEpoch   84 | Train Loss:     0.3882 | Validation Loss:     0.3508\nEpoch   85 | Train Loss:     0.3879 | Validation Loss:     0.3505\nEpoch   86 | Train Loss:     0.3876 | Validation Loss:     0.3502\nEpoch   87 | Train Loss:     0.3874 | Validation Loss:     0.3499\nEpoch   88 | Train Loss:     0.3873 | Validation Loss:     0.3497\nEpoch   89 | Train Loss:     0.3871 | Validation Loss:     0.3495\nEpoch   90 | Train Loss:     0.3869 | Validation Loss:     0.3494\nEpoch   91 | Train Loss:     0.3868 | Validation Loss:     0.3491\nEpoch   92 | Train Loss:     0.3867 | Validation Loss:     0.3490\nEpoch   93 | Train Loss:     0.3865 | Validation Loss:     0.3488\nEpoch   94 | Train Loss:     0.3865 | Validation Loss:     0.3486\nEpoch   95 | Train Loss:     0.3864 | Validation Loss:     0.3485\nEpoch   96 | Train Loss:     0.3862 | Validation Loss:     0.3483\nEpoch   97 | Train Loss:     0.3862 | Validation Loss:     0.3481\nEpoch   98 | Train Loss:     0.3860 | Validation Loss:     0.3480\nEpoch   99 | Train Loss:     0.3860 | Validation Loss:     0.3477\nEpoch  100 | Train Loss:     0.3857 | Validation Loss:     0.3474\nEpoch  101 | Train Loss:     0.3855 | Validation Loss:     0.3471\nEpoch  102 | Train Loss:     0.3855 | Validation Loss:     0.3476\nEpoch  103 | Train Loss:     0.3851 | Validation Loss:     0.3464\nEpoch  104 | Train Loss:     0.3848 | Validation Loss:     0.3460\nEpoch  105 | Train Loss:     0.3846 | Validation Loss:     0.3458\nEpoch  106 | Train Loss:     0.3842 | Validation Loss:     0.3453\nEpoch  107 | Train Loss:     0.3838 | Validation Loss:     0.3447\nEpoch  108 | Train Loss:     0.3833 | Validation Loss:     0.3437\nEpoch  109 | Train Loss:     0.3825 | Validation Loss:     0.3429\nEpoch  110 | Train Loss:     0.3815 | Validation Loss:     0.3409\nEpoch  111 | Train Loss:     0.3800 | Validation Loss:     0.3386\nEpoch  112 | Train Loss:     0.3782 | Validation Loss:     0.3358\nEpoch  113 | Train Loss:     0.3756 | Validation Loss:     0.3346\nEpoch  114 | Train Loss:     0.3722 | Validation Loss:     0.3289\nEpoch  115 | Train Loss:     0.3688 | Validation Loss:     0.3249\nEpoch  116 | Train Loss:     0.3653 | Validation Loss:     0.3219\nEpoch  117 | Train Loss:     0.3631 | Validation Loss:     0.3189\nEpoch  118 | Train Loss:     0.3604 | Validation Loss:     0.3177\nEpoch  119 | Train Loss:     0.3599 | Validation Loss:     0.3164\nEpoch  120 | Train Loss:     0.3583 | Validation Loss:     0.3158\nEpoch  121 | Train Loss:     0.3578 | Validation Loss:     0.3152\nEpoch  122 | Train Loss:     0.3573 | Validation Loss:     0.3150\nEpoch  123 | Train Loss:     0.3572 | Validation Loss:     0.3154\nEpoch  124 | Train Loss:     0.3572 | Validation Loss:     0.3158\nEpoch  125 | Train Loss:     0.3570 | Validation Loss:     0.3146\nEpoch  126 | Train Loss:     0.3567 | Validation Loss:     0.3147\nEpoch  127 | Train Loss:     0.3568 | Validation Loss:     0.3146\nEpoch  128 | Train Loss:     0.3566 | Validation Loss:     0.3144\nEpoch  129 | Train Loss:     0.3565 | Validation Loss:     0.3142\nEpoch  130 | Train Loss:     0.3565 | Validation Loss:     0.3142\nEpoch  131 | Train Loss:     0.3564 | Validation Loss:     0.3142\nEpoch  132 | Train Loss:     0.3562 | Validation Loss:     0.3140\nEpoch  133 | Train Loss:     0.3561 | Validation Loss:     0.3140\nEpoch  134 | Train Loss:     0.3562 | Validation Loss:     0.3143\nEpoch  135 | Train Loss:     0.3562 | Validation Loss:     0.3145\nEpoch  136 | Train Loss:     0.3561 | Validation Loss:     0.3138\nEpoch  137 | Train Loss:     0.3560 | Validation Loss:     0.3138\nEpoch  138 | Train Loss:     0.3560 | Validation Loss:     0.3138\nEpoch  139 | Train Loss:     0.3559 | Validation Loss:     0.3138\nEpoch  140 | Train Loss:     0.3559 | Validation Loss:     0.3138\nEpoch  141 | Train Loss:     0.3560 | Validation Loss:     0.3137\nEpoch  142 | Train Loss:     0.3558 | Validation Loss:     0.3136\nEpoch  143 | Train Loss:     0.3560 | Validation Loss:     0.3138\nEpoch  144 | Train Loss:     0.3559 | Validation Loss:     0.3135\nEpoch  145 | Train Loss:     0.3557 | Validation Loss:     0.3135\nEpoch  146 | Train Loss:     0.3557 | Validation Loss:     0.3136\nEpoch  147 | Train Loss:     0.3557 | Validation Loss:     0.3135\nEpoch  148 | Train Loss:     0.3556 | Validation Loss:     0.3135\nEpoch  149 | Train Loss:     0.3559 | Validation Loss:     0.3138\nEpoch  150 | Train Loss:     0.3558 | Validation Loss:     0.3134\nEpoch  151 | Train Loss:     0.3556 | Validation Loss:     0.3134\n","output_type":"stream"}]},{"cell_type":"code","source":"optimizer = optim.Adam(model_prepu.parameters(), lr=0.000001)\nevaluate_model(model_prepu, custom_train_loader, criterion, optimizer)","metadata":{"execution":{"iopub.status.busy":"2024-06-15T18:17:55.388612Z","iopub.execute_input":"2024-06-15T18:17:55.389021Z"},"trusted":true},"execution_count":null,"outputs":[{"name":"stdout","text":"Epoch    1 | Train Loss:     0.0726 | Validation Loss:     0.0602\nEpoch    2 | Train Loss:     0.0510 | Validation Loss:     0.0396\nEpoch    3 | Train Loss:     0.0327 | Validation Loss:     0.0254\nEpoch    4 | Train Loss:     0.0228 | Validation Loss:     0.0203\nEpoch    5 | Train Loss:     0.0201 | Validation Loss:     0.0194\nEpoch    6 | Train Loss:     0.0195 | Validation Loss:     0.0190\nEpoch    7 | Train Loss:     0.0191 | Validation Loss:     0.0186\nEpoch    8 | Train Loss:     0.0186 | Validation Loss:     0.0181\nEpoch    9 | Train Loss:     0.0181 | Validation Loss:     0.0176\nEpoch   10 | Train Loss:     0.0175 | Validation Loss:     0.0169\nEpoch   11 | Train Loss:     0.0168 | Validation Loss:     0.0162\nEpoch   12 | Train Loss:     0.0160 | Validation Loss:     0.0154\nEpoch   13 | Train Loss:     0.0151 | Validation Loss:     0.0144\nEpoch   14 | Train Loss:     0.0141 | Validation Loss:     0.0133\nEpoch   15 | Train Loss:     0.0129 | Validation Loss:     0.0121\nEpoch   16 | Train Loss:     0.0116 | Validation Loss:     0.0108\n","output_type":"stream"}]},{"cell_type":"code","source":"optimizer = optim.Adam(model_prepu.parameters(), lr=0.000001)\nevaluate_model(model_prepu, custom_train_loader, criterion, optimizer)","metadata":{"execution":{"iopub.status.busy":"2024-06-15T18:13:08.857616Z","iopub.execute_input":"2024-06-15T18:13:08.858040Z","iopub.status.idle":"2024-06-15T18:15:55.929763Z","shell.execute_reply.started":"2024-06-15T18:13:08.858007Z","shell.execute_reply":"2024-06-15T18:15:55.927334Z"},"trusted":true},"execution_count":46,"outputs":[{"name":"stdout","text":"Epoch    1 | Train Loss:     0.1256 | Validation Loss:     0.1085\nEpoch    2 | Train Loss:     0.0968 | Validation Loss:     0.0819\nEpoch    3 | Train Loss:     0.0715 | Validation Loss:     0.0583\nEpoch    4 | Train Loss:     0.0488 | Validation Loss:     0.0374\nEpoch    5 | Train Loss:     0.0307 | Validation Loss:     0.0239\nEpoch    6 | Train Loss:     0.0218 | Validation Loss:     0.0196\nEpoch    7 | Train Loss:     0.0196 | Validation Loss:     0.0189\nEpoch    8 | Train Loss:     0.0190 | Validation Loss:     0.0185\nEpoch    9 | Train Loss:     0.0185 | Validation Loss:     0.0180\nEpoch   10 | Train Loss:     0.0181 | Validation Loss:     0.0176\nEpoch   11 | Train Loss:     0.0176 | Validation Loss:     0.0170\nEpoch   12 | Train Loss:     0.0170 | Validation Loss:     0.0165\nEpoch   13 | Train Loss:     0.0164 | Validation Loss:     0.0159\nEpoch   14 | Train Loss:     0.0157 | Validation Loss:     0.0152\nEpoch   15 | Train Loss:     0.0150 | Validation Loss:     0.0144\nEpoch   16 | Train Loss:     0.0142 | Validation Loss:     0.0135\nEpoch   17 | Train Loss:     0.0132 | Validation Loss:     0.0126\nEpoch   18 | Train Loss:     0.0122 | Validation Loss:     0.0116\nEpoch   19 | Train Loss:     0.0111 | Validation Loss:     0.0104\nEpoch   20 | Train Loss:     0.0099 | Validation Loss:     0.0093\nEpoch   21 | Train Loss:     0.0087 | Validation Loss:     0.0081\nEpoch   22 | Train Loss:     0.0075 | Validation Loss:     0.0069\nEpoch   23 | Train Loss:     0.0063 | Validation Loss:     0.0058\nEpoch   24 | Train Loss:     0.0052 | Validation Loss:     0.0048\nEpoch   25 | Train Loss:     0.0043 | Validation Loss:     0.0039\nEpoch   26 | Train Loss:     0.0035 | Validation Loss:     0.0032\nEpoch   27 | Train Loss:     0.0028 | Validation Loss:     0.0026\nEpoch   28 | Train Loss:     0.0023 | Validation Loss:     0.0022\nEpoch   29 | Train Loss:     0.0020 | Validation Loss:     0.0019\nEpoch   30 | Train Loss:     0.0017 | Validation Loss:     0.0017\nEpoch   31 | Train Loss:     0.0016 | Validation Loss:     0.0015\nEpoch   32 | Train Loss:     0.0015 | Validation Loss:     0.0014\nEpoch   33 | Train Loss:     0.0014 | Validation Loss:     0.0014\nEpoch   34 | Train Loss:     0.0014 | Validation Loss:     0.0014\nEpoch   35 | Train Loss:     0.0013 | Validation Loss:     0.0013\nEpoch   36 | Train Loss:     0.0013 | Validation Loss:     0.0013\nEpoch   37 | Train Loss:     0.0013 | Validation Loss:     0.0013\nEpoch   38 | Train Loss:     0.0013 | Validation Loss:     0.0013\nEpoch   39 | Train Loss:     0.0013 | Validation Loss:     0.0013\nEpoch   40 | Train Loss:     0.0013 | Validation Loss:     0.0013\nEpoch   41 | Train Loss:     0.0013 | Validation Loss:     0.0013\nEpoch   42 | Train Loss:     0.0013 | Validation Loss:     0.0013\nEpoch   43 | Train Loss:     0.0013 | Validation Loss:     0.0013\nEpoch   44 | Train Loss:     0.0013 | Validation Loss:     0.0013\nEpoch   45 | Train Loss:     0.0013 | Validation Loss:     0.0013\nEpoch   46 | Train Loss:     0.0012 | Validation Loss:     0.0012\nEpoch   47 | Train Loss:     0.0012 | Validation Loss:     0.0012\nEpoch   48 | Train Loss:     0.0012 | Validation Loss:     0.0012\nEpoch   49 | Train Loss:     0.0012 | Validation Loss:     0.0012\nEpoch   50 | Train Loss:     0.0012 | Validation Loss:     0.0012\nEpoch   51 | Train Loss:     0.0012 | Validation Loss:     0.0012\nEpoch   52 | Train Loss:     0.0012 | Validation Loss:     0.0012\nEpoch   53 | Train Loss:     0.0012 | Validation Loss:     0.0012\nEpoch   54 | Train Loss:     0.0012 | Validation Loss:     0.0012\nEpoch   55 | Train Loss:     0.0012 | Validation Loss:     0.0012\nEpoch   56 | Train Loss:     0.0011 | Validation Loss:     0.0011\nEpoch   57 | Train Loss:     0.0011 | Validation Loss:     0.0011\nEpoch   58 | Train Loss:     0.0011 | Validation Loss:     0.0011\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","Cell \u001b[0;32mIn[46], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m optimizer \u001b[38;5;241m=\u001b[39m optim\u001b[38;5;241m.\u001b[39mAdam(model_prepu\u001b[38;5;241m.\u001b[39mparameters(), lr\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.000001\u001b[39m)\n\u001b[0;32m----> 2\u001b[0m \u001b[43mevaluate_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_prepu\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcustom_train_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m)\u001b[49m\n","Cell \u001b[0;32mIn[32], line 143\u001b[0m, in \u001b[0;36mevaluate_model\u001b[0;34m(model, custom_train_loader, criterion, optimizer)\u001b[0m\n\u001b[1;32m    141\u001b[0m     plt\u001b[38;5;241m.\u001b[39mlegend()\n\u001b[1;32m    142\u001b[0m     plt\u001b[38;5;241m.\u001b[39mgrid(\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m--> 143\u001b[0m     \u001b[43mplt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msavefig\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mimage_folder\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m/epoch_\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mepoch\u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[38;5;132;43;01m:\u001b[39;49;00m\u001b[38;5;124;43m04d\u001b[39;49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m.png\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    144\u001b[0m     plt\u001b[38;5;241m.\u001b[39mclose()\n\u001b[1;32m    146\u001b[0m \u001b[38;5;66;03m# Create a GIF from the saved images\u001b[39;00m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/matplotlib/pyplot.py:1024\u001b[0m, in \u001b[0;36msavefig\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m   1022\u001b[0m fig \u001b[38;5;241m=\u001b[39m gcf()\n\u001b[1;32m   1023\u001b[0m res \u001b[38;5;241m=\u001b[39m fig\u001b[38;5;241m.\u001b[39msavefig(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m-> 1024\u001b[0m \u001b[43mfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcanvas\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdraw_idle\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Need this if 'transparent=True', to reset colors.\u001b[39;00m\n\u001b[1;32m   1025\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m res\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/matplotlib/backend_bases.py:2082\u001b[0m, in \u001b[0;36mFigureCanvasBase.draw_idle\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2080\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_is_idle_drawing:\n\u001b[1;32m   2081\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_idle_draw_cntx():\n\u001b[0;32m-> 2082\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdraw\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/matplotlib/backends/backend_agg.py:400\u001b[0m, in \u001b[0;36mFigureCanvasAgg.draw\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    396\u001b[0m \u001b[38;5;66;03m# Acquire a lock on the shared font cache.\u001b[39;00m\n\u001b[1;32m    397\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m RendererAgg\u001b[38;5;241m.\u001b[39mlock, \\\n\u001b[1;32m    398\u001b[0m      (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtoolbar\u001b[38;5;241m.\u001b[39m_wait_cursor_for_draw_cm() \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtoolbar\n\u001b[1;32m    399\u001b[0m       \u001b[38;5;28;01melse\u001b[39;00m nullcontext()):\n\u001b[0;32m--> 400\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfigure\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdraw\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrenderer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    401\u001b[0m     \u001b[38;5;66;03m# A GUI class may be need to update a window using this draw, so\u001b[39;00m\n\u001b[1;32m    402\u001b[0m     \u001b[38;5;66;03m# don't forget to call the superclass.\u001b[39;00m\n\u001b[1;32m    403\u001b[0m     \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mdraw()\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/matplotlib/artist.py:95\u001b[0m, in \u001b[0;36m_finalize_rasterization.<locals>.draw_wrapper\u001b[0;34m(artist, renderer, *args, **kwargs)\u001b[0m\n\u001b[1;32m     93\u001b[0m \u001b[38;5;129m@wraps\u001b[39m(draw)\n\u001b[1;32m     94\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdraw_wrapper\u001b[39m(artist, renderer, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m---> 95\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mdraw\u001b[49m\u001b[43m(\u001b[49m\u001b[43martist\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrenderer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     96\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m renderer\u001b[38;5;241m.\u001b[39m_rasterizing:\n\u001b[1;32m     97\u001b[0m         renderer\u001b[38;5;241m.\u001b[39mstop_rasterizing()\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/matplotlib/artist.py:72\u001b[0m, in \u001b[0;36mallow_rasterization.<locals>.draw_wrapper\u001b[0;34m(artist, renderer)\u001b[0m\n\u001b[1;32m     69\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m artist\u001b[38;5;241m.\u001b[39mget_agg_filter() \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     70\u001b[0m         renderer\u001b[38;5;241m.\u001b[39mstart_filter()\n\u001b[0;32m---> 72\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mdraw\u001b[49m\u001b[43m(\u001b[49m\u001b[43martist\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrenderer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     73\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     74\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m artist\u001b[38;5;241m.\u001b[39mget_agg_filter() \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/matplotlib/figure.py:3175\u001b[0m, in \u001b[0;36mFigure.draw\u001b[0;34m(self, renderer)\u001b[0m\n\u001b[1;32m   3172\u001b[0m         \u001b[38;5;66;03m# ValueError can occur when resizing a window.\u001b[39;00m\n\u001b[1;32m   3174\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpatch\u001b[38;5;241m.\u001b[39mdraw(renderer)\n\u001b[0;32m-> 3175\u001b[0m \u001b[43mmimage\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_draw_list_compositing_images\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   3176\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrenderer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43martists\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msuppressComposite\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3178\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m sfig \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msubfigs:\n\u001b[1;32m   3179\u001b[0m     sfig\u001b[38;5;241m.\u001b[39mdraw(renderer)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/matplotlib/image.py:131\u001b[0m, in \u001b[0;36m_draw_list_compositing_images\u001b[0;34m(renderer, parent, artists, suppress_composite)\u001b[0m\n\u001b[1;32m    129\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m not_composite \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m has_images:\n\u001b[1;32m    130\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m a \u001b[38;5;129;01min\u001b[39;00m artists:\n\u001b[0;32m--> 131\u001b[0m         \u001b[43ma\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdraw\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrenderer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    132\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    133\u001b[0m     \u001b[38;5;66;03m# Composite any adjacent images together\u001b[39;00m\n\u001b[1;32m    134\u001b[0m     image_group \u001b[38;5;241m=\u001b[39m []\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/matplotlib/artist.py:72\u001b[0m, in \u001b[0;36mallow_rasterization.<locals>.draw_wrapper\u001b[0;34m(artist, renderer)\u001b[0m\n\u001b[1;32m     69\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m artist\u001b[38;5;241m.\u001b[39mget_agg_filter() \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     70\u001b[0m         renderer\u001b[38;5;241m.\u001b[39mstart_filter()\n\u001b[0;32m---> 72\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mdraw\u001b[49m\u001b[43m(\u001b[49m\u001b[43martist\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrenderer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     73\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     74\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m artist\u001b[38;5;241m.\u001b[39mget_agg_filter() \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/matplotlib/axes/_base.py:3064\u001b[0m, in \u001b[0;36m_AxesBase.draw\u001b[0;34m(self, renderer)\u001b[0m\n\u001b[1;32m   3061\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m artists_rasterized:\n\u001b[1;32m   3062\u001b[0m     _draw_rasterized(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfigure, artists_rasterized, renderer)\n\u001b[0;32m-> 3064\u001b[0m \u001b[43mmimage\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_draw_list_compositing_images\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   3065\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrenderer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43martists\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfigure\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msuppressComposite\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3067\u001b[0m renderer\u001b[38;5;241m.\u001b[39mclose_group(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124maxes\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m   3068\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstale \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/matplotlib/image.py:131\u001b[0m, in \u001b[0;36m_draw_list_compositing_images\u001b[0;34m(renderer, parent, artists, suppress_composite)\u001b[0m\n\u001b[1;32m    129\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m not_composite \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m has_images:\n\u001b[1;32m    130\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m a \u001b[38;5;129;01min\u001b[39;00m artists:\n\u001b[0;32m--> 131\u001b[0m         \u001b[43ma\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdraw\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrenderer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    132\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    133\u001b[0m     \u001b[38;5;66;03m# Composite any adjacent images together\u001b[39;00m\n\u001b[1;32m    134\u001b[0m     image_group \u001b[38;5;241m=\u001b[39m []\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/matplotlib/artist.py:72\u001b[0m, in \u001b[0;36mallow_rasterization.<locals>.draw_wrapper\u001b[0;34m(artist, renderer)\u001b[0m\n\u001b[1;32m     69\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m artist\u001b[38;5;241m.\u001b[39mget_agg_filter() \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     70\u001b[0m         renderer\u001b[38;5;241m.\u001b[39mstart_filter()\n\u001b[0;32m---> 72\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mdraw\u001b[49m\u001b[43m(\u001b[49m\u001b[43martist\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrenderer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     73\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     74\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m artist\u001b[38;5;241m.\u001b[39mget_agg_filter() \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/matplotlib/axis.py:1395\u001b[0m, in \u001b[0;36mAxis.draw\u001b[0;34m(self, renderer, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1392\u001b[0m     tick\u001b[38;5;241m.\u001b[39mdraw(renderer)\n\u001b[1;32m   1394\u001b[0m \u001b[38;5;66;03m# Shift label away from axes to avoid overlapping ticklabels.\u001b[39;00m\n\u001b[0;32m-> 1395\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_update_label_position\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrenderer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1396\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlabel\u001b[38;5;241m.\u001b[39mdraw(renderer)\n\u001b[1;32m   1398\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_update_offset_text_position(tlb1, tlb2)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/matplotlib/axis.py:2614\u001b[0m, in \u001b[0;36mYAxis._update_label_position\u001b[0;34m(self, renderer)\u001b[0m\n\u001b[1;32m   2610\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[1;32m   2612\u001b[0m \u001b[38;5;66;03m# get bounding boxes for this axis and any siblings\u001b[39;00m\n\u001b[1;32m   2613\u001b[0m \u001b[38;5;66;03m# that have been set by `fig.align_ylabels()`\u001b[39;00m\n\u001b[0;32m-> 2614\u001b[0m bboxes, bboxes2 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_tick_boxes_siblings\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrenderer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrenderer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2615\u001b[0m x, y \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlabel\u001b[38;5;241m.\u001b[39mget_position()\n\u001b[1;32m   2616\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlabel_position \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mleft\u001b[39m\u001b[38;5;124m'\u001b[39m:\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/matplotlib/axis.py:2149\u001b[0m, in \u001b[0;36mAxis._get_tick_boxes_siblings\u001b[0;34m(self, renderer)\u001b[0m\n\u001b[1;32m   2147\u001b[0m axis \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(ax, \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00maxis_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124maxis\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   2148\u001b[0m ticks_to_draw \u001b[38;5;241m=\u001b[39m axis\u001b[38;5;241m.\u001b[39m_update_ticks()\n\u001b[0;32m-> 2149\u001b[0m tlb, tlb2 \u001b[38;5;241m=\u001b[39m \u001b[43maxis\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_ticklabel_bboxes\u001b[49m\u001b[43m(\u001b[49m\u001b[43mticks_to_draw\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrenderer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2150\u001b[0m bboxes\u001b[38;5;241m.\u001b[39mextend(tlb)\n\u001b[1;32m   2151\u001b[0m bboxes2\u001b[38;5;241m.\u001b[39mextend(tlb2)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/matplotlib/axis.py:1316\u001b[0m, in \u001b[0;36mAxis._get_ticklabel_bboxes\u001b[0;34m(self, ticks, renderer)\u001b[0m\n\u001b[1;32m   1314\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m renderer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1315\u001b[0m     renderer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfigure\u001b[38;5;241m.\u001b[39m_get_renderer()\n\u001b[0;32m-> 1316\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m ([tick\u001b[38;5;241m.\u001b[39mlabel1\u001b[38;5;241m.\u001b[39mget_window_extent(renderer)\n\u001b[1;32m   1317\u001b[0m          \u001b[38;5;28;01mfor\u001b[39;00m tick \u001b[38;5;129;01min\u001b[39;00m ticks \u001b[38;5;28;01mif\u001b[39;00m tick\u001b[38;5;241m.\u001b[39mlabel1\u001b[38;5;241m.\u001b[39mget_visible()],\n\u001b[1;32m   1318\u001b[0m         [tick\u001b[38;5;241m.\u001b[39mlabel2\u001b[38;5;241m.\u001b[39mget_window_extent(renderer)\n\u001b[1;32m   1319\u001b[0m          \u001b[38;5;28;01mfor\u001b[39;00m tick \u001b[38;5;129;01min\u001b[39;00m ticks \u001b[38;5;28;01mif\u001b[39;00m tick\u001b[38;5;241m.\u001b[39mlabel2\u001b[38;5;241m.\u001b[39mget_visible()])\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/matplotlib/axis.py:1316\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m   1314\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m renderer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1315\u001b[0m     renderer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfigure\u001b[38;5;241m.\u001b[39m_get_renderer()\n\u001b[0;32m-> 1316\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m ([\u001b[43mtick\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlabel1\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_window_extent\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrenderer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1317\u001b[0m          \u001b[38;5;28;01mfor\u001b[39;00m tick \u001b[38;5;129;01min\u001b[39;00m ticks \u001b[38;5;28;01mif\u001b[39;00m tick\u001b[38;5;241m.\u001b[39mlabel1\u001b[38;5;241m.\u001b[39mget_visible()],\n\u001b[1;32m   1318\u001b[0m         [tick\u001b[38;5;241m.\u001b[39mlabel2\u001b[38;5;241m.\u001b[39mget_window_extent(renderer)\n\u001b[1;32m   1319\u001b[0m          \u001b[38;5;28;01mfor\u001b[39;00m tick \u001b[38;5;129;01min\u001b[39;00m ticks \u001b[38;5;28;01mif\u001b[39;00m tick\u001b[38;5;241m.\u001b[39mlabel2\u001b[38;5;241m.\u001b[39mget_visible()])\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/matplotlib/text.py:958\u001b[0m, in \u001b[0;36mText.get_window_extent\u001b[0;34m(self, renderer, dpi)\u001b[0m\n\u001b[1;32m    953\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_renderer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    954\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m    955\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot get window extent of text w/o renderer. You likely \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    956\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwant to call \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfigure.draw_without_rendering()\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m first.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 958\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m cbook\u001b[38;5;241m.\u001b[39m_setattr_cm(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfigure, dpi\u001b[38;5;241m=\u001b[39mdpi):\n\u001b[1;32m    959\u001b[0m     bbox, info, descent \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_layout(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_renderer)\n\u001b[1;32m    960\u001b[0m     x, y \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_unitless_position()\n","File \u001b[0;32m/opt/conda/lib/python3.10/contextlib.py:135\u001b[0m, in \u001b[0;36m_GeneratorContextManager.__enter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    133\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkwds, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfunc\n\u001b[1;32m    134\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 135\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgen\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    136\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m:\n\u001b[1;32m    137\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgenerator didn\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt yield\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/matplotlib/cbook/__init__.py:2009\u001b[0m, in \u001b[0;36m_setattr_cm\u001b[0;34m(obj, **kwargs)\u001b[0m\n\u001b[1;32m   2004\u001b[0m \u001b[38;5;129m@contextlib\u001b[39m\u001b[38;5;241m.\u001b[39mcontextmanager\n\u001b[1;32m   2005\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_setattr_cm\u001b[39m(obj, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m   2006\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   2007\u001b[0m \u001b[38;5;124;03m    Temporarily set some attributes; restore original state at context exit.\u001b[39;00m\n\u001b[1;32m   2008\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 2009\u001b[0m     sentinel \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mobject\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2010\u001b[0m     origs \u001b[38;5;241m=\u001b[39m {}\n\u001b[1;32m   2011\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m attr \u001b[38;5;129;01min\u001b[39;00m kwargs:\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "],"ename":"KeyboardInterrupt","evalue":"","output_type":"error"},{"output_type":"display_data","data":{"text/plain":"<Figure size 1000x500 with 1 Axes>","image/png":"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"},"metadata":{}}]},{"cell_type":"code","source":"optimizer = optim.Adam(model_prepu.parameters(), lr=0.000001)\nevaluate_model(model_prepu, custom_train_loader, criterion, optimizer)","metadata":{"execution":{"iopub.status.busy":"2024-06-15T18:07:12.531808Z","iopub.execute_input":"2024-06-15T18:07:12.532241Z","iopub.status.idle":"2024-06-15T18:09:37.960617Z","shell.execute_reply.started":"2024-06-15T18:07:12.532208Z","shell.execute_reply":"2024-06-15T18:09:37.958958Z"},"trusted":true},"execution_count":41,"outputs":[{"name":"stdout","text":"Epoch    1 | Train Loss:     0.2435 | Validation Loss:     0.2315\nEpoch    2 | Train Loss:     0.2169 | Validation Loss:     0.2060\nEpoch    3 | Train Loss:     0.1925 | Validation Loss:     0.1826\nEpoch    4 | Train Loss:     0.1704 | Validation Loss:     0.1614\nEpoch    5 | Train Loss:     0.1503 | Validation Loss:     0.1423\nEpoch    6 | Train Loss:     0.1324 | Validation Loss:     0.1252\nEpoch    7 | Train Loss:     0.1164 | Validation Loss:     0.1100\nEpoch    8 | Train Loss:     0.1022 | Validation Loss:     0.0966\nEpoch    9 | Train Loss:     0.0898 | Validation Loss:     0.0849\nEpoch   10 | Train Loss:     0.0789 | Validation Loss:     0.0747\nEpoch   11 | Train Loss:     0.0695 | Validation Loss:     0.0659\nEpoch   12 | Train Loss:     0.0614 | Validation Loss:     0.0583\nEpoch   13 | Train Loss:     0.0545 | Validation Loss:     0.0519\nEpoch   14 | Train Loss:     0.0486 | Validation Loss:     0.0464\nEpoch   15 | Train Loss:     0.0437 | Validation Loss:     0.0418\nEpoch   16 | Train Loss:     0.0395 | Validation Loss:     0.0379\nEpoch   17 | Train Loss:     0.0360 | Validation Loss:     0.0346\nEpoch   18 | Train Loss:     0.0331 | Validation Loss:     0.0320\nEpoch   19 | Train Loss:     0.0307 | Validation Loss:     0.0297\nEpoch   20 | Train Loss:     0.0288 | Validation Loss:     0.0279\nEpoch   21 | Train Loss:     0.0272 | Validation Loss:     0.0265\nEpoch   22 | Train Loss:     0.0259 | Validation Loss:     0.0253\nEpoch   23 | Train Loss:     0.0248 | Validation Loss:     0.0243\nEpoch   24 | Train Loss:     0.0240 | Validation Loss:     0.0235\nEpoch   25 | Train Loss:     0.0234 | Validation Loss:     0.0229\nEpoch   26 | Train Loss:     0.0228 | Validation Loss:     0.0224\nEpoch   27 | Train Loss:     0.0224 | Validation Loss:     0.0220\nEpoch   28 | Train Loss:     0.0221 | Validation Loss:     0.0217\nEpoch   29 | Train Loss:     0.0219 | Validation Loss:     0.0215\nEpoch   30 | Train Loss:     0.0217 | Validation Loss:     0.0213\nEpoch   31 | Train Loss:     0.0216 | Validation Loss:     0.0212\nEpoch   32 | Train Loss:     0.0215 | Validation Loss:     0.0211\nEpoch   33 | Train Loss:     0.0214 | Validation Loss:     0.0210\nEpoch   34 | Train Loss:     0.0213 | Validation Loss:     0.0210\nEpoch   35 | Train Loss:     0.0213 | Validation Loss:     0.0209\nEpoch   36 | Train Loss:     0.0213 | Validation Loss:     0.0209\nEpoch   37 | Train Loss:     0.0212 | Validation Loss:     0.0209\nEpoch   38 | Train Loss:     0.0212 | Validation Loss:     0.0209\nEpoch   39 | Train Loss:     0.0212 | Validation Loss:     0.0209\nEpoch   40 | Train Loss:     0.0212 | Validation Loss:     0.0208\nEpoch   41 | Train Loss:     0.0212 | Validation Loss:     0.0208\nEpoch   42 | Train Loss:     0.0212 | Validation Loss:     0.0208\nEpoch   43 | Train Loss:     0.0212 | Validation Loss:     0.0208\nEpoch   44 | Train Loss:     0.0212 | Validation Loss:     0.0208\nEpoch   45 | Train Loss:     0.0212 | Validation Loss:     0.0208\nEpoch   46 | Train Loss:     0.0212 | Validation Loss:     0.0208\nEpoch   47 | Train Loss:     0.0212 | Validation Loss:     0.0208\nEpoch   48 | Train Loss:     0.0212 | Validation Loss:     0.0208\nEpoch   49 | Train Loss:     0.0212 | Validation Loss:     0.0208\nEpoch   50 | Train Loss:     0.0212 | Validation Loss:     0.0208\nEpoch   51 | Train Loss:     0.0212 | Validation Loss:     0.0208\nEpoch   52 | Train Loss:     0.0212 | Validation Loss:     0.0208\nEpoch   53 | Train Loss:     0.0212 | Validation Loss:     0.0208\nEpoch   54 | Train Loss:     0.0212 | Validation Loss:     0.0208\nEpoch   55 | Train Loss:     0.0212 | Validation Loss:     0.0208\nEpoch   56 | Train Loss:     0.0212 | Validation Loss:     0.0208\nEpoch   57 | Train Loss:     0.0212 | Validation Loss:     0.0208\nEpoch   58 | Train Loss:     0.0212 | Validation Loss:     0.0208\nEpoch   59 | Train Loss:     0.0212 | Validation Loss:     0.0208\nEpoch   60 | Train Loss:     0.0212 | Validation Loss:     0.0208\nEpoch   61 | Train Loss:     0.0212 | Validation Loss:     0.0208\nEpoch   62 | Train Loss:     0.0212 | Validation Loss:     0.0208\nEpoch   63 | Train Loss:     0.0212 | Validation Loss:     0.0208\nEpoch   64 | Train Loss:     0.0212 | Validation Loss:     0.0208\nEpoch   65 | Train Loss:     0.0212 | Validation Loss:     0.0208\nEpoch   66 | Train Loss:     0.0212 | Validation Loss:     0.0208\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","Cell \u001b[0;32mIn[41], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m optimizer \u001b[38;5;241m=\u001b[39m optim\u001b[38;5;241m.\u001b[39mAdam(model_prepu\u001b[38;5;241m.\u001b[39mparameters(), lr\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.000001\u001b[39m)\n\u001b[0;32m----> 2\u001b[0m \u001b[43mevaluate_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_prepu\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcustom_train_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m)\u001b[49m\n","Cell \u001b[0;32mIn[32], line 71\u001b[0m, in \u001b[0;36mevaluate_model\u001b[0;34m(model, custom_train_loader, criterion, optimizer)\u001b[0m\n\u001b[1;32m     69\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_epochs):\n\u001b[1;32m     70\u001b[0m     running_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.0\u001b[39m\n\u001b[0;32m---> 71\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m inputs, labels \u001b[38;5;129;01min\u001b[39;00m custom_train_loader\u001b[38;5;241m.\u001b[39mget_train_loader():\n\u001b[1;32m     72\u001b[0m         inputs, labels \u001b[38;5;241m=\u001b[39m inputs\u001b[38;5;241m.\u001b[39mto(device), labels\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     73\u001b[0m         optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py:630\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    627\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    628\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    629\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 630\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    631\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    632\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    633\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    634\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py:674\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    672\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    673\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 674\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    675\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[1;32m    676\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py:51\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     49\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 51\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py:51\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     49\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 51\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataset.py:208\u001b[0m, in \u001b[0;36mTensorDataset.__getitem__\u001b[0;34m(self, index)\u001b[0m\n\u001b[1;32m    207\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__getitem__\u001b[39m(\u001b[38;5;28mself\u001b[39m, index):\n\u001b[0;32m--> 208\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mtuple\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtensor\u001b[49m\u001b[43m[\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtensor\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtensors\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataset.py:208\u001b[0m, in \u001b[0;36m<genexpr>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    207\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__getitem__\u001b[39m(\u001b[38;5;28mself\u001b[39m, index):\n\u001b[0;32m--> 208\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mtuple\u001b[39m(tensor[index] \u001b[38;5;28;01mfor\u001b[39;00m tensor \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtensors)\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "],"ename":"KeyboardInterrupt","evalue":"","output_type":"error"}]},{"cell_type":"code","source":"optimizer = optim.Adam(model_prepu.parameters(), lr=0.000001)\nevaluate_model(model_prepu, custom_train_loader, criterion, optimizer)","metadata":{"execution":{"iopub.status.busy":"2024-06-15T17:49:15.088778Z","iopub.execute_input":"2024-06-15T17:49:15.089202Z","iopub.status.idle":"2024-06-15T17:57:18.717827Z","shell.execute_reply.started":"2024-06-15T17:49:15.089170Z","shell.execute_reply":"2024-06-15T17:57:18.715932Z"},"trusted":true},"execution_count":15,"outputs":[{"name":"stdout","text":"Epoch    1 | Train Loss:    10.4650 | Validation Loss:     8.3437\nEpoch    2 | Train Loss:     5.8716 | Validation Loss:     2.9814\nEpoch    3 | Train Loss:     1.7122 | Validation Loss:     0.8699\nEpoch    4 | Train Loss:     0.6292 | Validation Loss:     0.4801\nEpoch    5 | Train Loss:     0.3817 | Validation Loss:     0.3244\nEpoch    6 | Train Loss:     0.2651 | Validation Loss:     0.2360\nEpoch    7 | Train Loss:     0.1987 | Validation Loss:     0.1839\nEpoch    8 | Train Loss:     0.1599 | Validation Loss:     0.1529\nEpoch    9 | Train Loss:     0.1364 | Validation Loss:     0.1337\nEpoch   10 | Train Loss:     0.1214 | Validation Loss:     0.1207\nEpoch   11 | Train Loss:     0.1110 | Validation Loss:     0.1111\nEpoch   12 | Train Loss:     0.1029 | Validation Loss:     0.1035\nEpoch   13 | Train Loss:     0.0960 | Validation Loss:     0.0967\nEpoch   14 | Train Loss:     0.0899 | Validation Loss:     0.0904\nEpoch   15 | Train Loss:     0.0840 | Validation Loss:     0.0846\nEpoch   16 | Train Loss:     0.0785 | Validation Loss:     0.0790\nEpoch   17 | Train Loss:     0.0733 | Validation Loss:     0.0737\nEpoch   18 | Train Loss:     0.0683 | Validation Loss:     0.0687\nEpoch   19 | Train Loss:     0.0636 | Validation Loss:     0.0640\nEpoch   20 | Train Loss:     0.0591 | Validation Loss:     0.0596\nEpoch   21 | Train Loss:     0.0549 | Validation Loss:     0.0554\nEpoch   22 | Train Loss:     0.0510 | Validation Loss:     0.0514\nEpoch   23 | Train Loss:     0.0473 | Validation Loss:     0.0477\nEpoch   24 | Train Loss:     0.0438 | Validation Loss:     0.0443\nEpoch   25 | Train Loss:     0.0406 | Validation Loss:     0.0411\nEpoch   26 | Train Loss:     0.0376 | Validation Loss:     0.0381\nEpoch   27 | Train Loss:     0.0348 | Validation Loss:     0.0353\nEpoch   28 | Train Loss:     0.0322 | Validation Loss:     0.0328\nEpoch   29 | Train Loss:     0.0298 | Validation Loss:     0.0304\nEpoch   30 | Train Loss:     0.0276 | Validation Loss:     0.0282\nEpoch   31 | Train Loss:     0.0256 | Validation Loss:     0.0262\nEpoch   32 | Train Loss:     0.0237 | Validation Loss:     0.0243\nEpoch   33 | Train Loss:     0.0220 | Validation Loss:     0.0227\nEpoch   34 | Train Loss:     0.0205 | Validation Loss:     0.0211\nEpoch   35 | Train Loss:     0.0191 | Validation Loss:     0.0197\nEpoch   36 | Train Loss:     0.0178 | Validation Loss:     0.0185\nEpoch   37 | Train Loss:     0.0167 | Validation Loss:     0.0173\nEpoch   38 | Train Loss:     0.0157 | Validation Loss:     0.0163\nEpoch   39 | Train Loss:     0.0147 | Validation Loss:     0.0154\nEpoch   40 | Train Loss:     0.0139 | Validation Loss:     0.0145\nEpoch   41 | Train Loss:     0.0131 | Validation Loss:     0.0138\nEpoch   42 | Train Loss:     0.0125 | Validation Loss:     0.0131\nEpoch   43 | Train Loss:     0.0119 | Validation Loss:     0.0125\nEpoch   44 | Train Loss:     0.0113 | Validation Loss:     0.0120\nEpoch   45 | Train Loss:     0.0109 | Validation Loss:     0.0115\nEpoch   46 | Train Loss:     0.0104 | Validation Loss:     0.0110\nEpoch   47 | Train Loss:     0.0100 | Validation Loss:     0.0106\nEpoch   48 | Train Loss:     0.0097 | Validation Loss:     0.0102\nEpoch   49 | Train Loss:     0.0093 | Validation Loss:     0.0099\nEpoch   50 | Train Loss:     0.0090 | Validation Loss:     0.0096\nEpoch   51 | Train Loss:     0.0088 | Validation Loss:     0.0093\nEpoch   52 | Train Loss:     0.0085 | Validation Loss:     0.0090\nEpoch   53 | Train Loss:     0.0082 | Validation Loss:     0.0087\nEpoch   54 | Train Loss:     0.0080 | Validation Loss:     0.0085\nEpoch   55 | Train Loss:     0.0077 | Validation Loss:     0.0082\nEpoch   56 | Train Loss:     0.0075 | Validation Loss:     0.0080\nEpoch   57 | Train Loss:     0.0073 | Validation Loss:     0.0077\nEpoch   58 | Train Loss:     0.0071 | Validation Loss:     0.0075\nEpoch   59 | Train Loss:     0.0069 | Validation Loss:     0.0073\nEpoch   60 | Train Loss:     0.0066 | Validation Loss:     0.0070\nEpoch   61 | Train Loss:     0.0064 | Validation Loss:     0.0068\nEpoch   62 | Train Loss:     0.0062 | Validation Loss:     0.0066\nEpoch   63 | Train Loss:     0.0060 | Validation Loss:     0.0064\nEpoch   64 | Train Loss:     0.0058 | Validation Loss:     0.0062\nEpoch   65 | Train Loss:     0.0056 | Validation Loss:     0.0060\nEpoch   66 | Train Loss:     0.0054 | Validation Loss:     0.0058\nEpoch   67 | Train Loss:     0.0053 | Validation Loss:     0.0056\nEpoch   68 | Train Loss:     0.0051 | Validation Loss:     0.0054\nEpoch   69 | Train Loss:     0.0049 | Validation Loss:     0.0052\nEpoch   70 | Train Loss:     0.0047 | Validation Loss:     0.0050\nEpoch   71 | Train Loss:     0.0045 | Validation Loss:     0.0048\nEpoch   72 | Train Loss:     0.0044 | Validation Loss:     0.0046\nEpoch   73 | Train Loss:     0.0042 | Validation Loss:     0.0044\nEpoch   74 | Train Loss:     0.0040 | Validation Loss:     0.0043\nEpoch   75 | Train Loss:     0.0039 | Validation Loss:     0.0041\nEpoch   76 | Train Loss:     0.0037 | Validation Loss:     0.0040\nEpoch   77 | Train Loss:     0.0036 | Validation Loss:     0.0038\nEpoch   78 | Train Loss:     0.0035 | Validation Loss:     0.0037\nEpoch   79 | Train Loss:     0.0033 | Validation Loss:     0.0035\nEpoch   80 | Train Loss:     0.0032 | Validation Loss:     0.0034\nEpoch   81 | Train Loss:     0.0031 | Validation Loss:     0.0033\nEpoch   82 | Train Loss:     0.0029 | Validation Loss:     0.0031\nEpoch   83 | Train Loss:     0.0028 | Validation Loss:     0.0030\nEpoch   84 | Train Loss:     0.0027 | Validation Loss:     0.0029\nEpoch   85 | Train Loss:     0.0026 | Validation Loss:     0.0028\nEpoch   86 | Train Loss:     0.0025 | Validation Loss:     0.0027\nEpoch   87 | Train Loss:     0.0024 | Validation Loss:     0.0026\nEpoch   88 | Train Loss:     0.0023 | Validation Loss:     0.0025\nEpoch   89 | Train Loss:     0.0022 | Validation Loss:     0.0024\nEpoch   90 | Train Loss:     0.0021 | Validation Loss:     0.0023\nEpoch   91 | Train Loss:     0.0021 | Validation Loss:     0.0022\nEpoch   92 | Train Loss:     0.0020 | Validation Loss:     0.0021\nEpoch   93 | Train Loss:     0.0019 | Validation Loss:     0.0020\nEpoch   94 | Train Loss:     0.0018 | Validation Loss:     0.0019\nEpoch   95 | Train Loss:     0.0018 | Validation Loss:     0.0019\nEpoch   96 | Train Loss:     0.0017 | Validation Loss:     0.0018\nEpoch   97 | Train Loss:     0.0016 | Validation Loss:     0.0017\nEpoch   98 | Train Loss:     0.0016 | Validation Loss:     0.0017\nEpoch   99 | Train Loss:     0.0015 | Validation Loss:     0.0016\nEpoch  100 | Train Loss:     0.0015 | Validation Loss:     0.0016\nEpoch  101 | Train Loss:     0.0014 | Validation Loss:     0.0015\nEpoch  102 | Train Loss:     0.0014 | Validation Loss:     0.0015\nEpoch  103 | Train Loss:     0.0013 | Validation Loss:     0.0014\nEpoch  104 | Train Loss:     0.0013 | Validation Loss:     0.0013\nEpoch  105 | Train Loss:     0.0012 | Validation Loss:     0.0013\nEpoch  106 | Train Loss:     0.0012 | Validation Loss:     0.0013\nEpoch  107 | Train Loss:     0.0011 | Validation Loss:     0.0012\nEpoch  108 | Train Loss:     0.0011 | Validation Loss:     0.0012\nEpoch  109 | Train Loss:     0.0011 | Validation Loss:     0.0011\nEpoch  110 | Train Loss:     0.0010 | Validation Loss:     0.0011\nEpoch  111 | Train Loss:     0.0010 | Validation Loss:     0.0010\nEpoch  112 | Train Loss:     0.0009 | Validation Loss:     0.0010\nEpoch  113 | Train Loss:     0.0009 | Validation Loss:     0.0010\nEpoch  114 | Train Loss:     0.0009 | Validation Loss:     0.0009\nEpoch  115 | Train Loss:     0.0009 | Validation Loss:     0.0009\nEpoch  116 | Train Loss:     0.0008 | Validation Loss:     0.0009\nEpoch  117 | Train Loss:     0.0008 | Validation Loss:     0.0008\nEpoch  118 | Train Loss:     0.0008 | Validation Loss:     0.0008\nEpoch  119 | Train Loss:     0.0007 | Validation Loss:     0.0008\nEpoch  120 | Train Loss:     0.0007 | Validation Loss:     0.0007\nEpoch  121 | Train Loss:     0.0007 | Validation Loss:     0.0007\nEpoch  122 | Train Loss:     0.0007 | Validation Loss:     0.0007\nEpoch  123 | Train Loss:     0.0006 | Validation Loss:     0.0007\nEpoch  124 | Train Loss:     0.0006 | Validation Loss:     0.0006\nEpoch  125 | Train Loss:     0.0006 | Validation Loss:     0.0006\nEpoch  126 | Train Loss:     0.0006 | Validation Loss:     0.0006\nEpoch  127 | Train Loss:     0.0005 | Validation Loss:     0.0006\nEpoch  128 | Train Loss:     0.0005 | Validation Loss:     0.0005\nEpoch  129 | Train Loss:     0.0005 | Validation Loss:     0.0005\nEpoch  130 | Train Loss:     0.0005 | Validation Loss:     0.0005\nEpoch  131 | Train Loss:     0.0005 | Validation Loss:     0.0005\nEpoch  132 | Train Loss:     0.0004 | Validation Loss:     0.0005\nEpoch  133 | Train Loss:     0.0004 | Validation Loss:     0.0004\nEpoch  134 | Train Loss:     0.0004 | Validation Loss:     0.0004\nEpoch  135 | Train Loss:     0.0004 | Validation Loss:     0.0004\nEpoch  136 | Train Loss:     0.0004 | Validation Loss:     0.0004\nEpoch  137 | Train Loss:     0.0004 | Validation Loss:     0.0004\nEpoch  138 | Train Loss:     0.0003 | Validation Loss:     0.0004\nEpoch  139 | Train Loss:     0.0003 | Validation Loss:     0.0003\nEpoch  140 | Train Loss:     0.0003 | Validation Loss:     0.0003\nEpoch  141 | Train Loss:     0.0003 | Validation Loss:     0.0003\nEpoch  142 | Train Loss:     0.0003 | Validation Loss:     0.0003\nEpoch  143 | Train Loss:     0.0003 | Validation Loss:     0.0003\nEpoch  144 | Train Loss:     0.0003 | Validation Loss:     0.0003\nEpoch  145 | Train Loss:     0.0002 | Validation Loss:     0.0003\nEpoch  146 | Train Loss:     0.0002 | Validation Loss:     0.0003\nEpoch  147 | Train Loss:     0.0002 | Validation Loss:     0.0003\nEpoch  148 | Train Loss:     0.0002 | Validation Loss:     0.0002\nEpoch  149 | Train Loss:     0.0002 | Validation Loss:     0.0002\nEpoch  150 | Train Loss:     0.0002 | Validation Loss:     0.0002\nEpoch  151 | Train Loss:     0.0002 | Validation Loss:     0.0002\nEpoch  152 | Train Loss:     0.0002 | Validation Loss:     0.0002\nEpoch  153 | Train Loss:     0.0002 | Validation Loss:     0.0002\nEpoch  154 | Train Loss:     0.0002 | Validation Loss:     0.0002\nEpoch  155 | Train Loss:     0.0002 | Validation Loss:     0.0002\nEpoch  156 | Train Loss:     0.0002 | Validation Loss:     0.0002\nEpoch  157 | Train Loss:     0.0002 | Validation Loss:     0.0002\nEpoch  158 | Train Loss:     0.0001 | Validation Loss:     0.0002\nEpoch  159 | Train Loss:     0.0001 | Validation Loss:     0.0002\nEpoch  160 | Train Loss:     0.0001 | Validation Loss:     0.0002\nEpoch  161 | Train Loss:     0.0001 | Validation Loss:     0.0001\nEpoch  162 | Train Loss:     0.0001 | Validation Loss:     0.0001\nEpoch  163 | Train Loss:     0.0001 | Validation Loss:     0.0001\nEpoch  164 | Train Loss:     0.0001 | Validation Loss:     0.0001\nEpoch  165 | Train Loss:     0.0001 | Validation Loss:     0.0001\nEpoch  166 | Train Loss:     0.0001 | Validation Loss:     0.0001\nEpoch  167 | Train Loss:     0.0001 | Validation Loss:     0.0001\nEpoch  168 | Train Loss:     0.0001 | Validation Loss:     0.0001\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","Cell \u001b[0;32mIn[15], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m optimizer \u001b[38;5;241m=\u001b[39m optim\u001b[38;5;241m.\u001b[39mAdam(model_prepu\u001b[38;5;241m.\u001b[39mparameters(), lr\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.000001\u001b[39m)\n\u001b[0;32m----> 2\u001b[0m \u001b[43mevaluate_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_prepu\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcustom_train_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m)\u001b[49m\n","Cell \u001b[0;32mIn[4], line 89\u001b[0m, in \u001b[0;36mevaluate_model\u001b[0;34m(model, custom_train_loader, criterion, optimizer)\u001b[0m\n\u001b[1;32m     86\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m name, param \u001b[38;5;129;01min\u001b[39;00m model\u001b[38;5;241m.\u001b[39mnamed_parameters():\n\u001b[1;32m     87\u001b[0m     parameters\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mparam\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 89\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     90\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m     92\u001b[0m running_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem()\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/_tensor.py:492\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    482\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    483\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    484\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    485\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    490\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    491\u001b[0m     )\n\u001b[0;32m--> 492\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    493\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    494\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/autograd/__init__.py:251\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    246\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    248\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    249\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    250\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 251\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    252\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    253\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    254\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    255\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    256\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    257\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    258\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    259\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "],"ename":"KeyboardInterrupt","evalue":"","output_type":"error"}]},{"cell_type":"code","source":"optimizer = optim.Adam(model_prepu.parameters(), lr=0.00001)\nevaluate_model(model_prepu, custom_train_loader, criterion, optimizer)","metadata":{"execution":{"iopub.status.busy":"2024-06-15T02:58:08.304933Z","iopub.execute_input":"2024-06-15T02:58:08.305332Z","iopub.status.idle":"2024-06-15T03:34:15.298177Z","shell.execute_reply.started":"2024-06-15T02:58:08.305302Z","shell.execute_reply":"2024-06-15T03:34:15.294023Z"},"trusted":true},"execution_count":12,"outputs":[{"name":"stdout","text":"Epoch    1 | Train Loss:     1.8894 | Validation Loss:     1.6232\nEpoch    2 | Train Loss:     1.6350 | Validation Loss:     1.5837\nEpoch    3 | Train Loss:     1.6092 | Validation Loss:     1.5665\nEpoch    4 | Train Loss:     1.5960 | Validation Loss:     1.5550\nEpoch    5 | Train Loss:     1.5856 | Validation Loss:     1.5445\nEpoch    6 | Train Loss:     1.5763 | Validation Loss:     1.5347\nEpoch    7 | Train Loss:     1.5674 | Validation Loss:     1.5278\nEpoch    8 | Train Loss:     1.5589 | Validation Loss:     1.5192\nEpoch    9 | Train Loss:     1.5511 | Validation Loss:     1.5093\nEpoch   10 | Train Loss:     1.5421 | Validation Loss:     1.5011\nEpoch   11 | Train Loss:     1.5331 | Validation Loss:     1.4929\nEpoch   12 | Train Loss:     1.5237 | Validation Loss:     1.4834\nEpoch   13 | Train Loss:     1.5140 | Validation Loss:     1.4742\nEpoch   14 | Train Loss:     1.5032 | Validation Loss:     1.4622\nEpoch   15 | Train Loss:     1.4901 | Validation Loss:     1.4507\nEpoch   16 | Train Loss:     1.4767 | Validation Loss:     1.4375\nEpoch   17 | Train Loss:     1.4614 | Validation Loss:     1.4210\nEpoch   18 | Train Loss:     1.4430 | Validation Loss:     1.4062\nEpoch   19 | Train Loss:     1.4229 | Validation Loss:     1.3814\nEpoch   20 | Train Loss:     1.3983 | Validation Loss:     1.3571\nEpoch   21 | Train Loss:     1.3599 | Validation Loss:     1.2968\nEpoch   22 | Train Loss:     1.3203 | Validation Loss:     1.2663\nEpoch   23 | Train Loss:     1.2880 | Validation Loss:     1.2371\nEpoch   24 | Train Loss:     1.2535 | Validation Loss:     1.2023\nEpoch   25 | Train Loss:     1.2142 | Validation Loss:     1.1655\nEpoch   26 | Train Loss:     1.1684 | Validation Loss:     1.1228\nEpoch   27 | Train Loss:     1.1151 | Validation Loss:     1.0769\nEpoch   28 | Train Loss:     1.0574 | Validation Loss:     1.0279\nEpoch   29 | Train Loss:     1.0072 | Validation Loss:     0.9951\nEpoch   30 | Train Loss:     0.9736 | Validation Loss:     0.9726\nEpoch   31 | Train Loss:     0.9491 | Validation Loss:     0.9549\nEpoch   32 | Train Loss:     0.9283 | Validation Loss:     0.9355\nEpoch   33 | Train Loss:     0.9086 | Validation Loss:     0.9177\nEpoch   34 | Train Loss:     0.8894 | Validation Loss:     0.9005\nEpoch   35 | Train Loss:     0.8709 | Validation Loss:     0.8827\nEpoch   36 | Train Loss:     0.8522 | Validation Loss:     0.8653\nEpoch   37 | Train Loss:     0.8340 | Validation Loss:     0.8472\nEpoch   38 | Train Loss:     0.8157 | Validation Loss:     0.8293\nEpoch   39 | Train Loss:     0.7981 | Validation Loss:     0.8127\nEpoch   40 | Train Loss:     0.7812 | Validation Loss:     0.7955\nEpoch   41 | Train Loss:     0.7642 | Validation Loss:     0.7797\nEpoch   42 | Train Loss:     0.7486 | Validation Loss:     0.7644\nEpoch   43 | Train Loss:     0.7377 | Validation Loss:     0.7520\nEpoch   44 | Train Loss:     0.7218 | Validation Loss:     0.7390\nEpoch   45 | Train Loss:     0.7097 | Validation Loss:     0.7283\nEpoch   46 | Train Loss:     0.6984 | Validation Loss:     0.7165\nEpoch   47 | Train Loss:     0.6867 | Validation Loss:     0.7068\nEpoch   48 | Train Loss:     0.6759 | Validation Loss:     0.6941\nEpoch   49 | Train Loss:     0.6650 | Validation Loss:     0.6834\nEpoch   50 | Train Loss:     0.6546 | Validation Loss:     0.6722\nEpoch   51 | Train Loss:     0.6444 | Validation Loss:     0.6626\nEpoch   52 | Train Loss:     0.6342 | Validation Loss:     0.6520\nEpoch   53 | Train Loss:     0.6243 | Validation Loss:     0.6417\nEpoch   54 | Train Loss:     0.6144 | Validation Loss:     0.6338\nEpoch   55 | Train Loss:     0.6055 | Validation Loss:     0.6223\nEpoch   56 | Train Loss:     0.5954 | Validation Loss:     0.6111\nEpoch   57 | Train Loss:     0.5859 | Validation Loss:     0.6018\nEpoch   58 | Train Loss:     0.5788 | Validation Loss:     0.5932\nEpoch   59 | Train Loss:     0.5673 | Validation Loss:     0.5828\nEpoch   60 | Train Loss:     0.5576 | Validation Loss:     0.5725\nEpoch   61 | Train Loss:     0.5477 | Validation Loss:     0.5661\nEpoch   62 | Train Loss:     0.5384 | Validation Loss:     0.5537\nEpoch   63 | Train Loss:     0.5294 | Validation Loss:     0.5434\nEpoch   64 | Train Loss:     0.5193 | Validation Loss:     0.5166\nEpoch   65 | Train Loss:     0.4686 | Validation Loss:     0.4718\nEpoch   66 | Train Loss:     0.4485 | Validation Loss:     0.4601\nEpoch   67 | Train Loss:     0.4344 | Validation Loss:     0.4443\nEpoch   68 | Train Loss:     0.4235 | Validation Loss:     0.4363\nEpoch   69 | Train Loss:     0.4137 | Validation Loss:     0.4257\nEpoch   70 | Train Loss:     0.4059 | Validation Loss:     0.4163\nEpoch   71 | Train Loss:     0.3971 | Validation Loss:     0.4080\nEpoch   72 | Train Loss:     0.3905 | Validation Loss:     0.4033\nEpoch   73 | Train Loss:     0.3830 | Validation Loss:     0.3942\nEpoch   74 | Train Loss:     0.3781 | Validation Loss:     0.3883\nEpoch   75 | Train Loss:     0.3713 | Validation Loss:     0.3831\nEpoch   76 | Train Loss:     0.3657 | Validation Loss:     0.3816\nEpoch   77 | Train Loss:     0.3615 | Validation Loss:     0.3803\nEpoch   78 | Train Loss:     0.3562 | Validation Loss:     0.3682\nEpoch   79 | Train Loss:     0.3515 | Validation Loss:     0.3652\nEpoch   80 | Train Loss:     0.3482 | Validation Loss:     0.3645\nEpoch   81 | Train Loss:     0.3440 | Validation Loss:     0.3577\nEpoch   82 | Train Loss:     0.3388 | Validation Loss:     0.3527\nEpoch   83 | Train Loss:     0.3355 | Validation Loss:     0.3486\nEpoch   84 | Train Loss:     0.3325 | Validation Loss:     0.3495\nEpoch   85 | Train Loss:     0.3303 | Validation Loss:     0.3420\nEpoch   86 | Train Loss:     0.3263 | Validation Loss:     0.3395\nEpoch   87 | Train Loss:     0.3229 | Validation Loss:     0.3360\nEpoch   88 | Train Loss:     0.3207 | Validation Loss:     0.3338\nEpoch   89 | Train Loss:     0.3193 | Validation Loss:     0.3382\nEpoch   90 | Train Loss:     0.3165 | Validation Loss:     0.3293\nEpoch   91 | Train Loss:     0.3129 | Validation Loss:     0.3285\nEpoch   92 | Train Loss:     0.3119 | Validation Loss:     0.3247\nEpoch   93 | Train Loss:     0.3093 | Validation Loss:     0.3219\nEpoch   94 | Train Loss:     0.3098 | Validation Loss:     0.3203\nEpoch   95 | Train Loss:     0.3050 | Validation Loss:     0.3192\nEpoch   96 | Train Loss:     0.3037 | Validation Loss:     0.3170\nEpoch   97 | Train Loss:     0.3023 | Validation Loss:     0.3203\nEpoch   98 | Train Loss:     0.3018 | Validation Loss:     0.3159\nEpoch   99 | Train Loss:     0.2850 | Validation Loss:     0.2726\nEpoch  100 | Train Loss:     0.2530 | Validation Loss:     0.2668\nEpoch  101 | Train Loss:     0.2464 | Validation Loss:     0.2614\nEpoch  102 | Train Loss:     0.2408 | Validation Loss:     0.2540\nEpoch  103 | Train Loss:     0.2388 | Validation Loss:     0.2554\nEpoch  104 | Train Loss:     0.2366 | Validation Loss:     0.2489\nEpoch  105 | Train Loss:     0.2378 | Validation Loss:     0.2472\nEpoch  106 | Train Loss:     0.2329 | Validation Loss:     0.2441\nEpoch  107 | Train Loss:     0.2314 | Validation Loss:     0.2416\nEpoch  108 | Train Loss:     0.2284 | Validation Loss:     0.2440\nEpoch  109 | Train Loss:     0.2279 | Validation Loss:     0.2465\nEpoch  110 | Train Loss:     0.2281 | Validation Loss:     0.2499\nEpoch  111 | Train Loss:     0.2247 | Validation Loss:     0.2415\nEpoch  112 | Train Loss:     0.2221 | Validation Loss:     0.2352\nEpoch  113 | Train Loss:     0.2235 | Validation Loss:     0.2350\nEpoch  114 | Train Loss:     0.2212 | Validation Loss:     0.2306\nEpoch  115 | Train Loss:     0.2180 | Validation Loss:     0.2298\nEpoch  116 | Train Loss:     0.2185 | Validation Loss:     0.2372\nEpoch  117 | Train Loss:     0.2167 | Validation Loss:     0.2277\nEpoch  118 | Train Loss:     0.2146 | Validation Loss:     0.2240\nEpoch  119 | Train Loss:     0.2128 | Validation Loss:     0.2233\nEpoch  120 | Train Loss:     0.2111 | Validation Loss:     0.2210\nEpoch  121 | Train Loss:     0.2105 | Validation Loss:     0.2229\nEpoch  122 | Train Loss:     0.2071 | Validation Loss:     0.2152\nEpoch  123 | Train Loss:     0.2059 | Validation Loss:     0.2156\nEpoch  124 | Train Loss:     0.2042 | Validation Loss:     0.2107\nEpoch  125 | Train Loss:     0.2019 | Validation Loss:     0.2114\nEpoch  126 | Train Loss:     0.2008 | Validation Loss:     0.2077\nEpoch  127 | Train Loss:     0.2001 | Validation Loss:     0.2140\nEpoch  128 | Train Loss:     0.1991 | Validation Loss:     0.2064\nEpoch  129 | Train Loss:     0.1972 | Validation Loss:     0.2034\nEpoch  130 | Train Loss:     0.1962 | Validation Loss:     0.2024\nEpoch  131 | Train Loss:     0.1955 | Validation Loss:     0.2091\nEpoch  132 | Train Loss:     0.1939 | Validation Loss:     0.1999\nEpoch  161 | Train Loss:     0.1741 | Validation Loss:     0.1777\nEpoch  162 | Train Loss:     0.1737 | Validation Loss:     0.1800\nEpoch  163 | Train Loss:     0.1726 | Validation Loss:     0.1758\nEpoch  164 | Train Loss:     0.1724 | Validation Loss:     0.1759\nEpoch  165 | Train Loss:     0.1718 | Validation Loss:     0.1766\nEpoch  166 | Train Loss:     0.1716 | Validation Loss:     0.1750\nEpoch  167 | Train Loss:     0.1710 | Validation Loss:     0.1754\nEpoch  168 | Train Loss:     0.1706 | Validation Loss:     0.1756\nEpoch  169 | Train Loss:     0.1704 | Validation Loss:     0.1726\nEpoch  170 | Train Loss:     0.1697 | Validation Loss:     0.1744\nEpoch  171 | Train Loss:     0.1697 | Validation Loss:     0.1733\nEpoch  172 | Train Loss:     0.1688 | Validation Loss:     0.1719\nEpoch  173 | Train Loss:     0.1685 | Validation Loss:     0.1712\nEpoch  174 | Train Loss:     0.1680 | Validation Loss:     0.1705\nEpoch  175 | Train Loss:     0.1676 | Validation Loss:     0.1712\nEpoch  176 | Train Loss:     0.1676 | Validation Loss:     0.1716\nEpoch  177 | Train Loss:     0.1672 | Validation Loss:     0.1697\nEpoch  178 | Train Loss:     0.1661 | Validation Loss:     0.1681\nEpoch  179 | Train Loss:     0.1662 | Validation Loss:     0.1681\nEpoch  180 | Train Loss:     0.1656 | Validation Loss:     0.1676\nEpoch  181 | Train Loss:     0.1650 | Validation Loss:     0.1667\nEpoch  182 | Train Loss:     0.1646 | Validation Loss:     0.1667\nEpoch  183 | Train Loss:     0.1641 | Validation Loss:     0.1671\nEpoch  184 | Train Loss:     0.1639 | Validation Loss:     0.1657\nEpoch  185 | Train Loss:     0.1634 | Validation Loss:     0.1669\nEpoch  186 | Train Loss:     0.1628 | Validation Loss:     0.1640\nEpoch  187 | Train Loss:     0.1624 | Validation Loss:     0.1673\nEpoch  188 | Train Loss:     0.1623 | Validation Loss:     0.1634\nEpoch  189 | Train Loss:     0.1620 | Validation Loss:     0.1675\nEpoch  190 | Train Loss:     0.1616 | Validation Loss:     0.1644\nEpoch  191 | Train Loss:     0.1609 | Validation Loss:     0.1654\nEpoch  192 | Train Loss:     0.1605 | Validation Loss:     0.1659\nEpoch  193 | Train Loss:     0.1603 | Validation Loss:     0.1631\nEpoch  194 | Train Loss:     0.1596 | Validation Loss:     0.1623\nEpoch  195 | Train Loss:     0.1590 | Validation Loss:     0.1640\nEpoch  196 | Train Loss:     0.1591 | Validation Loss:     0.1627\nEpoch  197 | Train Loss:     0.1584 | Validation Loss:     0.1617\nEpoch  198 | Train Loss:     0.1585 | Validation Loss:     0.1611\nEpoch  199 | Train Loss:     0.1576 | Validation Loss:     0.1599\nEpoch  200 | Train Loss:     0.1570 | Validation Loss:     0.1610\nEpoch  201 | Train Loss:     0.1566 | Validation Loss:     0.1612\nEpoch  202 | Train Loss:     0.1564 | Validation Loss:     0.1584\nEpoch  203 | Train Loss:     0.1557 | Validation Loss:     0.1574\nEpoch  204 | Train Loss:     0.1551 | Validation Loss:     0.1587\nEpoch  205 | Train Loss:     0.1546 | Validation Loss:     0.1575\nEpoch  206 | Train Loss:     0.1543 | Validation Loss:     0.1554\nEpoch  207 | Train Loss:     0.1542 | Validation Loss:     0.1560\nEpoch  208 | Train Loss:     0.1537 | Validation Loss:     0.1570\nEpoch  209 | Train Loss:     0.1533 | Validation Loss:     0.1550\nEpoch  210 | Train Loss:     0.1524 | Validation Loss:     0.1567\nEpoch  211 | Train Loss:     0.1521 | Validation Loss:     0.1547\nEpoch  212 | Train Loss:     0.1519 | Validation Loss:     0.1531\nEpoch  213 | Train Loss:     0.1517 | Validation Loss:     0.1525\nEpoch  214 | Train Loss:     0.1506 | Validation Loss:     0.1545\nEpoch  215 | Train Loss:     0.1503 | Validation Loss:     0.1531\nEpoch  216 | Train Loss:     0.1498 | Validation Loss:     0.1527\nEpoch  217 | Train Loss:     0.1494 | Validation Loss:     0.1504\nEpoch  218 | Train Loss:     0.1486 | Validation Loss:     0.1514\nEpoch  219 | Train Loss:     0.1483 | Validation Loss:     0.1498\nEpoch  220 | Train Loss:     0.1478 | Validation Loss:     0.1515\nEpoch  221 | Train Loss:     0.1470 | Validation Loss:     0.1511\nEpoch  222 | Train Loss:     0.1463 | Validation Loss:     0.1484\nEpoch  223 | Train Loss:     0.1459 | Validation Loss:     0.1488\nEpoch  224 | Train Loss:     0.1451 | Validation Loss:     0.1481\nEpoch  225 | Train Loss:     0.1447 | Validation Loss:     0.1458\nEpoch  226 | Train Loss:     0.1440 | Validation Loss:     0.1462\nEpoch  227 | Train Loss:     0.1436 | Validation Loss:     0.1449\nEpoch  228 | Train Loss:     0.1427 | Validation Loss:     0.1448\nEpoch  229 | Train Loss:     0.1420 | Validation Loss:     0.1441\nEpoch  230 | Train Loss:     0.1417 | Validation Loss:     0.1437\nEpoch  231 | Train Loss:     0.1411 | Validation Loss:     0.1433\nEpoch  232 | Train Loss:     0.1404 | Validation Loss:     0.1434\nEpoch  233 | Train Loss:     0.1395 | Validation Loss:     0.1424\nEpoch  234 | Train Loss:     0.1393 | Validation Loss:     0.1418\nEpoch  235 | Train Loss:     0.1386 | Validation Loss:     0.1452\nEpoch  236 | Train Loss:     0.1381 | Validation Loss:     0.1395\nEpoch  237 | Train Loss:     0.1371 | Validation Loss:     0.1388\nEpoch  238 | Train Loss:     0.1362 | Validation Loss:     0.1379\nEpoch  239 | Train Loss:     0.1360 | Validation Loss:     0.1384\nEpoch  240 | Train Loss:     0.1347 | Validation Loss:     0.1397\nEpoch  241 | Train Loss:     0.1342 | Validation Loss:     0.1364\nEpoch  242 | Train Loss:     0.1337 | Validation Loss:     0.1400\nEpoch  243 | Train Loss:     0.1335 | Validation Loss:     0.1361\nEpoch  244 | Train Loss:     0.1319 | Validation Loss:     0.1337\nEpoch  245 | Train Loss:     0.1315 | Validation Loss:     0.1350\nEpoch  246 | Train Loss:     0.1309 | Validation Loss:     0.1328\nEpoch  247 | Train Loss:     0.1309 | Validation Loss:     0.1337\nEpoch  248 | Train Loss:     0.1293 | Validation Loss:     0.1322\nEpoch  249 | Train Loss:     0.1286 | Validation Loss:     0.1311\nEpoch  250 | Train Loss:     0.1277 | Validation Loss:     0.1297\nEpoch  251 | Train Loss:     0.1275 | Validation Loss:     0.1317\nEpoch  252 | Train Loss:     0.1261 | Validation Loss:     0.1262\nEpoch  253 | Train Loss:     0.1252 | Validation Loss:     0.1293\nEpoch  254 | Train Loss:     0.1250 | Validation Loss:     0.1321\nEpoch  255 | Train Loss:     0.1242 | Validation Loss:     0.1219\nEpoch  256 | Train Loss:     0.1220 | Validation Loss:     0.1202\nEpoch  257 | Train Loss:     0.1201 | Validation Loss:     0.1280\nEpoch  258 | Train Loss:     0.1201 | Validation Loss:     0.1213\nEpoch  259 | Train Loss:     0.1192 | Validation Loss:     0.1172\nEpoch  260 | Train Loss:     0.1177 | Validation Loss:     0.1195\nEpoch  261 | Train Loss:     0.1169 | Validation Loss:     0.1170\nEpoch  262 | Train Loss:     0.1159 | Validation Loss:     0.1150\nEpoch  263 | Train Loss:     0.1151 | Validation Loss:     0.1160\nEpoch  264 | Train Loss:     0.1146 | Validation Loss:     0.1143\nEpoch  265 | Train Loss:     0.1141 | Validation Loss:     0.1129\nEpoch  266 | Train Loss:     0.1125 | Validation Loss:     0.1127\nEpoch  267 | Train Loss:     0.1111 | Validation Loss:     0.1121\nEpoch  268 | Train Loss:     0.1112 | Validation Loss:     0.1104\nEpoch  269 | Train Loss:     0.1097 | Validation Loss:     0.1099\nEpoch  270 | Train Loss:     0.1103 | Validation Loss:     0.1112\nEpoch  271 | Train Loss:     0.1081 | Validation Loss:     0.1096\nEpoch  272 | Train Loss:     0.1076 | Validation Loss:     0.1079\nEpoch  273 | Train Loss:     0.1072 | Validation Loss:     0.1158\nEpoch  274 | Train Loss:     0.1068 | Validation Loss:     0.1055\nEpoch  275 | Train Loss:     0.1049 | Validation Loss:     0.1049\nEpoch  276 | Train Loss:     0.1044 | Validation Loss:     0.1037\nEpoch  277 | Train Loss:     0.1029 | Validation Loss:     0.1032\nEpoch  278 | Train Loss:     0.1016 | Validation Loss:     0.1060\nEpoch  279 | Train Loss:     0.1025 | Validation Loss:     0.1013\nEpoch  280 | Train Loss:     0.0999 | Validation Loss:     0.1012\nEpoch  281 | Train Loss:     0.0997 | Validation Loss:     0.1015\nEpoch  282 | Train Loss:     0.0984 | Validation Loss:     0.0991\nEpoch  283 | Train Loss:     0.0977 | Validation Loss:     0.1000\nEpoch  284 | Train Loss:     0.0970 | Validation Loss:     0.0989\nEpoch  285 | Train Loss:     0.0964 | Validation Loss:     0.0967\nEpoch  286 | Train Loss:     0.0946 | Validation Loss:     0.0959\nEpoch  287 | Train Loss:     0.0933 | Validation Loss:     0.0945\nEpoch  288 | Train Loss:     0.0924 | Validation Loss:     0.0967\nEpoch  289 | Train Loss:     0.0919 | Validation Loss:     0.0921\nEpoch  290 | Train Loss:     0.0906 | Validation Loss:     0.0931\nEpoch  291 | Train Loss:     0.0901 | Validation Loss:     0.0915\nEpoch  292 | Train Loss:     0.0887 | Validation Loss:     0.0890\nEpoch  293 | Train Loss:     0.0877 | Validation Loss:     0.0881\nEpoch  294 | Train Loss:     0.0867 | Validation Loss:     0.0879\nEpoch  295 | Train Loss:     0.0864 | Validation Loss:     0.0884\nEpoch  296 | Train Loss:     0.0845 | Validation Loss:     0.0855\nEpoch  297 | Train Loss:     0.0844 | Validation Loss:     0.0851\nEpoch  298 | Train Loss:     0.0834 | Validation Loss:     0.0833\nEpoch  299 | Train Loss:     0.0822 | Validation Loss:     0.0839\nEpoch  300 | Train Loss:     0.0813 | Validation Loss:     0.0825\nEpoch  301 | Train Loss:     0.0804 | Validation Loss:     0.0814\nEpoch  302 | Train Loss:     0.0801 | Validation Loss:     0.0811\nEpoch  303 | Train Loss:     0.0788 | Validation Loss:     0.0795\nEpoch  304 | Train Loss:     0.0773 | Validation Loss:     0.0786\nEpoch  305 | Train Loss:     0.0766 | Validation Loss:     0.0788\nEpoch  306 | Train Loss:     0.0765 | Validation Loss:     0.0776\nEpoch  307 | Train Loss:     0.0753 | Validation Loss:     0.0804\nEpoch  308 | Train Loss:     0.0742 | Validation Loss:     0.0750\nEpoch  309 | Train Loss:     0.0735 | Validation Loss:     0.0754\nEpoch  310 | Train Loss:     0.0721 | Validation Loss:     0.0731\nEpoch  311 | Train Loss:     0.0715 | Validation Loss:     0.0731\nEpoch  312 | Train Loss:     0.0702 | Validation Loss:     0.0711\nEpoch  313 | Train Loss:     0.0693 | Validation Loss:     0.0711\nEpoch  314 | Train Loss:     0.0685 | Validation Loss:     0.0699\nEpoch  315 | Train Loss:     0.0683 | Validation Loss:     0.0699\nEpoch  316 | Train Loss:     0.0669 | Validation Loss:     0.0685\nEpoch  317 | Train Loss:     0.0664 | Validation Loss:     0.0670\nEpoch  318 | Train Loss:     0.0652 | Validation Loss:     0.0671\nEpoch  319 | Train Loss:     0.0646 | Validation Loss:     0.0662\nEpoch  320 | Train Loss:     0.0632 | Validation Loss:     0.0645\nEpoch  321 | Train Loss:     0.0629 | Validation Loss:     0.0645\nEpoch  322 | Train Loss:     0.0620 | Validation Loss:     0.0631\nEpoch  323 | Train Loss:     0.0609 | Validation Loss:     0.0620\nEpoch  324 | Train Loss:     0.0602 | Validation Loss:     0.0616\nEpoch  325 | Train Loss:     0.0591 | Validation Loss:     0.0601\nEpoch  326 | Train Loss:     0.0583 | Validation Loss:     0.0613\nEpoch  327 | Train Loss:     0.0581 | Validation Loss:     0.0595\nEpoch  328 | Train Loss:     0.0562 | Validation Loss:     0.0584\nEpoch  329 | Train Loss:     0.0559 | Validation Loss:     0.0572\nEpoch  330 | Train Loss:     0.0543 | Validation Loss:     0.0568\nEpoch  331 | Train Loss:     0.0535 | Validation Loss:     0.0546\nEpoch  332 | Train Loss:     0.0532 | Validation Loss:     0.0543\nEpoch  333 | Train Loss:     0.0523 | Validation Loss:     0.0530\nEpoch  334 | Train Loss:     0.0517 | Validation Loss:     0.0524\nEpoch  335 | Train Loss:     0.0505 | Validation Loss:     0.0516\nEpoch  336 | Train Loss:     0.0497 | Validation Loss:     0.0505\nEpoch  337 | Train Loss:     0.0492 | Validation Loss:     0.0507\nEpoch  338 | Train Loss:     0.0480 | Validation Loss:     0.0491\nEpoch  339 | Train Loss:     0.0478 | Validation Loss:     0.0484\nEpoch  340 | Train Loss:     0.0462 | Validation Loss:     0.0472\nEpoch  341 | Train Loss:     0.0460 | Validation Loss:     0.0462\nEpoch  342 | Train Loss:     0.0452 | Validation Loss:     0.0460\nEpoch  343 | Train Loss:     0.0441 | Validation Loss:     0.0450\nEpoch  344 | Train Loss:     0.0444 | Validation Loss:     0.0440\nEpoch  345 | Train Loss:     0.0424 | Validation Loss:     0.0435\nEpoch  346 | Train Loss:     0.0415 | Validation Loss:     0.0435\nEpoch  347 | Train Loss:     0.0417 | Validation Loss:     0.0420\nEpoch  348 | Train Loss:     0.0413 | Validation Loss:     0.0412\nEpoch  349 | Train Loss:     0.0393 | Validation Loss:     0.0397\nEpoch  350 | Train Loss:     0.0390 | Validation Loss:     0.0392\nEpoch  351 | Train Loss:     0.0377 | Validation Loss:     0.0388\nEpoch  352 | Train Loss:     0.0381 | Validation Loss:     0.0372\nEpoch  353 | Train Loss:     0.0358 | Validation Loss:     0.0363\nEpoch  354 | Train Loss:     0.0355 | Validation Loss:     0.0434\nEpoch  355 | Train Loss:     0.0346 | Validation Loss:     0.0352\nEpoch  356 | Train Loss:     0.0340 | Validation Loss:     0.0342\nEpoch  357 | Train Loss:     0.0330 | Validation Loss:     0.0339\nEpoch  358 | Train Loss:     0.0335 | Validation Loss:     0.0325\nEpoch  359 | Train Loss:     0.0316 | Validation Loss:     0.0323\nEpoch  360 | Train Loss:     0.0312 | Validation Loss:     0.0316\nEpoch  361 | Train Loss:     0.0312 | Validation Loss:     0.0313\nEpoch  362 | Train Loss:     0.0299 | Validation Loss:     0.0301\nEpoch  363 | Train Loss:     0.0296 | Validation Loss:     0.0293\nEpoch  364 | Train Loss:     0.0290 | Validation Loss:     0.0288\nEpoch  365 | Train Loss:     0.0280 | Validation Loss:     0.0279\nEpoch  366 | Train Loss:     0.0274 | Validation Loss:     0.0277\nEpoch  367 | Train Loss:     0.0264 | Validation Loss:     0.0263\nEpoch  368 | Train Loss:     0.0267 | Validation Loss:     0.0262\nEpoch  369 | Train Loss:     0.0267 | Validation Loss:     0.0271\nEpoch  370 | Train Loss:     0.0246 | Validation Loss:     0.0252\nEpoch  371 | Train Loss:     0.0242 | Validation Loss:     0.0239\nEpoch  372 | Train Loss:     0.0237 | Validation Loss:     0.0242\nEpoch  373 | Train Loss:     0.0231 | Validation Loss:     0.0254\nEpoch  374 | Train Loss:     0.0226 | Validation Loss:     0.0230\nEpoch  375 | Train Loss:     0.0216 | Validation Loss:     0.0218\nEpoch  376 | Train Loss:     0.0213 | Validation Loss:     0.0227\nEpoch  377 | Train Loss:     0.0220 | Validation Loss:     0.0203\nEpoch  378 | Train Loss:     0.0198 | Validation Loss:     0.0198\nEpoch  379 | Train Loss:     0.0191 | Validation Loss:     0.0200\nEpoch  380 | Train Loss:     0.0197 | Validation Loss:     0.0190\nEpoch  381 | Train Loss:     0.0185 | Validation Loss:     0.0195\nEpoch  382 | Train Loss:     0.0181 | Validation Loss:     0.0184\nEpoch  383 | Train Loss:     0.0174 | Validation Loss:     0.0180\nEpoch  384 | Train Loss:     0.0168 | Validation Loss:     0.0165\nEpoch  385 | Train Loss:     0.0166 | Validation Loss:     0.0160\nEpoch  386 | Train Loss:     0.0161 | Validation Loss:     0.0159\nEpoch  387 | Train Loss:     0.0166 | Validation Loss:     0.0159\nEpoch  388 | Train Loss:     0.0149 | Validation Loss:     0.0150\nEpoch  389 | Train Loss:     0.0146 | Validation Loss:     0.0145\nEpoch  390 | Train Loss:     0.0141 | Validation Loss:     0.0140\nEpoch  391 | Train Loss:     0.0138 | Validation Loss:     0.0136\nEpoch  392 | Train Loss:     0.0134 | Validation Loss:     0.0144\nEpoch  393 | Train Loss:     0.0129 | Validation Loss:     0.0124\nEpoch  394 | Train Loss:     0.0126 | Validation Loss:     0.0131\nEpoch  395 | Train Loss:     0.0121 | Validation Loss:     0.0130\nEpoch  396 | Train Loss:     0.0122 | Validation Loss:     0.0113\nEpoch  397 | Train Loss:     0.0114 | Validation Loss:     0.0117\nEpoch  398 | Train Loss:     0.0116 | Validation Loss:     0.0116\nEpoch  399 | Train Loss:     0.0114 | Validation Loss:     0.0104\nEpoch  400 | Train Loss:     0.0103 | Validation Loss:     0.0104\nEpoch  401 | Train Loss:     0.0103 | Validation Loss:     0.0105\nEpoch  402 | Train Loss:     0.0098 | Validation Loss:     0.0096\nEpoch  403 | Train Loss:     0.0100 | Validation Loss:     0.0098\nEpoch  404 | Train Loss:     0.0093 | Validation Loss:     0.0099\nEpoch  405 | Train Loss:     0.0096 | Validation Loss:     0.0088\nEpoch  406 | Train Loss:     0.0088 | Validation Loss:     0.0082\nEpoch  407 | Train Loss:     0.0082 | Validation Loss:     0.0081\nEpoch  408 | Train Loss:     0.0081 | Validation Loss:     0.0078\nEpoch  409 | Train Loss:     0.0085 | Validation Loss:     0.0078\nEpoch  410 | Train Loss:     0.0080 | Validation Loss:     0.0077\nEpoch  411 | Train Loss:     0.0077 | Validation Loss:     0.0090\nEpoch  412 | Train Loss:     0.0080 | Validation Loss:     0.0071\nEpoch  413 | Train Loss:     0.0080 | Validation Loss:     0.0067\nEpoch  414 | Train Loss:     0.0066 | Validation Loss:     0.0064\nEpoch  415 | Train Loss:     0.0065 | Validation Loss:     0.0076\nEpoch  416 | Train Loss:     0.0065 | Validation Loss:     0.0077\nEpoch  417 | Train Loss:     0.0062 | Validation Loss:     0.0065\nEpoch  418 | Train Loss:     0.0060 | Validation Loss:     0.0060\nEpoch  419 | Train Loss:     0.0066 | Validation Loss:     0.0060\nEpoch  420 | Train Loss:     0.0058 | Validation Loss:     0.0060\nEpoch  421 | Train Loss:     0.0061 | Validation Loss:     0.0057\nEpoch  422 | Train Loss:     0.0064 | Validation Loss:     0.0066\nEpoch  423 | Train Loss:     0.0056 | Validation Loss:     0.0068\nEpoch  424 | Train Loss:     0.0054 | Validation Loss:     0.0057\nEpoch  425 | Train Loss:     0.0055 | Validation Loss:     0.0051\nEpoch  426 | Train Loss:     0.0055 | Validation Loss:     0.0058\nEpoch  427 | Train Loss:     0.0050 | Validation Loss:     0.0047\nEpoch  428 | Train Loss:     0.0054 | Validation Loss:     0.0054\nEpoch  429 | Train Loss:     0.0047 | Validation Loss:     0.0050\nEpoch  430 | Train Loss:     0.0051 | Validation Loss:     0.0066\nEpoch  431 | Train Loss:     0.0052 | Validation Loss:     0.0046\nEpoch  432 | Train Loss:     0.0049 | Validation Loss:     0.0041\nEpoch  433 | Train Loss:     0.0047 | Validation Loss:     0.0053\nEpoch  434 | Train Loss:     0.0046 | Validation Loss:     0.0058\nEpoch  435 | Train Loss:     0.0044 | Validation Loss:     0.0045\nEpoch  436 | Train Loss:     0.0041 | Validation Loss:     0.0050\nEpoch  437 | Train Loss:     0.0040 | Validation Loss:     0.0071\nEpoch  438 | Train Loss:     0.0047 | Validation Loss:     0.0050\nEpoch  439 | Train Loss:     0.0042 | Validation Loss:     0.0042\nEpoch  440 | Train Loss:     0.0040 | Validation Loss:     0.0055\nEpoch  441 | Train Loss:     0.0048 | Validation Loss:     0.0039\nEpoch  442 | Train Loss:     0.0041 | Validation Loss:     0.0054\nEpoch  443 | Train Loss:     0.0037 | Validation Loss:     0.0036\nEpoch  444 | Train Loss:     0.0038 | Validation Loss:     0.0039\nEpoch  445 | Train Loss:     0.0039 | Validation Loss:     0.0058\nEpoch  446 | Train Loss:     0.0040 | Validation Loss:     0.0040\nEpoch  447 | Train Loss:     0.0047 | Validation Loss:     0.0042\nEpoch  448 | Train Loss:     0.0039 | Validation Loss:     0.0049\nEpoch  449 | Train Loss:     0.0033 | Validation Loss:     0.0037\nEpoch  450 | Train Loss:     0.0036 | Validation Loss:     0.0034\nEpoch  451 | Train Loss:     0.0033 | Validation Loss:     0.0040\nEpoch  452 | Train Loss:     0.0034 | Validation Loss:     0.0036\nEpoch  453 | Train Loss:     0.0037 | Validation Loss:     0.0043\nEpoch  454 | Train Loss:     0.0043 | Validation Loss:     0.0045\nEpoch  455 | Train Loss:     0.0041 | Validation Loss:     0.0036\nEpoch  456 | Train Loss:     0.0038 | Validation Loss:     0.0035\nEpoch  457 | Train Loss:     0.0034 | Validation Loss:     0.0037\nEpoch  458 | Train Loss:     0.0034 | Validation Loss:     0.0033\nEpoch  459 | Train Loss:     0.0043 | Validation Loss:     0.0032\nEpoch  460 | Train Loss:     0.0031 | Validation Loss:     0.0046\nEpoch  461 | Train Loss:     0.0038 | Validation Loss:     0.0039\nEpoch  462 | Train Loss:     0.0037 | Validation Loss:     0.0036\nEpoch  463 | Train Loss:     0.0033 | Validation Loss:     0.0030\nEpoch  464 | Train Loss:     0.0032 | Validation Loss:     0.0034\nEpoch  465 | Train Loss:     0.0029 | Validation Loss:     0.0040\nEpoch  466 | Train Loss:     0.0033 | Validation Loss:     0.0040\nEpoch  467 | Train Loss:     0.0038 | Validation Loss:     0.0051\nEpoch  468 | Train Loss:     0.0030 | Validation Loss:     0.0045\nEpoch  469 | Train Loss:     0.0041 | Validation Loss:     0.0037\nEpoch  470 | Train Loss:     0.0030 | Validation Loss:     0.0028\nEpoch  471 | Train Loss:     0.0037 | Validation Loss:     0.0046\nEpoch  472 | Train Loss:     0.0034 | Validation Loss:     0.0034\nEpoch  473 | Train Loss:     0.0030 | Validation Loss:     0.0032\nEpoch  474 | Train Loss:     0.0030 | Validation Loss:     0.0063\nEpoch  475 | Train Loss:     0.0034 | Validation Loss:     0.0036\nEpoch  476 | Train Loss:     0.0030 | Validation Loss:     0.0037\nEpoch  477 | Train Loss:     0.0032 | Validation Loss:     0.0034\nEpoch  478 | Train Loss:     0.0034 | Validation Loss:     0.0043\nEpoch  479 | Train Loss:     0.0032 | Validation Loss:     0.0029\nEpoch  480 | Train Loss:     0.0029 | Validation Loss:     0.0028\nEpoch  481 | Train Loss:     0.0031 | Validation Loss:     0.0031\nEpoch  482 | Train Loss:     0.0028 | Validation Loss:     0.0031\nEpoch  483 | Train Loss:     0.0034 | Validation Loss:     0.0033\nEpoch  484 | Train Loss:     0.0031 | Validation Loss:     0.0030\nEpoch  485 | Train Loss:     0.0026 | Validation Loss:     0.0029\nEpoch  486 | Train Loss:     0.0032 | Validation Loss:     0.0034\nEpoch  487 | Train Loss:     0.0040 | Validation Loss:     0.0053\nEpoch  488 | Train Loss:     0.0046 | Validation Loss:     0.0042\nEpoch  489 | Train Loss:     0.0040 | Validation Loss:     0.0030\nEpoch  490 | Train Loss:     0.0033 | Validation Loss:     0.0039\nEpoch  491 | Train Loss:     0.0028 | Validation Loss:     0.0028\nEpoch  492 | Train Loss:     0.0029 | Validation Loss:     0.0030\nEpoch  493 | Train Loss:     0.0026 | Validation Loss:     0.0033\nEpoch  494 | Train Loss:     0.0029 | Validation Loss:     0.0043\nEpoch  495 | Train Loss:     0.0028 | Validation Loss:     0.0048\nEpoch  496 | Train Loss:     0.0039 | Validation Loss:     0.0029\nEpoch  497 | Train Loss:     0.0029 | Validation Loss:     0.0025\nEpoch  499 | Train Loss:     0.0028 | Validation Loss:     0.0044\nEpoch  500 | Train Loss:     0.0027 | Validation Loss:     0.0027\nEpoch  529 | Train Loss:     0.0025 | Validation Loss:     0.0040\nEpoch  530 | Train Loss:     0.0028 | Validation Loss:     0.0035\nEpoch  531 | Train Loss:     0.0032 | Validation Loss:     0.0053\nEpoch  532 | Train Loss:     0.0027 | Validation Loss:     0.0031\nEpoch  533 | Train Loss:     0.0026 | Validation Loss:     0.0037\nEpoch  534 | Train Loss:     0.0028 | Validation Loss:     0.0026\nEpoch  535 | Train Loss:     0.0029 | Validation Loss:     0.0047\nEpoch  536 | Train Loss:     0.0028 | Validation Loss:     0.0023\nEpoch  537 | Train Loss:     0.0026 | Validation Loss:     0.0033\nEpoch  538 | Train Loss:     0.0024 | Validation Loss:     0.0038\nEpoch  539 | Train Loss:     0.0024 | Validation Loss:     0.0032\nEpoch  540 | Train Loss:     0.0034 | Validation Loss:     0.0027\nEpoch  541 | Train Loss:     0.0026 | Validation Loss:     0.0034\nEpoch  542 | Train Loss:     0.0026 | Validation Loss:     0.0023\nEpoch  543 | Train Loss:     0.0025 | Validation Loss:     0.0030\nEpoch  544 | Train Loss:     0.0023 | Validation Loss:     0.0023\nEpoch  545 | Train Loss:     0.0026 | Validation Loss:     0.0024\nEpoch  546 | Train Loss:     0.0028 | Validation Loss:     0.0023\nEpoch  547 | Train Loss:     0.0026 | Validation Loss:     0.0024\nEpoch  548 | Train Loss:     0.0024 | Validation Loss:     0.0035\nEpoch  549 | Train Loss:     0.0026 | Validation Loss:     0.0026\nEpoch  550 | Train Loss:     0.0026 | Validation Loss:     0.0022\nEpoch  551 | Train Loss:     0.0024 | Validation Loss:     0.0024\nEpoch  552 | Train Loss:     0.0025 | Validation Loss:     0.0023\nEpoch  553 | Train Loss:     0.0022 | Validation Loss:     0.0023\nEpoch  554 | Train Loss:     0.0026 | Validation Loss:     0.0052\nEpoch  555 | Train Loss:     0.0030 | Validation Loss:     0.0033\nEpoch  556 | Train Loss:     0.0024 | Validation Loss:     0.0022\nEpoch  557 | Train Loss:     0.0023 | Validation Loss:     0.0025\nEpoch  558 | Train Loss:     0.0023 | Validation Loss:     0.0028\nEpoch  559 | Train Loss:     0.0025 | Validation Loss:     0.0026\nEpoch  560 | Train Loss:     0.0023 | Validation Loss:     0.0022\nEpoch  561 | Train Loss:     0.0039 | Validation Loss:     0.0049\nEpoch  562 | Train Loss:     0.0028 | Validation Loss:     0.0033\nEpoch  563 | Train Loss:     0.0024 | Validation Loss:     0.0020\nEpoch  564 | Train Loss:     0.0024 | Validation Loss:     0.0049\nEpoch  565 | Train Loss:     0.0026 | Validation Loss:     0.0029\nEpoch  566 | Train Loss:     0.0022 | Validation Loss:     0.0022\nEpoch  567 | Train Loss:     0.0027 | Validation Loss:     0.0026\nEpoch  568 | Train Loss:     0.0027 | Validation Loss:     0.0025\nEpoch  569 | Train Loss:     0.0031 | Validation Loss:     0.0037\nEpoch  570 | Train Loss:     0.0027 | Validation Loss:     0.0028\nEpoch  571 | Train Loss:     0.0026 | Validation Loss:     0.0024\nEpoch  572 | Train Loss:     0.0027 | Validation Loss:     0.0024\nEpoch  573 | Train Loss:     0.0022 | Validation Loss:     0.0020\nEpoch  574 | Train Loss:     0.0024 | Validation Loss:     0.0022\nEpoch  575 | Train Loss:     0.0024 | Validation Loss:     0.0025\nEpoch  576 | Train Loss:     0.0024 | Validation Loss:     0.0031\nEpoch  577 | Train Loss:     0.0027 | Validation Loss:     0.0021\nEpoch  578 | Train Loss:     0.0020 | Validation Loss:     0.0020\nEpoch  579 | Train Loss:     0.0025 | Validation Loss:     0.0026\nEpoch  580 | Train Loss:     0.0022 | Validation Loss:     0.0022\nEpoch  581 | Train Loss:     0.0022 | Validation Loss:     0.0025\nEpoch  582 | Train Loss:     0.0023 | Validation Loss:     0.0021\nEpoch  583 | Train Loss:     0.0030 | Validation Loss:     0.0023\nEpoch  584 | Train Loss:     0.0032 | Validation Loss:     0.0029\nEpoch  585 | Train Loss:     0.0027 | Validation Loss:     0.0019\nEpoch  586 | Train Loss:     0.0028 | Validation Loss:     0.0027\nEpoch  587 | Train Loss:     0.0027 | Validation Loss:     0.0032\nEpoch  588 | Train Loss:     0.0022 | Validation Loss:     0.0030\nEpoch  589 | Train Loss:     0.0022 | Validation Loss:     0.0026\nEpoch  590 | Train Loss:     0.0028 | Validation Loss:     0.0038\nEpoch  591 | Train Loss:     0.0025 | Validation Loss:     0.0028\nEpoch  592 | Train Loss:     0.0023 | Validation Loss:     0.0022\nEpoch  593 | Train Loss:     0.0026 | Validation Loss:     0.0027\nEpoch  594 | Train Loss:     0.0028 | Validation Loss:     0.0022\nEpoch  595 | Train Loss:     0.0022 | Validation Loss:     0.0024\nEpoch  596 | Train Loss:     0.0020 | Validation Loss:     0.0035\nEpoch  597 | Train Loss:     0.0023 | Validation Loss:     0.0020\nEpoch  598 | Train Loss:     0.0020 | Validation Loss:     0.0021\nEpoch  599 | Train Loss:     0.0021 | Validation Loss:     0.0018\nEpoch  600 | Train Loss:     0.0021 | Validation Loss:     0.0029\nEpoch  601 | Train Loss:     0.0025 | Validation Loss:     0.0022\nEpoch  602 | Train Loss:     0.0023 | Validation Loss:     0.0035\nEpoch  603 | Train Loss:     0.0027 | Validation Loss:     0.0025\nEpoch  604 | Train Loss:     0.0020 | Validation Loss:     0.0023\nEpoch  605 | Train Loss:     0.0022 | Validation Loss:     0.0025\nEpoch  606 | Train Loss:     0.0026 | Validation Loss:     0.0024\nEpoch  607 | Train Loss:     0.0031 | Validation Loss:     0.0021\nEpoch  608 | Train Loss:     0.0022 | Validation Loss:     0.0018\nEpoch  609 | Train Loss:     0.0023 | Validation Loss:     0.0026\nEpoch  610 | Train Loss:     0.0026 | Validation Loss:     0.0042\nEpoch  611 | Train Loss:     0.0040 | Validation Loss:     0.0028\nEpoch  612 | Train Loss:     0.0022 | Validation Loss:     0.0020\nEpoch  613 | Train Loss:     0.0021 | Validation Loss:     0.0022\nEpoch  614 | Train Loss:     0.0020 | Validation Loss:     0.0025\nEpoch  615 | Train Loss:     0.0021 | Validation Loss:     0.0025\nEpoch  616 | Train Loss:     0.0023 | Validation Loss:     0.0023\nEpoch  617 | Train Loss:     0.0025 | Validation Loss:     0.0027\nEpoch  618 | Train Loss:     0.0022 | Validation Loss:     0.0019\nEpoch  619 | Train Loss:     0.0023 | Validation Loss:     0.0034\nEpoch  620 | Train Loss:     0.0027 | Validation Loss:     0.0040\nEpoch  621 | Train Loss:     0.0025 | Validation Loss:     0.0023\nEpoch  622 | Train Loss:     0.0021 | Validation Loss:     0.0019\nEpoch  623 | Train Loss:     0.0021 | Validation Loss:     0.0018\nEpoch  624 | Train Loss:     0.0020 | Validation Loss:     0.0030\nEpoch  625 | Train Loss:     0.0020 | Validation Loss:     0.0022\nEpoch  626 | Train Loss:     0.0020 | Validation Loss:     0.0023\nEpoch  627 | Train Loss:     0.0024 | Validation Loss:     0.0027\nEpoch  628 | Train Loss:     0.0020 | Validation Loss:     0.0019\nEpoch  629 | Train Loss:     0.0023 | Validation Loss:     0.0018\nEpoch  630 | Train Loss:     0.0025 | Validation Loss:     0.0021\nEpoch  631 | Train Loss:     0.0022 | Validation Loss:     0.0019\nEpoch  632 | Train Loss:     0.0022 | Validation Loss:     0.0022\nEpoch  633 | Train Loss:     0.0022 | Validation Loss:     0.0019\nEpoch  634 | Train Loss:     0.0027 | Validation Loss:     0.0023\nEpoch  635 | Train Loss:     0.0025 | Validation Loss:     0.0029\nEpoch  636 | Train Loss:     0.0020 | Validation Loss:     0.0026\nEpoch  637 | Train Loss:     0.0019 | Validation Loss:     0.0024\nEpoch  638 | Train Loss:     0.0021 | Validation Loss:     0.0019\nEpoch  639 | Train Loss:     0.0021 | Validation Loss:     0.0019\nEpoch  640 | Train Loss:     0.0026 | Validation Loss:     0.0018\nEpoch  641 | Train Loss:     0.0027 | Validation Loss:     0.0024\nEpoch  642 | Train Loss:     0.0021 | Validation Loss:     0.0017\nEpoch  643 | Train Loss:     0.0020 | Validation Loss:     0.0019\nEpoch  644 | Train Loss:     0.0018 | Validation Loss:     0.0017\nEpoch  645 | Train Loss:     0.0022 | Validation Loss:     0.0022\nEpoch  646 | Train Loss:     0.0020 | Validation Loss:     0.0018\nEpoch  647 | Train Loss:     0.0022 | Validation Loss:     0.0022\nEpoch  648 | Train Loss:     0.0020 | Validation Loss:     0.0020\nEpoch  649 | Train Loss:     0.0021 | Validation Loss:     0.0021\nEpoch  650 | Train Loss:     0.0024 | Validation Loss:     0.0018\nEpoch  651 | Train Loss:     0.0021 | Validation Loss:     0.0022\nEpoch  652 | Train Loss:     0.0019 | Validation Loss:     0.0017\nEpoch  653 | Train Loss:     0.0019 | Validation Loss:     0.0019\nEpoch  654 | Train Loss:     0.0018 | Validation Loss:     0.0016\nEpoch  655 | Train Loss:     0.0019 | Validation Loss:     0.0022\nEpoch  656 | Train Loss:     0.0019 | Validation Loss:     0.0018\nEpoch  657 | Train Loss:     0.0022 | Validation Loss:     0.0062\nEpoch  658 | Train Loss:     0.0026 | Validation Loss:     0.0027\nEpoch  659 | Train Loss:     0.0021 | Validation Loss:     0.0020\nEpoch  660 | Train Loss:     0.0020 | Validation Loss:     0.0021\nEpoch  661 | Train Loss:     0.0023 | Validation Loss:     0.0018\nEpoch  662 | Train Loss:     0.0034 | Validation Loss:     0.0023\nEpoch  663 | Train Loss:     0.0025 | Validation Loss:     0.0019\nEpoch  664 | Train Loss:     0.0019 | Validation Loss:     0.0019\nEpoch  665 | Train Loss:     0.0020 | Validation Loss:     0.0022\nEpoch  666 | Train Loss:     0.0026 | Validation Loss:     0.0040\nEpoch  667 | Train Loss:     0.0021 | Validation Loss:     0.0016\nEpoch  668 | Train Loss:     0.0019 | Validation Loss:     0.0020\nEpoch  669 | Train Loss:     0.0019 | Validation Loss:     0.0024\nEpoch  670 | Train Loss:     0.0020 | Validation Loss:     0.0020\nEpoch  671 | Train Loss:     0.0019 | Validation Loss:     0.0016\nEpoch  672 | Train Loss:     0.0017 | Validation Loss:     0.0016\nEpoch  673 | Train Loss:     0.0021 | Validation Loss:     0.0018\nEpoch  674 | Train Loss:     0.0023 | Validation Loss:     0.0032\nEpoch  675 | Train Loss:     0.0029 | Validation Loss:     0.0022\nEpoch  676 | Train Loss:     0.0018 | Validation Loss:     0.0015\nEpoch  677 | Train Loss:     0.0018 | Validation Loss:     0.0020\nEpoch  678 | Train Loss:     0.0022 | Validation Loss:     0.0017\nEpoch  679 | Train Loss:     0.0018 | Validation Loss:     0.0016\nEpoch  680 | Train Loss:     0.0017 | Validation Loss:     0.0020\nEpoch  681 | Train Loss:     0.0019 | Validation Loss:     0.0020\nEpoch  682 | Train Loss:     0.0019 | Validation Loss:     0.0016\nEpoch  683 | Train Loss:     0.0018 | Validation Loss:     0.0021\nEpoch  684 | Train Loss:     0.0024 | Validation Loss:     0.0070\nEpoch  685 | Train Loss:     0.0032 | Validation Loss:     0.0040\nEpoch  686 | Train Loss:     0.0021 | Validation Loss:     0.0023\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","Cell \u001b[0;32mIn[12], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m optimizer \u001b[38;5;241m=\u001b[39m optim\u001b[38;5;241m.\u001b[39mAdam(model_prepu\u001b[38;5;241m.\u001b[39mparameters(), lr\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.00001\u001b[39m)\n\u001b[0;32m----> 2\u001b[0m \u001b[43mevaluate_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_prepu\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcustom_train_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m)\u001b[49m\n","Cell \u001b[0;32mIn[2], line 74\u001b[0m, in \u001b[0;36mevaluate_model\u001b[0;34m(model, custom_train_loader, criterion, optimizer)\u001b[0m\n\u001b[1;32m     72\u001b[0m inputs, labels \u001b[38;5;241m=\u001b[39m inputs\u001b[38;5;241m.\u001b[39mto(device), labels\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     73\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m---> 74\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mview\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m129\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     75\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(outputs, labels\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m))\n\u001b[1;32m     77\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39misnan(loss):\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n","Cell \u001b[0;32mIn[11], line 28\u001b[0m, in \u001b[0;36mPairwiseCustomActivationNetwork.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     26\u001b[0m             out \u001b[38;5;241m=\u001b[39m layer(concatenated_outputs)\n\u001b[1;32m     27\u001b[0m \u001b[38;5;66;03m#             out = custom_layer(out, concatenated_outputs)\u001b[39;00m\n\u001b[0;32m---> 28\u001b[0m             out \u001b[38;5;241m=\u001b[39m \u001b[43mcustom_layer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     29\u001b[0m             outputs\u001b[38;5;241m.\u001b[39mappend(out)\n\u001b[1;32m     31\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m outputs[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n","Cell \u001b[0;32mIn[6], line 7\u001b[0m, in \u001b[0;36mCustomActivationLayerTwo.forward\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, inputs):\n\u001b[0;32m----> 7\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwhere\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m<\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbeta\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "],"ename":"KeyboardInterrupt","evalue":"","output_type":"error"}]},{"cell_type":"code","source":"optimizer = optim.Adam(model_prepu.parameters(), lr=0.000001)\nevaluate_model(model_prepu, custom_train_loader, criterion, optimizer)","metadata":{"execution":{"iopub.status.busy":"2024-06-15T02:21:58.681996Z","iopub.execute_input":"2024-06-15T02:21:58.682349Z","iopub.status.idle":"2024-06-15T02:24:54.168663Z","shell.execute_reply.started":"2024-06-15T02:21:58.682323Z","shell.execute_reply":"2024-06-15T02:24:54.167080Z"},"trusted":true},"execution_count":22,"outputs":[{"name":"stdout","text":"Epoch    1 | Train Loss:     1.6478 | Validation Loss:     1.6153\nEpoch    2 | Train Loss:     1.5846 | Validation Loss:     1.5767\nEpoch    3 | Train Loss:     1.5490 | Validation Loss:     1.5534\nEpoch    4 | Train Loss:     1.5245 | Validation Loss:     1.5353\nEpoch    5 | Train Loss:     1.5053 | Validation Loss:     1.5214\nEpoch    6 | Train Loss:     1.4903 | Validation Loss:     1.5103\nEpoch    7 | Train Loss:     1.4789 | Validation Loss:     1.5026\nEpoch    8 | Train Loss:     1.4708 | Validation Loss:     1.4978\nEpoch    9 | Train Loss:     1.4653 | Validation Loss:     1.4952\nEpoch   10 | Train Loss:     1.4618 | Validation Loss:     1.4936\nEpoch   11 | Train Loss:     1.4598 | Validation Loss:     1.4932\nEpoch   12 | Train Loss:     1.4586 | Validation Loss:     1.4933\nEpoch   13 | Train Loss:     1.4580 | Validation Loss:     1.4933\nEpoch   14 | Train Loss:     1.4577 | Validation Loss:     1.4932\nEpoch   15 | Train Loss:     1.4576 | Validation Loss:     1.4940\nEpoch   16 | Train Loss:     1.4574 | Validation Loss:     1.4936\nEpoch   17 | Train Loss:     1.4575 | Validation Loss:     1.4935\nEpoch   18 | Train Loss:     1.4575 | Validation Loss:     1.4937\nEpoch   19 | Train Loss:     1.4574 | Validation Loss:     1.4942\nEpoch   20 | Train Loss:     1.4574 | Validation Loss:     1.4940\nEpoch   21 | Train Loss:     1.4573 | Validation Loss:     1.4938\nEpoch   22 | Train Loss:     1.4575 | Validation Loss:     1.4939\nEpoch   23 | Train Loss:     1.4575 | Validation Loss:     1.4941\nEpoch   24 | Train Loss:     1.4574 | Validation Loss:     1.4939\nEpoch   25 | Train Loss:     1.4574 | Validation Loss:     1.4939\nEpoch   26 | Train Loss:     1.4573 | Validation Loss:     1.4942\nEpoch   27 | Train Loss:     1.4574 | Validation Loss:     1.4938\nEpoch   28 | Train Loss:     1.4575 | Validation Loss:     1.4938\nEpoch   29 | Train Loss:     1.4574 | Validation Loss:     1.4939\nEpoch   30 | Train Loss:     1.4574 | Validation Loss:     1.4941\nEpoch   31 | Train Loss:     1.4573 | Validation Loss:     1.4939\nEpoch   32 | Train Loss:     1.4574 | Validation Loss:     1.4939\nEpoch   33 | Train Loss:     1.4575 | Validation Loss:     1.4937\nEpoch   34 | Train Loss:     1.4573 | Validation Loss:     1.4940\nEpoch   35 | Train Loss:     1.4574 | Validation Loss:     1.4941\nEpoch   36 | Train Loss:     1.4574 | Validation Loss:     1.4941\nEpoch   37 | Train Loss:     1.4575 | Validation Loss:     1.4939\nEpoch   38 | Train Loss:     1.4574 | Validation Loss:     1.4941\nEpoch   39 | Train Loss:     1.4575 | Validation Loss:     1.4936\nEpoch   40 | Train Loss:     1.4573 | Validation Loss:     1.4937\nEpoch   41 | Train Loss:     1.4574 | Validation Loss:     1.4937\nEpoch   42 | Train Loss:     1.4573 | Validation Loss:     1.4940\nEpoch   43 | Train Loss:     1.4573 | Validation Loss:     1.4939\nEpoch   44 | Train Loss:     1.4573 | Validation Loss:     1.4937\nEpoch   45 | Train Loss:     1.4573 | Validation Loss:     1.4937\nEpoch   46 | Train Loss:     1.4573 | Validation Loss:     1.4937\nEpoch   47 | Train Loss:     1.4573 | Validation Loss:     1.4937\nEpoch   48 | Train Loss:     1.4574 | Validation Loss:     1.4938\nEpoch   49 | Train Loss:     1.4573 | Validation Loss:     1.4938\nEpoch   50 | Train Loss:     1.4572 | Validation Loss:     1.4938\nEpoch   51 | Train Loss:     1.4572 | Validation Loss:     1.4940\nEpoch   52 | Train Loss:     1.4572 | Validation Loss:     1.4937\nEpoch   53 | Train Loss:     1.4572 | Validation Loss:     1.4938\nEpoch   54 | Train Loss:     1.4573 | Validation Loss:     1.4937\nEpoch   55 | Train Loss:     1.4574 | Validation Loss:     1.4942\nEpoch   56 | Train Loss:     1.4572 | Validation Loss:     1.4937\nEpoch   57 | Train Loss:     1.4572 | Validation Loss:     1.4937\nEpoch   58 | Train Loss:     1.4572 | Validation Loss:     1.4936\nEpoch   59 | Train Loss:     1.4572 | Validation Loss:     1.4936\nEpoch   60 | Train Loss:     1.4572 | Validation Loss:     1.4936\nEpoch   61 | Train Loss:     1.4572 | Validation Loss:     1.4939\nEpoch   62 | Train Loss:     1.4572 | Validation Loss:     1.4939\nEpoch   63 | Train Loss:     1.4573 | Validation Loss:     1.4939\nEpoch   64 | Train Loss:     1.4572 | Validation Loss:     1.4936\nEpoch   65 | Train Loss:     1.4573 | Validation Loss:     1.4935\nEpoch   66 | Train Loss:     1.4573 | Validation Loss:     1.4935\nEpoch   67 | Train Loss:     1.4572 | Validation Loss:     1.4936\nEpoch   68 | Train Loss:     1.4571 | Validation Loss:     1.4935\nEpoch   69 | Train Loss:     1.4572 | Validation Loss:     1.4935\nEpoch   70 | Train Loss:     1.4572 | Validation Loss:     1.4938\nEpoch   71 | Train Loss:     1.4571 | Validation Loss:     1.4935\nEpoch   72 | Train Loss:     1.4571 | Validation Loss:     1.4934\nEpoch   73 | Train Loss:     1.4573 | Validation Loss:     1.4936\nEpoch   74 | Train Loss:     1.4572 | Validation Loss:     1.4939\nEpoch   75 | Train Loss:     1.4571 | Validation Loss:     1.4935\nEpoch   76 | Train Loss:     1.4572 | Validation Loss:     1.4937\nEpoch   77 | Train Loss:     1.4571 | Validation Loss:     1.4937\nEpoch   78 | Train Loss:     1.4571 | Validation Loss:     1.4938\nEpoch   79 | Train Loss:     1.4571 | Validation Loss:     1.4934\nEpoch   80 | Train Loss:     1.4571 | Validation Loss:     1.4935\nEpoch   81 | Train Loss:     1.4571 | Validation Loss:     1.4937\nEpoch   82 | Train Loss:     1.4571 | Validation Loss:     1.4937\nEpoch   83 | Train Loss:     1.4571 | Validation Loss:     1.4936\nEpoch   84 | Train Loss:     1.4570 | Validation Loss:     1.4937\nEpoch   85 | Train Loss:     1.4570 | Validation Loss:     1.4935\nEpoch   86 | Train Loss:     1.4570 | Validation Loss:     1.4939\nEpoch   87 | Train Loss:     1.4571 | Validation Loss:     1.4937\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","Cell \u001b[0;32mIn[22], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m optimizer \u001b[38;5;241m=\u001b[39m optim\u001b[38;5;241m.\u001b[39mAdam(model_prepu\u001b[38;5;241m.\u001b[39mparameters(), lr\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.000001\u001b[39m)\n\u001b[0;32m----> 2\u001b[0m \u001b[43mevaluate_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_prepu\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcustom_train_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m)\u001b[49m\n","Cell \u001b[0;32mIn[6], line 74\u001b[0m, in \u001b[0;36mevaluate_model\u001b[0;34m(model, custom_train_loader, criterion, optimizer)\u001b[0m\n\u001b[1;32m     72\u001b[0m inputs, labels \u001b[38;5;241m=\u001b[39m inputs\u001b[38;5;241m.\u001b[39mto(device), labels\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     73\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m---> 74\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mview\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     75\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(outputs, labels\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m))\n\u001b[1;32m     77\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39misnan(loss):\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n","Cell \u001b[0;32mIn[21], line 28\u001b[0m, in \u001b[0;36mPairwiseCustomActivationNetwork.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     26\u001b[0m             out \u001b[38;5;241m=\u001b[39m layer(concatenated_outputs)\n\u001b[1;32m     27\u001b[0m \u001b[38;5;66;03m#             out = custom_layer(out, concatenated_outputs)\u001b[39;00m\n\u001b[0;32m---> 28\u001b[0m             out \u001b[38;5;241m=\u001b[39m \u001b[43mcustom_layer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     29\u001b[0m             outputs\u001b[38;5;241m.\u001b[39mappend(out)\n\u001b[1;32m     31\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m outputs[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n","Cell \u001b[0;32mIn[16], line 7\u001b[0m, in \u001b[0;36mCustomActivationLayerTwo.forward\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, inputs):\n\u001b[0;32m----> 7\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwhere\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m<\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbeta\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "],"ename":"KeyboardInterrupt","evalue":"","output_type":"error"}]},{"cell_type":"code","source":"optimizer = optim.Adam(model_prepu.parameters(), lr=0.000001)\nevaluate_model(model_prepu, custom_train_loader, criterion, optimizer)","metadata":{"execution":{"iopub.status.busy":"2024-06-14T21:00:09.002382Z","iopub.execute_input":"2024-06-14T21:00:09.002758Z"},"trusted":true},"execution_count":null,"outputs":[{"name":"stdout","text":"Epoch    1 | Train Loss:     1.0042 | Validation Loss:     1.0352\nEpoch    2 | Train Loss:     1.0042 | Validation Loss:     1.0352\nEpoch    3 | Train Loss:     1.0042 | Validation Loss:     1.0352\nEpoch    4 | Train Loss:     1.0042 | Validation Loss:     1.0352\nEpoch    5 | Train Loss:     1.0042 | Validation Loss:     1.0352\nEpoch    6 | Train Loss:     1.0042 | Validation Loss:     1.0352\nEpoch    7 | Train Loss:     1.0042 | Validation Loss:     1.0352\nEpoch    8 | Train Loss:     1.0042 | Validation Loss:     1.0352\nEpoch    9 | Train Loss:     1.0042 | Validation Loss:     1.0352\nEpoch   10 | Train Loss:     1.0042 | Validation Loss:     1.0352\nEpoch   11 | Train Loss:     1.0042 | Validation Loss:     1.0352\nEpoch   12 | Train Loss:     1.0042 | Validation Loss:     1.0352\nEpoch   13 | Train Loss:     1.0042 | Validation Loss:     1.0352\nEpoch   14 | Train Loss:     1.0042 | Validation Loss:     1.0352\nEpoch   15 | Train Loss:     1.0042 | Validation Loss:     1.0352\nEpoch   16 | Train Loss:     1.0042 | Validation Loss:     1.0352\nEpoch   17 | Train Loss:     1.0042 | Validation Loss:     1.0352\nEpoch   18 | Train Loss:     1.0042 | Validation Loss:     1.0352\nEpoch   19 | Train Loss:     1.0042 | Validation Loss:     1.0352\nEpoch   20 | Train Loss:     1.0042 | Validation Loss:     1.0352\nEpoch   21 | Train Loss:     1.0042 | Validation Loss:     1.0352\nEpoch   22 | Train Loss:     1.0042 | Validation Loss:     1.0352\nEpoch   23 | Train Loss:     1.0042 | Validation Loss:     1.0352\nEpoch   24 | Train Loss:     1.0042 | Validation Loss:     1.0352\nEpoch   25 | Train Loss:     1.0042 | Validation Loss:     1.0352\nEpoch   26 | Train Loss:     1.0042 | Validation Loss:     1.0352\nEpoch   27 | Train Loss:     1.0042 | Validation Loss:     1.0352\nEpoch   28 | Train Loss:     1.0042 | Validation Loss:     1.0352\nEpoch   29 | Train Loss:     1.0042 | Validation Loss:     1.0352\nEpoch   30 | Train Loss:     1.0042 | Validation Loss:     1.0352\nEpoch   31 | Train Loss:     1.0042 | Validation Loss:     1.0352\nEpoch   32 | Train Loss:     1.0042 | Validation Loss:     1.0352\nEpoch   33 | Train Loss:     1.0042 | Validation Loss:     1.0352\nEpoch   34 | Train Loss:     1.0042 | Validation Loss:     1.0352\nEpoch   35 | Train Loss:     1.0042 | Validation Loss:     1.0352\nEpoch   36 | Train Loss:     1.0042 | Validation Loss:     1.0352\nEpoch   37 | Train Loss:     1.0042 | Validation Loss:     1.0352\nEpoch   38 | Train Loss:     1.0042 | Validation Loss:     1.0352\nEpoch   39 | Train Loss:     1.0042 | Validation Loss:     1.0352\nEpoch   40 | Train Loss:     1.0042 | Validation Loss:     1.0352\nEpoch   55 | Train Loss:     1.0042 | Validation Loss:     1.0352\nEpoch   56 | Train Loss:     1.0042 | Validation Loss:     1.0352\nEpoch   57 | Train Loss:     1.0042 | Validation Loss:     1.0352\nEpoch   58 | Train Loss:     1.0042 | Validation Loss:     1.0352\nEpoch   59 | Train Loss:     1.0042 | Validation Loss:     1.0352\nEpoch   60 | Train Loss:     1.0042 | Validation Loss:     1.0352\nEpoch   61 | Train Loss:     1.0042 | Validation Loss:     1.0352\nEpoch   62 | Train Loss:     1.0042 | Validation Loss:     1.0352\nEpoch   63 | Train Loss:     1.0042 | Validation Loss:     1.0352\nEpoch   64 | Train Loss:     1.0042 | Validation Loss:     1.0352\nEpoch   65 | Train Loss:     1.0042 | Validation Loss:     1.0352\nEpoch   66 | Train Loss:     1.0042 | Validation Loss:     1.0352\nEpoch   67 | Train Loss:     1.0042 | Validation Loss:     1.0352\nEpoch   68 | Train Loss:     1.0042 | Validation Loss:     1.0352\nEpoch   69 | Train Loss:     1.0042 | Validation Loss:     1.0352\nEpoch   70 | Train Loss:     1.0042 | Validation Loss:     1.0352\nEpoch   71 | Train Loss:     1.0042 | Validation Loss:     1.0352\nEpoch   72 | Train Loss:     1.0042 | Validation Loss:     1.0352\nEpoch   73 | Train Loss:     1.0042 | Validation Loss:     1.0352\nEpoch   74 | Train Loss:     1.0042 | Validation Loss:     1.0352\nEpoch   75 | Train Loss:     1.0042 | Validation Loss:     1.0352\nEpoch   76 | Train Loss:     1.0042 | Validation Loss:     1.0352\n","output_type":"stream"}]},{"cell_type":"code","source":"evaluate_model(model_prepu, custom_train_loader, criterion, optimizer)","metadata":{"execution":{"iopub.status.busy":"2024-06-14T19:59:00.551980Z","iopub.execute_input":"2024-06-14T19:59:00.552830Z","iopub.status.idle":"2024-06-14T20:55:55.756059Z","shell.execute_reply.started":"2024-06-14T19:59:00.552798Z","shell.execute_reply":"2024-06-14T20:55:55.754361Z"},"trusted":true},"execution_count":124,"outputs":[{"name":"stdout","text":"Epoch    1 | Train Loss:     1.6789 | Validation Loss:     1.5830\nEpoch    2 | Train Loss:     1.5242 | Validation Loss:     1.4780\nEpoch    3 | Train Loss:     1.4754 | Validation Loss:     1.4677\nEpoch    4 | Train Loss:     1.4647 | Validation Loss:     1.4606\nEpoch    5 | Train Loss:     1.4568 | Validation Loss:     1.4557\nEpoch    6 | Train Loss:     1.4504 | Validation Loss:     1.4514\nEpoch    7 | Train Loss:     1.4457 | Validation Loss:     1.4481\nEpoch    8 | Train Loss:     1.4417 | Validation Loss:     1.4460\nEpoch    9 | Train Loss:     1.4379 | Validation Loss:     1.4438\nEpoch   10 | Train Loss:     1.4353 | Validation Loss:     1.4411\nEpoch   11 | Train Loss:     1.4317 | Validation Loss:     1.4405\nEpoch   12 | Train Loss:     1.4295 | Validation Loss:     1.4372\nEpoch   13 | Train Loss:     1.4269 | Validation Loss:     1.4357\nEpoch   14 | Train Loss:     1.4241 | Validation Loss:     1.4339\nEpoch   15 | Train Loss:     1.4220 | Validation Loss:     1.4311\nEpoch   16 | Train Loss:     1.4192 | Validation Loss:     1.4316\nEpoch   17 | Train Loss:     1.4169 | Validation Loss:     1.4270\nEpoch   18 | Train Loss:     1.4139 | Validation Loss:     1.4246\nEpoch   19 | Train Loss:     1.4109 | Validation Loss:     1.4204\nEpoch   20 | Train Loss:     1.4084 | Validation Loss:     1.4176\nEpoch   21 | Train Loss:     1.4048 | Validation Loss:     1.4143\nEpoch   22 | Train Loss:     1.4015 | Validation Loss:     1.4111\nEpoch   23 | Train Loss:     1.3979 | Validation Loss:     1.4071\nEpoch   24 | Train Loss:     1.3942 | Validation Loss:     1.4061\nEpoch   25 | Train Loss:     1.3904 | Validation Loss:     1.4011\nEpoch   26 | Train Loss:     1.3861 | Validation Loss:     1.3947\nEpoch   27 | Train Loss:     1.3815 | Validation Loss:     1.3898\nEpoch   28 | Train Loss:     1.3763 | Validation Loss:     1.3846\nEpoch   29 | Train Loss:     1.3706 | Validation Loss:     1.3787\nEpoch   30 | Train Loss:     1.3643 | Validation Loss:     1.3727\nEpoch   31 | Train Loss:     1.3572 | Validation Loss:     1.3655\nEpoch   32 | Train Loss:     1.3488 | Validation Loss:     1.3549\nEpoch   33 | Train Loss:     1.3388 | Validation Loss:     1.3441\nEpoch   34 | Train Loss:     1.3264 | Validation Loss:     1.3314\nEpoch   35 | Train Loss:     1.3119 | Validation Loss:     1.3145\nEpoch   36 | Train Loss:     1.2935 | Validation Loss:     1.2941\nEpoch   37 | Train Loss:     1.2726 | Validation Loss:     1.2710\nEpoch   38 | Train Loss:     1.2493 | Validation Loss:     1.2461\nEpoch   39 | Train Loss:     1.2240 | Validation Loss:     1.2212\nEpoch   40 | Train Loss:     1.1991 | Validation Loss:     1.1972\nEpoch   41 | Train Loss:     1.1744 | Validation Loss:     1.1726\nEpoch   42 | Train Loss:     1.1505 | Validation Loss:     1.1499\nEpoch   43 | Train Loss:     1.1280 | Validation Loss:     1.1280\nEpoch   44 | Train Loss:     1.1048 | Validation Loss:     1.1067\nEpoch   45 | Train Loss:     1.0820 | Validation Loss:     1.0842\nEpoch   46 | Train Loss:     1.0573 | Validation Loss:     1.0619\nEpoch   47 | Train Loss:     1.0316 | Validation Loss:     1.0338\nEpoch   48 | Train Loss:     1.0028 | Validation Loss:     1.0070\nEpoch   49 | Train Loss:     0.9725 | Validation Loss:     0.9762\nEpoch   50 | Train Loss:     0.9411 | Validation Loss:     0.9464\nEpoch   51 | Train Loss:     0.9106 | Validation Loss:     0.9202\nEpoch   52 | Train Loss:     0.8825 | Validation Loss:     0.8920\nEpoch   53 | Train Loss:     0.8572 | Validation Loss:     0.8692\nEpoch   54 | Train Loss:     0.8352 | Validation Loss:     0.8500\nEpoch   55 | Train Loss:     0.8188 | Validation Loss:     0.8333\nEpoch   56 | Train Loss:     0.8009 | Validation Loss:     0.8201\nEpoch   57 | Train Loss:     0.7885 | Validation Loss:     0.8115\nEpoch   58 | Train Loss:     0.7785 | Validation Loss:     0.8008\nEpoch   59 | Train Loss:     0.7700 | Validation Loss:     0.7941\nEpoch   60 | Train Loss:     0.7632 | Validation Loss:     0.7901\nEpoch   61 | Train Loss:     0.7587 | Validation Loss:     0.7847\nEpoch   62 | Train Loss:     0.7545 | Validation Loss:     0.7811\nEpoch   63 | Train Loss:     0.7493 | Validation Loss:     0.7775\nEpoch   64 | Train Loss:     0.7468 | Validation Loss:     0.7753\nEpoch   65 | Train Loss:     0.7436 | Validation Loss:     0.7724\nEpoch   66 | Train Loss:     0.7426 | Validation Loss:     0.7709\nEpoch   67 | Train Loss:     0.7398 | Validation Loss:     0.7692\nEpoch   68 | Train Loss:     0.7379 | Validation Loss:     0.7679\nEpoch   69 | Train Loss:     0.7361 | Validation Loss:     0.7654\nEpoch   70 | Train Loss:     0.7341 | Validation Loss:     0.7643\nEpoch   71 | Train Loss:     0.7334 | Validation Loss:     0.7646\nEpoch   72 | Train Loss:     0.7315 | Validation Loss:     0.7614\nEpoch   73 | Train Loss:     0.7302 | Validation Loss:     0.7640\nEpoch   74 | Train Loss:     0.7292 | Validation Loss:     0.7592\nEpoch   75 | Train Loss:     0.7280 | Validation Loss:     0.7581\nEpoch   76 | Train Loss:     0.7273 | Validation Loss:     0.7622\nEpoch   77 | Train Loss:     0.7254 | Validation Loss:     0.7563\nEpoch   78 | Train Loss:     0.7253 | Validation Loss:     0.7553\nEpoch   79 | Train Loss:     0.7232 | Validation Loss:     0.7544\nEpoch   80 | Train Loss:     0.7222 | Validation Loss:     0.7588\nEpoch   81 | Train Loss:     0.7218 | Validation Loss:     0.7527\nEpoch   82 | Train Loss:     0.7222 | Validation Loss:     0.7549\nEpoch   83 | Train Loss:     0.7200 | Validation Loss:     0.7512\nEpoch   84 | Train Loss:     0.7193 | Validation Loss:     0.7512\nEpoch   85 | Train Loss:     0.7185 | Validation Loss:     0.7498\nEpoch   86 | Train Loss:     0.7170 | Validation Loss:     0.7490\nEpoch   87 | Train Loss:     0.7167 | Validation Loss:     0.7503\nEpoch   88 | Train Loss:     0.7156 | Validation Loss:     0.7477\nEpoch   89 | Train Loss:     0.7156 | Validation Loss:     0.7469\nEpoch   90 | Train Loss:     0.7144 | Validation Loss:     0.7476\nEpoch   91 | Train Loss:     0.7136 | Validation Loss:     0.7458\nEpoch   92 | Train Loss:     0.7126 | Validation Loss:     0.7492\nEpoch   93 | Train Loss:     0.7128 | Validation Loss:     0.7445\nEpoch   94 | Train Loss:     0.7116 | Validation Loss:     0.7451\nEpoch   95 | Train Loss:     0.7110 | Validation Loss:     0.7439\nEpoch   96 | Train Loss:     0.7104 | Validation Loss:     0.7460\nEpoch   97 | Train Loss:     0.7104 | Validation Loss:     0.7424\nEpoch   98 | Train Loss:     0.7089 | Validation Loss:     0.7418\nEpoch   99 | Train Loss:     0.7083 | Validation Loss:     0.7413\nEpoch  100 | Train Loss:     0.7078 | Validation Loss:     0.7404\nEpoch  101 | Train Loss:     0.7070 | Validation Loss:     0.7416\nEpoch  102 | Train Loss:     0.7060 | Validation Loss:     0.7405\nEpoch  103 | Train Loss:     0.7060 | Validation Loss:     0.7394\nEpoch  104 | Train Loss:     0.7050 | Validation Loss:     0.7383\nEpoch  105 | Train Loss:     0.7048 | Validation Loss:     0.7381\nEpoch  106 | Train Loss:     0.7044 | Validation Loss:     0.7385\nEpoch  107 | Train Loss:     0.7035 | Validation Loss:     0.7391\nEpoch  108 | Train Loss:     0.7031 | Validation Loss:     0.7364\nEpoch  109 | Train Loss:     0.7032 | Validation Loss:     0.7365\nEpoch  110 | Train Loss:     0.7023 | Validation Loss:     0.7375\nEpoch  111 | Train Loss:     0.7009 | Validation Loss:     0.7342\nEpoch  112 | Train Loss:     0.6999 | Validation Loss:     0.7344\nEpoch  113 | Train Loss:     0.6998 | Validation Loss:     0.7335\nEpoch  114 | Train Loss:     0.6998 | Validation Loss:     0.7327\nEpoch  115 | Train Loss:     0.6987 | Validation Loss:     0.7326\nEpoch  116 | Train Loss:     0.6986 | Validation Loss:     0.7313\nEpoch  117 | Train Loss:     0.6974 | Validation Loss:     0.7307\nEpoch  118 | Train Loss:     0.6965 | Validation Loss:     0.7301\nEpoch  119 | Train Loss:     0.6962 | Validation Loss:     0.7333\nEpoch  120 | Train Loss:     0.6972 | Validation Loss:     0.7294\nEpoch  121 | Train Loss:     0.6947 | Validation Loss:     0.7284\nEpoch  122 | Train Loss:     0.6938 | Validation Loss:     0.7276\nEpoch  123 | Train Loss:     0.6934 | Validation Loss:     0.7269\nEpoch  124 | Train Loss:     0.6927 | Validation Loss:     0.7263\nEpoch  125 | Train Loss:     0.6920 | Validation Loss:     0.7272\nEpoch  126 | Train Loss:     0.6910 | Validation Loss:     0.7248\nEpoch  127 | Train Loss:     0.6910 | Validation Loss:     0.7243\nEpoch  128 | Train Loss:     0.6893 | Validation Loss:     0.7249\nEpoch  129 | Train Loss:     0.6891 | Validation Loss:     0.7226\nEpoch  130 | Train Loss:     0.6888 | Validation Loss:     0.7222\nEpoch  131 | Train Loss:     0.6869 | Validation Loss:     0.7208\nEpoch  132 | Train Loss:     0.6865 | Validation Loss:     0.7200\nEpoch  133 | Train Loss:     0.6854 | Validation Loss:     0.7203\nEpoch  134 | Train Loss:     0.6851 | Validation Loss:     0.7178\nEpoch  135 | Train Loss:     0.6829 | Validation Loss:     0.7179\nEpoch  136 | Train Loss:     0.6823 | Validation Loss:     0.7168\nEpoch  137 | Train Loss:     0.6812 | Validation Loss:     0.7149\nEpoch  138 | Train Loss:     0.6804 | Validation Loss:     0.7140\nEpoch  139 | Train Loss:     0.6792 | Validation Loss:     0.7124\nEpoch  140 | Train Loss:     0.6773 | Validation Loss:     0.7121\nEpoch  141 | Train Loss:     0.6766 | Validation Loss:     0.7101\nEpoch  142 | Train Loss:     0.6757 | Validation Loss:     0.7112\nEpoch  143 | Train Loss:     0.6743 | Validation Loss:     0.7077\nEpoch  144 | Train Loss:     0.6731 | Validation Loss:     0.7121\nEpoch  145 | Train Loss:     0.6716 | Validation Loss:     0.7050\nEpoch  146 | Train Loss:     0.6703 | Validation Loss:     0.7035\nEpoch  147 | Train Loss:     0.6681 | Validation Loss:     0.7017\nEpoch  148 | Train Loss:     0.6676 | Validation Loss:     0.7041\nEpoch  149 | Train Loss:     0.6658 | Validation Loss:     0.6987\nEpoch  150 | Train Loss:     0.6639 | Validation Loss:     0.6971\nEpoch  151 | Train Loss:     0.6628 | Validation Loss:     0.6952\nEpoch  152 | Train Loss:     0.6600 | Validation Loss:     0.6940\nEpoch  153 | Train Loss:     0.6590 | Validation Loss:     0.6924\nEpoch  154 | Train Loss:     0.6578 | Validation Loss:     0.6923\nEpoch  155 | Train Loss:     0.6558 | Validation Loss:     0.6899\nEpoch  156 | Train Loss:     0.6541 | Validation Loss:     0.6874\nEpoch  157 | Train Loss:     0.6524 | Validation Loss:     0.6851\nEpoch  158 | Train Loss:     0.6506 | Validation Loss:     0.6838\nEpoch  159 | Train Loss:     0.6494 | Validation Loss:     0.6812\nEpoch  160 | Train Loss:     0.6475 | Validation Loss:     0.6798\nEpoch  161 | Train Loss:     0.6453 | Validation Loss:     0.6782\nEpoch  162 | Train Loss:     0.6441 | Validation Loss:     0.6760\nEpoch  163 | Train Loss:     0.6428 | Validation Loss:     0.6746\nEpoch  164 | Train Loss:     0.6406 | Validation Loss:     0.6725\nEpoch  165 | Train Loss:     0.6397 | Validation Loss:     0.6709\nEpoch  166 | Train Loss:     0.6377 | Validation Loss:     0.6701\nEpoch  167 | Train Loss:     0.6366 | Validation Loss:     0.6689\nEpoch  168 | Train Loss:     0.6351 | Validation Loss:     0.6668\nEpoch  169 | Train Loss:     0.6339 | Validation Loss:     0.6655\nEpoch  170 | Train Loss:     0.6325 | Validation Loss:     0.6646\nEpoch  171 | Train Loss:     0.6318 | Validation Loss:     0.6626\nEpoch  172 | Train Loss:     0.6299 | Validation Loss:     0.6618\nEpoch  173 | Train Loss:     0.6293 | Validation Loss:     0.6606\nEpoch  174 | Train Loss:     0.6281 | Validation Loss:     0.6593\nEpoch  175 | Train Loss:     0.6268 | Validation Loss:     0.6582\nEpoch  176 | Train Loss:     0.6259 | Validation Loss:     0.6572\nEpoch  177 | Train Loss:     0.6253 | Validation Loss:     0.6564\nEpoch  178 | Train Loss:     0.6242 | Validation Loss:     0.6550\nEpoch  179 | Train Loss:     0.6237 | Validation Loss:     0.6553\nEpoch  180 | Train Loss:     0.6229 | Validation Loss:     0.6532\nEpoch  181 | Train Loss:     0.6221 | Validation Loss:     0.6525\nEpoch  182 | Train Loss:     0.6212 | Validation Loss:     0.6522\nEpoch  183 | Train Loss:     0.6209 | Validation Loss:     0.6510\nEpoch  184 | Train Loss:     0.6203 | Validation Loss:     0.6505\nEpoch  185 | Train Loss:     0.6203 | Validation Loss:     0.6498\nEpoch  186 | Train Loss:     0.6191 | Validation Loss:     0.6490\nEpoch  187 | Train Loss:     0.6185 | Validation Loss:     0.6492\nEpoch  188 | Train Loss:     0.6182 | Validation Loss:     0.6491\nEpoch  189 | Train Loss:     0.6178 | Validation Loss:     0.6481\nEpoch  190 | Train Loss:     0.6170 | Validation Loss:     0.6469\nEpoch  191 | Train Loss:     0.6167 | Validation Loss:     0.6462\nEpoch  192 | Train Loss:     0.6162 | Validation Loss:     0.6461\nEpoch  193 | Train Loss:     0.6158 | Validation Loss:     0.6452\nEpoch  194 | Train Loss:     0.6154 | Validation Loss:     0.6450\nEpoch  195 | Train Loss:     0.6149 | Validation Loss:     0.6446\nEpoch  196 | Train Loss:     0.6144 | Validation Loss:     0.6439\nEpoch  197 | Train Loss:     0.6141 | Validation Loss:     0.6435\nEpoch  198 | Train Loss:     0.6136 | Validation Loss:     0.6429\nEpoch  199 | Train Loss:     0.6134 | Validation Loss:     0.6425\nEpoch  200 | Train Loss:     0.6127 | Validation Loss:     0.6427\nEpoch  201 | Train Loss:     0.6125 | Validation Loss:     0.6418\nEpoch  202 | Train Loss:     0.6121 | Validation Loss:     0.6418\nEpoch  203 | Train Loss:     0.6116 | Validation Loss:     0.6411\nEpoch  204 | Train Loss:     0.6115 | Validation Loss:     0.6406\nEpoch  205 | Train Loss:     0.6107 | Validation Loss:     0.6400\nEpoch  206 | Train Loss:     0.6105 | Validation Loss:     0.6393\nEpoch  207 | Train Loss:     0.6105 | Validation Loss:     0.6389\nEpoch  208 | Train Loss:     0.6099 | Validation Loss:     0.6384\nEpoch  209 | Train Loss:     0.6092 | Validation Loss:     0.6391\nEpoch  210 | Train Loss:     0.6092 | Validation Loss:     0.6382\nEpoch  211 | Train Loss:     0.6086 | Validation Loss:     0.6376\nEpoch  212 | Train Loss:     0.6082 | Validation Loss:     0.6368\nEpoch  213 | Train Loss:     0.6076 | Validation Loss:     0.6364\nEpoch  214 | Train Loss:     0.6074 | Validation Loss:     0.6362\nEpoch  215 | Train Loss:     0.6069 | Validation Loss:     0.6355\nEpoch  216 | Train Loss:     0.6065 | Validation Loss:     0.6352\nEpoch  217 | Train Loss:     0.6060 | Validation Loss:     0.6347\nEpoch  218 | Train Loss:     0.6056 | Validation Loss:     0.6347\nEpoch  219 | Train Loss:     0.6052 | Validation Loss:     0.6336\nEpoch  220 | Train Loss:     0.6047 | Validation Loss:     0.6332\nEpoch  221 | Train Loss:     0.6044 | Validation Loss:     0.6329\nEpoch  222 | Train Loss:     0.6040 | Validation Loss:     0.6326\nEpoch  223 | Train Loss:     0.6035 | Validation Loss:     0.6321\nEpoch  224 | Train Loss:     0.6031 | Validation Loss:     0.6317\nEpoch  225 | Train Loss:     0.6028 | Validation Loss:     0.6314\nEpoch  226 | Train Loss:     0.6024 | Validation Loss:     0.6309\nEpoch  227 | Train Loss:     0.6019 | Validation Loss:     0.6301\nEpoch  228 | Train Loss:     0.6013 | Validation Loss:     0.6299\nEpoch  229 | Train Loss:     0.6011 | Validation Loss:     0.6293\nEpoch  230 | Train Loss:     0.6005 | Validation Loss:     0.6289\nEpoch  231 | Train Loss:     0.6002 | Validation Loss:     0.6284\nEpoch  232 | Train Loss:     0.5998 | Validation Loss:     0.6280\nEpoch  233 | Train Loss:     0.5993 | Validation Loss:     0.6275\nEpoch  234 | Train Loss:     0.5987 | Validation Loss:     0.6270\nEpoch  235 | Train Loss:     0.5983 | Validation Loss:     0.6271\nEpoch  236 | Train Loss:     0.5982 | Validation Loss:     0.6269\nEpoch  237 | Train Loss:     0.5976 | Validation Loss:     0.6256\nEpoch  238 | Train Loss:     0.5971 | Validation Loss:     0.6260\nEpoch  239 | Train Loss:     0.5968 | Validation Loss:     0.6253\nEpoch  240 | Train Loss:     0.5963 | Validation Loss:     0.6244\nEpoch  241 | Train Loss:     0.5958 | Validation Loss:     0.6241\nEpoch  242 | Train Loss:     0.5954 | Validation Loss:     0.6236\nEpoch  243 | Train Loss:     0.5948 | Validation Loss:     0.6238\nEpoch  244 | Train Loss:     0.5943 | Validation Loss:     0.6228\nEpoch  245 | Train Loss:     0.5941 | Validation Loss:     0.6218\nEpoch  246 | Train Loss:     0.5935 | Validation Loss:     0.6213\nEpoch  247 | Train Loss:     0.5927 | Validation Loss:     0.6210\nEpoch  248 | Train Loss:     0.5924 | Validation Loss:     0.6205\nEpoch  249 | Train Loss:     0.5917 | Validation Loss:     0.6198\nEpoch  250 | Train Loss:     0.5913 | Validation Loss:     0.6193\nEpoch  251 | Train Loss:     0.5908 | Validation Loss:     0.6188\nEpoch  252 | Train Loss:     0.5899 | Validation Loss:     0.6182\nEpoch  253 | Train Loss:     0.5894 | Validation Loss:     0.6176\nEpoch  254 | Train Loss:     0.5888 | Validation Loss:     0.6169\nEpoch  255 | Train Loss:     0.5880 | Validation Loss:     0.6164\nEpoch  256 | Train Loss:     0.5874 | Validation Loss:     0.6155\nEpoch  257 | Train Loss:     0.5868 | Validation Loss:     0.6160\nEpoch  258 | Train Loss:     0.5861 | Validation Loss:     0.6143\nEpoch  259 | Train Loss:     0.5853 | Validation Loss:     0.6138\nEpoch  260 | Train Loss:     0.5845 | Validation Loss:     0.6138\nEpoch  261 | Train Loss:     0.5835 | Validation Loss:     0.6117\nEpoch  262 | Train Loss:     0.5827 | Validation Loss:     0.6111\nEpoch  263 | Train Loss:     0.5821 | Validation Loss:     0.6101\nEpoch  264 | Train Loss:     0.5809 | Validation Loss:     0.6094\nEpoch  265 | Train Loss:     0.5798 | Validation Loss:     0.6081\nEpoch  266 | Train Loss:     0.5787 | Validation Loss:     0.6069\nEpoch  267 | Train Loss:     0.5776 | Validation Loss:     0.6058\nEpoch  268 | Train Loss:     0.5764 | Validation Loss:     0.6046\nEpoch  269 | Train Loss:     0.5752 | Validation Loss:     0.6035\nEpoch  270 | Train Loss:     0.5739 | Validation Loss:     0.6022\nEpoch  271 | Train Loss:     0.5727 | Validation Loss:     0.6008\nEpoch  272 | Train Loss:     0.5710 | Validation Loss:     0.5993\nEpoch  273 | Train Loss:     0.5695 | Validation Loss:     0.5980\nEpoch  274 | Train Loss:     0.5678 | Validation Loss:     0.5961\nEpoch  275 | Train Loss:     0.5663 | Validation Loss:     0.5943\nEpoch  276 | Train Loss:     0.5645 | Validation Loss:     0.5926\nEpoch  277 | Train Loss:     0.5624 | Validation Loss:     0.5907\nEpoch  278 | Train Loss:     0.5603 | Validation Loss:     0.5885\nEpoch  279 | Train Loss:     0.5582 | Validation Loss:     0.5864\nEpoch  280 | Train Loss:     0.5557 | Validation Loss:     0.5840\nEpoch  281 | Train Loss:     0.5534 | Validation Loss:     0.5815\nEpoch  282 | Train Loss:     0.5508 | Validation Loss:     0.5788\nEpoch  283 | Train Loss:     0.5481 | Validation Loss:     0.5761\nEpoch  284 | Train Loss:     0.5452 | Validation Loss:     0.5733\nEpoch  285 | Train Loss:     0.5422 | Validation Loss:     0.5700\nEpoch  286 | Train Loss:     0.5388 | Validation Loss:     0.5666\nEpoch  287 | Train Loss:     0.5354 | Validation Loss:     0.5635\nEpoch  288 | Train Loss:     0.5317 | Validation Loss:     0.5594\nEpoch  289 | Train Loss:     0.5279 | Validation Loss:     0.5558\nEpoch  290 | Train Loss:     0.5238 | Validation Loss:     0.5514\nEpoch  291 | Train Loss:     0.5197 | Validation Loss:     0.5469\nEpoch  292 | Train Loss:     0.5151 | Validation Loss:     0.5423\nEpoch  293 | Train Loss:     0.5104 | Validation Loss:     0.5374\nEpoch  294 | Train Loss:     0.5053 | Validation Loss:     0.5323\nEpoch  295 | Train Loss:     0.5001 | Validation Loss:     0.5272\nEpoch  296 | Train Loss:     0.4949 | Validation Loss:     0.5214\nEpoch  297 | Train Loss:     0.4892 | Validation Loss:     0.5156\nEpoch  298 | Train Loss:     0.4834 | Validation Loss:     0.5098\nEpoch  299 | Train Loss:     0.4775 | Validation Loss:     0.5038\nEpoch  300 | Train Loss:     0.4712 | Validation Loss:     0.4973\nEpoch  301 | Train Loss:     0.4653 | Validation Loss:     0.4913\nEpoch  302 | Train Loss:     0.4590 | Validation Loss:     0.4849\nEpoch  303 | Train Loss:     0.4527 | Validation Loss:     0.4787\nEpoch  304 | Train Loss:     0.4469 | Validation Loss:     0.4726\nEpoch  319 | Train Loss:     0.3748 | Validation Loss:     0.3989\nEpoch  320 | Train Loss:     0.3716 | Validation Loss:     0.3956\nEpoch  321 | Train Loss:     0.3686 | Validation Loss:     0.3926\nEpoch  322 | Train Loss:     0.3662 | Validation Loss:     0.3902\nEpoch  323 | Train Loss:     0.3640 | Validation Loss:     0.3881\nEpoch  324 | Train Loss:     0.3620 | Validation Loss:     0.3861\nEpoch  325 | Train Loss:     0.3605 | Validation Loss:     0.3843\nEpoch  326 | Train Loss:     0.3588 | Validation Loss:     0.3826\nEpoch  327 | Train Loss:     0.3573 | Validation Loss:     0.3813\nEpoch  328 | Train Loss:     0.3559 | Validation Loss:     0.3798\nEpoch  329 | Train Loss:     0.3548 | Validation Loss:     0.3784\nEpoch  330 | Train Loss:     0.3534 | Validation Loss:     0.3768\nEpoch  331 | Train Loss:     0.3522 | Validation Loss:     0.3755\nEpoch  332 | Train Loss:     0.3509 | Validation Loss:     0.3743\nEpoch  333 | Train Loss:     0.3498 | Validation Loss:     0.3728\nEpoch  334 | Train Loss:     0.3484 | Validation Loss:     0.3715\nEpoch  335 | Train Loss:     0.3473 | Validation Loss:     0.3702\nEpoch  336 | Train Loss:     0.3462 | Validation Loss:     0.3691\nEpoch  337 | Train Loss:     0.3452 | Validation Loss:     0.3681\nEpoch  338 | Train Loss:     0.3442 | Validation Loss:     0.3673\nEpoch  339 | Train Loss:     0.3434 | Validation Loss:     0.3660\nEpoch  340 | Train Loss:     0.3426 | Validation Loss:     0.3652\nEpoch  341 | Train Loss:     0.3419 | Validation Loss:     0.3647\nEpoch  342 | Train Loss:     0.3413 | Validation Loss:     0.3639\nEpoch  343 | Train Loss:     0.3408 | Validation Loss:     0.3635\nEpoch  344 | Train Loss:     0.3403 | Validation Loss:     0.3627\nEpoch  345 | Train Loss:     0.3398 | Validation Loss:     0.3623\nEpoch  346 | Train Loss:     0.3394 | Validation Loss:     0.3617\nEpoch  347 | Train Loss:     0.3390 | Validation Loss:     0.3614\nEpoch  348 | Train Loss:     0.3387 | Validation Loss:     0.3609\nEpoch  349 | Train Loss:     0.3384 | Validation Loss:     0.3606\nEpoch  350 | Train Loss:     0.3381 | Validation Loss:     0.3603\nEpoch  351 | Train Loss:     0.3378 | Validation Loss:     0.3601\nEpoch  352 | Train Loss:     0.3376 | Validation Loss:     0.3597\nEpoch  353 | Train Loss:     0.3374 | Validation Loss:     0.3595\nEpoch  354 | Train Loss:     0.3371 | Validation Loss:     0.3593\nEpoch  355 | Train Loss:     0.3370 | Validation Loss:     0.3592\nEpoch  356 | Train Loss:     0.3368 | Validation Loss:     0.3589\nEpoch  357 | Train Loss:     0.3366 | Validation Loss:     0.3587\nEpoch  358 | Train Loss:     0.3364 | Validation Loss:     0.3586\nEpoch  359 | Train Loss:     0.3363 | Validation Loss:     0.3584\nEpoch  360 | Train Loss:     0.3362 | Validation Loss:     0.3583\nEpoch  361 | Train Loss:     0.3361 | Validation Loss:     0.3582\nEpoch  362 | Train Loss:     0.3359 | Validation Loss:     0.3581\nEpoch  363 | Train Loss:     0.3358 | Validation Loss:     0.3579\nEpoch  364 | Train Loss:     0.3357 | Validation Loss:     0.3578\nEpoch  365 | Train Loss:     0.3357 | Validation Loss:     0.3577\nEpoch  366 | Train Loss:     0.3356 | Validation Loss:     0.3576\nEpoch  367 | Train Loss:     0.3355 | Validation Loss:     0.3575\nEpoch  368 | Train Loss:     0.3354 | Validation Loss:     0.3575\nEpoch  369 | Train Loss:     0.3354 | Validation Loss:     0.3575\nEpoch  370 | Train Loss:     0.3353 | Validation Loss:     0.3574\nEpoch  371 | Train Loss:     0.3352 | Validation Loss:     0.3573\nEpoch  372 | Train Loss:     0.3352 | Validation Loss:     0.3572\nEpoch  373 | Train Loss:     0.3351 | Validation Loss:     0.3572\nEpoch  374 | Train Loss:     0.3351 | Validation Loss:     0.3571\nEpoch  375 | Train Loss:     0.3350 | Validation Loss:     0.3571\nEpoch  376 | Train Loss:     0.3350 | Validation Loss:     0.3570\nEpoch  377 | Train Loss:     0.3350 | Validation Loss:     0.3570\nEpoch  378 | Train Loss:     0.3350 | Validation Loss:     0.3570\nEpoch  379 | Train Loss:     0.3349 | Validation Loss:     0.3570\nEpoch  380 | Train Loss:     0.3349 | Validation Loss:     0.3569\nEpoch  381 | Train Loss:     0.3348 | Validation Loss:     0.3569\nEpoch  382 | Train Loss:     0.3348 | Validation Loss:     0.3568\nEpoch  383 | Train Loss:     0.3348 | Validation Loss:     0.3568\nEpoch  384 | Train Loss:     0.3348 | Validation Loss:     0.3568\nEpoch  385 | Train Loss:     0.3348 | Validation Loss:     0.3569\nEpoch  386 | Train Loss:     0.3347 | Validation Loss:     0.3567\nEpoch  387 | Train Loss:     0.3347 | Validation Loss:     0.3567\nEpoch  388 | Train Loss:     0.3347 | Validation Loss:     0.3567\nEpoch  389 | Train Loss:     0.3347 | Validation Loss:     0.3567\nEpoch  390 | Train Loss:     0.3346 | Validation Loss:     0.3567\nEpoch  391 | Train Loss:     0.3347 | Validation Loss:     0.3567\nEpoch  392 | Train Loss:     0.3347 | Validation Loss:     0.3566\nEpoch  393 | Train Loss:     0.3346 | Validation Loss:     0.3566\nEpoch  394 | Train Loss:     0.3346 | Validation Loss:     0.3566\nEpoch  395 | Train Loss:     0.3346 | Validation Loss:     0.3566\nEpoch  396 | Train Loss:     0.3346 | Validation Loss:     0.3566\nEpoch  397 | Train Loss:     0.3346 | Validation Loss:     0.3566\nEpoch  398 | Train Loss:     0.3346 | Validation Loss:     0.3566\nEpoch  399 | Train Loss:     0.3345 | Validation Loss:     0.3565\nEpoch  400 | Train Loss:     0.3345 | Validation Loss:     0.3566\nEpoch  401 | Train Loss:     0.3345 | Validation Loss:     0.3565\nEpoch  402 | Train Loss:     0.3345 | Validation Loss:     0.3565\nEpoch  403 | Train Loss:     0.3345 | Validation Loss:     0.3566\nEpoch  404 | Train Loss:     0.3345 | Validation Loss:     0.3565\nEpoch  405 | Train Loss:     0.3345 | Validation Loss:     0.3565\nEpoch  406 | Train Loss:     0.3345 | Validation Loss:     0.3565\nEpoch  407 | Train Loss:     0.3345 | Validation Loss:     0.3565\nEpoch  408 | Train Loss:     0.3345 | Validation Loss:     0.3565\nEpoch  409 | Train Loss:     0.3345 | Validation Loss:     0.3565\nEpoch  410 | Train Loss:     0.3345 | Validation Loss:     0.3565\nEpoch  411 | Train Loss:     0.3345 | Validation Loss:     0.3565\nEpoch  412 | Train Loss:     0.3345 | Validation Loss:     0.3565\nEpoch  413 | Train Loss:     0.3345 | Validation Loss:     0.3565\nEpoch  414 | Train Loss:     0.3345 | Validation Loss:     0.3565\nEpoch  415 | Train Loss:     0.3345 | Validation Loss:     0.3565\nEpoch  416 | Train Loss:     0.3345 | Validation Loss:     0.3565\nEpoch  417 | Train Loss:     0.3345 | Validation Loss:     0.3565\nEpoch  418 | Train Loss:     0.3345 | Validation Loss:     0.3565\nEpoch  419 | Train Loss:     0.3345 | Validation Loss:     0.3564\nEpoch  420 | Train Loss:     0.3344 | Validation Loss:     0.3565\nEpoch  421 | Train Loss:     0.3344 | Validation Loss:     0.3564\nEpoch  422 | Train Loss:     0.3345 | Validation Loss:     0.3565\nEpoch  423 | Train Loss:     0.3344 | Validation Loss:     0.3564\nEpoch  424 | Train Loss:     0.3344 | Validation Loss:     0.3564\nEpoch  425 | Train Loss:     0.3344 | Validation Loss:     0.3565\nEpoch  426 | Train Loss:     0.3344 | Validation Loss:     0.3564\nEpoch  427 | Train Loss:     0.3344 | Validation Loss:     0.3564\nEpoch  428 | Train Loss:     0.3344 | Validation Loss:     0.3564\nEpoch  429 | Train Loss:     0.3344 | Validation Loss:     0.3564\nEpoch  430 | Train Loss:     0.3344 | Validation Loss:     0.3565\nEpoch  431 | Train Loss:     0.3345 | Validation Loss:     0.3564\nEpoch  432 | Train Loss:     0.3344 | Validation Loss:     0.3564\nEpoch  433 | Train Loss:     0.3344 | Validation Loss:     0.3564\nEpoch  434 | Train Loss:     0.3344 | Validation Loss:     0.3564\nEpoch  435 | Train Loss:     0.3344 | Validation Loss:     0.3564\nEpoch  436 | Train Loss:     0.3344 | Validation Loss:     0.3565\nEpoch  437 | Train Loss:     0.3344 | Validation Loss:     0.3564\nEpoch  438 | Train Loss:     0.3344 | Validation Loss:     0.3564\nEpoch  439 | Train Loss:     0.3344 | Validation Loss:     0.3564\nEpoch  440 | Train Loss:     0.3344 | Validation Loss:     0.3564\nEpoch  441 | Train Loss:     0.3344 | Validation Loss:     0.3564\nEpoch  442 | Train Loss:     0.3344 | Validation Loss:     0.3564\nEpoch  443 | Train Loss:     0.3344 | Validation Loss:     0.3564\nEpoch  444 | Train Loss:     0.3344 | Validation Loss:     0.3564\nEpoch  445 | Train Loss:     0.3344 | Validation Loss:     0.3564\nEpoch  446 | Train Loss:     0.3344 | Validation Loss:     0.3564\nEpoch  447 | Train Loss:     0.3344 | Validation Loss:     0.3564\nEpoch  448 | Train Loss:     0.3344 | Validation Loss:     0.3564\nEpoch  449 | Train Loss:     0.3344 | Validation Loss:     0.3564\nEpoch  450 | Train Loss:     0.3344 | Validation Loss:     0.3564\nEpoch  451 | Train Loss:     0.3344 | Validation Loss:     0.3564\nEpoch  452 | Train Loss:     0.3344 | Validation Loss:     0.3564\nEpoch  453 | Train Loss:     0.3344 | Validation Loss:     0.3564\nEpoch  454 | Train Loss:     0.3344 | Validation Loss:     0.3564\nEpoch  455 | Train Loss:     0.3344 | Validation Loss:     0.3564\nEpoch  456 | Train Loss:     0.3344 | Validation Loss:     0.3564\nEpoch  457 | Train Loss:     0.3344 | Validation Loss:     0.3564\nEpoch  458 | Train Loss:     0.3344 | Validation Loss:     0.3564\nEpoch  459 | Train Loss:     0.3344 | Validation Loss:     0.3564\nEpoch  460 | Train Loss:     0.3344 | Validation Loss:     0.3563\nEpoch  461 | Train Loss:     0.3343 | Validation Loss:     0.3563\nEpoch  462 | Train Loss:     0.3344 | Validation Loss:     0.3563\nEpoch  463 | Train Loss:     0.3344 | Validation Loss:     0.3564\nEpoch  464 | Train Loss:     0.3344 | Validation Loss:     0.3563\nEpoch  465 | Train Loss:     0.3344 | Validation Loss:     0.3565\nEpoch  466 | Train Loss:     0.3344 | Validation Loss:     0.3563\nEpoch  467 | Train Loss:     0.3343 | Validation Loss:     0.3563\nEpoch  468 | Train Loss:     0.3343 | Validation Loss:     0.3563\nEpoch  469 | Train Loss:     0.3344 | Validation Loss:     0.3564\nEpoch  470 | Train Loss:     0.3343 | Validation Loss:     0.3563\nEpoch  471 | Train Loss:     0.3343 | Validation Loss:     0.3563\nEpoch  472 | Train Loss:     0.3343 | Validation Loss:     0.3564\nEpoch  473 | Train Loss:     0.3343 | Validation Loss:     0.3564\nEpoch  474 | Train Loss:     0.3343 | Validation Loss:     0.3563\nEpoch  475 | Train Loss:     0.3343 | Validation Loss:     0.3563\nEpoch  476 | Train Loss:     0.3344 | Validation Loss:     0.3565\nEpoch  477 | Train Loss:     0.3344 | Validation Loss:     0.3563\nEpoch  478 | Train Loss:     0.3343 | Validation Loss:     0.3563\nEpoch  479 | Train Loss:     0.3343 | Validation Loss:     0.3563\nEpoch  480 | Train Loss:     0.3343 | Validation Loss:     0.3563\nEpoch  481 | Train Loss:     0.3343 | Validation Loss:     0.3563\nEpoch  482 | Train Loss:     0.3343 | Validation Loss:     0.3563\nEpoch  483 | Train Loss:     0.3343 | Validation Loss:     0.3563\nEpoch  484 | Train Loss:     0.3343 | Validation Loss:     0.3563\nEpoch  485 | Train Loss:     0.3343 | Validation Loss:     0.3563\nEpoch  486 | Train Loss:     0.3343 | Validation Loss:     0.3564\nEpoch  487 | Train Loss:     0.3343 | Validation Loss:     0.3563\nEpoch  488 | Train Loss:     0.3343 | Validation Loss:     0.3563\nEpoch  489 | Train Loss:     0.3343 | Validation Loss:     0.3563\nEpoch  490 | Train Loss:     0.3343 | Validation Loss:     0.3563\nEpoch  491 | Train Loss:     0.3343 | Validation Loss:     0.3563\nEpoch  492 | Train Loss:     0.3343 | Validation Loss:     0.3563\nEpoch  493 | Train Loss:     0.3343 | Validation Loss:     0.3563\nEpoch  494 | Train Loss:     0.3343 | Validation Loss:     0.3563\nEpoch  495 | Train Loss:     0.3343 | Validation Loss:     0.3563\nEpoch  496 | Train Loss:     0.3343 | Validation Loss:     0.3563\nEpoch  497 | Train Loss:     0.3343 | Validation Loss:     0.3563\nEpoch  498 | Train Loss:     0.3343 | Validation Loss:     0.3564\nEpoch  499 | Train Loss:     0.3343 | Validation Loss:     0.3564\nEpoch  500 | Train Loss:     0.3343 | Validation Loss:     0.3563\nEpoch  501 | Train Loss:     0.3343 | Validation Loss:     0.3563\nEpoch  502 | Train Loss:     0.3343 | Validation Loss:     0.3563\nEpoch  503 | Train Loss:     0.3343 | Validation Loss:     0.3563\nEpoch  504 | Train Loss:     0.3343 | Validation Loss:     0.3563\nEpoch  505 | Train Loss:     0.3343 | Validation Loss:     0.3563\nEpoch  506 | Train Loss:     0.3343 | Validation Loss:     0.3563\nEpoch  507 | Train Loss:     0.3343 | Validation Loss:     0.3563\nEpoch  508 | Train Loss:     0.3343 | Validation Loss:     0.3563\nEpoch  509 | Train Loss:     0.3343 | Validation Loss:     0.3563\nEpoch  510 | Train Loss:     0.3343 | Validation Loss:     0.3563\nEpoch  511 | Train Loss:     0.3343 | Validation Loss:     0.3563\nEpoch  512 | Train Loss:     0.3344 | Validation Loss:     0.3563\nEpoch  513 | Train Loss:     0.3343 | Validation Loss:     0.3563\nEpoch  514 | Train Loss:     0.3343 | Validation Loss:     0.3563\nEpoch  515 | Train Loss:     0.3343 | Validation Loss:     0.3563\nEpoch  516 | Train Loss:     0.3343 | Validation Loss:     0.3563\nEpoch  517 | Train Loss:     0.3343 | Validation Loss:     0.3563\nEpoch  518 | Train Loss:     0.3343 | Validation Loss:     0.3563\nEpoch  519 | Train Loss:     0.3343 | Validation Loss:     0.3563\nEpoch  520 | Train Loss:     0.3343 | Validation Loss:     0.3563\nEpoch  521 | Train Loss:     0.3343 | Validation Loss:     0.3563\nEpoch  522 | Train Loss:     0.3343 | Validation Loss:     0.3563\nEpoch  523 | Train Loss:     0.3343 | Validation Loss:     0.3563\nEpoch  524 | Train Loss:     0.3343 | Validation Loss:     0.3563\nEpoch  525 | Train Loss:     0.3343 | Validation Loss:     0.3563\nEpoch  526 | Train Loss:     0.3343 | Validation Loss:     0.3563\nEpoch  527 | Train Loss:     0.3343 | Validation Loss:     0.3563\nEpoch  528 | Train Loss:     0.3343 | Validation Loss:     0.3563\nEpoch  529 | Train Loss:     0.3343 | Validation Loss:     0.3563\nEpoch  530 | Train Loss:     0.3343 | Validation Loss:     0.3563\nEpoch  531 | Train Loss:     0.3343 | Validation Loss:     0.3563\nEpoch  532 | Train Loss:     0.3343 | Validation Loss:     0.3563\nEpoch  533 | Train Loss:     0.3343 | Validation Loss:     0.3563\nEpoch  534 | Train Loss:     0.3343 | Validation Loss:     0.3563\nEpoch  535 | Train Loss:     0.3343 | Validation Loss:     0.3563\nEpoch  536 | Train Loss:     0.3343 | Validation Loss:     0.3563\nEpoch  537 | Train Loss:     0.3343 | Validation Loss:     0.3563\nEpoch  538 | Train Loss:     0.3343 | Validation Loss:     0.3563\nEpoch  539 | Train Loss:     0.3343 | Validation Loss:     0.3563\nEpoch  540 | Train Loss:     0.3343 | Validation Loss:     0.3563\nEpoch  541 | Train Loss:     0.3343 | Validation Loss:     0.3563\nEpoch  542 | Train Loss:     0.3343 | Validation Loss:     0.3563\nEpoch  543 | Train Loss:     0.3343 | Validation Loss:     0.3563\nEpoch  544 | Train Loss:     0.3343 | Validation Loss:     0.3563\nEpoch  545 | Train Loss:     0.3343 | Validation Loss:     0.3563\nEpoch  546 | Train Loss:     0.3343 | Validation Loss:     0.3563\nEpoch  547 | Train Loss:     0.3343 | Validation Loss:     0.3563\nEpoch  548 | Train Loss:     0.3343 | Validation Loss:     0.3563\nEpoch  549 | Train Loss:     0.3343 | Validation Loss:     0.3563\nEpoch  550 | Train Loss:     0.3343 | Validation Loss:     0.3563\nEpoch  551 | Train Loss:     0.3343 | Validation Loss:     0.3563\nEpoch  552 | Train Loss:     0.3343 | Validation Loss:     0.3563\nEpoch  553 | Train Loss:     0.3343 | Validation Loss:     0.3563\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","Cell \u001b[0;32mIn[124], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mevaluate_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_prepu\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcustom_train_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m)\u001b[49m\n","Cell \u001b[0;32mIn[117], line 74\u001b[0m, in \u001b[0;36mevaluate_model\u001b[0;34m(model, custom_train_loader, criterion, optimizer)\u001b[0m\n\u001b[1;32m     72\u001b[0m inputs, labels \u001b[38;5;241m=\u001b[39m inputs\u001b[38;5;241m.\u001b[39mto(device), labels\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     73\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m---> 74\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mview\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     75\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(outputs, labels\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m))\n\u001b[1;32m     77\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39misnan(loss):\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n","Cell \u001b[0;32mIn[123], line 23\u001b[0m, in \u001b[0;36mPairwiseCustomActivationNetwork.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     20\u001b[0m outputs \u001b[38;5;241m=\u001b[39m [x]\n\u001b[1;32m     22\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m layer, custom_layer \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayers, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcustom_layers):\n\u001b[0;32m---> 23\u001b[0m     concatenated_outputs \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcat\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     24\u001b[0m     out \u001b[38;5;241m=\u001b[39m layer(concatenated_outputs)\n\u001b[1;32m     25\u001b[0m     out \u001b[38;5;241m=\u001b[39m custom_layer(out, concatenated_outputs)\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "],"ename":"KeyboardInterrupt","evalue":"","output_type":"error"}]},{"cell_type":"code","source":"for name, param in model_prepu.named_parameters():\n    print(name, param.data)","metadata":{"execution":{"iopub.status.busy":"2024-06-14T20:56:03.010466Z","iopub.execute_input":"2024-06-14T20:56:03.010841Z","iopub.status.idle":"2024-06-14T20:56:03.085581Z","shell.execute_reply.started":"2024-06-14T20:56:03.010810Z","shell.execute_reply":"2024-06-14T20:56:03.084697Z"},"trusted":true},"execution_count":125,"outputs":[{"name":"stdout","text":"layers.0.weight tensor([[1.0070e+00, 3.6396e-03],\n        [8.8297e-04, 1.0067e+00]], device='cuda:0')\nlayers.1.weight tensor([[1.0069e+00, 3.5527e-03, 6.9651e-03, 3.6081e-03],\n        [8.9187e-04, 1.0066e+00, 8.6083e-04, 6.6673e-03],\n        [6.8773e-03, 3.5485e-03, 1.0069e+00, 3.6012e-03],\n        [9.2136e-04, 6.6493e-03, 8.9023e-04, 1.0067e+00]], device='cuda:0')\nlayers.2.weight tensor([[1.0069e+00, 3.4079e-03, 6.9394e-03, 3.4633e-03, 6.9434e-03, 3.5169e-03,\n         6.9441e-03, 3.5708e-03],\n        [8.8185e-04, 1.0066e+00, 8.5015e-04, 6.5970e-03, 8.1908e-04, 6.6094e-03,\n         7.8294e-04, 6.6235e-03],\n        [6.8762e-03, 3.3886e-03, 1.0069e+00, 3.4415e-03, 6.8815e-03, 3.4925e-03,\n         6.8803e-03, 3.5437e-03],\n        [9.2128e-04, 6.5552e-03, 8.8958e-04, 1.0066e+00, 8.5845e-04, 6.5834e-03,\n         8.2223e-04, 6.5978e-03],\n        [6.7686e-03, 3.3622e-03, 6.7703e-03, 3.4123e-03, 1.0068e+00, 3.4604e-03,\n         6.7690e-03, 3.5086e-03],\n        [9.6673e-04, 6.5491e-03, 9.3492e-04, 6.5653e-03, 9.0368e-04, 1.0066e+00,\n         8.6725e-04, 6.5938e-03],\n        [6.7096e-03, 3.3223e-03, 6.7101e-03, 3.3699e-03, 6.7101e-03, 3.4155e-03,\n         1.0067e+00, 3.4611e-03],\n        [9.9614e-04, 6.5478e-03, 9.6424e-04, 6.5644e-03, 9.3294e-04, 6.5783e-03,\n         8.9637e-04, 1.0066e+00]], device='cuda:0')\nlayers.3.weight tensor([[1.0071e+00, 3.0666e-03, 7.1542e-03, 3.1246e-03, 7.1586e-03, 3.1808e-03,\n         7.1595e-03, 3.2372e-03, 7.1699e-03, 3.2883e-03, 7.1681e-03, 3.3435e-03,\n         7.1650e-03, 3.3978e-03, 7.1567e-03, 3.4513e-03],\n        [8.9428e-04, 1.0064e+00, 8.6150e-04, 6.4647e-03, 8.2935e-04, 6.4766e-03,\n         7.9186e-04, 6.4901e-03, 7.6777e-04, 6.4968e-03, 7.2655e-04, 6.5091e-03,\n         6.8649e-04, 6.5190e-03, 6.4075e-04, 6.5304e-03],\n        [6.8154e-03, 2.9916e-03, 1.0068e+00, 3.0453e-03, 6.8165e-03, 3.0973e-03,\n         6.8129e-03, 3.1494e-03, 6.8194e-03, 3.1964e-03, 6.8130e-03, 3.2472e-03,\n         6.8055e-03, 3.2969e-03, 6.7927e-03, 3.3459e-03],\n        [9.2863e-04, 6.4383e-03, 8.9578e-04, 1.0064e+00, 8.6355e-04, 6.4657e-03,\n         8.2590e-04, 6.4796e-03, 8.0181e-04, 6.4868e-03, 7.6038e-04, 6.4995e-03,\n         7.2011e-04, 6.5098e-03, 6.7406e-04, 6.5216e-03],\n        [6.7053e-03, 2.9876e-03, 6.7047e-03, 3.0388e-03, 1.0067e+00, 3.0881e-03,\n         6.6988e-03, 3.1375e-03, 6.7044e-03, 3.1819e-03, 6.6964e-03, 3.2300e-03,\n         6.6878e-03, 3.2768e-03, 6.6736e-03, 3.3229e-03],\n        [9.7071e-04, 6.4041e-03, 9.3782e-04, 6.4194e-03, 9.0554e-04, 1.0064e+00,\n         8.6780e-04, 6.4462e-03, 8.4368e-04, 6.4537e-03, 8.0212e-04, 6.4667e-03,\n         7.6169e-04, 6.4772e-03, 7.1544e-04, 6.4893e-03],\n        [6.5831e-03, 2.9733e-03, 6.5814e-03, 3.0216e-03, 6.5792e-03, 3.0681e-03,\n         1.0066e+00, 3.1145e-03, 6.5778e-03, 3.1563e-03, 6.5690e-03, 3.2013e-03,\n         6.5595e-03, 3.2450e-03, 6.5447e-03, 3.2881e-03],\n        [9.9914e-04, 6.3970e-03, 9.6614e-04, 6.4127e-03, 9.3378e-04, 6.4256e-03,\n         8.9589e-04, 1.0064e+00, 8.7175e-04, 6.4480e-03, 8.3000e-04, 6.4615e-03,\n         7.8938e-04, 6.4724e-03, 7.4284e-04, 6.4849e-03],\n        [6.4742e-03, 2.9455e-03, 6.4718e-03, 2.9911e-03, 6.4687e-03, 3.0350e-03,\n         6.4619e-03, 3.0788e-03, 1.0065e+00, 3.1181e-03, 6.4562e-03, 3.1604e-03,\n         6.4461e-03, 3.2014e-03, 6.4312e-03, 3.2417e-03],\n        [1.0303e-03, 6.3884e-03, 9.9723e-04, 6.4046e-03, 9.6479e-04, 6.4180e-03,\n         9.2675e-04, 6.4330e-03, 9.0258e-04, 1.0064e+00, 8.6062e-04, 6.4551e-03,\n         8.1981e-04, 6.4665e-03, 7.7296e-04, 6.4794e-03],\n        [6.3317e-03, 2.9040e-03, 6.3287e-03, 2.9464e-03, 6.3252e-03, 2.9871e-03,\n         6.3182e-03, 3.0276e-03, 6.3214e-03, 3.0640e-03, 1.0063e+00, 3.1031e-03,\n         6.3019e-03, 3.1408e-03, 6.2876e-03, 3.1779e-03],\n        [1.0587e-03, 6.3823e-03, 1.0255e-03, 6.3988e-03, 9.9297e-04, 6.4126e-03,\n         9.5479e-04, 6.4281e-03, 9.3062e-04, 6.4367e-03, 8.8846e-04, 1.0064e+00,\n         8.4744e-04, 6.4628e-03, 8.0031e-04, 6.4761e-03],\n        [6.1998e-03, 2.8573e-03, 6.1965e-03, 2.8967e-03, 6.1929e-03, 2.9344e-03,\n         6.1860e-03, 2.9721e-03, 6.1887e-03, 3.0058e-03, 6.1797e-03, 3.0418e-03,\n         1.0062e+00, 3.0766e-03, 6.1570e-03, 3.1107e-03],\n        [1.0914e-03, 6.3669e-03, 1.0581e-03, 6.3840e-03, 1.0255e-03, 6.3982e-03,\n         9.8716e-04, 6.4141e-03, 9.6296e-04, 6.4232e-03, 9.2061e-04, 6.4379e-03,\n         8.7935e-04, 1.0064e+00, 8.3190e-04, 6.4640e-03],\n        [6.1739e-03, 2.7732e-03, 6.1705e-03, 2.8107e-03, 6.1667e-03, 2.8466e-03,\n         6.1598e-03, 2.8824e-03, 6.1624e-03, 2.9145e-03, 6.1536e-03, 2.9487e-03,\n         6.1445e-03, 2.9816e-03, 1.0061e+00, 3.0139e-03],\n        [1.1229e-03, 6.3577e-03, 1.0896e-03, 6.3751e-03, 1.0568e-03, 6.3897e-03,\n         1.0184e-03, 6.4061e-03, 9.9417e-04, 6.4155e-03, 9.5161e-04, 6.4306e-03,\n         9.1016e-04, 6.4434e-03, 8.6243e-04, 1.0065e+00]], device='cuda:0')\nlayers.4.weight tensor([[1.0071e+00, 2.1719e-03, 7.1433e-03,  ..., 2.9229e-03, 7.0681e-03,\n         2.9742e-03],\n        [9.8573e-04, 1.0060e+00, 9.5201e-04,  ..., 6.1185e-03, 3.9167e-04,\n         6.1246e-03],\n        [7.0185e-03, 2.2148e-03, 1.0070e+00,  ..., 2.9363e-03, 6.9201e-03,\n         2.9850e-03],\n        ...,\n        [1.3692e-03, 6.0969e-03, 1.3338e-03,  ..., 1.0063e+00, 7.1927e-04,\n         6.2981e-03],\n        [5.3296e-03, 1.7246e-03, 5.3261e-03,  ..., 2.0396e-03, 1.0053e+00,\n         2.0588e-03],\n        [1.3980e-03, 6.0900e-03, 1.3626e-03,  ..., 6.2838e-03, 7.4532e-04,\n         1.0063e+00]], device='cuda:0')\nlayers.5.weight tensor([[ 1.0070e+00,  1.1222e-03,  7.0519e-03,  ...,  2.6652e-03,\n          6.7798e-03,  2.7153e-03],\n        [ 8.9764e-04,  1.0058e+00,  8.6360e-04,  ...,  5.7646e-03,\n         -2.8944e-04,  5.7628e-03],\n        [ 7.0184e-03,  1.1018e-03,  1.0070e+00,  ...,  2.6121e-03,\n          6.7473e-03,  2.6606e-03],\n        ...,\n        [ 1.9519e-03,  5.5713e-03,  1.9171e-03,  ...,  1.0060e+00,\n          4.2655e-04,  6.0255e-03],\n        [ 4.4680e-03,  7.6800e-05,  4.4691e-03,  ...,  7.1548e-04,\n          1.0046e+00,  7.3105e-04],\n        [ 1.9841e-03,  5.5678e-03,  1.9493e-03,  ...,  6.0200e-03,\n          4.5186e-04,  1.0060e+00]], device='cuda:0')\nlayers.6.weight tensor([[ 1.0083e+00,  1.4078e-03,  8.2962e-03,  ...,  4.9508e-03,\n          7.5848e-03,  5.0012e-03],\n        [ 3.0933e-04,  1.0054e+00,  2.7940e-04,  ...,  4.6365e-03,\n         -1.1590e-03,  4.6275e-03],\n        [ 8.4802e-03,  1.3476e-03,  1.0085e+00,  ...,  4.8425e-03,\n          7.7486e-03,  4.8922e-03],\n        ...,\n        [ 2.8594e-03,  5.0280e-03,  2.8269e-03,  ...,  1.0061e+00,\n         -2.7554e-04,  6.0693e-03],\n        [ 3.1627e-03, -1.7859e-03,  3.1761e-03,  ..., -6.0050e-04,\n          1.0039e+00, -5.8955e-04],\n        [ 2.8983e-03,  5.0241e-03,  2.8659e-03,  ...,  6.0735e-03,\n         -2.5750e-04,  1.0061e+00]], device='cuda:0')\nlayers.7.weight tensor([[ 1.0103e+00,  5.0886e-04,  1.0233e-02,  ...,  5.3141e-03,\n          4.7541e-03,  5.3452e-03],\n        [-4.2010e-04,  1.0064e+00, -4.5764e-04,  ...,  5.8195e-03,\n         -2.6633e-03,  5.8288e-03],\n        [ 1.0044e-02,  4.8948e-04,  1.0100e+00,  ...,  5.1741e-03,\n          4.6822e-03,  5.2046e-03],\n        ...,\n        [ 4.6009e-03,  4.0472e-03,  4.5884e-03,  ...,  1.0073e+00,\n          1.1779e-03,  7.2886e-03],\n        [ 1.8487e-03, -2.2547e-03,  1.8723e-03,  ..., -9.3691e-04,\n          1.0040e+00, -9.3768e-04],\n        [ 4.6108e-03,  4.0486e-03,  4.5984e-03,  ...,  7.2771e-03,\n          1.1925e-03,  1.0073e+00]], device='cuda:0')\nlayers.8.weight tensor([[ 1.0020e+00,  7.9795e-04,  2.0445e-03,  ...,  2.5421e-03,\n          2.6191e-04,  2.5458e-03],\n        [ 1.5934e-03,  9.9977e-01,  1.5986e-03,  ..., -1.2255e-03,\n          1.5153e-03, -1.2314e-03],\n        [ 2.0122e-03,  7.7824e-04,  1.0020e+00,  ...,  2.5034e-03,\n          2.5455e-04,  2.5070e-03],\n        ...,\n        [ 4.5870e-04,  3.5988e-03,  4.5201e-04,  ...,  1.0031e+00,\n         -8.1031e-04,  3.0659e-03],\n        [ 1.0467e-03,  2.9683e-06,  1.0505e-03,  ..., -5.6042e-04,\n          1.0022e+00, -5.6081e-04],\n        [ 4.5826e-04,  3.6119e-03,  4.5160e-04,  ...,  3.0752e-03,\n         -8.1010e-04,  1.0031e+00]], device='cuda:0')\nlayers.9.weight tensor([[-0.0002,  0.0003, -0.0002,  ..., -0.0014,  0.0007, -0.0014]],\n       device='cuda:0')\nlayers.9.bias tensor([0.0003], device='cuda:0')\ncustom_layers.0.linear_sigmoid.weight tensor([[0.0011, 0.0070],\n        [0.0009, 0.0067]], device='cuda:0')\ncustom_layers.0.linear_sigmoid.bias tensor([0.0070, 0.0067], device='cuda:0')\ncustom_layers.1.linear_sigmoid.weight tensor([[0.0009, 0.0070, 0.0010, 0.0070],\n        [0.0009, 0.0067, 0.0009, 0.0067],\n        [0.0013, 0.0069, 0.0014, 0.0069],\n        [0.0009, 0.0067, 0.0009, 0.0067]], device='cuda:0')\ncustom_layers.1.linear_sigmoid.bias tensor([0.0070, 0.0067, 0.0069, 0.0067], device='cuda:0')\ncustom_layers.2.linear_sigmoid.weight tensor([[0.0006, 0.0069, 0.0007, 0.0069, 0.0008, 0.0069, 0.0009, 0.0069],\n        [0.0009, 0.0066, 0.0009, 0.0066, 0.0008, 0.0066, 0.0008, 0.0066],\n        [0.0009, 0.0069, 0.0010, 0.0069, 0.0011, 0.0069, 0.0012, 0.0069],\n        [0.0009, 0.0066, 0.0009, 0.0066, 0.0008, 0.0066, 0.0008, 0.0066],\n        [0.0013, 0.0068, 0.0014, 0.0068, 0.0014, 0.0068, 0.0015, 0.0068],\n        [0.0009, 0.0066, 0.0009, 0.0066, 0.0008, 0.0066, 0.0008, 0.0066],\n        [0.0017, 0.0067, 0.0018, 0.0067, 0.0019, 0.0067, 0.0020, 0.0067],\n        [0.0009, 0.0066, 0.0008, 0.0066, 0.0008, 0.0066, 0.0008, 0.0066]],\n       device='cuda:0')\ncustom_layers.2.linear_sigmoid.bias tensor([0.0069, 0.0066, 0.0069, 0.0066, 0.0068, 0.0066, 0.0067, 0.0066],\n       device='cuda:0')\ncustom_layers.3.linear_sigmoid.weight tensor([[-9.9743e-05,  7.1133e-03, -3.5454e-05,  7.1186e-03,  2.7664e-05,\n          7.1219e-03,  1.0561e-04,  7.1260e-03,  1.4926e-04,  7.1298e-03,\n          2.3826e-04,  7.1322e-03,  3.2582e-04,  7.1325e-03,  4.3098e-04,\n          7.1342e-03],\n        [ 9.0334e-04,  6.4546e-03,  8.7062e-04,  6.4695e-03,  8.3853e-04,\n          6.4817e-03,  8.0108e-04,  6.4954e-03,  7.7707e-04,  6.5025e-03,\n          7.3589e-04,  6.5150e-03,  6.9587e-04,  6.5251e-03,  6.5015e-04,\n          6.5368e-03],\n        [ 1.4698e-04,  6.8145e-03,  2.1309e-04,  6.8147e-03,  2.7778e-04,\n          6.8132e-03,  3.5768e-04,  6.8123e-03,  4.0186e-04,  6.8113e-03,\n          4.9275e-04,  6.8089e-03,  5.8177e-04,  6.8044e-03,  6.8868e-04,\n          6.8014e-03],\n        [ 8.9618e-04,  6.4599e-03,  8.6341e-04,  6.4745e-03,  8.3129e-04,\n          6.4863e-03,  7.9379e-04,  6.4998e-03,  7.6976e-04,  6.5064e-03,\n          7.2854e-04,  6.5187e-03,  6.8849e-04,  6.5285e-03,  6.4274e-04,\n          6.5397e-03],\n        [ 4.8029e-04,  6.7200e-03,  5.4921e-04,  6.7185e-03,  6.1632e-04,\n          6.7152e-03,  6.9922e-04,  6.7126e-03,  7.4446e-04,  6.7101e-03,\n          8.3837e-04,  6.7060e-03,  9.2995e-04,  6.6999e-03,  1.0398e-03,\n          6.6952e-03],\n        [ 8.9646e-04,  6.4409e-03,  8.6368e-04,  6.4550e-03,  8.3155e-04,\n          6.4663e-03,  7.9409e-04,  6.4793e-03,  7.7001e-04,  6.4855e-03,\n          7.2882e-04,  6.4972e-03,  6.8882e-04,  6.5065e-03,  6.4314e-04,\n          6.5173e-03],\n        [ 8.4592e-04,  6.6081e-03,  9.1713e-04,  6.6050e-03,  9.8617e-04,\n          6.6002e-03,  1.0713e-03,  6.5960e-03,  1.1173e-03,  6.5922e-03,\n          1.2133e-03,  6.5867e-03,  1.3064e-03,  6.5793e-03,  1.4179e-03,\n          6.5734e-03],\n        [ 8.8353e-04,  6.4511e-03,  8.5072e-04,  6.4648e-03,  8.1852e-04,\n          6.4758e-03,  7.8098e-04,  6.4884e-03,  7.5692e-04,  6.4942e-03,\n          7.1569e-04,  6.5056e-03,  6.7566e-04,  6.5144e-03,  6.2997e-04,\n          6.5249e-03],\n        [ 1.1257e-03,  6.5150e-03,  1.1978e-03,  6.5104e-03,  1.2675e-03,\n          6.5044e-03,  1.3533e-03,  6.4989e-03,  1.3992e-03,  6.4940e-03,\n          1.4955e-03,  6.4872e-03,  1.5885e-03,  6.4788e-03,  1.6997e-03,\n          6.4719e-03],\n        [ 8.7425e-04,  6.4546e-03,  8.4136e-04,  6.4680e-03,  8.0912e-04,\n          6.4787e-03,  7.7152e-04,  6.4910e-03,  7.4744e-04,  6.4965e-03,\n          7.0613e-04,  6.5075e-03,  6.6605e-04,  6.5160e-03,  6.2030e-04,\n          6.5262e-03],\n        [ 1.5045e-03,  6.3741e-03,  1.5769e-03,  6.3686e-03,  1.6464e-03,\n          6.3616e-03,  1.7319e-03,  6.3554e-03,  1.7773e-03,  6.3498e-03,\n          1.8726e-03,  6.3424e-03,  1.9641e-03,  6.3335e-03,  2.0730e-03,\n          6.3261e-03],\n        [ 8.6109e-04,  6.4662e-03,  8.2812e-04,  6.4792e-03,  7.9586e-04,\n          6.4895e-03,  7.5821e-04,  6.5015e-03,  7.3411e-04,  6.5066e-03,\n          6.9277e-04,  6.5174e-03,  6.5266e-04,  6.5255e-03,  6.0690e-04,\n          6.5353e-03],\n        [ 1.8545e-03,  6.2392e-03,  1.9259e-03,  6.2331e-03,  1.9943e-03,\n          6.2257e-03,  2.0781e-03,  6.2191e-03,  2.1225e-03,  6.2132e-03,\n          2.2153e-03,  6.2057e-03,  2.3039e-03,  6.1968e-03,  2.4089e-03,\n          6.1895e-03],\n        [ 8.5120e-04,  6.4677e-03,  8.1818e-04,  6.4805e-03,  7.8586e-04,\n          6.4905e-03,  7.4815e-04,  6.5021e-03,  7.2403e-04,  6.5070e-03,\n          6.8263e-04,  6.5173e-03,  6.4250e-04,  6.5252e-03,  5.9668e-04,\n          6.5346e-03],\n        [ 2.2639e-03,  6.2103e-03,  2.3346e-03,  6.2036e-03,  2.4018e-03,\n          6.1958e-03,  2.4840e-03,  6.1888e-03,  2.5272e-03,  6.1827e-03,\n          2.6175e-03,  6.1751e-03,  2.7032e-03,  6.1663e-03,  2.8043e-03,\n          6.1590e-03],\n        [ 8.4135e-04,  6.4769e-03,  8.0826e-04,  6.4894e-03,  7.7590e-04,\n          6.4990e-03,  7.3813e-04,  6.5103e-03,  7.1401e-04,  6.5150e-03,\n          6.7256e-04,  6.5250e-03,  6.3239e-04,  6.5325e-03,  5.8652e-04,\n          6.5416e-03]], device='cuda:0')\ncustom_layers.3.linear_sigmoid.bias tensor([0.0071, 0.0065, 0.0068, 0.0065, 0.0067, 0.0064, 0.0066, 0.0065, 0.0065,\n        0.0065, 0.0064, 0.0065, 0.0062, 0.0065, 0.0062, 0.0065],\n       device='cuda:0')\ncustom_layers.4.linear_sigmoid.weight tensor([[-0.0014,  0.0070, -0.0014,  ...,  0.0070, -0.0004,  0.0070],\n        [ 0.0010,  0.0060,  0.0010,  ...,  0.0062,  0.0004,  0.0062],\n        [-0.0011,  0.0069, -0.0011,  ...,  0.0069, -0.0001,  0.0068],\n        ...,\n        [ 0.0008,  0.0064,  0.0008,  ...,  0.0064,  0.0002,  0.0064],\n        [ 0.0025,  0.0053,  0.0026,  ...,  0.0051,  0.0034,  0.0051],\n        [ 0.0008,  0.0064,  0.0007,  ...,  0.0064,  0.0002,  0.0064]],\n       device='cuda:0')\ncustom_layers.4.linear_sigmoid.bias tensor([0.0070, 0.0060, 0.0069, 0.0061, 0.0068, 0.0061, 0.0066, 0.0061, 0.0065,\n        0.0061, 0.0066, 0.0062, 0.0065, 0.0062, 0.0063, 0.0063, 0.0063, 0.0064,\n        0.0062, 0.0064, 0.0061, 0.0063, 0.0059, 0.0063, 0.0058, 0.0063, 0.0056,\n        0.0064, 0.0054, 0.0064, 0.0053, 0.0064], device='cuda:0')\ncustom_layers.5.linear_sigmoid.weight tensor([[-2.4030e-03,  6.6140e-03, -2.3935e-03,  ...,  6.5152e-03,\n         -1.2412e-03,  6.5071e-03],\n        [ 1.1308e-03,  5.8565e-03,  1.0983e-03,  ...,  6.0251e-03,\n         -8.0990e-05,  6.0290e-03],\n        [-2.2806e-03,  6.6043e-03, -2.2701e-03,  ...,  6.4965e-03,\n         -1.0744e-03,  6.4883e-03],\n        ...,\n        [ 7.4677e-04,  6.2689e-03,  7.1068e-04,  ...,  6.1970e-03,\n         -4.8503e-04,  6.1934e-03],\n        [ 1.8267e-03,  3.8046e-03,  1.8710e-03,  ...,  3.6276e-03,\n          3.2032e-03,  3.6242e-03],\n        [ 7.2866e-04,  6.2949e-03,  6.9249e-04,  ...,  6.2134e-03,\n         -5.0025e-04,  6.2095e-03]], device='cuda:0')\ncustom_layers.5.linear_sigmoid.bias tensor([0.0066, 0.0059, 0.0066, 0.0059, 0.0066, 0.0059, 0.0066, 0.0059, 0.0065,\n        0.0059, 0.0065, 0.0059, 0.0064, 0.0059, 0.0063, 0.0059, 0.0063, 0.0059,\n        0.0063, 0.0059, 0.0061, 0.0059, 0.0060, 0.0059, 0.0059, 0.0059, 0.0058,\n        0.0059, 0.0056, 0.0059, 0.0055, 0.0059, 0.0058, 0.0058, 0.0057, 0.0059,\n        0.0055, 0.0059, 0.0053, 0.0059, 0.0052, 0.0059, 0.0056, 0.0061, 0.0054,\n        0.0062, 0.0052, 0.0062, 0.0052, 0.0062, 0.0050, 0.0063, 0.0048, 0.0062,\n        0.0045, 0.0062, 0.0044, 0.0062, 0.0041, 0.0063, 0.0040, 0.0063, 0.0038,\n        0.0063], device='cuda:0')\ncustom_layers.6.linear_sigmoid.weight tensor([[-0.0023,  0.0067, -0.0023,  ...,  0.0070, -0.0002,  0.0070],\n        [ 0.0012,  0.0056,  0.0012,  ...,  0.0062, -0.0006,  0.0062],\n        [-0.0023,  0.0068, -0.0023,  ...,  0.0070, -0.0004,  0.0070],\n        ...,\n        [ 0.0003,  0.0065,  0.0002,  ...,  0.0056, -0.0013,  0.0056],\n        [ 0.0008,  0.0014,  0.0008,  ...,  0.0013,  0.0017,  0.0013],\n        [ 0.0002,  0.0065,  0.0002,  ...,  0.0056, -0.0013,  0.0056]],\n       device='cuda:0')\ncustom_layers.6.linear_sigmoid.bias tensor([0.0067, 0.0056, 0.0068, 0.0056, 0.0069, 0.0056, 0.0070, 0.0056, 0.0071,\n        0.0055, 0.0072, 0.0055, 0.0073, 0.0053, 0.0074, 0.0053, 0.0074, 0.0053,\n        0.0064, 0.0053, 0.0065, 0.0053, 0.0065, 0.0054, 0.0065, 0.0054, 0.0065,\n        0.0054, 0.0064, 0.0054, 0.0078, 0.0055, 0.0071, 0.0054, 0.0070, 0.0055,\n        0.0069, 0.0055, 0.0065, 0.0055, 0.0065, 0.0055, 0.0064, 0.0055, 0.0063,\n        0.0056, 0.0062, 0.0056, 0.0061, 0.0056, 0.0060, 0.0056, 0.0058, 0.0056,\n        0.0056, 0.0057, 0.0054, 0.0057, 0.0052, 0.0057, 0.0050, 0.0057, 0.0047,\n        0.0057, 0.0063, 0.0056, 0.0061, 0.0056, 0.0059, 0.0056, 0.0057, 0.0057,\n        0.0055, 0.0057, 0.0053, 0.0057, 0.0051, 0.0057, 0.0048, 0.0058, 0.0047,\n        0.0058, 0.0044, 0.0058, 0.0041, 0.0058, 0.0038, 0.0058, 0.0036, 0.0059,\n        0.0034, 0.0059, 0.0032, 0.0059, 0.0030, 0.0059, 0.0033, 0.0059, 0.0031,\n        0.0060, 0.0029, 0.0060, 0.0027, 0.0060, 0.0026, 0.0061, 0.0024, 0.0063,\n        0.0023, 0.0064, 0.0022, 0.0065, 0.0021, 0.0066, 0.0020, 0.0066, 0.0019,\n        0.0064, 0.0018, 0.0064, 0.0017, 0.0064, 0.0016, 0.0065, 0.0015, 0.0065,\n        0.0014, 0.0065], device='cuda:0')\ncustom_layers.7.linear_sigmoid.weight tensor([[-3.3314e-03,  5.7403e-03, -3.2933e-03,  ...,  4.5912e-03,\n         -1.5703e-03,  4.5878e-03],\n        [ 2.2495e-03,  4.6554e-03,  2.2203e-03,  ...,  6.9698e-03,\n         -1.3243e-03,  6.9904e-03],\n        [-3.2790e-03,  5.7197e-03, -3.2416e-03,  ...,  4.5791e-03,\n         -1.5119e-03,  4.5753e-03],\n        ...,\n        [-5.9173e-05,  5.6399e-03, -7.2502e-05,  ...,  3.0534e-03,\n         -1.8072e-03,  3.0521e-03],\n        [ 1.6397e-03,  7.7071e-04,  1.6305e-03,  ...,  1.1900e-03,\n          4.7905e-04,  1.1904e-03],\n        [-6.3875e-05,  5.6591e-03, -7.7147e-05,  ...,  3.0609e-03,\n         -1.8100e-03,  3.0596e-03]], device='cuda:0')\ncustom_layers.7.linear_sigmoid.bias tensor([0.0057, 0.0047, 0.0057, 0.0046, 0.0057, 0.0045, 0.0057, 0.0046, 0.0056,\n        0.0054, 0.0056, 0.0046, 0.0055, 0.0046, 0.0055, 0.0047, 0.0055, 0.0047,\n        0.0054, 0.0048, 0.0054, 0.0048, 0.0054, 0.0049, 0.0055, 0.0049, 0.0055,\n        0.0050, 0.0054, 0.0050, 0.0054, 0.0050, 0.0054, 0.0051, 0.0054, 0.0051,\n        0.0053, 0.0051, 0.0052, 0.0051, 0.0051, 0.0052, 0.0050, 0.0052, 0.0049,\n        0.0052, 0.0048, 0.0052, 0.0047, 0.0052, 0.0044, 0.0052, 0.0044, 0.0052,\n        0.0044, 0.0052, 0.0044, 0.0052, 0.0044, 0.0052, 0.0043, 0.0052, 0.0043,\n        0.0052, 0.0045, 0.0052, 0.0044, 0.0052, 0.0044, 0.0052, 0.0044, 0.0052,\n        0.0044, 0.0051, 0.0044, 0.0051, 0.0044, 0.0051, 0.0043, 0.0051, 0.0043,\n        0.0051, 0.0043, 0.0051, 0.0043, 0.0051, 0.0042, 0.0051, 0.0042, 0.0051,\n        0.0041, 0.0051, 0.0041, 0.0050, 0.0040, 0.0049, 0.0041, 0.0043, 0.0040,\n        0.0045, 0.0040, 0.0046, 0.0039, 0.0046, 0.0039, 0.0047, 0.0039, 0.0047,\n        0.0039, 0.0048, 0.0039, 0.0048, 0.0038, 0.0048, 0.0038, 0.0048, 0.0038,\n        0.0048, 0.0038, 0.0049, 0.0038, 0.0049, 0.0038, 0.0049, 0.0037, 0.0049,\n        0.0037, 0.0049, 0.0041, 0.0052, 0.0041, 0.0052, 0.0040, 0.0052, 0.0040,\n        0.0052, 0.0040, 0.0052, 0.0039, 0.0051, 0.0039, 0.0046, 0.0039, 0.0047,\n        0.0039, 0.0048, 0.0040, 0.0048, 0.0040, 0.0048, 0.0039, 0.0049, 0.0039,\n        0.0049, 0.0039, 0.0049, 0.0039, 0.0049, 0.0042, 0.0050, 0.0042, 0.0049,\n        0.0042, 0.0050, 0.0042, 0.0050, 0.0038, 0.0050, 0.0037, 0.0050, 0.0037,\n        0.0051, 0.0037, 0.0051, 0.0036, 0.0051, 0.0036, 0.0051, 0.0036, 0.0051,\n        0.0035, 0.0051, 0.0035, 0.0052, 0.0034, 0.0052, 0.0034, 0.0052, 0.0033,\n        0.0052, 0.0033, 0.0052, 0.0036, 0.0051, 0.0036, 0.0051, 0.0035, 0.0052,\n        0.0035, 0.0052, 0.0034, 0.0052, 0.0033, 0.0052, 0.0032, 0.0052, 0.0031,\n        0.0052, 0.0030, 0.0053, 0.0029, 0.0053, 0.0026, 0.0053, 0.0025, 0.0053,\n        0.0023, 0.0053, 0.0022, 0.0053, 0.0021, 0.0054, 0.0020, 0.0054, 0.0020,\n        0.0054, 0.0019, 0.0054, 0.0018, 0.0054, 0.0017, 0.0054, 0.0016, 0.0054,\n        0.0016, 0.0055, 0.0015, 0.0055, 0.0014, 0.0055, 0.0013, 0.0055, 0.0012,\n        0.0055, 0.0012, 0.0056, 0.0011, 0.0056, 0.0010, 0.0056, 0.0009, 0.0056,\n        0.0008, 0.0056, 0.0008, 0.0057], device='cuda:0')\ncustom_layers.8.linear_sigmoid.weight tensor([[-3.7879e-03, -2.9122e-04, -3.7686e-03,  ..., -4.5594e-03,\n          3.5762e-03, -4.5913e-03],\n        [ 2.4221e-03,  5.4644e-04,  2.3904e-03,  ...,  3.1854e-03,\n         -1.2702e-03,  3.1927e-03],\n        [-3.8232e-03, -2.2278e-04, -3.8040e-03,  ..., -4.5422e-03,\n          3.5797e-03, -4.5741e-03],\n        ...,\n        [ 8.7763e-03,  1.0235e-03,  8.7971e-03,  ...,  3.5504e-03,\n          7.5331e-03,  3.5583e-03],\n        [-5.1234e-05, -5.3784e-03, -2.6379e-05,  ..., -5.1425e-03,\n          3.3391e-03, -5.1475e-03],\n        [ 8.7882e-03,  1.0284e-03,  8.8090e-03,  ...,  3.5548e-03,\n          7.5431e-03,  3.5627e-03]], device='cuda:0')\ncustom_layers.8.linear_sigmoid.bias tensor([-2.9122e-04,  5.4644e-04, -2.2278e-04,  5.8008e-04, -1.6920e-04,\n         6.3051e-04, -2.1669e-05,  6.6765e-04,  7.0980e-05,  7.4225e-04,\n         3.0915e-04,  7.8278e-04,  3.9481e-04,  8.4155e-04,  4.9129e-04,\n         8.8467e-04,  6.4383e-04,  9.9994e-04,  6.8928e-04,  1.0453e-03,\n         7.0265e-04,  1.1083e-03,  7.2487e-04,  1.1556e-03,  7.4055e-04,\n         1.2389e-03,  7.5334e-04,  1.2878e-03,  7.4683e-04,  1.3526e-03,\n         7.3591e-04,  1.4028e-03,  3.3943e-04,  1.5649e-03,  3.2916e-04,\n         1.6116e-03,  3.2098e-04,  1.6606e-03,  3.1096e-04,  1.7050e-03,\n         3.0631e-04,  1.7329e-03,  2.9900e-04,  1.7685e-03,  2.9258e-04,\n         1.7635e-03,  2.8563e-04,  1.7784e-03,  2.8919e-04,  2.3880e-03,\n         2.8243e-04,  1.9535e-03,  2.7709e-04,  8.8195e-04,  2.7256e-04,\n         8.5692e-04,  2.7228e-04,  1.1985e-03,  2.7395e-04,  1.1799e-03,\n         2.8224e-04,  1.1883e-03,  3.0512e-04,  1.2041e-03,  3.5564e-04,\n         1.3332e-03,  3.5066e-04,  1.3499e-03,  3.4848e-04,  1.3705e-03,\n         3.4708e-04,  1.3880e-03,  3.4567e-04,  1.4144e-03,  3.4662e-04,\n         1.4360e-03,  3.4813e-04,  1.4694e-03,  3.4854e-04,  1.5147e-03,\n         3.3693e-04,  1.6634e-03,  3.3044e-04,  1.7439e-03,  3.2349e-04,\n         1.7680e-03,  3.0976e-04,  1.7786e-03,  2.8747e-04,  1.7837e-03,\n         2.5687e-04,  1.7480e-03,  2.1472e-04,  1.5532e-03,  1.5907e-04,\n         1.5105e-03,  1.0462e-04,  1.4693e-03,  3.3030e-05,  1.4617e-03,\n        -7.1308e-05,  1.4486e-03, -2.3602e-04,  1.4370e-03, -1.0338e-03,\n         1.4191e-03, -2.4064e-03,  1.4063e-03, -2.5302e-03,  1.3924e-03,\n        -2.9414e-03,  1.3844e-03, -3.2730e-03,  1.3756e-03, -3.5856e-03,\n         1.3750e-03, -3.8627e-03,  1.3770e-03, -4.0451e-03,  1.3777e-03,\n        -4.1962e-03,  1.3788e-03, -4.8915e-03,  1.3806e-03, -5.1444e-03,\n         1.3820e-03, -5.3199e-03,  1.3832e-03, -4.4568e-03,  9.9983e-04,\n        -5.4912e-03,  9.9882e-04, -5.7873e-03,  9.9977e-04, -6.0388e-03,\n         1.0009e-03, -6.2610e-03,  1.0062e-03, -6.5251e-03,  1.0148e-03,\n        -6.7107e-03,  1.1211e-03, -6.8321e-03,  1.1374e-03, -6.8951e-03,\n         1.1663e-03, -4.9449e-03,  1.1771e-03, -5.0920e-03,  1.1908e-03,\n        -5.1626e-03,  1.2017e-03, -5.2079e-03,  1.2195e-03, -5.2368e-03,\n         1.2314e-03, -5.2519e-03,  1.2461e-03, -5.2566e-03,  1.2586e-03,\n        -5.3120e-03,  1.3038e-03, -5.3009e-03,  1.3158e-03, -5.2834e-03,\n         1.3292e-03, -5.2630e-03,  1.3417e-03, -5.2394e-03,  1.3585e-03,\n        -5.2149e-03,  1.3715e-03, -5.1883e-03,  1.3863e-03, -5.1626e-03,\n         1.4001e-03, -5.1385e-03,  1.4215e-03, -5.1157e-03,  1.4356e-03,\n        -5.0904e-03,  1.4508e-03, -5.0676e-03,  1.4631e-03, -5.0449e-03,\n         1.4764e-03, -5.0254e-03,  1.4870e-03, -5.0067e-03,  1.4975e-03,\n        -4.9908e-03,  1.5072e-03, -4.9129e-03,  1.5516e-03, -4.8942e-03,\n         1.5755e-03, -4.8772e-03,  1.6043e-03, -4.8635e-03,  1.6324e-03,\n        -4.8476e-03,  1.6633e-03, -4.8370e-03,  1.6883e-03, -4.8261e-03,\n         1.7156e-03, -4.8171e-03,  1.7513e-03, -4.7930e-03,  1.8299e-03,\n        -4.7850e-03,  1.8949e-03, -4.7756e-03,  1.8780e-03, -4.7674e-03,\n         1.8278e-03, -4.7535e-03,  1.7731e-03, -4.7448e-03,  1.7481e-03,\n        -4.7339e-03,  1.7274e-03, -4.7243e-03,  1.7120e-03, -4.6325e-03,\n         1.6755e-03, -4.6230e-03,  1.6621e-03, -4.6105e-03,  1.6527e-03,\n        -4.5992e-03,  1.6666e-03, -4.5783e-03,  2.1340e-03, -4.5650e-03,\n         1.3487e-03, -4.5466e-03,  1.3881e-03, -4.5291e-03,  1.4236e-03,\n        -4.4703e-03,  1.4535e-03, -4.4460e-03,  1.4651e-03, -4.4157e-03,\n         2.6267e-03, -4.3840e-03,  3.0040e-03, -4.3288e-03,  3.0034e-03,\n        -4.2801e-03,  2.9161e-03, -4.2098e-03,  2.7412e-03, -4.1157e-03,\n         2.6546e-03, -5.7056e-03,  2.6143e-03, -5.6830e-03,  2.4451e-03,\n        -5.6600e-03,  2.1505e-03, -5.6392e-03,  1.5330e-03, -5.6177e-03,\n         1.3545e-03, -5.5976e-03, -1.5018e-03, -5.5765e-03, -1.6625e-03,\n        -5.5567e-03,  1.6442e-03, -5.5413e-03,  2.3493e-03, -5.5241e-03,\n         2.6803e-03, -5.5034e-03,  2.8750e-03, -5.4849e-03,  3.0059e-03,\n        -5.4674e-03,  3.1271e-03, -5.4529e-03,  3.3085e-03, -5.4391e-03,\n         3.6322e-03, -5.4275e-03,  3.9344e-03, -5.4335e-03,  4.3076e-03,\n        -5.4215e-03,  4.5882e-03, -5.4097e-03,  4.8037e-03, -5.4000e-03,\n         4.9857e-03, -5.3899e-03,  5.1795e-03, -5.3823e-03,  5.4246e-03,\n        -5.3748e-03,  5.6268e-03, -5.3696e-03,  5.8018e-03, -5.3642e-03,\n         6.0961e-03, -5.3616e-03,  6.2030e-03, -5.3582e-03,  6.0148e-03,\n        -5.3561e-03,  5.8456e-03, -5.3524e-03,  5.7279e-03, -5.3509e-03,\n         5.5777e-03, -5.3486e-03,  5.4009e-03, -5.3475e-03,  5.2684e-03,\n        -5.3408e-03,  4.8635e-03, -5.3387e-03,  4.6363e-03, -5.3358e-03,\n         4.4819e-03, -5.3340e-03,  4.2327e-03, -5.3301e-03,  3.7356e-03,\n        -5.3285e-03,  3.3149e-03, -5.3259e-03,  3.1086e-03, -5.3243e-03,\n         3.0452e-03, -5.3185e-03,  2.8323e-03, -5.3166e-03,  2.4243e-03,\n        -5.3135e-03,  1.8960e-03, -5.3114e-03,  1.9650e-03, -5.3067e-03,\n         1.9784e-03, -5.3042e-03,  1.9681e-03, -5.3005e-03,  3.3496e-04,\n        -5.2977e-03,  4.2486e-04, -5.2820e-03,  4.7370e-04, -5.2786e-03,\n         2.0547e-03, -5.2739e-03,  2.3090e-03, -5.2699e-03,  2.4108e-03,\n        -5.2631e-03,  2.4742e-03, -5.2594e-03,  2.4769e-03, -5.2536e-03,\n         2.4721e-03, -5.2486e-03,  2.4452e-03, -5.2368e-03,  2.4285e-03,\n        -5.2311e-03,  2.3944e-03, -5.2238e-03,  2.1183e-03, -5.2176e-03,\n         2.1157e-03, -5.2077e-03,  2.1301e-03, -5.2007e-03,  2.1338e-03,\n        -5.1920e-03,  4.0911e-04, -5.1843e-03,  5.0755e-04, -5.0444e-03,\n         6.6447e-04, -5.0364e-03,  6.7343e-04, -5.0257e-03,  6.8208e-04,\n        -5.0154e-03,  6.9081e-04, -4.9987e-03,  6.9920e-04, -4.9851e-03,\n         7.0791e-04, -4.9674e-03,  7.2318e-04, -4.9488e-03,  7.2300e-04,\n        -4.9041e-03,  7.2381e-04, -4.8345e-03,  7.2744e-04, -4.7980e-03,\n         7.3149e-04, -4.7550e-03,  7.3670e-04, -4.6830e-03,  7.4150e-04,\n        -4.5901e-03,  7.4747e-04, -4.3622e-03,  7.5330e-04, -3.2231e-03,\n         7.5968e-04, -4.0277e-03,  7.6529e-04, -4.0388e-03,  7.7179e-04,\n        -3.9777e-03,  7.7803e-04, -4.9634e-03,  7.8464e-04, -4.6380e-03,\n         7.9067e-04, -4.6541e-03,  7.9733e-04, -4.6706e-03,  8.0369e-04,\n        -4.6913e-03,  8.1034e-04, -4.7178e-03,  8.1608e-04, -4.7408e-03,\n         8.2269e-04, -4.7628e-03,  8.2902e-04, -4.7853e-03,  8.3559e-04,\n        -4.8062e-03,  8.4159e-04, -4.8269e-03,  8.4806e-04, -4.8450e-03,\n         8.5425e-04, -4.8625e-03,  8.6066e-04, -4.9693e-03,  8.6328e-04,\n        -4.9878e-03,  8.6944e-04, -5.0041e-03,  8.7533e-04, -5.0205e-03,\n         8.8142e-04, -5.0347e-03,  8.8698e-04, -5.0490e-03,  8.9299e-04,\n        -5.0606e-03,  8.9874e-04, -5.0712e-03,  9.0468e-04, -5.0860e-03,\n         9.0976e-04, -5.0912e-03,  9.1561e-04, -5.0916e-03,  9.2120e-04,\n        -5.0957e-03,  9.2695e-04, -5.1034e-03,  9.3223e-04, -5.1136e-03,\n         9.3791e-04, -5.1239e-03,  9.4335e-04, -5.1361e-03,  9.4894e-04,\n        -5.1807e-03,  9.5300e-04, -5.1920e-03,  9.5858e-04, -5.2025e-03,\n         9.6386e-04, -5.2148e-03,  9.6929e-04, -5.2262e-03,  9.7424e-04,\n        -5.2416e-03,  9.7947e-04, -5.2532e-03,  9.8444e-04, -5.2665e-03,\n         9.8960e-04, -5.2827e-03,  9.9396e-04, -5.2956e-03,  9.9911e-04,\n        -5.3081e-03,  1.0041e-03, -5.3224e-03,  1.0091e-03, -5.3348e-03,\n         1.0137e-03, -5.3499e-03,  1.0187e-03, -5.3632e-03,  1.0235e-03,\n        -5.3784e-03,  1.0284e-03], device='cuda:0')\ncustom_layers.9.linear_sigmoid.weight tensor([[-0.0005,  0.0008, -0.0005,  ..., -0.0045, -0.0006, -0.0045]],\n       device='cuda:0')\ncustom_layers.9.linear_sigmoid.bias tensor([0.0008], device='cuda:0')\n","output_type":"stream"}]},{"cell_type":"code","source":"evaluate_model(model_prepu, custom_train_loader, criterion, optimizer)","metadata":{"execution":{"iopub.status.busy":"2024-06-14T15:53:00.891014Z","iopub.execute_input":"2024-06-14T15:53:00.891394Z","iopub.status.idle":"2024-06-14T18:43:46.361670Z","shell.execute_reply.started":"2024-06-14T15:53:00.891363Z","shell.execute_reply":"2024-06-14T18:43:46.360838Z"},"collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":89,"outputs":[{"name":"stdout","text":"Epoch    1 | Train Loss:     1.7041 | Validation Loss:     1.6738\nEpoch    2 | Train Loss:     1.6777 | Validation Loss:     1.6498\nEpoch    3 | Train Loss:     1.6469 | Validation Loss:     1.6227\nEpoch    4 | Train Loss:     1.6119 | Validation Loss:     1.5933\nEpoch    5 | Train Loss:     1.5750 | Validation Loss:     1.5649\nEpoch    6 | Train Loss:     1.5404 | Validation Loss:     1.5414\nEpoch    7 | Train Loss:     1.5123 | Validation Loss:     1.5261\nEpoch    8 | Train Loss:     1.4930 | Validation Loss:     1.5185\nEpoch    9 | Train Loss:     1.4818 | Validation Loss:     1.5158\nEpoch   10 | Train Loss:     1.4757 | Validation Loss:     1.5148\nEpoch   11 | Train Loss:     1.4723 | Validation Loss:     1.5138\nEpoch   12 | Train Loss:     1.4698 | Validation Loss:     1.5123\nEpoch   13 | Train Loss:     1.4676 | Validation Loss:     1.5111\nEpoch   14 | Train Loss:     1.4656 | Validation Loss:     1.5089\nEpoch   15 | Train Loss:     1.4637 | Validation Loss:     1.5067\nEpoch   16 | Train Loss:     1.4620 | Validation Loss:     1.5054\nEpoch   17 | Train Loss:     1.4603 | Validation Loss:     1.5032\nEpoch   18 | Train Loss:     1.4587 | Validation Loss:     1.5014\nEpoch   19 | Train Loss:     1.4572 | Validation Loss:     1.5003\nEpoch   20 | Train Loss:     1.4558 | Validation Loss:     1.4987\nEpoch   21 | Train Loss:     1.4544 | Validation Loss:     1.4971\nEpoch   22 | Train Loss:     1.4532 | Validation Loss:     1.4956\nEpoch   23 | Train Loss:     1.4520 | Validation Loss:     1.4946\nEpoch   24 | Train Loss:     1.4509 | Validation Loss:     1.4937\nEpoch   25 | Train Loss:     1.4498 | Validation Loss:     1.4921\nEpoch   26 | Train Loss:     1.4488 | Validation Loss:     1.4910\nEpoch   27 | Train Loss:     1.4478 | Validation Loss:     1.4899\nEpoch   28 | Train Loss:     1.4469 | Validation Loss:     1.4895\nEpoch   29 | Train Loss:     1.4460 | Validation Loss:     1.4881\nEpoch   30 | Train Loss:     1.4452 | Validation Loss:     1.4879\nEpoch   31 | Train Loss:     1.4443 | Validation Loss:     1.4864\nEpoch   32 | Train Loss:     1.4436 | Validation Loss:     1.4857\nEpoch   33 | Train Loss:     1.4428 | Validation Loss:     1.4848\nEpoch   34 | Train Loss:     1.4421 | Validation Loss:     1.4845\nEpoch   35 | Train Loss:     1.4413 | Validation Loss:     1.4832\nEpoch   36 | Train Loss:     1.4407 | Validation Loss:     1.4823\nEpoch   37 | Train Loss:     1.4400 | Validation Loss:     1.4817\nEpoch   38 | Train Loss:     1.4393 | Validation Loss:     1.4811\nEpoch   39 | Train Loss:     1.4387 | Validation Loss:     1.4804\nEpoch   40 | Train Loss:     1.4380 | Validation Loss:     1.4797\nEpoch   41 | Train Loss:     1.4374 | Validation Loss:     1.4792\nEpoch   42 | Train Loss:     1.4368 | Validation Loss:     1.4785\nEpoch   43 | Train Loss:     1.4363 | Validation Loss:     1.4779\nEpoch   44 | Train Loss:     1.4357 | Validation Loss:     1.4769\nEpoch   45 | Train Loss:     1.4351 | Validation Loss:     1.4762\nEpoch   46 | Train Loss:     1.4346 | Validation Loss:     1.4765\nEpoch   47 | Train Loss:     1.4340 | Validation Loss:     1.4759\nEpoch   48 | Train Loss:     1.4334 | Validation Loss:     1.4747\nEpoch   49 | Train Loss:     1.4329 | Validation Loss:     1.4744\nEpoch   50 | Train Loss:     1.4324 | Validation Loss:     1.4744\nEpoch   51 | Train Loss:     1.4318 | Validation Loss:     1.4737\nEpoch   52 | Train Loss:     1.4313 | Validation Loss:     1.4721\nEpoch   53 | Train Loss:     1.4307 | Validation Loss:     1.4718\nEpoch   54 | Train Loss:     1.4302 | Validation Loss:     1.4716\nEpoch   55 | Train Loss:     1.4297 | Validation Loss:     1.4711\nEpoch   56 | Train Loss:     1.4293 | Validation Loss:     1.4701\nEpoch   57 | Train Loss:     1.4287 | Validation Loss:     1.4701\nEpoch   58 | Train Loss:     1.4282 | Validation Loss:     1.4692\nEpoch   59 | Train Loss:     1.4277 | Validation Loss:     1.4688\nEpoch   60 | Train Loss:     1.4272 | Validation Loss:     1.4687\nEpoch   61 | Train Loss:     1.4268 | Validation Loss:     1.4677\nEpoch   62 | Train Loss:     1.4262 | Validation Loss:     1.4673\nEpoch   63 | Train Loss:     1.4257 | Validation Loss:     1.4670\nEpoch   64 | Train Loss:     1.4251 | Validation Loss:     1.4661\nEpoch   65 | Train Loss:     1.4247 | Validation Loss:     1.4655\nEpoch   66 | Train Loss:     1.4242 | Validation Loss:     1.4653\nEpoch   67 | Train Loss:     1.4236 | Validation Loss:     1.4643\nEpoch   68 | Train Loss:     1.4231 | Validation Loss:     1.4643\nEpoch   69 | Train Loss:     1.4226 | Validation Loss:     1.4631\nEpoch   80 | Train Loss:     1.4165 | Validation Loss:     1.4582\nEpoch   81 | Train Loss:     1.4159 | Validation Loss:     1.4570\nEpoch   82 | Train Loss:     1.4155 | Validation Loss:     1.4569\nEpoch   83 | Train Loss:     1.4147 | Validation Loss:     1.4557\nEpoch   84 | Train Loss:     1.4141 | Validation Loss:     1.4554\nEpoch   85 | Train Loss:     1.4135 | Validation Loss:     1.4547\nEpoch   86 | Train Loss:     1.4129 | Validation Loss:     1.4545\nEpoch   87 | Train Loss:     1.4123 | Validation Loss:     1.4537\nEpoch   88 | Train Loss:     1.4116 | Validation Loss:     1.4525\nEpoch   89 | Train Loss:     1.4110 | Validation Loss:     1.4518\nEpoch   90 | Train Loss:     1.4104 | Validation Loss:     1.4514\nEpoch   91 | Train Loss:     1.4097 | Validation Loss:     1.4508\nEpoch   92 | Train Loss:     1.4091 | Validation Loss:     1.4507\nEpoch   93 | Train Loss:     1.4084 | Validation Loss:     1.4501\nEpoch   94 | Train Loss:     1.4077 | Validation Loss:     1.4487\nEpoch   95 | Train Loss:     1.4070 | Validation Loss:     1.4481\nEpoch   96 | Train Loss:     1.4064 | Validation Loss:     1.4475\nEpoch   97 | Train Loss:     1.4059 | Validation Loss:     1.4463\nEpoch   98 | Train Loss:     1.4050 | Validation Loss:     1.4456\nEpoch   99 | Train Loss:     1.4043 | Validation Loss:     1.4452\nEpoch  100 | Train Loss:     1.4035 | Validation Loss:     1.4448\nEpoch  101 | Train Loss:     1.4029 | Validation Loss:     1.4435\nEpoch  102 | Train Loss:     1.4021 | Validation Loss:     1.4436\nEpoch  103 | Train Loss:     1.4013 | Validation Loss:     1.4426\nEpoch  104 | Train Loss:     1.4006 | Validation Loss:     1.4412\nEpoch  105 | Train Loss:     1.3998 | Validation Loss:     1.4413\nEpoch  106 | Train Loss:     1.3991 | Validation Loss:     1.4405\nEpoch  107 | Train Loss:     1.3983 | Validation Loss:     1.4400\nEpoch  108 | Train Loss:     1.3974 | Validation Loss:     1.4392\nEpoch  109 | Train Loss:     1.3966 | Validation Loss:     1.4381\nEpoch  110 | Train Loss:     1.3958 | Validation Loss:     1.4367\nEpoch  111 | Train Loss:     1.3949 | Validation Loss:     1.4358\nEpoch  112 | Train Loss:     1.3941 | Validation Loss:     1.4361\nEpoch  113 | Train Loss:     1.3932 | Validation Loss:     1.4341\nEpoch  114 | Train Loss:     1.3923 | Validation Loss:     1.4340\nEpoch  115 | Train Loss:     1.3914 | Validation Loss:     1.4330\nEpoch  116 | Train Loss:     1.3905 | Validation Loss:     1.4319\nEpoch  117 | Train Loss:     1.3896 | Validation Loss:     1.4310\nEpoch  118 | Train Loss:     1.3886 | Validation Loss:     1.4298\nEpoch  119 | Train Loss:     1.3877 | Validation Loss:     1.4286\nEpoch  120 | Train Loss:     1.3867 | Validation Loss:     1.4269\nEpoch  121 | Train Loss:     1.3856 | Validation Loss:     1.4268\nEpoch  122 | Train Loss:     1.3846 | Validation Loss:     1.4259\nEpoch  123 | Train Loss:     1.3836 | Validation Loss:     1.4249\nEpoch  124 | Train Loss:     1.3824 | Validation Loss:     1.4237\nEpoch  125 | Train Loss:     1.3814 | Validation Loss:     1.4228\nEpoch  126 | Train Loss:     1.3803 | Validation Loss:     1.4216\nEpoch  127 | Train Loss:     1.3791 | Validation Loss:     1.4197\nEpoch  128 | Train Loss:     1.3779 | Validation Loss:     1.4186\nEpoch  129 | Train Loss:     1.3766 | Validation Loss:     1.4179\nEpoch  130 | Train Loss:     1.3754 | Validation Loss:     1.4165\nEpoch  131 | Train Loss:     1.3741 | Validation Loss:     1.4143\nEpoch  132 | Train Loss:     1.3728 | Validation Loss:     1.4131\nEpoch  133 | Train Loss:     1.3714 | Validation Loss:     1.4124\nEpoch  134 | Train Loss:     1.3701 | Validation Loss:     1.4111\nEpoch  135 | Train Loss:     1.3686 | Validation Loss:     1.4091\nEpoch  136 | Train Loss:     1.3671 | Validation Loss:     1.4079\nEpoch  137 | Train Loss:     1.3655 | Validation Loss:     1.4063\nEpoch  138 | Train Loss:     1.3638 | Validation Loss:     1.4041\nEpoch  139 | Train Loss:     1.3622 | Validation Loss:     1.4030\nEpoch  140 | Train Loss:     1.3605 | Validation Loss:     1.4017\nEpoch  141 | Train Loss:     1.3588 | Validation Loss:     1.3990\nEpoch  142 | Train Loss:     1.3570 | Validation Loss:     1.3976\nEpoch  143 | Train Loss:     1.3550 | Validation Loss:     1.3954\nEpoch  144 | Train Loss:     1.3532 | Validation Loss:     1.3924\nEpoch  145 | Train Loss:     1.3511 | Validation Loss:     1.3901\nEpoch  146 | Train Loss:     1.3490 | Validation Loss:     1.3886\nEpoch  147 | Train Loss:     1.3468 | Validation Loss:     1.3871\nEpoch  148 | Train Loss:     1.3446 | Validation Loss:     1.3844\nEpoch  149 | Train Loss:     1.3423 | Validation Loss:     1.3817\nEpoch  150 | Train Loss:     1.3399 | Validation Loss:     1.3801\nEpoch  151 | Train Loss:     1.3374 | Validation Loss:     1.3772\nEpoch  152 | Train Loss:     1.3351 | Validation Loss:     1.3745\nEpoch  153 | Train Loss:     1.3322 | Validation Loss:     1.3718\nEpoch  154 | Train Loss:     1.3296 | Validation Loss:     1.3693\nEpoch  155 | Train Loss:     1.3268 | Validation Loss:     1.3664\nEpoch  156 | Train Loss:     1.3240 | Validation Loss:     1.3624\nEpoch  157 | Train Loss:     1.3210 | Validation Loss:     1.3594\nEpoch  158 | Train Loss:     1.3179 | Validation Loss:     1.3558\nEpoch  159 | Train Loss:     1.3149 | Validation Loss:     1.3535\nEpoch  160 | Train Loss:     1.3116 | Validation Loss:     1.3499\nEpoch  161 | Train Loss:     1.3083 | Validation Loss:     1.3463\nEpoch  162 | Train Loss:     1.3050 | Validation Loss:     1.3433\nEpoch  163 | Train Loss:     1.3016 | Validation Loss:     1.3396\nEpoch  164 | Train Loss:     1.2981 | Validation Loss:     1.3360\nEpoch  165 | Train Loss:     1.2945 | Validation Loss:     1.3323\nEpoch  166 | Train Loss:     1.2910 | Validation Loss:     1.3294\nEpoch  167 | Train Loss:     1.2872 | Validation Loss:     1.3245\nEpoch  168 | Train Loss:     1.2836 | Validation Loss:     1.3208\nEpoch  169 | Train Loss:     1.2798 | Validation Loss:     1.3176\nEpoch  170 | Train Loss:     1.2759 | Validation Loss:     1.3128\nEpoch  171 | Train Loss:     1.2721 | Validation Loss:     1.3094\nEpoch  172 | Train Loss:     1.2683 | Validation Loss:     1.3056\nEpoch  173 | Train Loss:     1.2643 | Validation Loss:     1.3016\nEpoch  174 | Train Loss:     1.2603 | Validation Loss:     1.2974\nEpoch  175 | Train Loss:     1.2563 | Validation Loss:     1.2933\nEpoch  176 | Train Loss:     1.2522 | Validation Loss:     1.2889\nEpoch  177 | Train Loss:     1.2483 | Validation Loss:     1.2860\nEpoch  178 | Train Loss:     1.2443 | Validation Loss:     1.2803\nEpoch  179 | Train Loss:     1.2401 | Validation Loss:     1.2771\nEpoch  180 | Train Loss:     1.2362 | Validation Loss:     1.2730\nEpoch  181 | Train Loss:     1.2321 | Validation Loss:     1.2676\nEpoch  182 | Train Loss:     1.2281 | Validation Loss:     1.2638\nEpoch  183 | Train Loss:     1.2241 | Validation Loss:     1.2596\nEpoch  184 | Train Loss:     1.2199 | Validation Loss:     1.2551\nEpoch  185 | Train Loss:     1.2160 | Validation Loss:     1.2515\nEpoch  186 | Train Loss:     1.2119 | Validation Loss:     1.2464\nEpoch  187 | Train Loss:     1.2082 | Validation Loss:     1.2437\nEpoch  188 | Train Loss:     1.2043 | Validation Loss:     1.2392\nEpoch  189 | Train Loss:     1.2002 | Validation Loss:     1.2347\nEpoch  190 | Train Loss:     1.1962 | Validation Loss:     1.2315\nEpoch  191 | Train Loss:     1.1924 | Validation Loss:     1.2268\nEpoch  192 | Train Loss:     1.1884 | Validation Loss:     1.2222\nEpoch  193 | Train Loss:     1.1846 | Validation Loss:     1.2190\nEpoch  194 | Train Loss:     1.1807 | Validation Loss:     1.2139\nEpoch  195 | Train Loss:     1.1769 | Validation Loss:     1.2102\nEpoch  196 | Train Loss:     1.1731 | Validation Loss:     1.2067\nEpoch  197 | Train Loss:     1.1692 | Validation Loss:     1.2021\nEpoch  198 | Train Loss:     1.1655 | Validation Loss:     1.1980\nEpoch  199 | Train Loss:     1.1617 | Validation Loss:     1.1946\nEpoch  200 | Train Loss:     1.1579 | Validation Loss:     1.1904\nEpoch  201 | Train Loss:     1.1540 | Validation Loss:     1.1863\nEpoch  202 | Train Loss:     1.1503 | Validation Loss:     1.1819\nEpoch  203 | Train Loss:     1.1467 | Validation Loss:     1.1780\nEpoch  204 | Train Loss:     1.1429 | Validation Loss:     1.1752\nEpoch  205 | Train Loss:     1.1390 | Validation Loss:     1.1709\nEpoch  206 | Train Loss:     1.1352 | Validation Loss:     1.1664\nEpoch  207 | Train Loss:     1.1314 | Validation Loss:     1.1637\nEpoch  208 | Train Loss:     1.1276 | Validation Loss:     1.1582\nEpoch  209 | Train Loss:     1.1238 | Validation Loss:     1.1547\nEpoch  210 | Train Loss:     1.1200 | Validation Loss:     1.1504\nEpoch  211 | Train Loss:     1.1160 | Validation Loss:     1.1462\nEpoch  212 | Train Loss:     1.1120 | Validation Loss:     1.1416\nEpoch  213 | Train Loss:     1.1081 | Validation Loss:     1.1374\nEpoch  214 | Train Loss:     1.1042 | Validation Loss:     1.1331\nEpoch  215 | Train Loss:     1.1001 | Validation Loss:     1.1298\nEpoch  216 | Train Loss:     1.0961 | Validation Loss:     1.1248\nEpoch  217 | Train Loss:     1.0924 | Validation Loss:     1.1210\nEpoch  218 | Train Loss:     1.0881 | Validation Loss:     1.1170\nEpoch  219 | Train Loss:     1.0840 | Validation Loss:     1.1127\nEpoch  220 | Train Loss:     1.0800 | Validation Loss:     1.1076\nEpoch  221 | Train Loss:     1.0758 | Validation Loss:     1.1034\nEpoch  222 | Train Loss:     1.0715 | Validation Loss:     1.0989\nEpoch  223 | Train Loss:     1.0675 | Validation Loss:     1.0948\nEpoch  224 | Train Loss:     1.0630 | Validation Loss:     1.0898\nEpoch  225 | Train Loss:     1.0588 | Validation Loss:     1.0859\nEpoch  226 | Train Loss:     1.0544 | Validation Loss:     1.0809\nEpoch  227 | Train Loss:     1.0501 | Validation Loss:     1.0763\nEpoch  228 | Train Loss:     1.0457 | Validation Loss:     1.0714\nEpoch  229 | Train Loss:     1.0413 | Validation Loss:     1.0669\nEpoch  230 | Train Loss:     1.0369 | Validation Loss:     1.0624\nEpoch  231 | Train Loss:     1.0322 | Validation Loss:     1.0581\nEpoch  232 | Train Loss:     1.0277 | Validation Loss:     1.0542\nEpoch  233 | Train Loss:     1.0232 | Validation Loss:     1.0480\nEpoch  234 | Train Loss:     1.0185 | Validation Loss:     1.0434\nEpoch  235 | Train Loss:     1.0138 | Validation Loss:     1.0390\nEpoch  236 | Train Loss:     1.0090 | Validation Loss:     1.0332\nEpoch  237 | Train Loss:     1.0042 | Validation Loss:     1.0283\nEpoch  238 | Train Loss:     0.9993 | Validation Loss:     1.0227\nEpoch  239 | Train Loss:     0.9946 | Validation Loss:     1.0179\nEpoch  240 | Train Loss:     0.9895 | Validation Loss:     1.0125\nEpoch  241 | Train Loss:     0.9844 | Validation Loss:     1.0071\nEpoch  242 | Train Loss:     0.9794 | Validation Loss:     1.0018\nEpoch  243 | Train Loss:     0.9743 | Validation Loss:     0.9972\nEpoch  244 | Train Loss:     0.9691 | Validation Loss:     0.9920\nEpoch  245 | Train Loss:     0.9641 | Validation Loss:     0.9862\nEpoch  246 | Train Loss:     0.9586 | Validation Loss:     0.9812\nEpoch  247 | Train Loss:     0.9534 | Validation Loss:     0.9755\nEpoch  248 | Train Loss:     0.9481 | Validation Loss:     0.9709\nEpoch  249 | Train Loss:     0.9430 | Validation Loss:     0.9649\nEpoch  250 | Train Loss:     0.9376 | Validation Loss:     0.9588\nEpoch  251 | Train Loss:     0.9324 | Validation Loss:     0.9538\nEpoch  252 | Train Loss:     0.9272 | Validation Loss:     0.9485\nEpoch  253 | Train Loss:     0.9219 | Validation Loss:     0.9434\nEpoch  254 | Train Loss:     0.9169 | Validation Loss:     0.9376\nEpoch  255 | Train Loss:     0.9117 | Validation Loss:     0.9330\nEpoch  256 | Train Loss:     0.9067 | Validation Loss:     0.9274\nEpoch  257 | Train Loss:     0.9018 | Validation Loss:     0.9223\nEpoch  258 | Train Loss:     0.8969 | Validation Loss:     0.9174\nEpoch  259 | Train Loss:     0.8921 | Validation Loss:     0.9130\nEpoch  260 | Train Loss:     0.8874 | Validation Loss:     0.9079\nEpoch  261 | Train Loss:     0.8827 | Validation Loss:     0.9032\nEpoch  262 | Train Loss:     0.8780 | Validation Loss:     0.8987\nEpoch  263 | Train Loss:     0.8735 | Validation Loss:     0.8937\nEpoch  264 | Train Loss:     0.8690 | Validation Loss:     0.8893\nEpoch  265 | Train Loss:     0.8647 | Validation Loss:     0.8845\nEpoch  266 | Train Loss:     0.8603 | Validation Loss:     0.8800\nEpoch  267 | Train Loss:     0.8560 | Validation Loss:     0.8767\nEpoch  268 | Train Loss:     0.8520 | Validation Loss:     0.8718\nEpoch  269 | Train Loss:     0.8479 | Validation Loss:     0.8676\nEpoch  270 | Train Loss:     0.8440 | Validation Loss:     0.8641\nEpoch  271 | Train Loss:     0.8402 | Validation Loss:     0.8611\nEpoch  272 | Train Loss:     0.8363 | Validation Loss:     0.8558\nEpoch  273 | Train Loss:     0.8326 | Validation Loss:     0.8520\nEpoch  274 | Train Loss:     0.8289 | Validation Loss:     0.8485\nEpoch  275 | Train Loss:     0.8253 | Validation Loss:     0.8462\nEpoch  276 | Train Loss:     0.8222 | Validation Loss:     0.8416\nEpoch  277 | Train Loss:     0.8188 | Validation Loss:     0.8381\nEpoch  278 | Train Loss:     0.8154 | Validation Loss:     0.8347\nEpoch  279 | Train Loss:     0.8121 | Validation Loss:     0.8310\nEpoch  280 | Train Loss:     0.8092 | Validation Loss:     0.8282\nEpoch  281 | Train Loss:     0.8062 | Validation Loss:     0.8257\nEpoch  282 | Train Loss:     0.8033 | Validation Loss:     0.8220\nEpoch  283 | Train Loss:     0.8003 | Validation Loss:     0.8191\nEpoch  284 | Train Loss:     0.7978 | Validation Loss:     0.8166\nEpoch  285 | Train Loss:     0.7952 | Validation Loss:     0.8140\nEpoch  286 | Train Loss:     0.7926 | Validation Loss:     0.8115\nEpoch  287 | Train Loss:     0.7901 | Validation Loss:     0.8096\nEpoch  288 | Train Loss:     0.7878 | Validation Loss:     0.8062\nEpoch  289 | Train Loss:     0.7855 | Validation Loss:     0.8041\nEpoch  290 | Train Loss:     0.7831 | Validation Loss:     0.8017\nEpoch  291 | Train Loss:     0.7810 | Validation Loss:     0.7999\nEpoch  292 | Train Loss:     0.7789 | Validation Loss:     0.7975\nEpoch  293 | Train Loss:     0.7769 | Validation Loss:     0.7958\nEpoch  294 | Train Loss:     0.7750 | Validation Loss:     0.7940\nEpoch  295 | Train Loss:     0.7730 | Validation Loss:     0.7934\nEpoch  296 | Train Loss:     0.7714 | Validation Loss:     0.7904\nEpoch  297 | Train Loss:     0.7696 | Validation Loss:     0.7882\nEpoch  298 | Train Loss:     0.7682 | Validation Loss:     0.7866\nEpoch  299 | Train Loss:     0.7664 | Validation Loss:     0.7861\nEpoch  300 | Train Loss:     0.7652 | Validation Loss:     0.7836\nEpoch  301 | Train Loss:     0.7637 | Validation Loss:     0.7829\nEpoch  302 | Train Loss:     0.7622 | Validation Loss:     0.7811\nEpoch  303 | Train Loss:     0.7607 | Validation Loss:     0.7797\nEpoch  304 | Train Loss:     0.7596 | Validation Loss:     0.7784\nEpoch  305 | Train Loss:     0.7581 | Validation Loss:     0.7781\nEpoch  306 | Train Loss:     0.7572 | Validation Loss:     0.7758\nEpoch  307 | Train Loss:     0.7560 | Validation Loss:     0.7757\nEpoch  308 | Train Loss:     0.7548 | Validation Loss:     0.7734\nEpoch  309 | Train Loss:     0.7538 | Validation Loss:     0.7732\nEpoch  310 | Train Loss:     0.7529 | Validation Loss:     0.7723\nEpoch  311 | Train Loss:     0.7519 | Validation Loss:     0.7712\nEpoch  312 | Train Loss:     0.7511 | Validation Loss:     0.7697\nEpoch  313 | Train Loss:     0.7501 | Validation Loss:     0.7701\nEpoch  314 | Train Loss:     0.7494 | Validation Loss:     0.7681\nEpoch  315 | Train Loss:     0.7485 | Validation Loss:     0.7674\nEpoch  316 | Train Loss:     0.7477 | Validation Loss:     0.7666\nEpoch  317 | Train Loss:     0.7469 | Validation Loss:     0.7668\nEpoch  318 | Train Loss:     0.7463 | Validation Loss:     0.7648\nEpoch  319 | Train Loss:     0.7457 | Validation Loss:     0.7642\nEpoch  320 | Train Loss:     0.7447 | Validation Loss:     0.7634\nEpoch  321 | Train Loss:     0.7442 | Validation Loss:     0.7629\nEpoch  322 | Train Loss:     0.7436 | Validation Loss:     0.7623\nEpoch  323 | Train Loss:     0.7429 | Validation Loss:     0.7616\nEpoch  324 | Train Loss:     0.7423 | Validation Loss:     0.7617\nEpoch  325 | Train Loss:     0.7417 | Validation Loss:     0.7609\nEpoch  326 | Train Loss:     0.7413 | Validation Loss:     0.7598\nEpoch  327 | Train Loss:     0.7407 | Validation Loss:     0.7607\nEpoch  328 | Train Loss:     0.7402 | Validation Loss:     0.7591\nEpoch  329 | Train Loss:     0.7396 | Validation Loss:     0.7594\nEpoch  330 | Train Loss:     0.7392 | Validation Loss:     0.7579\nEpoch  331 | Train Loss:     0.7386 | Validation Loss:     0.7583\nEpoch  332 | Train Loss:     0.7382 | Validation Loss:     0.7570\nEpoch  333 | Train Loss:     0.7377 | Validation Loss:     0.7575\nEpoch  334 | Train Loss:     0.7374 | Validation Loss:     0.7573\nEpoch  335 | Train Loss:     0.7370 | Validation Loss:     0.7558\nEpoch  336 | Train Loss:     0.7365 | Validation Loss:     0.7560\nEpoch  337 | Train Loss:     0.7361 | Validation Loss:     0.7559\nEpoch  338 | Train Loss:     0.7357 | Validation Loss:     0.7548\nEpoch  339 | Train Loss:     0.7353 | Validation Loss:     0.7541\nEpoch  340 | Train Loss:     0.7349 | Validation Loss:     0.7548\nEpoch  341 | Train Loss:     0.7345 | Validation Loss:     0.7531\nEpoch  342 | Train Loss:     0.7342 | Validation Loss:     0.7532\nEpoch  343 | Train Loss:     0.7338 | Validation Loss:     0.7526\nEpoch  344 | Train Loss:     0.7333 | Validation Loss:     0.7541\nEpoch  345 | Train Loss:     0.7332 | Validation Loss:     0.7527\nEpoch  346 | Train Loss:     0.7329 | Validation Loss:     0.7519\nEpoch  347 | Train Loss:     0.7326 | Validation Loss:     0.7510\nEpoch  348 | Train Loss:     0.7324 | Validation Loss:     0.7511\nEpoch  349 | Train Loss:     0.7321 | Validation Loss:     0.7510\nEpoch  350 | Train Loss:     0.7319 | Validation Loss:     0.7504\nEpoch  351 | Train Loss:     0.7313 | Validation Loss:     0.7516\nEpoch  352 | Train Loss:     0.7311 | Validation Loss:     0.7506\nEpoch  353 | Train Loss:     0.7308 | Validation Loss:     0.7507\nEpoch  354 | Train Loss:     0.7302 | Validation Loss:     0.7490\nEpoch  355 | Train Loss:     0.7301 | Validation Loss:     0.7492\nEpoch  356 | Train Loss:     0.7299 | Validation Loss:     0.7487\nEpoch  357 | Train Loss:     0.7296 | Validation Loss:     0.7486\nEpoch  358 | Train Loss:     0.7293 | Validation Loss:     0.7521\nEpoch  359 | Train Loss:     0.7293 | Validation Loss:     0.7480\nEpoch  360 | Train Loss:     0.7288 | Validation Loss:     0.7481\nEpoch  361 | Train Loss:     0.7286 | Validation Loss:     0.7476\nEpoch  362 | Train Loss:     0.7282 | Validation Loss:     0.7473\nEpoch  363 | Train Loss:     0.7282 | Validation Loss:     0.7467\nEpoch  364 | Train Loss:     0.7279 | Validation Loss:     0.7478\nEpoch  365 | Train Loss:     0.7275 | Validation Loss:     0.7465\nEpoch  366 | Train Loss:     0.7272 | Validation Loss:     0.7460\nEpoch  367 | Train Loss:     0.7274 | Validation Loss:     0.7462\nEpoch  368 | Train Loss:     0.7270 | Validation Loss:     0.7455\nEpoch  369 | Train Loss:     0.7267 | Validation Loss:     0.7461\nEpoch  370 | Train Loss:     0.7263 | Validation Loss:     0.7465\nEpoch  371 | Train Loss:     0.7261 | Validation Loss:     0.7453\nEpoch  372 | Train Loss:     0.7259 | Validation Loss:     0.7445\nEpoch  373 | Train Loss:     0.7258 | Validation Loss:     0.7443\nEpoch  374 | Train Loss:     0.7257 | Validation Loss:     0.7446\nEpoch  375 | Train Loss:     0.7253 | Validation Loss:     0.7455\nEpoch  376 | Train Loss:     0.7249 | Validation Loss:     0.7444\nEpoch  377 | Train Loss:     0.7248 | Validation Loss:     0.7447\nEpoch  378 | Train Loss:     0.7246 | Validation Loss:     0.7434\nEpoch  379 | Train Loss:     0.7245 | Validation Loss:     0.7437\nEpoch  380 | Train Loss:     0.7245 | Validation Loss:     0.7443\nEpoch  381 | Train Loss:     0.7241 | Validation Loss:     0.7428\nEpoch  382 | Train Loss:     0.7239 | Validation Loss:     0.7428\nEpoch  383 | Train Loss:     0.7236 | Validation Loss:     0.7424\nEpoch  384 | Train Loss:     0.7236 | Validation Loss:     0.7426\nEpoch  385 | Train Loss:     0.7232 | Validation Loss:     0.7424\nEpoch  386 | Train Loss:     0.7231 | Validation Loss:     0.7427\nEpoch  387 | Train Loss:     0.7229 | Validation Loss:     0.7423\nEpoch  388 | Train Loss:     0.7230 | Validation Loss:     0.7416\nEpoch  389 | Train Loss:     0.7225 | Validation Loss:     0.7420\nEpoch  390 | Train Loss:     0.7222 | Validation Loss:     0.7423\nEpoch  391 | Train Loss:     0.7222 | Validation Loss:     0.7408\nEpoch  392 | Train Loss:     0.7220 | Validation Loss:     0.7411\nEpoch  393 | Train Loss:     0.7217 | Validation Loss:     0.7424\nEpoch  394 | Train Loss:     0.7217 | Validation Loss:     0.7410\nEpoch  395 | Train Loss:     0.7214 | Validation Loss:     0.7402\nEpoch  396 | Train Loss:     0.7214 | Validation Loss:     0.7407\nEpoch  397 | Train Loss:     0.7211 | Validation Loss:     0.7413\nEpoch  398 | Train Loss:     0.7208 | Validation Loss:     0.7401\nEpoch  399 | Train Loss:     0.7206 | Validation Loss:     0.7394\nEpoch  400 | Train Loss:     0.7204 | Validation Loss:     0.7402\nEpoch  401 | Train Loss:     0.7204 | Validation Loss:     0.7391\nEpoch  402 | Train Loss:     0.7203 | Validation Loss:     0.7393\nEpoch  403 | Train Loss:     0.7200 | Validation Loss:     0.7396\nEpoch  404 | Train Loss:     0.7199 | Validation Loss:     0.7394\nEpoch  405 | Train Loss:     0.7197 | Validation Loss:     0.7388\nEpoch  406 | Train Loss:     0.7196 | Validation Loss:     0.7386\nEpoch  407 | Train Loss:     0.7194 | Validation Loss:     0.7387\nEpoch  408 | Train Loss:     0.7192 | Validation Loss:     0.7381\nEpoch  409 | Train Loss:     0.7190 | Validation Loss:     0.7384\nEpoch  410 | Train Loss:     0.7189 | Validation Loss:     0.7382\nEpoch  411 | Train Loss:     0.7187 | Validation Loss:     0.7378\nEpoch  412 | Train Loss:     0.7186 | Validation Loss:     0.7379\nEpoch  413 | Train Loss:     0.7184 | Validation Loss:     0.7376\nEpoch  414 | Train Loss:     0.7186 | Validation Loss:     0.7382\nEpoch  415 | Train Loss:     0.7181 | Validation Loss:     0.7379\nEpoch  416 | Train Loss:     0.7179 | Validation Loss:     0.7388\nEpoch  417 | Train Loss:     0.7178 | Validation Loss:     0.7372\nEpoch  418 | Train Loss:     0.7177 | Validation Loss:     0.7368\nEpoch  419 | Train Loss:     0.7176 | Validation Loss:     0.7364\nEpoch  420 | Train Loss:     0.7175 | Validation Loss:     0.7360\nEpoch  421 | Train Loss:     0.7173 | Validation Loss:     0.7366\nEpoch  422 | Train Loss:     0.7172 | Validation Loss:     0.7361\nEpoch  423 | Train Loss:     0.7169 | Validation Loss:     0.7368\nEpoch  424 | Train Loss:     0.7168 | Validation Loss:     0.7356\nEpoch  425 | Train Loss:     0.7165 | Validation Loss:     0.7360\nEpoch  426 | Train Loss:     0.7163 | Validation Loss:     0.7364\nEpoch  427 | Train Loss:     0.7163 | Validation Loss:     0.7352\nEpoch  428 | Train Loss:     0.7162 | Validation Loss:     0.7352\nEpoch  429 | Train Loss:     0.7160 | Validation Loss:     0.7348\nEpoch  430 | Train Loss:     0.7159 | Validation Loss:     0.7346\nEpoch  431 | Train Loss:     0.7160 | Validation Loss:     0.7356\nEpoch  432 | Train Loss:     0.7156 | Validation Loss:     0.7350\nEpoch  433 | Train Loss:     0.7155 | Validation Loss:     0.7342\nEpoch  434 | Train Loss:     0.7152 | Validation Loss:     0.7341\nEpoch  435 | Train Loss:     0.7153 | Validation Loss:     0.7343\nEpoch  436 | Train Loss:     0.7150 | Validation Loss:     0.7342\nEpoch  437 | Train Loss:     0.7148 | Validation Loss:     0.7343\nEpoch  438 | Train Loss:     0.7149 | Validation Loss:     0.7349\nEpoch  439 | Train Loss:     0.7145 | Validation Loss:     0.7343\nEpoch  440 | Train Loss:     0.7145 | Validation Loss:     0.7342\nEpoch  441 | Train Loss:     0.7142 | Validation Loss:     0.7331\nEpoch  442 | Train Loss:     0.7142 | Validation Loss:     0.7329\nEpoch  443 | Train Loss:     0.7141 | Validation Loss:     0.7331\nEpoch  444 | Train Loss:     0.7138 | Validation Loss:     0.7336\nEpoch  445 | Train Loss:     0.7137 | Validation Loss:     0.7327\nEpoch  446 | Train Loss:     0.7138 | Validation Loss:     0.7323\nEpoch  447 | Train Loss:     0.7135 | Validation Loss:     0.7333\nEpoch  448 | Train Loss:     0.7133 | Validation Loss:     0.7323\nEpoch  449 | Train Loss:     0.7134 | Validation Loss:     0.7322\nEpoch  450 | Train Loss:     0.7130 | Validation Loss:     0.7324\nEpoch  451 | Train Loss:     0.7128 | Validation Loss:     0.7319\nEpoch  452 | Train Loss:     0.7127 | Validation Loss:     0.7316\nEpoch  453 | Train Loss:     0.7127 | Validation Loss:     0.7316\nEpoch  454 | Train Loss:     0.7126 | Validation Loss:     0.7320\nEpoch  455 | Train Loss:     0.7128 | Validation Loss:     0.7316\nEpoch  456 | Train Loss:     0.7122 | Validation Loss:     0.7312\nEpoch  457 | Train Loss:     0.7121 | Validation Loss:     0.7309\nEpoch  458 | Train Loss:     0.7120 | Validation Loss:     0.7306\nEpoch  459 | Train Loss:     0.7119 | Validation Loss:     0.7310\nEpoch  460 | Train Loss:     0.7117 | Validation Loss:     0.7310\nEpoch  461 | Train Loss:     0.7117 | Validation Loss:     0.7304\nEpoch  462 | Train Loss:     0.7113 | Validation Loss:     0.7302\nEpoch  463 | Train Loss:     0.7113 | Validation Loss:     0.7307\nEpoch  464 | Train Loss:     0.7113 | Validation Loss:     0.7303\nEpoch  465 | Train Loss:     0.7109 | Validation Loss:     0.7297\nEpoch  466 | Train Loss:     0.7112 | Validation Loss:     0.7295\nEpoch  467 | Train Loss:     0.7110 | Validation Loss:     0.7301\nEpoch  468 | Train Loss:     0.7107 | Validation Loss:     0.7300\nEpoch  469 | Train Loss:     0.7106 | Validation Loss:     0.7300\nEpoch  470 | Train Loss:     0.7105 | Validation Loss:     0.7299\nEpoch  471 | Train Loss:     0.7103 | Validation Loss:     0.7294\nEpoch  482 | Train Loss:     0.7090 | Validation Loss:     0.7275\nEpoch  483 | Train Loss:     0.7090 | Validation Loss:     0.7286\nEpoch  484 | Train Loss:     0.7086 | Validation Loss:     0.7277\nEpoch  485 | Train Loss:     0.7084 | Validation Loss:     0.7280\nEpoch  486 | Train Loss:     0.7086 | Validation Loss:     0.7273\nEpoch  487 | Train Loss:     0.7083 | Validation Loss:     0.7278\nEpoch  488 | Train Loss:     0.7081 | Validation Loss:     0.7271\nEpoch  489 | Train Loss:     0.7080 | Validation Loss:     0.7275\nEpoch  490 | Train Loss:     0.7079 | Validation Loss:     0.7272\nEpoch  491 | Train Loss:     0.7078 | Validation Loss:     0.7272\nEpoch  492 | Train Loss:     0.7073 | Validation Loss:     0.7262\nEpoch  493 | Train Loss:     0.7075 | Validation Loss:     0.7266\nEpoch  494 | Train Loss:     0.7074 | Validation Loss:     0.7272\nEpoch  495 | Train Loss:     0.7072 | Validation Loss:     0.7261\nEpoch  496 | Train Loss:     0.7071 | Validation Loss:     0.7263\nEpoch  497 | Train Loss:     0.7069 | Validation Loss:     0.7257\nEpoch  498 | Train Loss:     0.7070 | Validation Loss:     0.7257\nEpoch  499 | Train Loss:     0.7068 | Validation Loss:     0.7260\nEpoch  500 | Train Loss:     0.7066 | Validation Loss:     0.7261\nEpoch  501 | Train Loss:     0.7064 | Validation Loss:     0.7265\nEpoch  502 | Train Loss:     0.7064 | Validation Loss:     0.7253\nEpoch  503 | Train Loss:     0.7062 | Validation Loss:     0.7246\nEpoch  504 | Train Loss:     0.7060 | Validation Loss:     0.7269\nEpoch  505 | Train Loss:     0.7059 | Validation Loss:     0.7244\nEpoch  506 | Train Loss:     0.7056 | Validation Loss:     0.7256\nEpoch  507 | Train Loss:     0.7058 | Validation Loss:     0.7249\nEpoch  508 | Train Loss:     0.7058 | Validation Loss:     0.7256\nEpoch  509 | Train Loss:     0.7055 | Validation Loss:     0.7240\nEpoch  510 | Train Loss:     0.7054 | Validation Loss:     0.7242\nEpoch  511 | Train Loss:     0.7052 | Validation Loss:     0.7240\nEpoch  512 | Train Loss:     0.7051 | Validation Loss:     0.7237\nEpoch  513 | Train Loss:     0.7047 | Validation Loss:     0.7253\nEpoch  514 | Train Loss:     0.7048 | Validation Loss:     0.7242\nEpoch  515 | Train Loss:     0.7046 | Validation Loss:     0.7233\nEpoch  516 | Train Loss:     0.7044 | Validation Loss:     0.7237\nEpoch  517 | Train Loss:     0.7043 | Validation Loss:     0.7247\nEpoch  518 | Train Loss:     0.7040 | Validation Loss:     0.7227\nEpoch  519 | Train Loss:     0.7041 | Validation Loss:     0.7235\nEpoch  520 | Train Loss:     0.7039 | Validation Loss:     0.7230\nEpoch  521 | Train Loss:     0.7037 | Validation Loss:     0.7228\nEpoch  522 | Train Loss:     0.7037 | Validation Loss:     0.7227\nEpoch  523 | Train Loss:     0.7035 | Validation Loss:     0.7222\nEpoch  524 | Train Loss:     0.7032 | Validation Loss:     0.7240\nEpoch  525 | Train Loss:     0.7034 | Validation Loss:     0.7232\nEpoch  526 | Train Loss:     0.7031 | Validation Loss:     0.7229\nEpoch  527 | Train Loss:     0.7029 | Validation Loss:     0.7223\nEpoch  528 | Train Loss:     0.7028 | Validation Loss:     0.7216\nEpoch  529 | Train Loss:     0.7029 | Validation Loss:     0.7221\nEpoch  530 | Train Loss:     0.7026 | Validation Loss:     0.7214\nEpoch  531 | Train Loss:     0.7024 | Validation Loss:     0.7213\nEpoch  532 | Train Loss:     0.7024 | Validation Loss:     0.7207\nEpoch  533 | Train Loss:     0.7021 | Validation Loss:     0.7228\nEpoch  534 | Train Loss:     0.7019 | Validation Loss:     0.7211\nEpoch  535 | Train Loss:     0.7018 | Validation Loss:     0.7212\nEpoch  536 | Train Loss:     0.7018 | Validation Loss:     0.7210\nEpoch  537 | Train Loss:     0.7016 | Validation Loss:     0.7209\nEpoch  538 | Train Loss:     0.7014 | Validation Loss:     0.7197\nEpoch  539 | Train Loss:     0.7013 | Validation Loss:     0.7205\nEpoch  540 | Train Loss:     0.7010 | Validation Loss:     0.7208\nEpoch  541 | Train Loss:     0.7010 | Validation Loss:     0.7194\nEpoch  542 | Train Loss:     0.7009 | Validation Loss:     0.7197\nEpoch  543 | Train Loss:     0.7007 | Validation Loss:     0.7206\nEpoch  544 | Train Loss:     0.7005 | Validation Loss:     0.7189\nEpoch  545 | Train Loss:     0.7005 | Validation Loss:     0.7192\nEpoch  546 | Train Loss:     0.7003 | Validation Loss:     0.7193\nEpoch  547 | Train Loss:     0.7001 | Validation Loss:     0.7191\nEpoch  548 | Train Loss:     0.6999 | Validation Loss:     0.7194\nEpoch  549 | Train Loss:     0.6998 | Validation Loss:     0.7189\nEpoch  550 | Train Loss:     0.6998 | Validation Loss:     0.7192\nEpoch  551 | Train Loss:     0.6995 | Validation Loss:     0.7181\nEpoch  552 | Train Loss:     0.6992 | Validation Loss:     0.7178\nEpoch  553 | Train Loss:     0.6995 | Validation Loss:     0.7182\nEpoch  554 | Train Loss:     0.6990 | Validation Loss:     0.7175\nEpoch  555 | Train Loss:     0.6988 | Validation Loss:     0.7189\nEpoch  556 | Train Loss:     0.6988 | Validation Loss:     0.7180\nEpoch  557 | Train Loss:     0.6989 | Validation Loss:     0.7168\nEpoch  558 | Train Loss:     0.6985 | Validation Loss:     0.7170\nEpoch  559 | Train Loss:     0.6985 | Validation Loss:     0.7177\nEpoch  560 | Train Loss:     0.6982 | Validation Loss:     0.7180\nEpoch  561 | Train Loss:     0.6983 | Validation Loss:     0.7163\nEpoch  562 | Train Loss:     0.6979 | Validation Loss:     0.7176\nEpoch  563 | Train Loss:     0.6978 | Validation Loss:     0.7160\nEpoch  575 | Train Loss:     0.6955 | Validation Loss:     0.7166\nEpoch  576 | Train Loss:     0.6956 | Validation Loss:     0.7140\nEpoch  577 | Train Loss:     0.6953 | Validation Loss:     0.7139\nEpoch  578 | Train Loss:     0.6952 | Validation Loss:     0.7144\nEpoch  579 | Train Loss:     0.6951 | Validation Loss:     0.7138\nEpoch  580 | Train Loss:     0.6948 | Validation Loss:     0.7130\nEpoch  581 | Train Loss:     0.6945 | Validation Loss:     0.7133\nEpoch  582 | Train Loss:     0.6943 | Validation Loss:     0.7125\nEpoch  583 | Train Loss:     0.6942 | Validation Loss:     0.7129\nEpoch  584 | Train Loss:     0.6941 | Validation Loss:     0.7126\nEpoch  585 | Train Loss:     0.6939 | Validation Loss:     0.7136\nEpoch  586 | Train Loss:     0.6937 | Validation Loss:     0.7127\nEpoch  587 | Train Loss:     0.6937 | Validation Loss:     0.7123\nEpoch  588 | Train Loss:     0.6935 | Validation Loss:     0.7118\nEpoch  589 | Train Loss:     0.6930 | Validation Loss:     0.7114\nEpoch  590 | Train Loss:     0.6928 | Validation Loss:     0.7114\nEpoch  591 | Train Loss:     0.6928 | Validation Loss:     0.7121\nEpoch  592 | Train Loss:     0.6925 | Validation Loss:     0.7111\nEpoch  593 | Train Loss:     0.6921 | Validation Loss:     0.7101\nEpoch  594 | Train Loss:     0.6922 | Validation Loss:     0.7099\nEpoch  595 | Train Loss:     0.6919 | Validation Loss:     0.7098\nEpoch  596 | Train Loss:     0.6917 | Validation Loss:     0.7094\nEpoch  597 | Train Loss:     0.6914 | Validation Loss:     0.7096\nEpoch  598 | Train Loss:     0.6914 | Validation Loss:     0.7102\nEpoch  599 | Train Loss:     0.6911 | Validation Loss:     0.7106\nEpoch  600 | Train Loss:     0.6909 | Validation Loss:     0.7091\nEpoch  601 | Train Loss:     0.6907 | Validation Loss:     0.7086\nEpoch  602 | Train Loss:     0.6906 | Validation Loss:     0.7088\nEpoch  603 | Train Loss:     0.6904 | Validation Loss:     0.7081\nEpoch  604 | Train Loss:     0.6901 | Validation Loss:     0.7093\nEpoch  605 | Train Loss:     0.6897 | Validation Loss:     0.7079\nEpoch  606 | Train Loss:     0.6895 | Validation Loss:     0.7078\nEpoch  607 | Train Loss:     0.6894 | Validation Loss:     0.7085\nEpoch  608 | Train Loss:     0.6893 | Validation Loss:     0.7077\nEpoch  609 | Train Loss:     0.6891 | Validation Loss:     0.7066\nEpoch  610 | Train Loss:     0.6891 | Validation Loss:     0.7082\nEpoch  611 | Train Loss:     0.6886 | Validation Loss:     0.7072\nEpoch  612 | Train Loss:     0.6881 | Validation Loss:     0.7057\nEpoch  613 | Train Loss:     0.6881 | Validation Loss:     0.7058\nEpoch  614 | Train Loss:     0.6877 | Validation Loss:     0.7062\nEpoch  615 | Train Loss:     0.6875 | Validation Loss:     0.7061\nEpoch  616 | Train Loss:     0.6872 | Validation Loss:     0.7052\nEpoch  617 | Train Loss:     0.6871 | Validation Loss:     0.7059\nEpoch  618 | Train Loss:     0.6869 | Validation Loss:     0.7042\nEpoch  619 | Train Loss:     0.6866 | Validation Loss:     0.7049\nEpoch  620 | Train Loss:     0.6863 | Validation Loss:     0.7045\nEpoch  621 | Train Loss:     0.6862 | Validation Loss:     0.7035\nEpoch  622 | Train Loss:     0.6859 | Validation Loss:     0.7036\nEpoch  623 | Train Loss:     0.6857 | Validation Loss:     0.7030\nEpoch  624 | Train Loss:     0.6854 | Validation Loss:     0.7035\nEpoch  625 | Train Loss:     0.6851 | Validation Loss:     0.7027\nEpoch  626 | Train Loss:     0.6849 | Validation Loss:     0.7025\nEpoch  627 | Train Loss:     0.6845 | Validation Loss:     0.7025\nEpoch  628 | Train Loss:     0.6844 | Validation Loss:     0.7026\nEpoch  629 | Train Loss:     0.6841 | Validation Loss:     0.7021\nEpoch  630 | Train Loss:     0.6840 | Validation Loss:     0.7017\nEpoch  631 | Train Loss:     0.6839 | Validation Loss:     0.7010\nEpoch  632 | Train Loss:     0.6834 | Validation Loss:     0.7007\nEpoch  633 | Train Loss:     0.6831 | Validation Loss:     0.7001\nEpoch  634 | Train Loss:     0.6828 | Validation Loss:     0.6999\nEpoch  635 | Train Loss:     0.6828 | Validation Loss:     0.7003\nEpoch  636 | Train Loss:     0.6823 | Validation Loss:     0.6998\nEpoch  637 | Train Loss:     0.6820 | Validation Loss:     0.6993\nEpoch  638 | Train Loss:     0.6819 | Validation Loss:     0.6988\nEpoch  639 | Train Loss:     0.6816 | Validation Loss:     0.6984\nEpoch  640 | Train Loss:     0.6812 | Validation Loss:     0.6981\nEpoch  641 | Train Loss:     0.6810 | Validation Loss:     0.6988\nEpoch  642 | Train Loss:     0.6807 | Validation Loss:     0.6984\nEpoch  643 | Train Loss:     0.6805 | Validation Loss:     0.6974\nEpoch  644 | Train Loss:     0.6802 | Validation Loss:     0.6970\nEpoch  645 | Train Loss:     0.6797 | Validation Loss:     0.6981\nEpoch  646 | Train Loss:     0.6795 | Validation Loss:     0.6981\nEpoch  647 | Train Loss:     0.6793 | Validation Loss:     0.6962\nEpoch  648 | Train Loss:     0.6790 | Validation Loss:     0.6968\nEpoch  649 | Train Loss:     0.6787 | Validation Loss:     0.6956\nEpoch  650 | Train Loss:     0.6784 | Validation Loss:     0.6954\nEpoch  651 | Train Loss:     0.6782 | Validation Loss:     0.6952\nEpoch  652 | Train Loss:     0.6779 | Validation Loss:     0.6948\nEpoch  653 | Train Loss:     0.6775 | Validation Loss:     0.6948\nEpoch  654 | Train Loss:     0.6773 | Validation Loss:     0.6941\nEpoch  655 | Train Loss:     0.6767 | Validation Loss:     0.6949\nEpoch  656 | Train Loss:     0.6766 | Validation Loss:     0.6934\nEpoch  657 | Train Loss:     0.6767 | Validation Loss:     0.6927\nEpoch  658 | Train Loss:     0.6760 | Validation Loss:     0.6925\nEpoch  659 | Train Loss:     0.6758 | Validation Loss:     0.6928\nEpoch  660 | Train Loss:     0.6754 | Validation Loss:     0.6921\nEpoch  661 | Train Loss:     0.6753 | Validation Loss:     0.6923\nEpoch  662 | Train Loss:     0.6751 | Validation Loss:     0.6916\nEpoch  673 | Train Loss:     0.6713 | Validation Loss:     0.6875\nEpoch  674 | Train Loss:     0.6711 | Validation Loss:     0.6887\nEpoch  675 | Train Loss:     0.6707 | Validation Loss:     0.6871\nEpoch  676 | Train Loss:     0.6703 | Validation Loss:     0.6867\nEpoch  677 | Train Loss:     0.6701 | Validation Loss:     0.6866\nEpoch  678 | Train Loss:     0.6697 | Validation Loss:     0.6857\nEpoch  679 | Train Loss:     0.6694 | Validation Loss:     0.6860\nEpoch  680 | Train Loss:     0.6692 | Validation Loss:     0.6848\nEpoch  681 | Train Loss:     0.6688 | Validation Loss:     0.6852\nEpoch  682 | Train Loss:     0.6682 | Validation Loss:     0.6839\nEpoch  683 | Train Loss:     0.6679 | Validation Loss:     0.6850\nEpoch  684 | Train Loss:     0.6677 | Validation Loss:     0.6830\nEpoch  685 | Train Loss:     0.6675 | Validation Loss:     0.6828\nEpoch  686 | Train Loss:     0.6670 | Validation Loss:     0.6830\nEpoch  687 | Train Loss:     0.6667 | Validation Loss:     0.6829\nEpoch  688 | Train Loss:     0.6662 | Validation Loss:     0.6817\nEpoch  689 | Train Loss:     0.6660 | Validation Loss:     0.6812\nEpoch  690 | Train Loss:     0.6656 | Validation Loss:     0.6818\nEpoch  691 | Train Loss:     0.6655 | Validation Loss:     0.6814\nEpoch  692 | Train Loss:     0.6650 | Validation Loss:     0.6803\nEpoch  693 | Train Loss:     0.6647 | Validation Loss:     0.6800\nEpoch  694 | Train Loss:     0.6644 | Validation Loss:     0.6803\nEpoch  695 | Train Loss:     0.6639 | Validation Loss:     0.6791\nEpoch  696 | Train Loss:     0.6638 | Validation Loss:     0.6794\nEpoch  697 | Train Loss:     0.6632 | Validation Loss:     0.6805\nEpoch  698 | Train Loss:     0.6632 | Validation Loss:     0.6783\nEpoch  699 | Train Loss:     0.6629 | Validation Loss:     0.6788\nEpoch  700 | Train Loss:     0.6623 | Validation Loss:     0.6778\nEpoch  701 | Train Loss:     0.6619 | Validation Loss:     0.6785\nEpoch  702 | Train Loss:     0.6616 | Validation Loss:     0.6775\nEpoch  703 | Train Loss:     0.6613 | Validation Loss:     0.6760\nEpoch  704 | Train Loss:     0.6609 | Validation Loss:     0.6777\nEpoch  705 | Train Loss:     0.6606 | Validation Loss:     0.6757\nEpoch  706 | Train Loss:     0.6602 | Validation Loss:     0.6760\nEpoch  707 | Train Loss:     0.6598 | Validation Loss:     0.6753\nEpoch  708 | Train Loss:     0.6595 | Validation Loss:     0.6754\nEpoch  709 | Train Loss:     0.6593 | Validation Loss:     0.6737\nEpoch  710 | Train Loss:     0.6588 | Validation Loss:     0.6748\nEpoch  711 | Train Loss:     0.6586 | Validation Loss:     0.6732\nEpoch  712 | Train Loss:     0.6581 | Validation Loss:     0.6733\nEpoch  713 | Train Loss:     0.6578 | Validation Loss:     0.6726\nEpoch  714 | Train Loss:     0.6574 | Validation Loss:     0.6737\nEpoch  715 | Train Loss:     0.6571 | Validation Loss:     0.6718\nEpoch  716 | Train Loss:     0.6568 | Validation Loss:     0.6717\nEpoch  717 | Train Loss:     0.6564 | Validation Loss:     0.6729\nEpoch  718 | Train Loss:     0.6564 | Validation Loss:     0.6710\nEpoch  719 | Train Loss:     0.6558 | Validation Loss:     0.6706\nEpoch  720 | Train Loss:     0.6555 | Validation Loss:     0.6705\nEpoch  721 | Train Loss:     0.6551 | Validation Loss:     0.6693\nEpoch  722 | Train Loss:     0.6548 | Validation Loss:     0.6692\nEpoch  723 | Train Loss:     0.6543 | Validation Loss:     0.6684\nEpoch  724 | Train Loss:     0.6541 | Validation Loss:     0.6681\nEpoch  725 | Train Loss:     0.6537 | Validation Loss:     0.6681\nEpoch  726 | Train Loss:     0.6535 | Validation Loss:     0.6679\nEpoch  727 | Train Loss:     0.6531 | Validation Loss:     0.6672\nEpoch  728 | Train Loss:     0.6528 | Validation Loss:     0.6664\nEpoch  729 | Train Loss:     0.6524 | Validation Loss:     0.6661\nEpoch  730 | Train Loss:     0.6520 | Validation Loss:     0.6659\nEpoch  731 | Train Loss:     0.6516 | Validation Loss:     0.6667\nEpoch  732 | Train Loss:     0.6515 | Validation Loss:     0.6651\nEpoch  733 | Train Loss:     0.6511 | Validation Loss:     0.6660\nEpoch  734 | Train Loss:     0.6509 | Validation Loss:     0.6645\nEpoch  735 | Train Loss:     0.6504 | Validation Loss:     0.6647\nEpoch  736 | Train Loss:     0.6502 | Validation Loss:     0.6638\nEpoch  737 | Train Loss:     0.6498 | Validation Loss:     0.6637\nEpoch  738 | Train Loss:     0.6493 | Validation Loss:     0.6627\nEpoch  739 | Train Loss:     0.6491 | Validation Loss:     0.6631\nEpoch  740 | Train Loss:     0.6489 | Validation Loss:     0.6626\nEpoch  741 | Train Loss:     0.6485 | Validation Loss:     0.6623\nEpoch  742 | Train Loss:     0.6482 | Validation Loss:     0.6611\nEpoch  743 | Train Loss:     0.6480 | Validation Loss:     0.6624\nEpoch  744 | Train Loss:     0.6475 | Validation Loss:     0.6624\nEpoch  745 | Train Loss:     0.6473 | Validation Loss:     0.6606\nEpoch  746 | Train Loss:     0.6470 | Validation Loss:     0.6605\nEpoch  747 | Train Loss:     0.6467 | Validation Loss:     0.6596\nEpoch  748 | Train Loss:     0.6463 | Validation Loss:     0.6599\nEpoch  749 | Train Loss:     0.6460 | Validation Loss:     0.6591\nEpoch  750 | Train Loss:     0.6455 | Validation Loss:     0.6587\nEpoch  751 | Train Loss:     0.6455 | Validation Loss:     0.6589\nEpoch  752 | Train Loss:     0.6451 | Validation Loss:     0.6578\nEpoch  753 | Train Loss:     0.6448 | Validation Loss:     0.6577\nEpoch  754 | Train Loss:     0.6446 | Validation Loss:     0.6586\nEpoch  755 | Train Loss:     0.6443 | Validation Loss:     0.6577\nEpoch  756 | Train Loss:     0.6439 | Validation Loss:     0.6575\nEpoch  757 | Train Loss:     0.6438 | Validation Loss:     0.6565\nEpoch  758 | Train Loss:     0.6433 | Validation Loss:     0.6558\nEpoch  759 | Train Loss:     0.6431 | Validation Loss:     0.6555\nEpoch  760 | Train Loss:     0.6427 | Validation Loss:     0.6565\nEpoch  761 | Train Loss:     0.6425 | Validation Loss:     0.6550\nEpoch  762 | Train Loss:     0.6423 | Validation Loss:     0.6545\nEpoch  763 | Train Loss:     0.6421 | Validation Loss:     0.6551\nEpoch  764 | Train Loss:     0.6416 | Validation Loss:     0.6537\nEpoch  765 | Train Loss:     0.6414 | Validation Loss:     0.6541\nEpoch  766 | Train Loss:     0.6411 | Validation Loss:     0.6539\nEpoch  767 | Train Loss:     0.6409 | Validation Loss:     0.6533\nEpoch  768 | Train Loss:     0.6407 | Validation Loss:     0.6535\nEpoch  769 | Train Loss:     0.6404 | Validation Loss:     0.6524\nEpoch  770 | Train Loss:     0.6402 | Validation Loss:     0.6531\nEpoch  771 | Train Loss:     0.6399 | Validation Loss:     0.6526\nEpoch  772 | Train Loss:     0.6395 | Validation Loss:     0.6519\nEpoch  773 | Train Loss:     0.6395 | Validation Loss:     0.6511\nEpoch  774 | Train Loss:     0.6392 | Validation Loss:     0.6508\nEpoch  775 | Train Loss:     0.6388 | Validation Loss:     0.6509\nEpoch  776 | Train Loss:     0.6385 | Validation Loss:     0.6503\nEpoch  777 | Train Loss:     0.6384 | Validation Loss:     0.6497\nEpoch  778 | Train Loss:     0.6381 | Validation Loss:     0.6498\nEpoch  779 | Train Loss:     0.6379 | Validation Loss:     0.6495\nEpoch  780 | Train Loss:     0.6376 | Validation Loss:     0.6492\nEpoch  781 | Train Loss:     0.6374 | Validation Loss:     0.6493\nEpoch  782 | Train Loss:     0.6371 | Validation Loss:     0.6483\nEpoch  783 | Train Loss:     0.6369 | Validation Loss:     0.6480\nEpoch  784 | Train Loss:     0.6368 | Validation Loss:     0.6487\nEpoch  785 | Train Loss:     0.6364 | Validation Loss:     0.6477\nEpoch  786 | Train Loss:     0.6363 | Validation Loss:     0.6476\nEpoch  787 | Train Loss:     0.6360 | Validation Loss:     0.6474\nEpoch  788 | Train Loss:     0.6357 | Validation Loss:     0.6481\nEpoch  789 | Train Loss:     0.6356 | Validation Loss:     0.6465\nEpoch  790 | Train Loss:     0.6353 | Validation Loss:     0.6470\nEpoch  791 | Train Loss:     0.6351 | Validation Loss:     0.6466\nEpoch  792 | Train Loss:     0.6348 | Validation Loss:     0.6461\nEpoch  793 | Train Loss:     0.6346 | Validation Loss:     0.6461\nEpoch  794 | Train Loss:     0.6345 | Validation Loss:     0.6458\nEpoch  795 | Train Loss:     0.6343 | Validation Loss:     0.6451\nEpoch  796 | Train Loss:     0.6340 | Validation Loss:     0.6450\nEpoch  797 | Train Loss:     0.6339 | Validation Loss:     0.6450\nEpoch  798 | Train Loss:     0.6336 | Validation Loss:     0.6453\nEpoch  799 | Train Loss:     0.6334 | Validation Loss:     0.6447\nEpoch  800 | Train Loss:     0.6332 | Validation Loss:     0.6449\nEpoch  801 | Train Loss:     0.6331 | Validation Loss:     0.6444\nEpoch  802 | Train Loss:     0.6330 | Validation Loss:     0.6436\nEpoch  803 | Train Loss:     0.6327 | Validation Loss:     0.6434\nEpoch  804 | Train Loss:     0.6324 | Validation Loss:     0.6430\nEpoch  805 | Train Loss:     0.6323 | Validation Loss:     0.6429\nEpoch  806 | Train Loss:     0.6323 | Validation Loss:     0.6426\nEpoch  807 | Train Loss:     0.6319 | Validation Loss:     0.6422\nEpoch  808 | Train Loss:     0.6317 | Validation Loss:     0.6424\nEpoch  809 | Train Loss:     0.6316 | Validation Loss:     0.6422\nEpoch  810 | Train Loss:     0.6314 | Validation Loss:     0.6417\nEpoch  811 | Train Loss:     0.6313 | Validation Loss:     0.6419\nEpoch  812 | Train Loss:     0.6310 | Validation Loss:     0.6417\nEpoch  813 | Train Loss:     0.6309 | Validation Loss:     0.6412\nEpoch  814 | Train Loss:     0.6307 | Validation Loss:     0.6413\nEpoch  815 | Train Loss:     0.6306 | Validation Loss:     0.6408\nEpoch  816 | Train Loss:     0.6304 | Validation Loss:     0.6404\nEpoch  817 | Train Loss:     0.6302 | Validation Loss:     0.6403\nEpoch  818 | Train Loss:     0.6301 | Validation Loss:     0.6401\nEpoch  819 | Train Loss:     0.6299 | Validation Loss:     0.6402\nEpoch  820 | Train Loss:     0.6297 | Validation Loss:     0.6397\nEpoch  821 | Train Loss:     0.6297 | Validation Loss:     0.6396\nEpoch  822 | Train Loss:     0.6296 | Validation Loss:     0.6399\nEpoch  823 | Train Loss:     0.6293 | Validation Loss:     0.6390\nEpoch  824 | Train Loss:     0.6292 | Validation Loss:     0.6389\nEpoch  825 | Train Loss:     0.6290 | Validation Loss:     0.6390\nEpoch  826 | Train Loss:     0.6289 | Validation Loss:     0.6392\nEpoch  827 | Train Loss:     0.6287 | Validation Loss:     0.6383\nEpoch  828 | Train Loss:     0.6285 | Validation Loss:     0.6383\nEpoch  829 | Train Loss:     0.6284 | Validation Loss:     0.6389\nEpoch  830 | Train Loss:     0.6283 | Validation Loss:     0.6396\nEpoch  831 | Train Loss:     0.6281 | Validation Loss:     0.6377\nEpoch  832 | Train Loss:     0.6281 | Validation Loss:     0.6382\nEpoch  833 | Train Loss:     0.6279 | Validation Loss:     0.6381\nEpoch  834 | Train Loss:     0.6278 | Validation Loss:     0.6380\nEpoch  835 | Train Loss:     0.6276 | Validation Loss:     0.6378\nEpoch  836 | Train Loss:     0.6275 | Validation Loss:     0.6373\nEpoch  837 | Train Loss:     0.6273 | Validation Loss:     0.6366\nEpoch  838 | Train Loss:     0.6273 | Validation Loss:     0.6370\nEpoch  839 | Train Loss:     0.6273 | Validation Loss:     0.6376\nEpoch  840 | Train Loss:     0.6270 | Validation Loss:     0.6364\nEpoch  841 | Train Loss:     0.6270 | Validation Loss:     0.6360\nEpoch  842 | Train Loss:     0.6268 | Validation Loss:     0.6363\nEpoch  843 | Train Loss:     0.6266 | Validation Loss:     0.6361\nEpoch  844 | Train Loss:     0.6265 | Validation Loss:     0.6359\nEpoch  845 | Train Loss:     0.6264 | Validation Loss:     0.6358\nEpoch  846 | Train Loss:     0.6263 | Validation Loss:     0.6357\nEpoch  847 | Train Loss:     0.6263 | Validation Loss:     0.6357\nEpoch  848 | Train Loss:     0.6261 | Validation Loss:     0.6354\nEpoch  849 | Train Loss:     0.6261 | Validation Loss:     0.6351\nEpoch  850 | Train Loss:     0.6259 | Validation Loss:     0.6355\nEpoch  851 | Train Loss:     0.6258 | Validation Loss:     0.6346\nEpoch  852 | Train Loss:     0.6257 | Validation Loss:     0.6354\nEpoch  853 | Train Loss:     0.6258 | Validation Loss:     0.6347\nEpoch  854 | Train Loss:     0.6257 | Validation Loss:     0.6343\nEpoch  855 | Train Loss:     0.6254 | Validation Loss:     0.6342\nEpoch  856 | Train Loss:     0.6253 | Validation Loss:     0.6343\nEpoch  857 | Train Loss:     0.6252 | Validation Loss:     0.6343\nEpoch  858 | Train Loss:     0.6251 | Validation Loss:     0.6342\nEpoch  859 | Train Loss:     0.6250 | Validation Loss:     0.6345\nEpoch  860 | Train Loss:     0.6248 | Validation Loss:     0.6333\nEpoch  861 | Train Loss:     0.6248 | Validation Loss:     0.6335\nEpoch  862 | Train Loss:     0.6246 | Validation Loss:     0.6337\nEpoch  863 | Train Loss:     0.6245 | Validation Loss:     0.6331\nEpoch  864 | Train Loss:     0.6246 | Validation Loss:     0.6338\nEpoch  865 | Train Loss:     0.6245 | Validation Loss:     0.6333\nEpoch  866 | Train Loss:     0.6243 | Validation Loss:     0.6329\nEpoch  867 | Train Loss:     0.6242 | Validation Loss:     0.6329\nEpoch  868 | Train Loss:     0.6241 | Validation Loss:     0.6334\nEpoch  869 | Train Loss:     0.6240 | Validation Loss:     0.6330\nEpoch  870 | Train Loss:     0.6239 | Validation Loss:     0.6321\nEpoch  871 | Train Loss:     0.6239 | Validation Loss:     0.6322\nEpoch  872 | Train Loss:     0.6237 | Validation Loss:     0.6324\nEpoch  873 | Train Loss:     0.6237 | Validation Loss:     0.6320\nEpoch  874 | Train Loss:     0.6236 | Validation Loss:     0.6320\nEpoch  875 | Train Loss:     0.6236 | Validation Loss:     0.6318\nEpoch  876 | Train Loss:     0.6234 | Validation Loss:     0.6318\nEpoch  877 | Train Loss:     0.6233 | Validation Loss:     0.6317\nEpoch  878 | Train Loss:     0.6233 | Validation Loss:     0.6322\nEpoch  879 | Train Loss:     0.6232 | Validation Loss:     0.6318\nEpoch  880 | Train Loss:     0.6231 | Validation Loss:     0.6318\nEpoch  881 | Train Loss:     0.6229 | Validation Loss:     0.6321\nEpoch  882 | Train Loss:     0.6229 | Validation Loss:     0.6314\nEpoch  883 | Train Loss:     0.6228 | Validation Loss:     0.6311\nEpoch  884 | Train Loss:     0.6228 | Validation Loss:     0.6311\nEpoch  885 | Train Loss:     0.6226 | Validation Loss:     0.6323\nEpoch  886 | Train Loss:     0.6225 | Validation Loss:     0.6317\nEpoch  887 | Train Loss:     0.6225 | Validation Loss:     0.6310\nEpoch  888 | Train Loss:     0.6224 | Validation Loss:     0.6307\nEpoch  889 | Train Loss:     0.6222 | Validation Loss:     0.6308\nEpoch  890 | Train Loss:     0.6222 | Validation Loss:     0.6304\nEpoch  891 | Train Loss:     0.6222 | Validation Loss:     0.6303\nEpoch  892 | Train Loss:     0.6221 | Validation Loss:     0.6308\nEpoch  893 | Train Loss:     0.6220 | Validation Loss:     0.6300\nEpoch  894 | Train Loss:     0.6219 | Validation Loss:     0.6296\nEpoch  895 | Train Loss:     0.6217 | Validation Loss:     0.6298\nEpoch  896 | Train Loss:     0.6217 | Validation Loss:     0.6296\nEpoch  897 | Train Loss:     0.6217 | Validation Loss:     0.6295\nEpoch  898 | Train Loss:     0.6216 | Validation Loss:     0.6300\nEpoch  899 | Train Loss:     0.6216 | Validation Loss:     0.6295\nEpoch  900 | Train Loss:     0.6214 | Validation Loss:     0.6296\nEpoch  901 | Train Loss:     0.6213 | Validation Loss:     0.6296\nEpoch  902 | Train Loss:     0.6212 | Validation Loss:     0.6309\nEpoch  903 | Train Loss:     0.6212 | Validation Loss:     0.6288\nEpoch  904 | Train Loss:     0.6211 | Validation Loss:     0.6290\nEpoch  905 | Train Loss:     0.6211 | Validation Loss:     0.6287\nEpoch  906 | Train Loss:     0.6209 | Validation Loss:     0.6290\nEpoch  907 | Train Loss:     0.6208 | Validation Loss:     0.6284\nEpoch  908 | Train Loss:     0.6207 | Validation Loss:     0.6287\nEpoch  909 | Train Loss:     0.6206 | Validation Loss:     0.6285\nEpoch  910 | Train Loss:     0.6205 | Validation Loss:     0.6281\nEpoch  911 | Train Loss:     0.6206 | Validation Loss:     0.6283\nEpoch  912 | Train Loss:     0.6204 | Validation Loss:     0.6283\nEpoch  913 | Train Loss:     0.6204 | Validation Loss:     0.6286\nEpoch  914 | Train Loss:     0.6203 | Validation Loss:     0.6279\nEpoch  915 | Train Loss:     0.6201 | Validation Loss:     0.6282\nEpoch  916 | Train Loss:     0.6201 | Validation Loss:     0.6279\nEpoch  917 | Train Loss:     0.6200 | Validation Loss:     0.6275\nEpoch  918 | Train Loss:     0.6200 | Validation Loss:     0.6280\nEpoch  919 | Train Loss:     0.6197 | Validation Loss:     0.6271\nEpoch  920 | Train Loss:     0.6197 | Validation Loss:     0.6277\nEpoch  921 | Train Loss:     0.6197 | Validation Loss:     0.6274\nEpoch  922 | Train Loss:     0.6196 | Validation Loss:     0.6279\nEpoch  923 | Train Loss:     0.6195 | Validation Loss:     0.6268\nEpoch  924 | Train Loss:     0.6195 | Validation Loss:     0.6268\nEpoch  925 | Train Loss:     0.6193 | Validation Loss:     0.6273\nEpoch  926 | Train Loss:     0.6193 | Validation Loss:     0.6268\nEpoch  927 | Train Loss:     0.6191 | Validation Loss:     0.6270\nEpoch  928 | Train Loss:     0.6191 | Validation Loss:     0.6266\nEpoch  929 | Train Loss:     0.6190 | Validation Loss:     0.6264\nEpoch  930 | Train Loss:     0.6189 | Validation Loss:     0.6267\nEpoch  931 | Train Loss:     0.6190 | Validation Loss:     0.6261\nEpoch  932 | Train Loss:     0.6187 | Validation Loss:     0.6270\nEpoch  933 | Train Loss:     0.6187 | Validation Loss:     0.6258\nEpoch  934 | Train Loss:     0.6186 | Validation Loss:     0.6259\nEpoch  935 | Train Loss:     0.6187 | Validation Loss:     0.6259\nEpoch  936 | Train Loss:     0.6184 | Validation Loss:     0.6258\nEpoch  937 | Train Loss:     0.6184 | Validation Loss:     0.6255\nEpoch  938 | Train Loss:     0.6182 | Validation Loss:     0.6255\nEpoch  939 | Train Loss:     0.6183 | Validation Loss:     0.6255\nEpoch  940 | Train Loss:     0.6181 | Validation Loss:     0.6254\nEpoch  941 | Train Loss:     0.6180 | Validation Loss:     0.6253\nEpoch  942 | Train Loss:     0.6179 | Validation Loss:     0.6254\nEpoch  943 | Train Loss:     0.6179 | Validation Loss:     0.6251\nEpoch  944 | Train Loss:     0.6178 | Validation Loss:     0.6253\nEpoch  945 | Train Loss:     0.6177 | Validation Loss:     0.6251\nEpoch  946 | Train Loss:     0.6177 | Validation Loss:     0.6252\nEpoch  947 | Train Loss:     0.6175 | Validation Loss:     0.6247\nEpoch  948 | Train Loss:     0.6174 | Validation Loss:     0.6248\nEpoch  949 | Train Loss:     0.6174 | Validation Loss:     0.6252\nEpoch  950 | Train Loss:     0.6173 | Validation Loss:     0.6243\nEpoch  951 | Train Loss:     0.6172 | Validation Loss:     0.6247\nEpoch  952 | Train Loss:     0.6172 | Validation Loss:     0.6241\nEpoch  953 | Train Loss:     0.6171 | Validation Loss:     0.6242\nEpoch  954 | Train Loss:     0.6170 | Validation Loss:     0.6240\nEpoch  955 | Train Loss:     0.6169 | Validation Loss:     0.6241\nEpoch  956 | Train Loss:     0.6167 | Validation Loss:     0.6237\nEpoch  957 | Train Loss:     0.6167 | Validation Loss:     0.6234\nEpoch  958 | Train Loss:     0.6167 | Validation Loss:     0.6234\nEpoch  959 | Train Loss:     0.6165 | Validation Loss:     0.6239\nEpoch  960 | Train Loss:     0.6164 | Validation Loss:     0.6231\nEpoch  961 | Train Loss:     0.6163 | Validation Loss:     0.6231\nEpoch  962 | Train Loss:     0.6162 | Validation Loss:     0.6229\nEpoch  963 | Train Loss:     0.6161 | Validation Loss:     0.6227\nEpoch  964 | Train Loss:     0.6158 | Validation Loss:     0.6225\nEpoch  965 | Train Loss:     0.6157 | Validation Loss:     0.6229\nEpoch  966 | Train Loss:     0.6155 | Validation Loss:     0.6221\nEpoch  967 | Train Loss:     0.6154 | Validation Loss:     0.6219\nEpoch  968 | Train Loss:     0.6152 | Validation Loss:     0.6218\nEpoch  969 | Train Loss:     0.6150 | Validation Loss:     0.6216\nEpoch  970 | Train Loss:     0.6149 | Validation Loss:     0.6214\nEpoch  971 | Train Loss:     0.6147 | Validation Loss:     0.6209\nEpoch  972 | Train Loss:     0.6147 | Validation Loss:     0.6209\nEpoch  973 | Train Loss:     0.6145 | Validation Loss:     0.6210\nEpoch  974 | Train Loss:     0.6144 | Validation Loss:     0.6208\nEpoch  975 | Train Loss:     0.6142 | Validation Loss:     0.6210\nEpoch  976 | Train Loss:     0.6141 | Validation Loss:     0.6201\nEpoch  977 | Train Loss:     0.6139 | Validation Loss:     0.6199\nEpoch  978 | Train Loss:     0.6138 | Validation Loss:     0.6199\nEpoch  979 | Train Loss:     0.6137 | Validation Loss:     0.6201\nEpoch  980 | Train Loss:     0.6136 | Validation Loss:     0.6200\nEpoch  981 | Train Loss:     0.6134 | Validation Loss:     0.6193\nEpoch  982 | Train Loss:     0.6134 | Validation Loss:     0.6191\nEpoch  983 | Train Loss:     0.6132 | Validation Loss:     0.6192\nEpoch  984 | Train Loss:     0.6132 | Validation Loss:     0.6192\nEpoch  985 | Train Loss:     0.6130 | Validation Loss:     0.6190\nEpoch  996 | Train Loss:     0.6117 | Validation Loss:     0.6175\nEpoch  997 | Train Loss:     0.6116 | Validation Loss:     0.6172\nEpoch  998 | Train Loss:     0.6114 | Validation Loss:     0.6177\nEpoch  999 | Train Loss:     0.6112 | Validation Loss:     0.6166\nEpoch 1000 | Train Loss:     0.6111 | Validation Loss:     0.6170\nEpoch 1001 | Train Loss:     0.6110 | Validation Loss:     0.6163\nEpoch 1002 | Train Loss:     0.6109 | Validation Loss:     0.6163\nEpoch 1003 | Train Loss:     0.6107 | Validation Loss:     0.6161\nEpoch 1004 | Train Loss:     0.6106 | Validation Loss:     0.6160\nEpoch 1005 | Train Loss:     0.6105 | Validation Loss:     0.6156\nEpoch 1006 | Train Loss:     0.6104 | Validation Loss:     0.6160\nEpoch 1007 | Train Loss:     0.6103 | Validation Loss:     0.6154\nEpoch 1008 | Train Loss:     0.6101 | Validation Loss:     0.6153\nEpoch 1009 | Train Loss:     0.6100 | Validation Loss:     0.6150\nEpoch 1010 | Train Loss:     0.6099 | Validation Loss:     0.6149\nEpoch 1011 | Train Loss:     0.6097 | Validation Loss:     0.6152\nEpoch 1012 | Train Loss:     0.6096 | Validation Loss:     0.6148\nEpoch 1013 | Train Loss:     0.6095 | Validation Loss:     0.6151\nEpoch 1014 | Train Loss:     0.6094 | Validation Loss:     0.6146\nEpoch 1015 | Train Loss:     0.6093 | Validation Loss:     0.6146\nEpoch 1016 | Train Loss:     0.6091 | Validation Loss:     0.6140\nEpoch 1017 | Train Loss:     0.6090 | Validation Loss:     0.6140\nEpoch 1018 | Train Loss:     0.6089 | Validation Loss:     0.6140\nEpoch 1019 | Train Loss:     0.6088 | Validation Loss:     0.6142\nEpoch 1020 | Train Loss:     0.6087 | Validation Loss:     0.6135\nEpoch 1021 | Train Loss:     0.6085 | Validation Loss:     0.6133\nEpoch 1022 | Train Loss:     0.6084 | Validation Loss:     0.6134\nEpoch 1023 | Train Loss:     0.6083 | Validation Loss:     0.6133\nEpoch 1024 | Train Loss:     0.6082 | Validation Loss:     0.6131\nEpoch 1025 | Train Loss:     0.6081 | Validation Loss:     0.6127\nEpoch 1026 | Train Loss:     0.6080 | Validation Loss:     0.6127\nEpoch 1027 | Train Loss:     0.6077 | Validation Loss:     0.6126\nEpoch 1028 | Train Loss:     0.6076 | Validation Loss:     0.6122\nEpoch 1029 | Train Loss:     0.6076 | Validation Loss:     0.6121\nEpoch 1030 | Train Loss:     0.6074 | Validation Loss:     0.6121\nEpoch 1031 | Train Loss:     0.6073 | Validation Loss:     0.6122\nEpoch 1032 | Train Loss:     0.6072 | Validation Loss:     0.6116\nEpoch 1033 | Train Loss:     0.6070 | Validation Loss:     0.6117\nEpoch 1034 | Train Loss:     0.6070 | Validation Loss:     0.6115\nEpoch 1035 | Train Loss:     0.6068 | Validation Loss:     0.6112\nEpoch 1036 | Train Loss:     0.6067 | Validation Loss:     0.6110\nEpoch 1037 | Train Loss:     0.6065 | Validation Loss:     0.6112\nEpoch 1038 | Train Loss:     0.6064 | Validation Loss:     0.6110\nEpoch 1039 | Train Loss:     0.6063 | Validation Loss:     0.6110\nEpoch 1040 | Train Loss:     0.6062 | Validation Loss:     0.6107\nEpoch 1041 | Train Loss:     0.6061 | Validation Loss:     0.6108\nEpoch 1042 | Train Loss:     0.6060 | Validation Loss:     0.6103\nEpoch 1043 | Train Loss:     0.6059 | Validation Loss:     0.6102\nEpoch 1044 | Train Loss:     0.6058 | Validation Loss:     0.6100\nEpoch 1045 | Train Loss:     0.6056 | Validation Loss:     0.6098\nEpoch 1046 | Train Loss:     0.6055 | Validation Loss:     0.6097\nEpoch 1047 | Train Loss:     0.6055 | Validation Loss:     0.6096\nEpoch 1048 | Train Loss:     0.6053 | Validation Loss:     0.6094\nEpoch 1049 | Train Loss:     0.6053 | Validation Loss:     0.6096\nEpoch 1050 | Train Loss:     0.6051 | Validation Loss:     0.6092\nEpoch 1051 | Train Loss:     0.6050 | Validation Loss:     0.6092\nEpoch 1052 | Train Loss:     0.6049 | Validation Loss:     0.6089\nEpoch 1053 | Train Loss:     0.6048 | Validation Loss:     0.6089\nEpoch 1054 | Train Loss:     0.6047 | Validation Loss:     0.6088\nEpoch 1055 | Train Loss:     0.6046 | Validation Loss:     0.6086\nEpoch 1056 | Train Loss:     0.6045 | Validation Loss:     0.6086\nEpoch 1057 | Train Loss:     0.6044 | Validation Loss:     0.6084\nEpoch 1058 | Train Loss:     0.6043 | Validation Loss:     0.6083\nEpoch 1059 | Train Loss:     0.6042 | Validation Loss:     0.6083\nEpoch 1060 | Train Loss:     0.6041 | Validation Loss:     0.6083\nEpoch 1061 | Train Loss:     0.6040 | Validation Loss:     0.6078\nEpoch 1062 | Train Loss:     0.6039 | Validation Loss:     0.6077\nEpoch 1063 | Train Loss:     0.6037 | Validation Loss:     0.6076\nEpoch 1064 | Train Loss:     0.6036 | Validation Loss:     0.6074\nEpoch 1065 | Train Loss:     0.6035 | Validation Loss:     0.6075\nEpoch 1066 | Train Loss:     0.6035 | Validation Loss:     0.6072\nEpoch 1067 | Train Loss:     0.6033 | Validation Loss:     0.6071\nEpoch 1068 | Train Loss:     0.6032 | Validation Loss:     0.6070\nEpoch 1069 | Train Loss:     0.6031 | Validation Loss:     0.6069\nEpoch 1070 | Train Loss:     0.6030 | Validation Loss:     0.6068\nEpoch 1071 | Train Loss:     0.6029 | Validation Loss:     0.6065\nEpoch 1072 | Train Loss:     0.6028 | Validation Loss:     0.6066\nEpoch 1073 | Train Loss:     0.6027 | Validation Loss:     0.6064\nEpoch 1074 | Train Loss:     0.6026 | Validation Loss:     0.6063\nEpoch 1075 | Train Loss:     0.6025 | Validation Loss:     0.6063\nEpoch 1076 | Train Loss:     0.6024 | Validation Loss:     0.6062\nEpoch 1077 | Train Loss:     0.6023 | Validation Loss:     0.6060\nEpoch 1078 | Train Loss:     0.6022 | Validation Loss:     0.6060\nEpoch 1079 | Train Loss:     0.6021 | Validation Loss:     0.6057\nEpoch 1080 | Train Loss:     0.6020 | Validation Loss:     0.6057\nEpoch 1081 | Train Loss:     0.6019 | Validation Loss:     0.6055\nEpoch 1082 | Train Loss:     0.6018 | Validation Loss:     0.6054\nEpoch 1083 | Train Loss:     0.6017 | Validation Loss:     0.6053\nEpoch 1084 | Train Loss:     0.6016 | Validation Loss:     0.6052\nEpoch 1085 | Train Loss:     0.6015 | Validation Loss:     0.6052\nEpoch 1086 | Train Loss:     0.6014 | Validation Loss:     0.6051\nEpoch 1087 | Train Loss:     0.6013 | Validation Loss:     0.6049\nEpoch 1088 | Train Loss:     0.6012 | Validation Loss:     0.6048\nEpoch 1089 | Train Loss:     0.6011 | Validation Loss:     0.6047\nEpoch 1090 | Train Loss:     0.6010 | Validation Loss:     0.6045\nEpoch 1091 | Train Loss:     0.6010 | Validation Loss:     0.6046\nEpoch 1092 | Train Loss:     0.6008 | Validation Loss:     0.6045\nEpoch 1093 | Train Loss:     0.6007 | Validation Loss:     0.6042\nEpoch 1094 | Train Loss:     0.6006 | Validation Loss:     0.6042\nEpoch 1095 | Train Loss:     0.6005 | Validation Loss:     0.6041\nEpoch 1096 | Train Loss:     0.6004 | Validation Loss:     0.6039\nEpoch 1097 | Train Loss:     0.6003 | Validation Loss:     0.6038\nEpoch 1098 | Train Loss:     0.6002 | Validation Loss:     0.6037\nEpoch 1099 | Train Loss:     0.6001 | Validation Loss:     0.6036\nEpoch 1100 | Train Loss:     0.6000 | Validation Loss:     0.6034\nEpoch 1101 | Train Loss:     0.5999 | Validation Loss:     0.6033\nEpoch 1102 | Train Loss:     0.5998 | Validation Loss:     0.6033\nEpoch 1103 | Train Loss:     0.5997 | Validation Loss:     0.6031\nEpoch 1104 | Train Loss:     0.5996 | Validation Loss:     0.6031\nEpoch 1105 | Train Loss:     0.5995 | Validation Loss:     0.6030\nEpoch 1106 | Train Loss:     0.5993 | Validation Loss:     0.6028\nEpoch 1107 | Train Loss:     0.5993 | Validation Loss:     0.6026\nEpoch 1108 | Train Loss:     0.5991 | Validation Loss:     0.6026\nEpoch 1109 | Train Loss:     0.5990 | Validation Loss:     0.6024\nEpoch 1110 | Train Loss:     0.5989 | Validation Loss:     0.6022\nEpoch 1111 | Train Loss:     0.5988 | Validation Loss:     0.6022\nEpoch 1112 | Train Loss:     0.5987 | Validation Loss:     0.6020\nEpoch 1113 | Train Loss:     0.5986 | Validation Loss:     0.6021\nEpoch 1114 | Train Loss:     0.5984 | Validation Loss:     0.6018\nEpoch 1115 | Train Loss:     0.5984 | Validation Loss:     0.6016\nEpoch 1116 | Train Loss:     0.5982 | Validation Loss:     0.6015\nEpoch 1117 | Train Loss:     0.5981 | Validation Loss:     0.6015\nEpoch 1118 | Train Loss:     0.5980 | Validation Loss:     0.6013\nEpoch 1119 | Train Loss:     0.5979 | Validation Loss:     0.6013\nEpoch 1120 | Train Loss:     0.5978 | Validation Loss:     0.6012\nEpoch 1121 | Train Loss:     0.5976 | Validation Loss:     0.6011\nEpoch 1122 | Train Loss:     0.5975 | Validation Loss:     0.6009\nEpoch 1123 | Train Loss:     0.5974 | Validation Loss:     0.6008\nEpoch 1124 | Train Loss:     0.5973 | Validation Loss:     0.6006\nEpoch 1125 | Train Loss:     0.5972 | Validation Loss:     0.6005\nEpoch 1126 | Train Loss:     0.5970 | Validation Loss:     0.6003\nEpoch 1127 | Train Loss:     0.5969 | Validation Loss:     0.6003\nEpoch 1128 | Train Loss:     0.5968 | Validation Loss:     0.6002\nEpoch 1129 | Train Loss:     0.5967 | Validation Loss:     0.6000\nEpoch 1130 | Train Loss:     0.5965 | Validation Loss:     0.5998\nEpoch 1131 | Train Loss:     0.5964 | Validation Loss:     0.5996\nEpoch 1132 | Train Loss:     0.5962 | Validation Loss:     0.5997\nEpoch 1133 | Train Loss:     0.5961 | Validation Loss:     0.5994\nEpoch 1134 | Train Loss:     0.5960 | Validation Loss:     0.5993\nEpoch 1135 | Train Loss:     0.5958 | Validation Loss:     0.5992\nEpoch 1136 | Train Loss:     0.5957 | Validation Loss:     0.5989\nEpoch 1137 | Train Loss:     0.5955 | Validation Loss:     0.5988\nEpoch 1138 | Train Loss:     0.5954 | Validation Loss:     0.5986\nEpoch 1139 | Train Loss:     0.5953 | Validation Loss:     0.5984\nEpoch 1140 | Train Loss:     0.5951 | Validation Loss:     0.5984\nEpoch 1141 | Train Loss:     0.5950 | Validation Loss:     0.5982\nEpoch 1142 | Train Loss:     0.5948 | Validation Loss:     0.5981\nEpoch 1143 | Train Loss:     0.5947 | Validation Loss:     0.5979\nEpoch 1144 | Train Loss:     0.5945 | Validation Loss:     0.5977\nEpoch 1145 | Train Loss:     0.5943 | Validation Loss:     0.5976\nEpoch 1146 | Train Loss:     0.5942 | Validation Loss:     0.5973\nEpoch 1147 | Train Loss:     0.5940 | Validation Loss:     0.5973\nEpoch 1148 | Train Loss:     0.5939 | Validation Loss:     0.5971\nEpoch 1149 | Train Loss:     0.5938 | Validation Loss:     0.5970\nEpoch 1150 | Train Loss:     0.5936 | Validation Loss:     0.5967\nEpoch 1151 | Train Loss:     0.5934 | Validation Loss:     0.5966\nEpoch 1152 | Train Loss:     0.5933 | Validation Loss:     0.5965\nEpoch 1153 | Train Loss:     0.5931 | Validation Loss:     0.5962\nEpoch 1154 | Train Loss:     0.5929 | Validation Loss:     0.5959\nEpoch 1155 | Train Loss:     0.5927 | Validation Loss:     0.5959\nEpoch 1156 | Train Loss:     0.5926 | Validation Loss:     0.5958\nEpoch 1157 | Train Loss:     0.5924 | Validation Loss:     0.5955\nEpoch 1158 | Train Loss:     0.5922 | Validation Loss:     0.5954\nEpoch 1159 | Train Loss:     0.5920 | Validation Loss:     0.5953\nEpoch 1160 | Train Loss:     0.5918 | Validation Loss:     0.5950\nEpoch 1161 | Train Loss:     0.5917 | Validation Loss:     0.5947\nEpoch 1162 | Train Loss:     0.5915 | Validation Loss:     0.5946\nEpoch 1163 | Train Loss:     0.5913 | Validation Loss:     0.5943\nEpoch 1164 | Train Loss:     0.5911 | Validation Loss:     0.5941\nEpoch 1165 | Train Loss:     0.5909 | Validation Loss:     0.5940\nEpoch 1166 | Train Loss:     0.5907 | Validation Loss:     0.5938\nEpoch 1167 | Train Loss:     0.5906 | Validation Loss:     0.5936\nEpoch 1168 | Train Loss:     0.5903 | Validation Loss:     0.5935\nEpoch 1169 | Train Loss:     0.5901 | Validation Loss:     0.5932\nEpoch 1170 | Train Loss:     0.5899 | Validation Loss:     0.5929\nEpoch 1171 | Train Loss:     0.5897 | Validation Loss:     0.5927\nEpoch 1172 | Train Loss:     0.5895 | Validation Loss:     0.5925\nEpoch 1173 | Train Loss:     0.5893 | Validation Loss:     0.5923\nEpoch 1174 | Train Loss:     0.5890 | Validation Loss:     0.5921\nEpoch 1175 | Train Loss:     0.5888 | Validation Loss:     0.5918\nEpoch 1176 | Train Loss:     0.5886 | Validation Loss:     0.5915\nEpoch 1177 | Train Loss:     0.5885 | Validation Loss:     0.5913\nEpoch 1178 | Train Loss:     0.5881 | Validation Loss:     0.5912\nEpoch 1179 | Train Loss:     0.5879 | Validation Loss:     0.5908\nEpoch 1180 | Train Loss:     0.5876 | Validation Loss:     0.5906\nEpoch 1191 | Train Loss:     0.5848 | Validation Loss:     0.5877\nEpoch 1192 | Train Loss:     0.5845 | Validation Loss:     0.5873\nEpoch 1193 | Train Loss:     0.5842 | Validation Loss:     0.5870\nEpoch 1194 | Train Loss:     0.5840 | Validation Loss:     0.5867\nEpoch 1195 | Train Loss:     0.5837 | Validation Loss:     0.5865\nEpoch 1196 | Train Loss:     0.5834 | Validation Loss:     0.5861\nEpoch 1197 | Train Loss:     0.5830 | Validation Loss:     0.5858\nEpoch 1198 | Train Loss:     0.5828 | Validation Loss:     0.5856\nEpoch 1199 | Train Loss:     0.5825 | Validation Loss:     0.5851\nEpoch 1200 | Train Loss:     0.5821 | Validation Loss:     0.5848\n","output_type":"stream"},{"name":"stderr","text":"/tmp/ipykernel_34/4051270088.py:142: DeprecationWarning: Starting with ImageIO v3 the behavior of this function will switch to that of iio.v3.imread. To keep the current behavior (and make this warning disappear) use `import imageio.v2 as imageio` or call `imageio.v2.imread` directly.\n  images.append(imageio.imread(filename))\n","output_type":"stream"}]},{"cell_type":"code","source":"for name, param in model_prepu.named_parameters():\n    print(name, param.data)","metadata":{"execution":{"iopub.status.busy":"2024-06-14T19:36:03.177731Z","iopub.execute_input":"2024-06-14T19:36:03.178134Z","iopub.status.idle":"2024-06-14T19:36:03.280948Z","shell.execute_reply.started":"2024-06-14T19:36:03.178101Z","shell.execute_reply":"2024-06-14T19:36:03.279956Z"},"collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":92,"outputs":[{"name":"stdout","text":"layers.0.weight tensor([[ 1.0016e+00, -2.3969e-03,  9.3801e-04, -9.3801e-04],\n        [-2.3969e-03,  1.0016e+00, -9.3801e-04,  9.3801e-04],\n        [ 7.2699e-04, -7.2699e-04,  1.0013e+00, -1.8896e-03],\n        [-7.2700e-04,  7.2700e-04, -1.8896e-03,  1.0013e+00]], device='cuda:0')\nlayers.0.bias tensor([ 0.0009, -0.0009,  0.0019, -0.0019], device='cuda:0')\nlayers.1.weight tensor([[ 1.0016e+00, -2.3814e-03,  9.1094e-04, -9.1094e-04,  2.3887e-03,\n         -2.3887e-03,  9.2674e-04, -9.2674e-04],\n        [-2.3814e-03,  1.0016e+00, -9.1094e-04,  9.1094e-04, -2.3887e-03,\n          2.3887e-03, -9.2674e-04,  9.2674e-04],\n        [ 7.2071e-04, -7.2071e-04,  1.0013e+00, -1.8817e-03,  7.1180e-04,\n         -7.1180e-04,  1.8842e-03, -1.8842e-03],\n        [-7.2071e-04,  7.2071e-04, -1.8817e-03,  1.0013e+00, -7.1180e-04,\n          7.1180e-04, -1.8842e-03,  1.8842e-03],\n        [ 2.3739e-03, -2.3739e-03,  9.0424e-04, -9.0424e-04,  1.0016e+00,\n         -2.3808e-03,  9.1966e-04, -9.1966e-04],\n        [-2.3739e-03,  2.3739e-03, -9.0424e-04,  9.0424e-04, -2.3808e-03,\n          1.0016e+00, -9.1966e-04,  9.1966e-04],\n        [ 7.2716e-04, -7.2716e-04,  1.8818e-03, -1.8818e-03,  7.1828e-04,\n         -7.1828e-04,  1.0013e+00, -1.8844e-03],\n        [-7.2716e-04,  7.2716e-04, -1.8818e-03,  1.8818e-03, -7.1828e-04,\n          7.1828e-04, -1.8844e-03,  1.0013e+00]], device='cuda:0')\nlayers.1.bias tensor([ 0.0009, -0.0009,  0.0019, -0.0019,  0.0009, -0.0009,  0.0019, -0.0019],\n       device='cuda:0')\nlayers.2.weight tensor([[ 1.0016e+00, -2.3682e-03,  8.7979e-04, -8.7979e-04,  2.3755e-03,\n         -2.3755e-03,  8.9552e-04, -8.9552e-04,  2.3825e-03, -2.3825e-03,\n          9.1104e-04, -9.1104e-04,  2.3904e-03, -2.3904e-03,  9.2641e-04,\n         -9.2641e-04],\n        [-2.3682e-03,  1.0016e+00, -8.7979e-04,  8.7979e-04, -2.3755e-03,\n          2.3755e-03, -8.9552e-04,  8.9552e-04, -2.3825e-03,  2.3825e-03,\n         -9.1104e-04,  9.1104e-04, -2.3904e-03,  2.3904e-03, -9.2641e-04,\n          9.2641e-04],\n        [ 7.0758e-04, -7.0758e-04,  1.0013e+00, -1.8658e-03,  6.9844e-04,\n         -6.9844e-04,  1.8681e-03, -1.8681e-03,  6.8909e-04, -6.8909e-04,\n          1.8701e-03, -1.8701e-03,  6.7930e-04, -6.7930e-04,  1.8722e-03,\n         -1.8722e-03],\n        [-7.0758e-04,  7.0758e-04, -1.8658e-03,  1.0013e+00, -6.9844e-04,\n          6.9844e-04, -1.8681e-03,  1.8681e-03, -6.8909e-04,  6.8909e-04,\n         -1.8701e-03,  1.8701e-03, -6.7930e-04,  6.7930e-04, -1.8722e-03,\n          1.8722e-03],\n        [ 2.3579e-03, -2.3579e-03,  8.7023e-04, -8.7023e-04,  1.0016e+00,\n         -2.3649e-03,  8.8556e-04, -8.8556e-04,  2.3715e-03, -2.3715e-03,\n          9.0070e-04, -9.0070e-04,  2.3789e-03, -2.3789e-03,  9.1564e-04,\n         -9.1564e-04],\n        [-2.3579e-03,  2.3579e-03, -8.7023e-04,  8.7023e-04, -2.3649e-03,\n          1.0016e+00, -8.8556e-04,  8.8556e-04, -2.3715e-03,  2.3715e-03,\n         -9.0070e-04,  9.0070e-04, -2.3789e-03,  2.3789e-03, -9.1564e-04,\n          9.1564e-04],\n        [ 7.1422e-04, -7.1422e-04,  1.8663e-03, -1.8663e-03,  7.0515e-04,\n         -7.0515e-04,  1.0013e+00, -1.8687e-03,  6.9585e-04, -6.9585e-04,\n          1.8708e-03, -1.8708e-03,  6.8612e-04, -6.8612e-04,  1.8730e-03,\n         -1.8730e-03],\n        [-7.1422e-04,  7.1422e-04, -1.8663e-03,  1.8663e-03, -7.0515e-04,\n          7.0515e-04, -1.8687e-03,  1.0013e+00, -6.9585e-04,  6.9585e-04,\n         -1.8708e-03,  1.8708e-03, -6.8612e-04,  6.8612e-04, -1.8730e-03,\n          1.8730e-03],\n        [ 2.3311e-03, -2.3311e-03,  8.3985e-04, -8.3985e-04,  2.3374e-03,\n         -2.3374e-03,  8.5468e-04, -8.5468e-04,  1.0015e+00, -2.3434e-03,\n          8.6929e-04, -8.6929e-04,  2.3501e-03, -2.3501e-03,  8.8372e-04,\n         -8.8372e-04],\n        [-2.3311e-03,  2.3311e-03, -8.3985e-04,  8.3985e-04, -2.3374e-03,\n          2.3374e-03, -8.5468e-04,  8.5468e-04, -2.3434e-03,  1.0015e+00,\n         -8.6930e-04,  8.6929e-04, -2.3501e-03,  2.3501e-03, -8.8372e-04,\n          8.8372e-04],\n        [ 7.2175e-04, -7.2175e-04,  1.8662e-03, -1.8662e-03,  7.1275e-04,\n         -7.1275e-04,  1.8688e-03, -1.8688e-03,  7.0352e-04, -7.0352e-04,\n          1.0013e+00, -1.8710e-03,  6.9387e-04, -6.9387e-04,  1.8733e-03,\n         -1.8733e-03],\n        [-7.2175e-04,  7.2175e-04, -1.8662e-03,  1.8662e-03, -7.1275e-04,\n          7.1275e-04, -1.8688e-03,  1.8688e-03, -7.0352e-04,  7.0352e-04,\n         -1.8710e-03,  1.0013e+00, -6.9387e-04,  6.9387e-04, -1.8733e-03,\n          1.8733e-03],\n        [ 2.3196e-03, -2.3196e-03,  8.3032e-04, -8.3032e-04,  2.3257e-03,\n         -2.3257e-03,  8.4476e-04, -8.4476e-04,  2.3313e-03, -2.3313e-03,\n          8.5899e-04, -8.5899e-04,  1.0015e+00, -2.3377e-03,  8.7304e-04,\n         -8.7304e-04],\n        [-2.3196e-03,  2.3196e-03, -8.3032e-04,  8.3032e-04, -2.3257e-03,\n          2.3257e-03, -8.4476e-04,  8.4476e-04, -2.3313e-03,  2.3313e-03,\n         -8.5899e-04,  8.5899e-04, -2.3377e-03,  1.0015e+00, -8.7304e-04,\n          8.7304e-04],\n        [ 7.2862e-04, -7.2862e-04,  1.8669e-03, -1.8669e-03,  7.1968e-04,\n         -7.1968e-04,  1.8695e-03, -1.8695e-03,  7.1051e-04, -7.1051e-04,\n          1.8718e-03, -1.8718e-03,  7.0092e-04, -7.0092e-04,  1.0013e+00,\n         -1.8742e-03],\n        [-7.2862e-04,  7.2862e-04, -1.8669e-03,  1.8669e-03, -7.1968e-04,\n          7.1968e-04, -1.8695e-03,  1.8695e-03, -7.1051e-04,  7.1051e-04,\n         -1.8718e-03,  1.8718e-03, -7.0092e-04,  7.0092e-04, -1.8742e-03,\n          1.0013e+00]], device='cuda:0')\nlayers.2.bias tensor([ 0.0009, -0.0009,  0.0019, -0.0019,  0.0009, -0.0009,  0.0019, -0.0019,\n         0.0008, -0.0008,  0.0019, -0.0019,  0.0008, -0.0008,  0.0019, -0.0019],\n       device='cuda:0')\nlayers.3.weight tensor([[ 1.0016e+00, -2.3007e-03,  7.6667e-04,  ..., -2.3486e-03,\n          8.6994e-04, -8.6994e-04],\n        [-2.3007e-03,  1.0016e+00, -7.6667e-04,  ...,  2.3486e-03,\n         -8.6994e-04,  8.6994e-04],\n        [ 6.8790e-04, -6.8790e-04,  1.0013e+00,  ..., -6.1676e-04,\n          1.8480e-03, -1.8480e-03],\n        ...,\n        [-2.2144e-03,  2.2144e-03, -6.9962e-04,  ...,  1.0014e+00,\n         -7.8478e-04,  7.8478e-04],\n        [ 7.3070e-04, -7.3070e-04,  1.8403e-03,  ..., -6.6112e-04,\n          1.0012e+00, -1.8557e-03],\n        [-7.3070e-04,  7.3070e-04, -1.8403e-03,  ...,  6.6112e-04,\n         -1.8557e-03,  1.0012e+00]], device='cuda:0')\nlayers.3.bias tensor([ 0.0008, -0.0008,  0.0018, -0.0018,  0.0008, -0.0008,  0.0018, -0.0018,\n         0.0008, -0.0008,  0.0018, -0.0018,  0.0007, -0.0007,  0.0018, -0.0018,\n         0.0007, -0.0007,  0.0018, -0.0018,  0.0007, -0.0007,  0.0018, -0.0018,\n         0.0007, -0.0007,  0.0018, -0.0018,  0.0007, -0.0007,  0.0018, -0.0018],\n       device='cuda:0')\nlayers.4.weight tensor([[ 1.0015e+00, -2.2087e-03,  6.2709e-04,  ..., -2.3050e-03,\n          8.3186e-04, -8.3186e-04],\n        [-2.2087e-03,  1.0015e+00, -6.2709e-04,  ...,  2.3050e-03,\n         -8.3186e-04,  8.3186e-04],\n        [ 6.4660e-04, -6.4660e-04,  1.0012e+00,  ..., -4.7275e-04,\n          1.8036e-03, -1.8036e-03],\n        ...,\n        [-1.9949e-03,  1.9949e-03, -4.0959e-04,  ...,  1.0012e+00,\n         -5.4994e-04,  5.4994e-04],\n        [ 7.4577e-04, -7.4577e-04,  1.7889e-03,  ..., -5.8178e-04,\n          1.0012e+00, -1.8174e-03],\n        [-7.4577e-04,  7.4577e-04, -1.7889e-03,  ...,  5.8178e-04,\n         -1.8174e-03,  1.0012e+00]], device='cuda:0')\nlayers.4.bias tensor([ 0.0006, -0.0006,  0.0018, -0.0018,  0.0006, -0.0006,  0.0018, -0.0018,\n         0.0006, -0.0006,  0.0018, -0.0018,  0.0006, -0.0006,  0.0018, -0.0018,\n         0.0006, -0.0006,  0.0018, -0.0018,  0.0006, -0.0006,  0.0018, -0.0018,\n         0.0006, -0.0006,  0.0018, -0.0018,  0.0005, -0.0005,  0.0018, -0.0018,\n         0.0005, -0.0005,  0.0018, -0.0018,  0.0005, -0.0005,  0.0018, -0.0018,\n         0.0005, -0.0005,  0.0018, -0.0018,  0.0005, -0.0005,  0.0018, -0.0018,\n         0.0005, -0.0005,  0.0018, -0.0018,  0.0004, -0.0004,  0.0018, -0.0018,\n         0.0004, -0.0004,  0.0018, -0.0018,  0.0004, -0.0004,  0.0018, -0.0018],\n       device='cuda:0')\nlayers.5.weight tensor([[ 1.0016e+00, -2.0574e-03,  3.2739e-04,  ..., -2.2488e-03,\n          7.1464e-04, -7.1464e-04],\n        [-2.0574e-03,  1.0016e+00, -3.2739e-04,  ...,  2.2488e-03,\n         -7.1464e-04,  7.1464e-04],\n        [ 6.1149e-04, -6.1149e-04,  1.0011e+00,  ..., -2.2422e-04,\n          1.6964e-03, -1.6964e-03],\n        ...,\n        [-1.6437e-03,  1.6437e-03,  5.2493e-05,  ...,  1.0009e+00,\n         -1.6625e-04,  1.6625e-04],\n        [ 8.0419e-04, -8.0419e-04,  1.7236e-03,  ..., -4.3878e-04,\n          1.0011e+00, -1.7798e-03],\n        [-8.0419e-04,  8.0419e-04, -1.7236e-03,  ...,  4.3878e-04,\n         -1.7798e-03,  1.0011e+00]], device='cuda:0')\nlayers.5.bias tensor([ 3.2739e-04, -3.2739e-04,  1.7200e-03, -1.7200e-03,  3.2756e-04,\n        -3.2756e-04,  1.7201e-03, -1.7201e-03,  3.2627e-04, -3.2627e-04,\n         1.7200e-03, -1.7200e-03,  3.2376e-04, -3.2376e-04,  1.7201e-03,\n        -1.7201e-03,  3.1991e-04, -3.1991e-04,  1.7202e-03, -1.7202e-03,\n         3.1538e-04, -3.1538e-04,  1.7205e-03, -1.7205e-03,  3.1008e-04,\n        -3.1008e-04,  1.7209e-03, -1.7209e-03,  3.0424e-04, -3.0424e-04,\n         1.7218e-03, -1.7218e-03,  2.9859e-04, -2.9859e-04,  1.7234e-03,\n        -1.7234e-03,  2.9256e-04, -2.9256e-04,  1.7269e-03, -1.7269e-03,\n         2.8688e-04, -2.8688e-04,  1.7290e-03, -1.7290e-03,  2.8233e-04,\n        -2.8233e-04,  1.7308e-03, -1.7308e-03,  2.8001e-04, -2.8001e-04,\n         1.7328e-03, -1.7328e-03,  2.8135e-04, -2.8135e-04,  1.7351e-03,\n        -1.7351e-03,  2.8506e-04, -2.8506e-04,  1.7372e-03, -1.7372e-03,\n         2.8022e-04, -2.8022e-04,  1.7392e-03, -1.7392e-03,  2.7404e-04,\n        -2.7404e-04,  1.7431e-03, -1.7431e-03,  2.5765e-04, -2.5765e-04,\n         1.7441e-03, -1.7441e-03,  2.3928e-04, -2.3928e-04,  1.7443e-03,\n        -1.7443e-03,  2.1992e-04, -2.1992e-04,  1.7439e-03, -1.7439e-03,\n         1.9865e-04, -1.9865e-04,  1.7424e-03, -1.7424e-03,  1.7367e-04,\n        -1.7367e-04,  1.7399e-03, -1.7399e-03,  1.4154e-04, -1.4154e-04,\n         1.7332e-03, -1.7332e-03,  1.1776e-04, -1.1776e-04,  1.7325e-03,\n        -1.7325e-03,  9.9854e-05, -9.9854e-05,  1.7310e-03, -1.7310e-03,\n         7.9528e-05, -7.9528e-05,  1.7291e-03, -1.7291e-03,  5.8475e-05,\n        -5.8474e-05,  1.7265e-03, -1.7265e-03,  3.6826e-05, -3.6825e-05,\n         1.7236e-03, -1.7236e-03,  1.5073e-05, -1.5073e-05,  1.7235e-03,\n        -1.7235e-03, -7.2192e-06,  7.2193e-06,  1.7236e-03, -1.7236e-03,\n        -2.9656e-05,  2.9656e-05,  1.7236e-03, -1.7236e-03, -5.2493e-05,\n         5.2493e-05,  1.7236e-03, -1.7236e-03], device='cuda:0')\nlayers.6.weight tensor([[ 1.0016e+00, -2.4345e-03,  3.0998e-04,  ..., -3.4372e-03,\n          1.4850e-03, -1.4850e-03],\n        [-2.4345e-03,  1.0016e+00, -3.0998e-04,  ...,  3.4372e-03,\n         -1.4850e-03,  1.4850e-03],\n        [ 5.3935e-04, -5.3935e-04,  1.0011e+00,  ...,  1.9894e-04,\n          1.4773e-03, -1.4773e-03],\n        ...,\n        [-1.1118e-03,  1.1118e-03,  7.5458e-04,  ...,  1.0007e+00,\n          3.4650e-04, -3.4650e-04],\n        [ 9.7943e-04, -9.7943e-04,  1.6743e-03,  ..., -2.1696e-04,\n          1.0010e+00, -1.8210e-03],\n        [-9.7943e-04,  9.7943e-04, -1.6743e-03,  ...,  2.1696e-04,\n         -1.8210e-03,  1.0010e+00]], device='cuda:0')\nlayers.6.bias tensor([ 0.0003, -0.0003,  0.0017, -0.0017,  0.0003, -0.0003,  0.0017, -0.0017,\n         0.0003, -0.0003,  0.0016, -0.0016,  0.0003, -0.0003,  0.0016, -0.0016,\n         0.0003, -0.0003,  0.0016, -0.0016,  0.0003, -0.0003,  0.0016, -0.0016,\n        -0.0003,  0.0003,  0.0016, -0.0016, -0.0003,  0.0003,  0.0016, -0.0016,\n        -0.0003,  0.0003,  0.0016, -0.0016, -0.0003,  0.0003,  0.0016, -0.0016,\n        -0.0003,  0.0003,  0.0016, -0.0016, -0.0003,  0.0003,  0.0016, -0.0016,\n        -0.0003,  0.0003,  0.0016, -0.0016, -0.0003,  0.0003,  0.0016, -0.0016,\n        -0.0004,  0.0004,  0.0016, -0.0016, -0.0004,  0.0004,  0.0016, -0.0016,\n        -0.0004,  0.0004,  0.0016, -0.0016, -0.0004,  0.0004,  0.0016, -0.0016,\n        -0.0004,  0.0004,  0.0016, -0.0016, -0.0004,  0.0004,  0.0016, -0.0016,\n        -0.0004,  0.0004,  0.0016, -0.0016, -0.0004,  0.0004,  0.0016, -0.0016,\n        -0.0004,  0.0004,  0.0016, -0.0016, -0.0004,  0.0004,  0.0016, -0.0016,\n        -0.0004,  0.0004,  0.0016, -0.0016, -0.0004,  0.0004,  0.0016, -0.0016,\n        -0.0004,  0.0004,  0.0016, -0.0016, -0.0004,  0.0004,  0.0016, -0.0016,\n        -0.0004,  0.0004,  0.0016, -0.0016, -0.0005,  0.0005,  0.0016, -0.0016,\n        -0.0005,  0.0005,  0.0016, -0.0016, -0.0005,  0.0005,  0.0016, -0.0016,\n        -0.0005,  0.0005,  0.0016, -0.0016, -0.0005,  0.0005,  0.0016, -0.0016,\n        -0.0005,  0.0005,  0.0016, -0.0016, -0.0005,  0.0005,  0.0016, -0.0016,\n        -0.0005,  0.0005,  0.0016, -0.0016, -0.0005,  0.0005,  0.0016, -0.0016,\n        -0.0005,  0.0005,  0.0016, -0.0016, -0.0005,  0.0005,  0.0016, -0.0016,\n        -0.0005,  0.0005,  0.0016, -0.0016, -0.0005,  0.0005,  0.0016, -0.0016,\n        -0.0005,  0.0005,  0.0017, -0.0017, -0.0005,  0.0005,  0.0017, -0.0017,\n        -0.0006,  0.0006,  0.0017, -0.0017, -0.0006,  0.0006,  0.0017, -0.0017,\n        -0.0006,  0.0006,  0.0017, -0.0017, -0.0006,  0.0006,  0.0017, -0.0017,\n        -0.0006,  0.0006,  0.0017, -0.0017, -0.0006,  0.0006,  0.0017, -0.0017,\n        -0.0006,  0.0006,  0.0017, -0.0017, -0.0006,  0.0006,  0.0017, -0.0017,\n        -0.0006,  0.0006,  0.0017, -0.0017, -0.0006,  0.0006,  0.0017, -0.0017,\n        -0.0007,  0.0007,  0.0017, -0.0017, -0.0007,  0.0007,  0.0017, -0.0017,\n        -0.0007,  0.0007,  0.0017, -0.0017, -0.0007,  0.0007,  0.0017, -0.0017,\n        -0.0007,  0.0007,  0.0017, -0.0017, -0.0007,  0.0007,  0.0017, -0.0017,\n        -0.0007,  0.0007,  0.0017, -0.0017, -0.0007,  0.0007,  0.0017, -0.0017,\n        -0.0007,  0.0007,  0.0017, -0.0017, -0.0008,  0.0008,  0.0017, -0.0017],\n       device='cuda:0')\nlayers.7.weight tensor([[ 1.0031e+00, -3.9171e-03, -1.0322e-04,  ..., -1.6967e-03,\n          9.6165e-04, -9.6165e-04],\n        [-3.9171e-03,  1.0031e+00,  1.0322e-04,  ...,  1.6967e-03,\n         -9.6165e-04,  9.6165e-04],\n        [ 6.5357e-04, -6.5357e-04,  1.0010e+00,  ...,  3.7529e-04,\n          1.4872e-03, -1.4872e-03],\n        ...,\n        [-6.2854e-04,  6.2854e-04,  1.0832e-03,  ...,  1.0007e+00,\n          5.9292e-04, -5.9292e-04],\n        [ 1.2742e-03, -1.2742e-03,  1.6136e-03,  ..., -5.7213e-04,\n          1.0011e+00, -2.2422e-03],\n        [-1.2742e-03,  1.2742e-03, -1.6136e-03,  ...,  5.7213e-04,\n         -2.2422e-03,  1.0011e+00]], device='cuda:0')\nlayers.7.bias tensor([-0.0001,  0.0001,  0.0018, -0.0018, -0.0001,  0.0001,  0.0018, -0.0018,\n        -0.0001,  0.0001,  0.0017, -0.0017, -0.0001,  0.0001,  0.0017, -0.0017,\n        -0.0001,  0.0001,  0.0017, -0.0017, -0.0001,  0.0001,  0.0017, -0.0017,\n        -0.0001,  0.0001,  0.0017, -0.0017, -0.0001,  0.0001,  0.0017, -0.0017,\n        -0.0001,  0.0001,  0.0017, -0.0017, -0.0001,  0.0001,  0.0017, -0.0017,\n        -0.0001,  0.0001,  0.0017, -0.0017, -0.0001,  0.0001,  0.0017, -0.0017,\n        -0.0001,  0.0001,  0.0017, -0.0017, -0.0001,  0.0001,  0.0017, -0.0017,\n        -0.0001,  0.0001,  0.0017, -0.0017, -0.0001,  0.0001,  0.0017, -0.0017,\n        -0.0001,  0.0001,  0.0017, -0.0017, -0.0001,  0.0001,  0.0017, -0.0017,\n        -0.0001,  0.0001,  0.0017, -0.0017, -0.0001,  0.0001,  0.0017, -0.0017,\n        -0.0001,  0.0001,  0.0017, -0.0017, -0.0002,  0.0002,  0.0017, -0.0017,\n        -0.0002,  0.0002,  0.0017, -0.0017, -0.0002,  0.0002,  0.0017, -0.0017,\n        -0.0002,  0.0002,  0.0017, -0.0017, -0.0002,  0.0002,  0.0017, -0.0017,\n        -0.0002,  0.0002,  0.0017, -0.0017, -0.0002,  0.0002,  0.0017, -0.0017,\n        -0.0003,  0.0003,  0.0017, -0.0017, -0.0003,  0.0003,  0.0017, -0.0017,\n        -0.0003,  0.0003,  0.0017, -0.0017, -0.0003,  0.0003,  0.0017, -0.0017,\n        -0.0003,  0.0003,  0.0017, -0.0017, -0.0003,  0.0003,  0.0017, -0.0017,\n        -0.0003,  0.0003,  0.0017, -0.0017, -0.0004,  0.0004,  0.0017, -0.0017,\n        -0.0004,  0.0004,  0.0017, -0.0017, -0.0004,  0.0004,  0.0017, -0.0017,\n        -0.0004,  0.0004,  0.0017, -0.0017, -0.0005,  0.0005,  0.0017, -0.0017,\n        -0.0005,  0.0005,  0.0017, -0.0017, -0.0005,  0.0005,  0.0017, -0.0017,\n        -0.0005,  0.0005,  0.0017, -0.0017, -0.0005,  0.0005,  0.0017, -0.0017,\n        -0.0006,  0.0006,  0.0017, -0.0017, -0.0006,  0.0006,  0.0017, -0.0017,\n        -0.0006,  0.0006,  0.0017, -0.0017, -0.0006,  0.0006,  0.0017, -0.0017,\n        -0.0006,  0.0006,  0.0017, -0.0017, -0.0006,  0.0006,  0.0017, -0.0017,\n        -0.0007,  0.0007,  0.0017, -0.0017, -0.0007,  0.0007,  0.0017, -0.0017,\n        -0.0007,  0.0007,  0.0017, -0.0017, -0.0007,  0.0007,  0.0017, -0.0017,\n        -0.0007,  0.0007,  0.0016, -0.0016, -0.0007,  0.0007,  0.0016, -0.0016,\n        -0.0007,  0.0007,  0.0016, -0.0016, -0.0008,  0.0008,  0.0016, -0.0016,\n        -0.0008,  0.0008,  0.0016, -0.0016, -0.0008,  0.0008,  0.0016, -0.0016,\n        -0.0008,  0.0008,  0.0016, -0.0016, -0.0008,  0.0008,  0.0016, -0.0016,\n        -0.0008,  0.0008,  0.0016, -0.0016, -0.0008,  0.0008,  0.0016, -0.0016,\n        -0.0009,  0.0009,  0.0017, -0.0017, -0.0009,  0.0009,  0.0017, -0.0017,\n        -0.0009,  0.0009,  0.0017, -0.0017, -0.0009,  0.0009,  0.0017, -0.0017,\n        -0.0010,  0.0010,  0.0016, -0.0016, -0.0010,  0.0010,  0.0016, -0.0016,\n        -0.0010,  0.0010,  0.0016, -0.0016, -0.0010,  0.0010,  0.0016, -0.0016,\n        -0.0010,  0.0010,  0.0016, -0.0016, -0.0011,  0.0011,  0.0016, -0.0016,\n        -0.0011,  0.0011,  0.0016, -0.0016, -0.0011,  0.0011,  0.0016, -0.0016,\n        -0.0011,  0.0011,  0.0016, -0.0016, -0.0011,  0.0011,  0.0016, -0.0016,\n        -0.0011,  0.0011,  0.0016, -0.0016, -0.0011,  0.0011,  0.0016, -0.0016,\n        -0.0011,  0.0011,  0.0016, -0.0016, -0.0011,  0.0011,  0.0016, -0.0016,\n        -0.0011,  0.0011,  0.0016, -0.0016, -0.0012,  0.0012,  0.0016, -0.0016,\n        -0.0012,  0.0012,  0.0016, -0.0016, -0.0012,  0.0012,  0.0016, -0.0016,\n        -0.0012,  0.0012,  0.0016, -0.0016, -0.0012,  0.0012,  0.0016, -0.0016,\n        -0.0011,  0.0011,  0.0016, -0.0016, -0.0011,  0.0011,  0.0016, -0.0016,\n        -0.0011,  0.0011,  0.0016, -0.0016, -0.0011,  0.0011,  0.0016, -0.0016,\n        -0.0011,  0.0011,  0.0016, -0.0016, -0.0011,  0.0011,  0.0016, -0.0016,\n        -0.0011,  0.0011,  0.0016, -0.0016, -0.0011,  0.0011,  0.0016, -0.0016,\n        -0.0011,  0.0011,  0.0016, -0.0016, -0.0011,  0.0011,  0.0016, -0.0016,\n        -0.0011,  0.0011,  0.0016, -0.0016, -0.0011,  0.0011,  0.0016, -0.0016,\n        -0.0011,  0.0011,  0.0016, -0.0016, -0.0011,  0.0011,  0.0016, -0.0016,\n        -0.0011,  0.0011,  0.0016, -0.0016, -0.0011,  0.0011,  0.0016, -0.0016,\n        -0.0011,  0.0011,  0.0016, -0.0016, -0.0011,  0.0011,  0.0016, -0.0016,\n        -0.0011,  0.0011,  0.0016, -0.0016, -0.0011,  0.0011,  0.0016, -0.0016,\n        -0.0011,  0.0011,  0.0016, -0.0016, -0.0011,  0.0011,  0.0016, -0.0016,\n        -0.0011,  0.0011,  0.0016, -0.0016, -0.0011,  0.0011,  0.0016, -0.0016,\n        -0.0011,  0.0011,  0.0016, -0.0016, -0.0011,  0.0011,  0.0016, -0.0016,\n        -0.0011,  0.0011,  0.0016, -0.0016, -0.0011,  0.0011,  0.0016, -0.0016,\n        -0.0011,  0.0011,  0.0016, -0.0016, -0.0011,  0.0011,  0.0016, -0.0016,\n        -0.0011,  0.0011,  0.0016, -0.0016, -0.0011,  0.0011,  0.0016, -0.0016,\n        -0.0011,  0.0011,  0.0016, -0.0016, -0.0011,  0.0011,  0.0016, -0.0016,\n        -0.0011,  0.0011,  0.0016, -0.0016, -0.0011,  0.0011,  0.0016, -0.0016,\n        -0.0011,  0.0011,  0.0016, -0.0016, -0.0011,  0.0011,  0.0016, -0.0016,\n        -0.0011,  0.0011,  0.0016, -0.0016, -0.0011,  0.0011,  0.0016, -0.0016],\n       device='cuda:0')\nlayers.8.weight tensor([[ 1.0008e+00, -7.7503e-04,  7.3453e-05,  ..., -1.5396e-04,\n          8.4032e-04, -8.4032e-04],\n        [-7.7503e-04,  1.0008e+00, -7.3453e-05,  ...,  1.5396e-04,\n         -8.4032e-04,  8.4032e-04],\n        [-1.1542e-04,  1.1542e-04,  1.0006e+00,  ...,  4.1488e-04,\n          3.3940e-04, -3.3940e-04],\n        ...,\n        [-2.5400e-04,  2.5400e-04,  1.7070e-04,  ...,  1.0006e+00,\n          3.7055e-04, -3.7055e-04],\n        [ 2.4232e-04, -2.4232e-04,  1.7838e-03,  ...,  3.4447e-04,\n          1.0007e+00, -1.3345e-03],\n        [-2.4232e-04,  2.4232e-04, -1.7838e-03,  ..., -3.4447e-04,\n         -1.3345e-03,  1.0007e+00]], device='cuda:0')\nlayers.8.bias tensor([ 7.3453e-05, -7.3453e-05,  8.5300e-04,  ...,  1.7070e-04,\n         1.7838e-03, -1.7838e-03], device='cuda:0')\nlayers.9.weight tensor([[-0.0004,  0.0004, -0.0002,  ..., -0.0002, -0.0005,  0.0005]],\n       device='cuda:0')\nlayers.9.bias tensor([-0.0002], device='cuda:0')\ncustom_layers.0.linear_sigmoid.weight tensor([[ 0.0014, -0.0014,  0.0024, -0.0024],\n        [ 0.0014, -0.0014,  0.0024, -0.0024],\n        [ 0.0007, -0.0007,  0.0019, -0.0019],\n        [ 0.0007, -0.0007,  0.0019, -0.0019]], device='cuda:0')\ncustom_layers.0.linear_sigmoid.bias tensor([0.0024, 0.0024, 0.0019, 0.0019], device='cuda:0')\ncustom_layers.1.linear_sigmoid.weight tensor([[ 0.0014, -0.0014,  0.0024, -0.0024,  0.0014, -0.0014,  0.0024, -0.0024],\n        [ 0.0014, -0.0014,  0.0024, -0.0024,  0.0014, -0.0014,  0.0024, -0.0024],\n        [ 0.0007, -0.0007,  0.0019, -0.0019,  0.0007, -0.0007,  0.0019, -0.0019],\n        [ 0.0007, -0.0007,  0.0019, -0.0019,  0.0007, -0.0007,  0.0019, -0.0019],\n        [ 0.0015, -0.0015,  0.0024, -0.0024,  0.0015, -0.0015,  0.0024, -0.0024],\n        [ 0.0015, -0.0015,  0.0024, -0.0024,  0.0015, -0.0015,  0.0024, -0.0024],\n        [ 0.0007, -0.0007,  0.0019, -0.0019,  0.0007, -0.0007,  0.0019, -0.0019],\n        [ 0.0007, -0.0007,  0.0019, -0.0019,  0.0007, -0.0007,  0.0019, -0.0019]],\n       device='cuda:0')\ncustom_layers.1.linear_sigmoid.bias tensor([0.0024, 0.0024, 0.0019, 0.0019, 0.0024, 0.0024, 0.0019, 0.0019],\n       device='cuda:0')\ncustom_layers.2.linear_sigmoid.weight tensor([[ 0.0014, -0.0014,  0.0023, -0.0023,  0.0014, -0.0014,  0.0023, -0.0023,\n          0.0014, -0.0014,  0.0024, -0.0024,  0.0014, -0.0014,  0.0024, -0.0024],\n        [ 0.0014, -0.0014,  0.0023, -0.0023,  0.0014, -0.0014,  0.0023, -0.0023,\n          0.0014, -0.0014,  0.0024, -0.0024,  0.0014, -0.0014,  0.0024, -0.0024],\n        [ 0.0007, -0.0007,  0.0019, -0.0019,  0.0007, -0.0007,  0.0019, -0.0019,\n          0.0007, -0.0007,  0.0019, -0.0019,  0.0007, -0.0007,  0.0019, -0.0019],\n        [ 0.0007, -0.0007,  0.0019, -0.0019,  0.0007, -0.0007,  0.0019, -0.0019,\n          0.0007, -0.0007,  0.0019, -0.0019,  0.0007, -0.0007,  0.0019, -0.0019],\n        [ 0.0014, -0.0014,  0.0023, -0.0023,  0.0014, -0.0014,  0.0023, -0.0023,\n          0.0015, -0.0015,  0.0024, -0.0024,  0.0015, -0.0015,  0.0024, -0.0024],\n        [ 0.0014, -0.0014,  0.0023, -0.0023,  0.0014, -0.0014,  0.0023, -0.0023,\n          0.0015, -0.0015,  0.0024, -0.0024,  0.0015, -0.0015,  0.0024, -0.0024],\n        [ 0.0007, -0.0007,  0.0019, -0.0019,  0.0007, -0.0007,  0.0019, -0.0019,\n          0.0007, -0.0007,  0.0019, -0.0019,  0.0007, -0.0007,  0.0019, -0.0019],\n        [ 0.0007, -0.0007,  0.0019, -0.0019,  0.0007, -0.0007,  0.0019, -0.0019,\n          0.0007, -0.0007,  0.0019, -0.0019,  0.0007, -0.0007,  0.0019, -0.0019],\n        [ 0.0015, -0.0015,  0.0023, -0.0023,  0.0015, -0.0015,  0.0023, -0.0023,\n          0.0015, -0.0015,  0.0023, -0.0023,  0.0015, -0.0015,  0.0023, -0.0023],\n        [ 0.0015, -0.0015,  0.0023, -0.0023,  0.0015, -0.0015,  0.0023, -0.0023,\n          0.0015, -0.0015,  0.0023, -0.0023,  0.0015, -0.0015,  0.0023, -0.0023],\n        [ 0.0007, -0.0007,  0.0019, -0.0019,  0.0007, -0.0007,  0.0019, -0.0019,\n          0.0007, -0.0007,  0.0019, -0.0019,  0.0007, -0.0007,  0.0019, -0.0019],\n        [ 0.0007, -0.0007,  0.0019, -0.0019,  0.0007, -0.0007,  0.0019, -0.0019,\n          0.0007, -0.0007,  0.0019, -0.0019,  0.0007, -0.0007,  0.0019, -0.0019],\n        [ 0.0015, -0.0015,  0.0023, -0.0023,  0.0015, -0.0015,  0.0023, -0.0023,\n          0.0015, -0.0015,  0.0023, -0.0023,  0.0016, -0.0016,  0.0023, -0.0023],\n        [ 0.0015, -0.0015,  0.0023, -0.0023,  0.0015, -0.0015,  0.0023, -0.0023,\n          0.0015, -0.0015,  0.0023, -0.0023,  0.0016, -0.0016,  0.0023, -0.0023],\n        [ 0.0007, -0.0007,  0.0019, -0.0019,  0.0007, -0.0007,  0.0019, -0.0019,\n          0.0007, -0.0007,  0.0019, -0.0019,  0.0007, -0.0007,  0.0019, -0.0019],\n        [ 0.0007, -0.0007,  0.0019, -0.0019,  0.0007, -0.0007,  0.0019, -0.0019,\n          0.0007, -0.0007,  0.0019, -0.0019,  0.0007, -0.0007,  0.0019, -0.0019]],\n       device='cuda:0')\ncustom_layers.2.linear_sigmoid.bias tensor([0.0023, 0.0023, 0.0019, 0.0019, 0.0023, 0.0023, 0.0019, 0.0019, 0.0023,\n        0.0023, 0.0019, 0.0019, 0.0023, 0.0023, 0.0019, 0.0019],\n       device='cuda:0')\ncustom_layers.3.linear_sigmoid.weight tensor([[ 0.0012, -0.0012,  0.0022,  ..., -0.0013,  0.0023, -0.0023],\n        [ 0.0012, -0.0012,  0.0022,  ..., -0.0013,  0.0023, -0.0023],\n        [ 0.0007, -0.0007,  0.0018,  ..., -0.0006,  0.0019, -0.0019],\n        ...,\n        [ 0.0016, -0.0016,  0.0022,  ..., -0.0017,  0.0022, -0.0022],\n        [ 0.0007, -0.0007,  0.0019,  ..., -0.0006,  0.0019, -0.0019],\n        [ 0.0007, -0.0007,  0.0019,  ..., -0.0006,  0.0019, -0.0019]],\n       device='cuda:0')\ncustom_layers.3.linear_sigmoid.bias tensor([0.0022, 0.0022, 0.0018, 0.0018, 0.0022, 0.0022, 0.0018, 0.0018, 0.0022,\n        0.0022, 0.0018, 0.0018, 0.0022, 0.0022, 0.0018, 0.0018, 0.0022, 0.0022,\n        0.0019, 0.0019, 0.0022, 0.0022, 0.0019, 0.0019, 0.0022, 0.0022, 0.0019,\n        0.0019, 0.0022, 0.0022, 0.0019, 0.0019], device='cuda:0')\ncustom_layers.4.linear_sigmoid.weight tensor([[ 0.0009, -0.0009,  0.0021,  ..., -0.0012,  0.0022, -0.0022],\n        [ 0.0009, -0.0009,  0.0021,  ..., -0.0012,  0.0022, -0.0022],\n        [ 0.0007, -0.0007,  0.0018,  ..., -0.0005,  0.0018, -0.0018],\n        ...,\n        [ 0.0016, -0.0016,  0.0019,  ..., -0.0017,  0.0020, -0.0020],\n        [ 0.0006, -0.0006,  0.0018,  ..., -0.0004,  0.0018, -0.0018],\n        [ 0.0006, -0.0006,  0.0018,  ..., -0.0004,  0.0018, -0.0018]],\n       device='cuda:0')\ncustom_layers.4.linear_sigmoid.bias tensor([0.0021, 0.0021, 0.0018, 0.0018, 0.0021, 0.0021, 0.0018, 0.0018, 0.0021,\n        0.0021, 0.0018, 0.0018, 0.0021, 0.0021, 0.0018, 0.0018, 0.0021, 0.0021,\n        0.0018, 0.0018, 0.0021, 0.0021, 0.0018, 0.0018, 0.0021, 0.0021, 0.0018,\n        0.0018, 0.0020, 0.0020, 0.0018, 0.0018, 0.0020, 0.0020, 0.0018, 0.0018,\n        0.0020, 0.0020, 0.0018, 0.0018, 0.0020, 0.0020, 0.0018, 0.0018, 0.0020,\n        0.0020, 0.0018, 0.0018, 0.0020, 0.0020, 0.0018, 0.0018, 0.0019, 0.0019,\n        0.0018, 0.0018, 0.0019, 0.0019, 0.0018, 0.0018, 0.0019, 0.0019, 0.0018,\n        0.0018], device='cuda:0')\ncustom_layers.5.linear_sigmoid.weight tensor([[ 0.0005, -0.0005,  0.0019,  ..., -0.0008,  0.0021, -0.0021],\n        [ 0.0005, -0.0005,  0.0019,  ..., -0.0008,  0.0021, -0.0021],\n        [ 0.0007, -0.0007,  0.0017,  ..., -0.0003,  0.0018, -0.0018],\n        ...,\n        [ 0.0013, -0.0013,  0.0012,  ..., -0.0015,  0.0013, -0.0013],\n        [ 0.0005, -0.0005,  0.0018,  ..., -0.0001,  0.0018, -0.0018],\n        [ 0.0005, -0.0005,  0.0018,  ..., -0.0001,  0.0018, -0.0018]],\n       device='cuda:0')\ncustom_layers.5.linear_sigmoid.bias tensor([0.0019, 0.0019, 0.0017, 0.0017, 0.0019, 0.0019, 0.0017, 0.0017, 0.0019,\n        0.0019, 0.0018, 0.0018, 0.0019, 0.0019, 0.0018, 0.0018, 0.0019, 0.0019,\n        0.0018, 0.0018, 0.0019, 0.0019, 0.0018, 0.0018, 0.0019, 0.0019, 0.0018,\n        0.0018, 0.0018, 0.0018, 0.0018, 0.0018, 0.0018, 0.0018, 0.0018, 0.0018,\n        0.0018, 0.0018, 0.0018, 0.0018, 0.0018, 0.0018, 0.0018, 0.0018, 0.0017,\n        0.0017, 0.0018, 0.0018, 0.0017, 0.0017, 0.0018, 0.0018, 0.0017, 0.0017,\n        0.0018, 0.0018, 0.0016, 0.0016, 0.0018, 0.0018, 0.0016, 0.0016, 0.0018,\n        0.0018, 0.0016, 0.0016, 0.0018, 0.0018, 0.0016, 0.0016, 0.0018, 0.0018,\n        0.0016, 0.0016, 0.0018, 0.0018, 0.0016, 0.0016, 0.0018, 0.0018, 0.0015,\n        0.0015, 0.0018, 0.0018, 0.0015, 0.0015, 0.0018, 0.0018, 0.0015, 0.0015,\n        0.0018, 0.0018, 0.0015, 0.0015, 0.0018, 0.0018, 0.0014, 0.0014, 0.0018,\n        0.0018, 0.0014, 0.0014, 0.0018, 0.0018, 0.0014, 0.0014, 0.0018, 0.0018,\n        0.0013, 0.0013, 0.0018, 0.0018, 0.0013, 0.0013, 0.0018, 0.0018, 0.0013,\n        0.0013, 0.0018, 0.0018, 0.0013, 0.0013, 0.0018, 0.0018, 0.0012, 0.0012,\n        0.0018, 0.0018], device='cuda:0')\ncustom_layers.6.linear_sigmoid.weight tensor([[ 0.0002, -0.0002,  0.0021,  ..., -0.0008,  0.0025, -0.0025],\n        [ 0.0002, -0.0002,  0.0021,  ..., -0.0008,  0.0025, -0.0025],\n        [ 0.0008, -0.0008,  0.0017,  ..., -0.0003,  0.0018, -0.0018],\n        ...,\n        [ 0.0005, -0.0005,  0.0002,  ..., -0.0008,  0.0003, -0.0003],\n        [ 0.0005, -0.0005,  0.0020,  ...,  0.0003,  0.0017, -0.0017],\n        [ 0.0005, -0.0005,  0.0020,  ...,  0.0003,  0.0017, -0.0017]],\n       device='cuda:0')\ncustom_layers.6.linear_sigmoid.bias tensor([0.0021, 0.0021, 0.0017, 0.0017, 0.0021, 0.0021, 0.0017, 0.0017, 0.0021,\n        0.0021, 0.0017, 0.0017, 0.0021, 0.0021, 0.0017, 0.0017, 0.0021, 0.0021,\n        0.0017, 0.0017, 0.0021, 0.0021, 0.0017, 0.0017, 0.0020, 0.0020, 0.0017,\n        0.0017, 0.0020, 0.0020, 0.0017, 0.0017, 0.0020, 0.0020, 0.0017, 0.0017,\n        0.0020, 0.0020, 0.0017, 0.0017, 0.0020, 0.0020, 0.0017, 0.0017, 0.0020,\n        0.0020, 0.0017, 0.0017, 0.0019, 0.0019, 0.0017, 0.0017, 0.0019, 0.0019,\n        0.0017, 0.0017, 0.0019, 0.0019, 0.0017, 0.0017, 0.0019, 0.0019, 0.0017,\n        0.0017, 0.0019, 0.0019, 0.0017, 0.0017, 0.0019, 0.0019, 0.0017, 0.0017,\n        0.0018, 0.0018, 0.0017, 0.0017, 0.0018, 0.0018, 0.0017, 0.0017, 0.0018,\n        0.0018, 0.0017, 0.0017, 0.0017, 0.0017, 0.0017, 0.0017, 0.0017, 0.0017,\n        0.0018, 0.0018, 0.0016, 0.0016, 0.0018, 0.0018, 0.0016, 0.0016, 0.0018,\n        0.0018, 0.0015, 0.0015, 0.0018, 0.0018, 0.0015, 0.0015, 0.0018, 0.0018,\n        0.0014, 0.0014, 0.0018, 0.0018, 0.0014, 0.0014, 0.0018, 0.0018, 0.0013,\n        0.0013, 0.0018, 0.0018, 0.0012, 0.0012, 0.0018, 0.0018, 0.0012, 0.0012,\n        0.0018, 0.0018, 0.0015, 0.0015, 0.0018, 0.0018, 0.0014, 0.0014, 0.0018,\n        0.0018, 0.0013, 0.0013, 0.0018, 0.0018, 0.0012, 0.0012, 0.0018, 0.0018,\n        0.0012, 0.0012, 0.0018, 0.0018, 0.0011, 0.0011, 0.0018, 0.0018, 0.0010,\n        0.0010, 0.0018, 0.0018, 0.0010, 0.0010, 0.0018, 0.0018, 0.0010, 0.0010,\n        0.0018, 0.0018, 0.0009, 0.0009, 0.0019, 0.0019, 0.0009, 0.0009, 0.0019,\n        0.0019, 0.0008, 0.0008, 0.0019, 0.0019, 0.0008, 0.0008, 0.0019, 0.0019,\n        0.0007, 0.0007, 0.0019, 0.0019, 0.0007, 0.0007, 0.0019, 0.0019, 0.0007,\n        0.0007, 0.0019, 0.0019, 0.0007, 0.0007, 0.0019, 0.0019, 0.0006, 0.0006,\n        0.0019, 0.0019, 0.0006, 0.0006, 0.0019, 0.0019, 0.0006, 0.0006, 0.0019,\n        0.0019, 0.0005, 0.0005, 0.0020, 0.0020, 0.0005, 0.0005, 0.0020, 0.0020,\n        0.0004, 0.0004, 0.0020, 0.0020, 0.0004, 0.0004, 0.0020, 0.0020, 0.0004,\n        0.0004, 0.0020, 0.0020, 0.0004, 0.0004, 0.0020, 0.0020, 0.0003, 0.0003,\n        0.0020, 0.0020, 0.0003, 0.0003, 0.0020, 0.0020, 0.0003, 0.0003, 0.0020,\n        0.0020, 0.0003, 0.0003, 0.0020, 0.0020, 0.0003, 0.0003, 0.0020, 0.0020,\n        0.0002, 0.0002, 0.0020, 0.0020], device='cuda:0')\ncustom_layers.7.linear_sigmoid.weight tensor([[-1.8578e-03,  1.8578e-03,  3.3281e-03,  ..., -1.2455e-04,\n          2.3448e-03, -2.3448e-03],\n        [-1.8578e-03,  1.8578e-03,  3.3281e-03,  ..., -1.2455e-04,\n          2.3448e-03, -2.3448e-03],\n        [ 1.0140e-03, -1.0140e-03,  1.5392e-03,  ..., -1.7977e-04,\n          1.9716e-03, -1.9716e-03],\n        ...,\n        [ 6.7312e-04, -6.7312e-04, -3.3924e-04,  ..., -1.7707e-04,\n         -6.5241e-05,  6.5241e-05],\n        [ 2.8568e-04, -2.8568e-04,  2.2270e-03,  ...,  6.7702e-04,\n          1.3241e-03, -1.3241e-03],\n        [ 2.8568e-04, -2.8568e-04,  2.2270e-03,  ...,  6.7702e-04,\n          1.3241e-03, -1.3241e-03]], device='cuda:0')\ncustom_layers.7.linear_sigmoid.bias tensor([ 3.3281e-03,  3.3281e-03,  1.5392e-03,  1.5392e-03,  3.3093e-03,\n         3.3093e-03,  1.5263e-03,  1.5263e-03,  3.2827e-03,  3.2827e-03,\n         1.5107e-03,  1.5107e-03,  3.2491e-03,  3.2491e-03,  1.4925e-03,\n         1.4925e-03,  3.2084e-03,  3.2084e-03,  1.4861e-03,  1.4861e-03,\n         3.1594e-03,  3.1594e-03,  1.4935e-03,  1.4935e-03,  3.1027e-03,\n         3.1027e-03,  1.5012e-03,  1.5012e-03,  3.0394e-03,  3.0394e-03,\n         1.5086e-03,  1.5086e-03,  2.9833e-03,  2.9833e-03,  1.5156e-03,\n         1.5156e-03,  2.9075e-03,  2.9075e-03,  1.5227e-03,  1.5227e-03,\n         2.8260e-03,  2.8260e-03,  1.5298e-03,  1.5298e-03,  2.7421e-03,\n         2.7421e-03,  1.5366e-03,  1.5366e-03,  2.6557e-03,  2.6557e-03,\n         1.5436e-03,  1.5436e-03,  2.5677e-03,  2.5677e-03,  1.5501e-03,\n         1.5501e-03,  2.4784e-03,  2.4784e-03,  1.5569e-03,  1.5569e-03,\n         2.3892e-03,  2.3892e-03,  1.5632e-03,  1.5632e-03,  2.3758e-03,\n         2.3758e-03,  1.5653e-03,  1.5653e-03,  2.2819e-03,  2.2819e-03,\n         1.5713e-03,  1.5713e-03,  2.1876e-03,  2.1876e-03,  1.5776e-03,\n         1.5776e-03,  2.0951e-03,  2.0951e-03,  1.5834e-03,  1.5834e-03,\n         2.0026e-03,  2.0026e-03,  1.5896e-03,  1.5896e-03,  1.9089e-03,\n         1.9089e-03,  1.5955e-03,  1.5955e-03,  1.8152e-03,  1.8151e-03,\n         1.6017e-03,  1.6017e-03,  1.7269e-03,  1.7269e-03,  1.6077e-03,\n         1.6077e-03,  1.6584e-03,  1.6584e-03,  1.6131e-03,  1.6131e-03,\n         1.5893e-03,  1.5893e-03,  1.6189e-03,  1.6189e-03,  1.5316e-03,\n         1.5316e-03,  1.6250e-03,  1.6250e-03,  1.4850e-03,  1.4850e-03,\n         1.6308e-03,  1.6308e-03,  1.4467e-03,  1.4467e-03,  1.6370e-03,\n         1.6370e-03,  1.4153e-03,  1.4153e-03,  1.6428e-03,  1.6428e-03,\n         1.3890e-03,  1.3890e-03,  1.6490e-03,  1.6490e-03,  1.3670e-03,\n         1.3670e-03,  1.6547e-03,  1.6547e-03,  1.4242e-03,  1.4242e-03,\n         1.6392e-03,  1.6392e-03,  1.3904e-03,  1.3904e-03,  1.6445e-03,\n         1.6445e-03,  1.3621e-03,  1.3621e-03,  1.6502e-03,  1.6502e-03,\n         1.3391e-03,  1.3391e-03,  1.6556e-03,  1.6556e-03,  1.3191e-03,\n         1.3191e-03,  1.6615e-03,  1.6615e-03,  1.3026e-03,  1.3026e-03,\n         1.6670e-03,  1.6670e-03,  1.2880e-03,  1.2880e-03,  1.6730e-03,\n         1.6730e-03,  1.2754e-03,  1.2754e-03,  1.6787e-03,  1.6787e-03,\n         1.2641e-03,  1.2641e-03,  1.6842e-03,  1.6842e-03,  1.2539e-03,\n         1.2539e-03,  1.6899e-03,  1.6899e-03,  1.2443e-03,  1.2443e-03,\n         1.6961e-03,  1.6961e-03,  1.2358e-03,  1.2358e-03,  1.7020e-03,\n         1.7020e-03,  1.2272e-03,  1.2272e-03,  1.7087e-03,  1.7087e-03,\n         1.2196e-03,  1.2196e-03,  1.7148e-03,  1.7148e-03,  1.2118e-03,\n         1.2118e-03,  1.7216e-03,  1.7216e-03,  1.2046e-03,  1.2046e-03,\n         1.7281e-03,  1.7281e-03,  1.1967e-03,  1.1967e-03,  1.7282e-03,\n         1.7282e-03,  1.1894e-03,  1.1894e-03,  1.7350e-03,  1.7350e-03,\n         1.1820e-03,  1.1820e-03,  1.7427e-03,  1.7427e-03,  1.1749e-03,\n         1.1749e-03,  1.7502e-03,  1.7502e-03,  1.1671e-03,  1.1671e-03,\n         1.7594e-03,  1.7594e-03,  1.1599e-03,  1.1599e-03,  1.7681e-03,\n         1.7681e-03,  1.1521e-03,  1.1521e-03,  1.7342e-03,  1.7342e-03,\n         1.1446e-03,  1.1446e-03,  1.7634e-03,  1.7634e-03,  1.1364e-03,\n         1.1364e-03,  1.7913e-03,  1.7913e-03,  1.1286e-03,  1.1286e-03,\n         1.8104e-03,  1.8104e-03,  1.1200e-03,  1.1200e-03,  1.8284e-03,\n         1.8284e-03,  1.1113e-03,  1.1113e-03,  1.8431e-03,  1.8431e-03,\n         1.1018e-03,  1.1018e-03,  1.8588e-03,  1.8588e-03,  1.0921e-03,\n         1.0921e-03,  1.8712e-03,  1.8712e-03,  1.0815e-03,  1.0815e-03,\n         1.8842e-03,  1.8842e-03,  1.0705e-03,  1.0705e-03,  1.8954e-03,\n         1.8954e-03,  1.0378e-03,  1.0378e-03,  1.7600e-03,  1.7600e-03,\n         1.0285e-03,  1.0285e-03,  1.7665e-03,  1.7665e-03,  1.0188e-03,\n         1.0188e-03,  1.7745e-03,  1.7745e-03,  1.0093e-03,  1.0093e-03,\n         1.7830e-03,  1.7830e-03,  9.9867e-04,  9.9867e-04,  1.7517e-03,\n         1.7517e-03,  9.8823e-04,  9.8823e-04,  1.7814e-03,  1.7814e-03,\n         9.7303e-04,  9.7303e-04,  1.8066e-03,  1.8066e-03,  9.6253e-04,\n         9.6253e-04,  1.8248e-03,  1.8248e-03,  9.5009e-04,  9.5009e-04,\n         1.8418e-03,  1.8418e-03,  9.3917e-04,  9.3917e-04,  1.8552e-03,\n         1.8552e-03,  9.2741e-04,  9.2741e-04,  1.8686e-03,  1.8686e-03,\n         9.1530e-04,  9.1530e-04,  1.8800e-03,  1.8800e-03,  9.0045e-04,\n         9.0045e-04,  1.8923e-03,  1.8923e-03,  8.8844e-04,  8.8844e-04,\n         1.9023e-03,  1.9023e-03,  8.7566e-04,  8.7566e-04,  1.9130e-03,\n         1.9130e-03,  8.6242e-04,  8.6242e-04,  1.9222e-03,  1.9222e-03,\n         8.5291e-04,  8.5291e-04,  1.9240e-03,  1.9240e-03,  8.3876e-04,\n         8.3876e-04,  1.9326e-03,  1.9326e-03,  8.2293e-04,  8.2293e-04,\n         1.9419e-03,  1.9419e-03,  8.0587e-04,  8.0586e-04,  1.9500e-03,\n         1.9500e-03,  7.8657e-04,  7.8657e-04,  1.9593e-03,  1.9593e-03,\n         7.6491e-04,  7.6491e-04,  1.9672e-03,  1.9672e-03,  7.3967e-04,\n         7.3967e-04,  1.9759e-03,  1.9759e-03,  7.1006e-04,  7.1006e-04,\n         1.9837e-03,  1.9837e-03,  6.8391e-04,  6.8391e-04,  1.9915e-03,\n         1.9915e-03,  6.4556e-04,  6.4556e-04,  1.9990e-03,  1.9990e-03,\n         6.0560e-04,  6.0560e-04,  2.0072e-03,  2.0072e-03,  5.6794e-04,\n         5.6794e-04,  2.0145e-03,  2.0145e-03,  5.3389e-04,  5.3389e-04,\n         2.0231e-03,  2.0231e-03,  5.0061e-04,  5.0061e-04,  2.0303e-03,\n         2.0303e-03,  4.6886e-04,  4.6886e-04,  2.0383e-03,  2.0383e-03,\n         4.3792e-04,  4.3792e-04,  2.0453e-03,  2.0453e-03,  5.6783e-04,\n         5.6783e-04,  2.0173e-03,  2.0173e-03,  5.2119e-04,  5.2119e-04,\n         2.0240e-03,  2.0240e-03,  4.7811e-04,  4.7811e-04,  2.0315e-03,\n         2.0315e-03,  4.3817e-04,  4.3817e-04,  2.0382e-03,  2.0382e-03,\n         4.0075e-04,  4.0075e-04,  2.0462e-03,  2.0462e-03,  3.6411e-04,\n         3.6411e-04,  2.0530e-03,  2.0530e-03,  3.2868e-04,  3.2868e-04,\n         2.0607e-03,  2.0607e-03,  2.9406e-04,  2.9406e-04,  2.0677e-03,\n         2.0677e-03,  2.6706e-04,  2.6706e-04,  2.0752e-03,  2.0752e-03,\n         2.3350e-04,  2.3350e-04,  2.0815e-03,  2.0815e-03,  2.0062e-04,\n         2.0062e-04,  2.0885e-03,  2.0885e-03,  1.6854e-04,  1.6854e-04,\n         2.0945e-03,  2.0945e-03,  1.3799e-04,  1.3799e-04,  2.1019e-03,\n         2.1019e-03,  1.0724e-04,  1.0724e-04,  2.1080e-03,  2.1080e-03,\n         7.7260e-05,  7.7260e-05,  2.1150e-03,  2.1150e-03,  4.7564e-05,\n         4.7564e-05,  2.1210e-03,  2.1210e-03,  4.7815e-05,  4.7814e-05,\n         2.1190e-03,  2.1190e-03,  1.7216e-05,  1.7217e-05,  2.1251e-03,\n         2.1251e-03, -1.2695e-05, -1.2695e-05,  2.1322e-03,  2.1322e-03,\n        -4.1889e-05, -4.1889e-05,  2.1383e-03,  2.1383e-03, -6.9390e-05,\n        -6.9390e-05,  2.1462e-03,  2.1462e-03, -9.6975e-05, -9.6975e-05,\n         2.1525e-03,  2.1525e-03, -1.2361e-04, -1.2361e-04,  2.1606e-03,\n         2.1606e-03, -1.5046e-04, -1.5046e-04,  2.1679e-03,  2.1679e-03,\n        -1.7029e-04, -1.7029e-04,  2.1754e-03,  2.1754e-03, -1.9628e-04,\n        -1.9628e-04,  2.1823e-03,  2.1823e-03, -2.2174e-04, -2.2174e-04,\n         2.1903e-03,  2.1903e-03, -2.4666e-04, -2.4666e-04,  2.1970e-03,\n         2.1970e-03, -2.6997e-04, -2.6997e-04,  2.2056e-03,  2.2056e-03,\n        -2.9371e-04, -2.9371e-04,  2.2123e-03,  2.2123e-03, -3.1664e-04,\n        -3.1664e-04,  2.2202e-03,  2.2202e-03, -3.3924e-04, -3.3924e-04,\n         2.2270e-03,  2.2270e-03], device='cuda:0')\ncustom_layers.8.linear_sigmoid.weight tensor([[-1.7023e-03,  1.7023e-03, -9.0151e-05,  ..., -1.4681e-03,\n         -2.0036e-03,  2.0036e-03],\n        [-1.7023e-03,  1.7023e-03, -9.0151e-05,  ..., -1.4681e-03,\n         -2.0036e-03,  2.0036e-03],\n        [ 1.9687e-03, -1.9687e-03,  9.2622e-04,  ...,  7.5486e-04,\n          3.3268e-03, -3.3268e-03],\n        ...,\n        [-1.4412e-04,  1.4412e-04, -2.4052e-03,  ..., -1.2934e-03,\n         -2.1493e-03,  2.1493e-03],\n        [ 5.2275e-03, -5.2275e-03,  8.0920e-04,  ..., -3.9496e-03,\n          2.1493e-03, -2.1493e-03],\n        [ 5.2275e-03, -5.2275e-03,  8.0920e-04,  ..., -3.9496e-03,\n          2.1493e-03, -2.1493e-03]], device='cuda:0')\ncustom_layers.8.linear_sigmoid.bias tensor([-9.0151e-05, -9.0151e-05,  9.2622e-04,  ..., -2.4052e-03,\n         8.0920e-04,  8.0920e-04], device='cuda:0')\ncustom_layers.9.linear_sigmoid.weight tensor([[-4.0186e-04,  4.0186e-04,  2.3176e-04,  ...,  4.1922e-05,\n         -2.0848e-03,  2.0848e-03]], device='cuda:0')\ncustom_layers.9.linear_sigmoid.bias tensor([0.0002], device='cuda:0')\n","output_type":"stream"}]},{"cell_type":"code","source":"evaluate_model(model_prepu, custom_train_loader, criterion, optimizer)","metadata":{"execution":{"iopub.status.busy":"2024-06-14T14:39:03.709226Z","iopub.execute_input":"2024-06-14T14:39:03.710064Z","iopub.status.idle":"2024-06-14T15:01:07.198926Z","shell.execute_reply.started":"2024-06-14T14:39:03.710020Z","shell.execute_reply":"2024-06-14T15:01:07.197381Z"},"trusted":true},"execution_count":84,"outputs":[{"name":"stdout","text":"Epoch    1 | Train Loss:     1.5059 | Validation Loss:     1.4955\nEpoch    2 | Train Loss:     1.4405 | Validation Loss:     1.4719\nEpoch    3 | Train Loss:     1.4296 | Validation Loss:     1.4593\nEpoch    4 | Train Loss:     1.4177 | Validation Loss:     1.4546\nEpoch    5 | Train Loss:     1.4016 | Validation Loss:     1.4400\nEpoch    6 | Train Loss:     1.3792 | Validation Loss:     1.4043\nEpoch    7 | Train Loss:     1.3296 | Validation Loss:     1.3167\nEpoch    8 | Train Loss:     1.2257 | Validation Loss:     1.2649\nEpoch    9 | Train Loss:     1.1420 | Validation Loss:     1.1086\nEpoch   10 | Train Loss:     1.0061 | Validation Loss:     0.9401\nEpoch   11 | Train Loss:     0.8566 | Validation Loss:     0.8420\nEpoch   12 | Train Loss:     0.7795 | Validation Loss:     0.7778\nEpoch   13 | Train Loss:     0.7539 | Validation Loss:     0.7637\nEpoch   14 | Train Loss:     0.7431 | Validation Loss:     0.7739\nEpoch   15 | Train Loss:     0.7346 | Validation Loss:     0.7497\nEpoch   16 | Train Loss:     0.7304 | Validation Loss:     0.7417\nEpoch   17 | Train Loss:     0.7238 | Validation Loss:     0.7500\nEpoch   18 | Train Loss:     0.7233 | Validation Loss:     0.7322\nEpoch   19 | Train Loss:     0.7166 | Validation Loss:     0.7322\nEpoch   30 | Train Loss:     0.6761 | Validation Loss:     0.6892\nEpoch   31 | Train Loss:     0.6695 | Validation Loss:     0.6808\nEpoch   32 | Train Loss:     0.6634 | Validation Loss:     0.6717\nEpoch   33 | Train Loss:     0.6542 | Validation Loss:     0.6662\nEpoch   34 | Train Loss:     0.6470 | Validation Loss:     0.6577\nEpoch   35 | Train Loss:     0.6416 | Validation Loss:     0.6524\nEpoch   36 | Train Loss:     0.6351 | Validation Loss:     0.6462\nEpoch   37 | Train Loss:     0.6294 | Validation Loss:     0.6364\nEpoch   38 | Train Loss:     0.6255 | Validation Loss:     0.6306\nEpoch   39 | Train Loss:     0.6223 | Validation Loss:     0.6286\nEpoch   40 | Train Loss:     0.6204 | Validation Loss:     0.6291\nEpoch   41 | Train Loss:     0.6183 | Validation Loss:     0.6219\nEpoch   42 | Train Loss:     0.6132 | Validation Loss:     0.6178\nEpoch   43 | Train Loss:     0.6112 | Validation Loss:     0.6188\nEpoch   44 | Train Loss:     0.6096 | Validation Loss:     0.6166\nEpoch   45 | Train Loss:     0.6067 | Validation Loss:     0.6126\nEpoch   46 | Train Loss:     0.6051 | Validation Loss:     0.6224\nEpoch   47 | Train Loss:     0.6023 | Validation Loss:     0.6048\nEpoch   48 | Train Loss:     0.5986 | Validation Loss:     0.6046\nEpoch   49 | Train Loss:     0.5965 | Validation Loss:     0.6002\nEpoch   50 | Train Loss:     0.5940 | Validation Loss:     0.5951\nEpoch   51 | Train Loss:     0.5890 | Validation Loss:     0.5918\nEpoch   52 | Train Loss:     0.5868 | Validation Loss:     0.5881\nEpoch   53 | Train Loss:     0.5821 | Validation Loss:     0.5886\nEpoch   54 | Train Loss:     0.5771 | Validation Loss:     0.5774\nEpoch   55 | Train Loss:     0.5702 | Validation Loss:     0.5717\nEpoch   56 | Train Loss:     0.5635 | Validation Loss:     0.5638\nEpoch   57 | Train Loss:     0.5559 | Validation Loss:     0.5543\nEpoch   58 | Train Loss:     0.5462 | Validation Loss:     0.5439\nEpoch   59 | Train Loss:     0.5352 | Validation Loss:     0.5345\nEpoch   60 | Train Loss:     0.5241 | Validation Loss:     0.5222\nEpoch   61 | Train Loss:     0.5103 | Validation Loss:     0.5062\nEpoch   62 | Train Loss:     0.4941 | Validation Loss:     0.4901\nEpoch   63 | Train Loss:     0.4777 | Validation Loss:     0.4728\nEpoch   64 | Train Loss:     0.4609 | Validation Loss:     0.4568\nEpoch   65 | Train Loss:     0.4434 | Validation Loss:     0.4422\nEpoch   66 | Train Loss:     0.4280 | Validation Loss:     0.4278\nEpoch   67 | Train Loss:     0.4157 | Validation Loss:     0.4176\nEpoch   68 | Train Loss:     0.4053 | Validation Loss:     0.4072\nEpoch   69 | Train Loss:     0.3977 | Validation Loss:     0.4022\nEpoch   70 | Train Loss:     0.3903 | Validation Loss:     0.3940\nEpoch   71 | Train Loss:     0.3833 | Validation Loss:     0.3861\nEpoch   72 | Train Loss:     0.3778 | Validation Loss:     0.3817\nEpoch   73 | Train Loss:     0.3743 | Validation Loss:     0.3815\nEpoch   74 | Train Loss:     0.3717 | Validation Loss:     0.3770\nEpoch   75 | Train Loss:     0.3686 | Validation Loss:     0.3725\nEpoch   76 | Train Loss:     0.3651 | Validation Loss:     0.3699\nEpoch   77 | Train Loss:     0.3619 | Validation Loss:     0.3654\nEpoch   78 | Train Loss:     0.3577 | Validation Loss:     0.3603\nEpoch   79 | Train Loss:     0.3544 | Validation Loss:     0.3593\nEpoch   80 | Train Loss:     0.3504 | Validation Loss:     0.3515\nEpoch   81 | Train Loss:     0.3457 | Validation Loss:     0.3486\nEpoch   82 | Train Loss:     0.3425 | Validation Loss:     0.3450\nEpoch   83 | Train Loss:     0.3394 | Validation Loss:     0.3413\nEpoch   84 | Train Loss:     0.3374 | Validation Loss:     0.3394\nEpoch   85 | Train Loss:     0.3359 | Validation Loss:     0.3379\nEpoch   86 | Train Loss:     0.3352 | Validation Loss:     0.3372\nEpoch   87 | Train Loss:     0.3346 | Validation Loss:     0.3365\nEpoch   88 | Train Loss:     0.3342 | Validation Loss:     0.3366\nEpoch   89 | Train Loss:     0.3339 | Validation Loss:     0.3359\nEpoch   90 | Train Loss:     0.3339 | Validation Loss:     0.3359\nEpoch   91 | Train Loss:     0.3336 | Validation Loss:     0.3357\nEpoch   92 | Train Loss:     0.3336 | Validation Loss:     0.3357\nEpoch   93 | Train Loss:     0.3336 | Validation Loss:     0.3357\nEpoch   94 | Train Loss:     0.3335 | Validation Loss:     0.3357\nEpoch   95 | Train Loss:     0.3334 | Validation Loss:     0.3357\nEpoch   96 | Train Loss:     0.3335 | Validation Loss:     0.3356\nEpoch   97 | Train Loss:     0.3334 | Validation Loss:     0.3356\nEpoch   98 | Train Loss:     0.3334 | Validation Loss:     0.3355\nEpoch   99 | Train Loss:     0.3337 | Validation Loss:     0.3357\nEpoch  100 | Train Loss:     0.3335 | Validation Loss:     0.3355\nEpoch  101 | Train Loss:     0.3334 | Validation Loss:     0.3355\nEpoch  102 | Train Loss:     0.3334 | Validation Loss:     0.3356\nEpoch  103 | Train Loss:     0.3333 | Validation Loss:     0.3356\nEpoch  104 | Train Loss:     0.3334 | Validation Loss:     0.3356\nEpoch  105 | Train Loss:     0.3333 | Validation Loss:     0.3356\nEpoch  106 | Train Loss:     0.3337 | Validation Loss:     0.3359\nEpoch  107 | Train Loss:     0.3334 | Validation Loss:     0.3357\nEpoch  108 | Train Loss:     0.3349 | Validation Loss:     0.3354\nEpoch  109 | Train Loss:     0.3333 | Validation Loss:     0.3354\nEpoch  110 | Train Loss:     0.3332 | Validation Loss:     0.3355\nEpoch  111 | Train Loss:     0.3332 | Validation Loss:     0.3354\nEpoch  112 | Train Loss:     0.3332 | Validation Loss:     0.3356\nEpoch  113 | Train Loss:     0.3335 | Validation Loss:     0.3371\nEpoch  114 | Train Loss:     0.3344 | Validation Loss:     0.3359\nEpoch  115 | Train Loss:     0.3336 | Validation Loss:     0.3355\nEpoch  116 | Train Loss:     0.3332 | Validation Loss:     0.3354\nEpoch  117 | Train Loss:     0.3332 | Validation Loss:     0.3354\nEpoch  118 | Train Loss:     0.3332 | Validation Loss:     0.3354\nEpoch  119 | Train Loss:     0.3332 | Validation Loss:     0.3354\nEpoch  120 | Train Loss:     0.3332 | Validation Loss:     0.3354\nEpoch  121 | Train Loss:     0.3332 | Validation Loss:     0.3357\nEpoch  122 | Train Loss:     0.3333 | Validation Loss:     0.3355\nEpoch  123 | Train Loss:     0.3333 | Validation Loss:     0.3356\nEpoch  124 | Train Loss:     0.3333 | Validation Loss:     0.3354\nEpoch  125 | Train Loss:     0.3334 | Validation Loss:     0.3355\nEpoch  126 | Train Loss:     0.3332 | Validation Loss:     0.3354\nEpoch  127 | Train Loss:     0.3356 | Validation Loss:     0.3356\nEpoch  128 | Train Loss:     0.3332 | Validation Loss:     0.3354\nEpoch  129 | Train Loss:     0.3332 | Validation Loss:     0.3356\nEpoch  130 | Train Loss:     0.3332 | Validation Loss:     0.3353\nEpoch  131 | Train Loss:     0.3333 | Validation Loss:     0.3354\nEpoch  132 | Train Loss:     0.3331 | Validation Loss:     0.3354\nEpoch  133 | Train Loss:     0.3332 | Validation Loss:     0.3359\nEpoch  134 | Train Loss:     0.3332 | Validation Loss:     0.3354\nEpoch  135 | Train Loss:     0.3332 | Validation Loss:     0.3353\nEpoch  136 | Train Loss:     0.3332 | Validation Loss:     0.3354\nEpoch  137 | Train Loss:     0.3349 | Validation Loss:     0.3356\nEpoch  138 | Train Loss:     0.3334 | Validation Loss:     0.3354\nEpoch  139 | Train Loss:     0.3332 | Validation Loss:     0.3354\nEpoch  140 | Train Loss:     0.3331 | Validation Loss:     0.3354\nEpoch  141 | Train Loss:     0.3331 | Validation Loss:     0.3353\nEpoch  142 | Train Loss:     0.3331 | Validation Loss:     0.3353\nEpoch  143 | Train Loss:     0.3339 | Validation Loss:     0.3360\nEpoch  144 | Train Loss:     0.3353 | Validation Loss:     0.3356\nEpoch  145 | Train Loss:     0.3332 | Validation Loss:     0.3353\nEpoch  146 | Train Loss:     0.3331 | Validation Loss:     0.3353\nEpoch  147 | Train Loss:     0.3331 | Validation Loss:     0.3353\nEpoch  148 | Train Loss:     0.3331 | Validation Loss:     0.3354\nEpoch  149 | Train Loss:     0.3332 | Validation Loss:     0.3353\nEpoch  150 | Train Loss:     0.3331 | Validation Loss:     0.3354\nEpoch  151 | Train Loss:     0.3332 | Validation Loss:     0.3353\nEpoch  152 | Train Loss:     0.3333 | Validation Loss:     0.3359\nEpoch  153 | Train Loss:     0.3332 | Validation Loss:     0.3353\nEpoch  154 | Train Loss:     0.3334 | Validation Loss:     0.3366\nEpoch  155 | Train Loss:     0.3342 | Validation Loss:     0.3358\nEpoch  156 | Train Loss:     0.3333 | Validation Loss:     0.3353\nEpoch  157 | Train Loss:     0.3332 | Validation Loss:     0.3357\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","Cell \u001b[0;32mIn[84], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mevaluate_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_prepu\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcustom_train_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m)\u001b[49m\n","Cell \u001b[0;32mIn[50], line 87\u001b[0m, in \u001b[0;36mevaluate_model\u001b[0;34m(model, custom_train_loader, criterion, optimizer)\u001b[0m\n\u001b[1;32m     85\u001b[0m parameters \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m     86\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m name, param \u001b[38;5;129;01min\u001b[39;00m model\u001b[38;5;241m.\u001b[39mnamed_parameters():\n\u001b[0;32m---> 87\u001b[0m     parameters\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mparam\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     89\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[1;32m     90\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/_tensor.py:934\u001b[0m, in \u001b[0;36mTensor.__format__\u001b[0;34m(self, format_spec)\u001b[0m\n\u001b[1;32m    932\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdim() \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mis_meta \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;129;01mis\u001b[39;00m Tensor:\n\u001b[1;32m    933\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mitem()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__format__\u001b[39m(format_spec)\n\u001b[0;32m--> 934\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mobject\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__format__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mformat_spec\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/parameter.py:63\u001b[0m, in \u001b[0;36mParameter.__repr__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     62\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__repr__\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m---> 63\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mParameter containing:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__repr__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/_tensor.py:431\u001b[0m, in \u001b[0;36mTensor.__repr__\u001b[0;34m(self, tensor_contents)\u001b[0m\n\u001b[1;32m    427\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    428\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__repr__\u001b[39m, (\u001b[38;5;28mself\u001b[39m,), \u001b[38;5;28mself\u001b[39m, tensor_contents\u001b[38;5;241m=\u001b[39mtensor_contents\n\u001b[1;32m    429\u001b[0m     )\n\u001b[1;32m    430\u001b[0m \u001b[38;5;66;03m# All strings are unicode in Python 3.\u001b[39;00m\n\u001b[0;32m--> 431\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_tensor_str\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_str\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtensor_contents\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtensor_contents\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/_tensor_str.py:664\u001b[0m, in \u001b[0;36m_str\u001b[0;34m(self, tensor_contents)\u001b[0m\n\u001b[1;32m    662\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad(), torch\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39m_python_dispatch\u001b[38;5;241m.\u001b[39m_disable_current_modes():\n\u001b[1;32m    663\u001b[0m     guard \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39m_C\u001b[38;5;241m.\u001b[39m_DisableFuncTorch()\n\u001b[0;32m--> 664\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_str_intern\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtensor_contents\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtensor_contents\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/_tensor_str.py:595\u001b[0m, in \u001b[0;36m_str_intern\u001b[0;34m(inp, tensor_contents)\u001b[0m\n\u001b[1;32m    593\u001b[0m                     tensor_str \u001b[38;5;241m=\u001b[39m _tensor_str(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mto_dense(), indent)\n\u001b[1;32m    594\u001b[0m                 \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 595\u001b[0m                     tensor_str \u001b[38;5;241m=\u001b[39m \u001b[43m_tensor_str\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindent\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    597\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayout \u001b[38;5;241m!=\u001b[39m torch\u001b[38;5;241m.\u001b[39mstrided:\n\u001b[1;32m    598\u001b[0m     suffixes\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlayout=\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayout))\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/_tensor_str.py:347\u001b[0m, in \u001b[0;36m_tensor_str\u001b[0;34m(self, indent)\u001b[0m\n\u001b[1;32m    343\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _tensor_str_with_formatter(\n\u001b[1;32m    344\u001b[0m         \u001b[38;5;28mself\u001b[39m, indent, summarize, real_formatter, imag_formatter\n\u001b[1;32m    345\u001b[0m     )\n\u001b[1;32m    346\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 347\u001b[0m     formatter \u001b[38;5;241m=\u001b[39m \u001b[43m_Formatter\u001b[49m\u001b[43m(\u001b[49m\u001b[43mget_summarized_data\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43msummarize\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    348\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _tensor_str_with_formatter(\u001b[38;5;28mself\u001b[39m, indent, summarize, formatter)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/_tensor_str.py:138\u001b[0m, in \u001b[0;36m_Formatter.__init__\u001b[0;34m(self, tensor)\u001b[0m\n\u001b[1;32m    134\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmax_width \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmax\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmax_width, \u001b[38;5;28mlen\u001b[39m(value_str))\n\u001b[1;32m    136\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    137\u001b[0m     nonzero_finite_vals \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mmasked_select(\n\u001b[0;32m--> 138\u001b[0m         tensor_view, \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43misfinite\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtensor_view\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;241m&\u001b[39m tensor_view\u001b[38;5;241m.\u001b[39mne(\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m    139\u001b[0m     )\n\u001b[1;32m    141\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m nonzero_finite_vals\u001b[38;5;241m.\u001b[39mnumel() \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    142\u001b[0m         \u001b[38;5;66;03m# no valid number, do nothing\u001b[39;00m\n\u001b[1;32m    143\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "],"ename":"KeyboardInterrupt","evalue":"","output_type":"error"}]},{"cell_type":"code","source":"evaluate_model(model_prepu, custom_train_loader, criterion, optimizer)","metadata":{"execution":{"iopub.status.busy":"2024-06-14T14:37:38.632888Z","iopub.execute_input":"2024-06-14T14:37:38.633791Z","iopub.status.idle":"2024-06-14T14:38:06.569794Z","shell.execute_reply.started":"2024-06-14T14:37:38.633757Z","shell.execute_reply":"2024-06-14T14:38:06.568553Z"},"collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":82,"outputs":[{"name":"stdout","text":"Epoch    1 | Train Loss:     1.7084 | Validation Loss:     1.6841\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","Cell \u001b[0;32mIn[82], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mevaluate_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_prepu\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcustom_train_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m)\u001b[49m\n","Cell \u001b[0;32mIn[50], line 87\u001b[0m, in \u001b[0;36mevaluate_model\u001b[0;34m(model, custom_train_loader, criterion, optimizer)\u001b[0m\n\u001b[1;32m     85\u001b[0m parameters \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m     86\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m name, param \u001b[38;5;129;01min\u001b[39;00m model\u001b[38;5;241m.\u001b[39mnamed_parameters():\n\u001b[0;32m---> 87\u001b[0m     parameters\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mparam\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     89\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[1;32m     90\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/_tensor.py:934\u001b[0m, in \u001b[0;36mTensor.__format__\u001b[0;34m(self, format_spec)\u001b[0m\n\u001b[1;32m    932\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdim() \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mis_meta \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;129;01mis\u001b[39;00m Tensor:\n\u001b[1;32m    933\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mitem()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__format__\u001b[39m(format_spec)\n\u001b[0;32m--> 934\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mobject\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__format__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mformat_spec\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/parameter.py:63\u001b[0m, in \u001b[0;36mParameter.__repr__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     62\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__repr__\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m---> 63\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mParameter containing:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__repr__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/_tensor.py:431\u001b[0m, in \u001b[0;36mTensor.__repr__\u001b[0;34m(self, tensor_contents)\u001b[0m\n\u001b[1;32m    427\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    428\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__repr__\u001b[39m, (\u001b[38;5;28mself\u001b[39m,), \u001b[38;5;28mself\u001b[39m, tensor_contents\u001b[38;5;241m=\u001b[39mtensor_contents\n\u001b[1;32m    429\u001b[0m     )\n\u001b[1;32m    430\u001b[0m \u001b[38;5;66;03m# All strings are unicode in Python 3.\u001b[39;00m\n\u001b[0;32m--> 431\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_tensor_str\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_str\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtensor_contents\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtensor_contents\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/_tensor_str.py:664\u001b[0m, in \u001b[0;36m_str\u001b[0;34m(self, tensor_contents)\u001b[0m\n\u001b[1;32m    662\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad(), torch\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39m_python_dispatch\u001b[38;5;241m.\u001b[39m_disable_current_modes():\n\u001b[1;32m    663\u001b[0m     guard \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39m_C\u001b[38;5;241m.\u001b[39m_DisableFuncTorch()\n\u001b[0;32m--> 664\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_str_intern\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtensor_contents\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtensor_contents\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/_tensor_str.py:595\u001b[0m, in \u001b[0;36m_str_intern\u001b[0;34m(inp, tensor_contents)\u001b[0m\n\u001b[1;32m    593\u001b[0m                     tensor_str \u001b[38;5;241m=\u001b[39m _tensor_str(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mto_dense(), indent)\n\u001b[1;32m    594\u001b[0m                 \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 595\u001b[0m                     tensor_str \u001b[38;5;241m=\u001b[39m \u001b[43m_tensor_str\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindent\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    597\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayout \u001b[38;5;241m!=\u001b[39m torch\u001b[38;5;241m.\u001b[39mstrided:\n\u001b[1;32m    598\u001b[0m     suffixes\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlayout=\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayout))\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/_tensor_str.py:347\u001b[0m, in \u001b[0;36m_tensor_str\u001b[0;34m(self, indent)\u001b[0m\n\u001b[1;32m    343\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _tensor_str_with_formatter(\n\u001b[1;32m    344\u001b[0m         \u001b[38;5;28mself\u001b[39m, indent, summarize, real_formatter, imag_formatter\n\u001b[1;32m    345\u001b[0m     )\n\u001b[1;32m    346\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 347\u001b[0m     formatter \u001b[38;5;241m=\u001b[39m \u001b[43m_Formatter\u001b[49m\u001b[43m(\u001b[49m\u001b[43mget_summarized_data\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43msummarize\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    348\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _tensor_str_with_formatter(\u001b[38;5;28mself\u001b[39m, indent, summarize, formatter)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/_tensor_str.py:138\u001b[0m, in \u001b[0;36m_Formatter.__init__\u001b[0;34m(self, tensor)\u001b[0m\n\u001b[1;32m    134\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmax_width \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmax\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmax_width, \u001b[38;5;28mlen\u001b[39m(value_str))\n\u001b[1;32m    136\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    137\u001b[0m     nonzero_finite_vals \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mmasked_select(\n\u001b[0;32m--> 138\u001b[0m         tensor_view, \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43misfinite\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtensor_view\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;241m&\u001b[39m tensor_view\u001b[38;5;241m.\u001b[39mne(\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m    139\u001b[0m     )\n\u001b[1;32m    141\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m nonzero_finite_vals\u001b[38;5;241m.\u001b[39mnumel() \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    142\u001b[0m         \u001b[38;5;66;03m# no valid number, do nothing\u001b[39;00m\n\u001b[1;32m    143\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "],"ename":"KeyboardInterrupt","evalue":"","output_type":"error"}]},{"cell_type":"code","source":"evaluate_model(model_prepu, custom_train_loader, criterion, optimizer)","metadata":{"execution":{"iopub.status.busy":"2024-06-14T14:00:22.594084Z","iopub.execute_input":"2024-06-14T14:00:22.594453Z","iopub.status.idle":"2024-06-14T14:32:39.231877Z","shell.execute_reply.started":"2024-06-14T14:00:22.594422Z","shell.execute_reply":"2024-06-14T14:32:39.229234Z"},"collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":72,"outputs":[{"name":"stdout","text":"Epoch    1 | Train Loss:     1.5841 | Validation Loss:     1.5215\nEpoch    2 | Train Loss:     1.4668 | Validation Loss:     1.5017\nEpoch    3 | Train Loss:     1.4520 | Validation Loss:     1.4924\nEpoch    4 | Train Loss:     1.4438 | Validation Loss:     1.4815\nEpoch    5 | Train Loss:     1.4379 | Validation Loss:     1.4763\nEpoch    6 | Train Loss:     1.4331 | Validation Loss:     1.4726\nEpoch    7 | Train Loss:     1.4292 | Validation Loss:     1.4637\nEpoch    8 | Train Loss:     1.4250 | Validation Loss:     1.4631\nEpoch    9 | Train Loss:     1.4207 | Validation Loss:     1.4555\nEpoch   10 | Train Loss:     1.4154 | Validation Loss:     1.4591\nEpoch   11 | Train Loss:     1.4108 | Validation Loss:     1.4448\nEpoch   12 | Train Loss:     1.4039 | Validation Loss:     1.4398\nEpoch   13 | Train Loss:     1.3987 | Validation Loss:     1.4351\nEpoch   14 | Train Loss:     1.3904 | Validation Loss:     1.4238\nEpoch   15 | Train Loss:     1.3812 | Validation Loss:     1.4233\nEpoch   16 | Train Loss:     1.3705 | Validation Loss:     1.4162\nEpoch   17 | Train Loss:     1.3551 | Validation Loss:     1.3860\nEpoch   18 | Train Loss:     1.3335 | Validation Loss:     1.3571\nEpoch   19 | Train Loss:     1.2998 | Validation Loss:     1.3241\nEpoch   20 | Train Loss:     1.2568 | Validation Loss:     1.2707\nEpoch   21 | Train Loss:     1.2075 | Validation Loss:     1.2209\nEpoch   22 | Train Loss:     1.1669 | Validation Loss:     1.1778\nEpoch   23 | Train Loss:     1.1203 | Validation Loss:     1.1243\nEpoch   24 | Train Loss:     1.0702 | Validation Loss:     1.0774\nEpoch   25 | Train Loss:     1.0104 | Validation Loss:     1.0001\nEpoch   26 | Train Loss:     0.9442 | Validation Loss:     0.9327\nEpoch   27 | Train Loss:     0.8834 | Validation Loss:     0.8769\nEpoch   28 | Train Loss:     0.8352 | Validation Loss:     0.8356\nEpoch   29 | Train Loss:     0.8046 | Validation Loss:     0.8108\nEpoch   30 | Train Loss:     0.7838 | Validation Loss:     0.7905\nEpoch   31 | Train Loss:     0.7691 | Validation Loss:     0.7932\nEpoch   32 | Train Loss:     0.7619 | Validation Loss:     0.7721\nEpoch   33 | Train Loss:     0.7536 | Validation Loss:     0.7669\nEpoch   34 | Train Loss:     0.7478 | Validation Loss:     0.7653\nEpoch   35 | Train Loss:     0.7437 | Validation Loss:     0.7589\nEpoch   36 | Train Loss:     0.7419 | Validation Loss:     0.7637\nEpoch   37 | Train Loss:     0.7394 | Validation Loss:     0.7564\nEpoch   38 | Train Loss:     0.7351 | Validation Loss:     0.7538\nEpoch   39 | Train Loss:     0.7339 | Validation Loss:     0.7493\nEpoch   40 | Train Loss:     0.7319 | Validation Loss:     0.7495\nEpoch   41 | Train Loss:     0.7298 | Validation Loss:     0.7538\nEpoch   42 | Train Loss:     0.7272 | Validation Loss:     0.7442\nEpoch   43 | Train Loss:     0.7260 | Validation Loss:     0.7435\nEpoch   44 | Train Loss:     0.7248 | Validation Loss:     0.7412\nEpoch   45 | Train Loss:     0.7236 | Validation Loss:     0.7399\nEpoch   46 | Train Loss:     0.7212 | Validation Loss:     0.7402\nEpoch   47 | Train Loss:     0.7202 | Validation Loss:     0.7416\nEpoch   48 | Train Loss:     0.7189 | Validation Loss:     0.7366\nEpoch   49 | Train Loss:     0.7167 | Validation Loss:     0.7356\nEpoch   50 | Train Loss:     0.7157 | Validation Loss:     0.7337\nEpoch   51 | Train Loss:     0.7151 | Validation Loss:     0.7327\nEpoch   52 | Train Loss:     0.7130 | Validation Loss:     0.7342\nEpoch   53 | Train Loss:     0.7134 | Validation Loss:     0.7309\nEpoch   65 | Train Loss:     0.6977 | Validation Loss:     0.7144\nEpoch   66 | Train Loss:     0.6953 | Validation Loss:     0.7120\nEpoch   67 | Train Loss:     0.6945 | Validation Loss:     0.7100\nEpoch   68 | Train Loss:     0.6937 | Validation Loss:     0.7094\nEpoch   69 | Train Loss:     0.6904 | Validation Loss:     0.7117\nEpoch   70 | Train Loss:     0.6881 | Validation Loss:     0.7052\nEpoch   71 | Train Loss:     0.6849 | Validation Loss:     0.7047\nEpoch   72 | Train Loss:     0.6850 | Validation Loss:     0.7050\nEpoch   73 | Train Loss:     0.6816 | Validation Loss:     0.6993\nEpoch   74 | Train Loss:     0.6785 | Validation Loss:     0.6939\nEpoch   75 | Train Loss:     0.6770 | Validation Loss:     0.6907\nEpoch   76 | Train Loss:     0.6736 | Validation Loss:     0.6905\nEpoch   77 | Train Loss:     0.6696 | Validation Loss:     0.6931\nEpoch   78 | Train Loss:     0.6666 | Validation Loss:     0.6805\nEpoch   79 | Train Loss:     0.6633 | Validation Loss:     0.6800\nEpoch   80 | Train Loss:     0.6608 | Validation Loss:     0.6745\nEpoch   81 | Train Loss:     0.6570 | Validation Loss:     0.6746\nEpoch   82 | Train Loss:     0.6536 | Validation Loss:     0.6686\nEpoch   83 | Train Loss:     0.6503 | Validation Loss:     0.6618\nEpoch   84 | Train Loss:     0.6471 | Validation Loss:     0.6601\nEpoch   85 | Train Loss:     0.6435 | Validation Loss:     0.6542\nEpoch   86 | Train Loss:     0.6405 | Validation Loss:     0.6550\nEpoch   87 | Train Loss:     0.6385 | Validation Loss:     0.6502\nEpoch   88 | Train Loss:     0.6360 | Validation Loss:     0.6465\nEpoch   89 | Train Loss:     0.6337 | Validation Loss:     0.6452\nEpoch   90 | Train Loss:     0.6329 | Validation Loss:     0.6455\nEpoch   91 | Train Loss:     0.6300 | Validation Loss:     0.6403\nEpoch   92 | Train Loss:     0.6280 | Validation Loss:     0.6396\nEpoch   93 | Train Loss:     0.6267 | Validation Loss:     0.6357\nEpoch   94 | Train Loss:     0.6256 | Validation Loss:     0.6346\nEpoch   95 | Train Loss:     0.6244 | Validation Loss:     0.6323\nEpoch   96 | Train Loss:     0.6229 | Validation Loss:     0.6303\nEpoch   97 | Train Loss:     0.6222 | Validation Loss:     0.6302\nEpoch   98 | Train Loss:     0.6211 | Validation Loss:     0.6310\nEpoch   99 | Train Loss:     0.6197 | Validation Loss:     0.6269\nEpoch  100 | Train Loss:     0.6197 | Validation Loss:     0.6263\nEpoch  101 | Train Loss:     0.6183 | Validation Loss:     0.6243\nEpoch  102 | Train Loss:     0.6174 | Validation Loss:     0.6232\nEpoch  103 | Train Loss:     0.6162 | Validation Loss:     0.6231\nEpoch  104 | Train Loss:     0.6155 | Validation Loss:     0.6216\nEpoch  105 | Train Loss:     0.6145 | Validation Loss:     0.6225\nEpoch  106 | Train Loss:     0.6137 | Validation Loss:     0.6189\nEpoch  107 | Train Loss:     0.6134 | Validation Loss:     0.6184\nEpoch  108 | Train Loss:     0.6115 | Validation Loss:     0.6189\nEpoch  109 | Train Loss:     0.6108 | Validation Loss:     0.6161\nEpoch  110 | Train Loss:     0.6098 | Validation Loss:     0.6169\nEpoch  111 | Train Loss:     0.6097 | Validation Loss:     0.6137\nEpoch  112 | Train Loss:     0.6081 | Validation Loss:     0.6127\nEpoch  113 | Train Loss:     0.6075 | Validation Loss:     0.6129\nEpoch  114 | Train Loss:     0.6068 | Validation Loss:     0.6116\nEpoch  115 | Train Loss:     0.6061 | Validation Loss:     0.6102\nEpoch  116 | Train Loss:     0.6049 | Validation Loss:     0.6085\nEpoch  117 | Train Loss:     0.6040 | Validation Loss:     0.6086\nEpoch  118 | Train Loss:     0.6027 | Validation Loss:     0.6078\nEpoch  119 | Train Loss:     0.6018 | Validation Loss:     0.6058\nEpoch  120 | Train Loss:     0.6009 | Validation Loss:     0.6051\nEpoch  121 | Train Loss:     0.5998 | Validation Loss:     0.6036\nEpoch  122 | Train Loss:     0.5991 | Validation Loss:     0.6040\nEpoch  123 | Train Loss:     0.5979 | Validation Loss:     0.6014\nEpoch  124 | Train Loss:     0.5965 | Validation Loss:     0.6005\nEpoch  125 | Train Loss:     0.5958 | Validation Loss:     0.5985\nEpoch  126 | Train Loss:     0.5941 | Validation Loss:     0.5983\nEpoch  127 | Train Loss:     0.5923 | Validation Loss:     0.5955\nEpoch  128 | Train Loss:     0.5904 | Validation Loss:     0.5950\nEpoch  129 | Train Loss:     0.5889 | Validation Loss:     0.5918\nEpoch  130 | Train Loss:     0.5870 | Validation Loss:     0.5903\nEpoch  131 | Train Loss:     0.5848 | Validation Loss:     0.5888\nEpoch  132 | Train Loss:     0.5824 | Validation Loss:     0.5852\nEpoch  133 | Train Loss:     0.5797 | Validation Loss:     0.5828\nEpoch  134 | Train Loss:     0.5769 | Validation Loss:     0.5787\nEpoch  135 | Train Loss:     0.5736 | Validation Loss:     0.5768\nEpoch  136 | Train Loss:     0.5698 | Validation Loss:     0.5711\nEpoch  137 | Train Loss:     0.5657 | Validation Loss:     0.5677\nEpoch  138 | Train Loss:     0.5613 | Validation Loss:     0.5619\nEpoch  139 | Train Loss:     0.5559 | Validation Loss:     0.5568\nEpoch  140 | Train Loss:     0.5501 | Validation Loss:     0.5507\nEpoch  141 | Train Loss:     0.5439 | Validation Loss:     0.5448\nEpoch  142 | Train Loss:     0.5370 | Validation Loss:     0.5370\nEpoch  143 | Train Loss:     0.5293 | Validation Loss:     0.5304\nEpoch  144 | Train Loss:     0.5208 | Validation Loss:     0.5218\nEpoch  145 | Train Loss:     0.5119 | Validation Loss:     0.5115\nEpoch  146 | Train Loss:     0.5019 | Validation Loss:     0.5017\nEpoch  147 | Train Loss:     0.4918 | Validation Loss:     0.4921\nEpoch  148 | Train Loss:     0.4814 | Validation Loss:     0.4812\nEpoch  149 | Train Loss:     0.4704 | Validation Loss:     0.4711\nEpoch  150 | Train Loss:     0.4595 | Validation Loss:     0.4602\nEpoch  151 | Train Loss:     0.4488 | Validation Loss:     0.4501\nEpoch  152 | Train Loss:     0.4390 | Validation Loss:     0.4406\nEpoch  153 | Train Loss:     0.4295 | Validation Loss:     0.4323\nEpoch  154 | Train Loss:     0.4209 | Validation Loss:     0.4245\nEpoch  155 | Train Loss:     0.4135 | Validation Loss:     0.4172\nEpoch  156 | Train Loss:     0.4066 | Validation Loss:     0.4109\nEpoch  157 | Train Loss:     0.4005 | Validation Loss:     0.4049\nEpoch  158 | Train Loss:     0.3946 | Validation Loss:     0.3985\nEpoch  159 | Train Loss:     0.3883 | Validation Loss:     0.3913\nEpoch  160 | Train Loss:     0.3817 | Validation Loss:     0.3846\nEpoch  161 | Train Loss:     0.3763 | Validation Loss:     0.3796\nEpoch  162 | Train Loss:     0.3719 | Validation Loss:     0.3757\nEpoch  163 | Train Loss:     0.3683 | Validation Loss:     0.3720\nEpoch  164 | Train Loss:     0.3651 | Validation Loss:     0.3689\nEpoch  165 | Train Loss:     0.3623 | Validation Loss:     0.3667\nEpoch  166 | Train Loss:     0.3595 | Validation Loss:     0.3631\nEpoch  167 | Train Loss:     0.3567 | Validation Loss:     0.3602\nEpoch  168 | Train Loss:     0.3539 | Validation Loss:     0.3567\nEpoch  169 | Train Loss:     0.3511 | Validation Loss:     0.3536\nEpoch  170 | Train Loss:     0.3484 | Validation Loss:     0.3512\nEpoch  171 | Train Loss:     0.3462 | Validation Loss:     0.3490\nEpoch  172 | Train Loss:     0.3443 | Validation Loss:     0.3469\nEpoch  173 | Train Loss:     0.3428 | Validation Loss:     0.3455\nEpoch  174 | Train Loss:     0.3413 | Validation Loss:     0.3439\nEpoch  175 | Train Loss:     0.3402 | Validation Loss:     0.3426\nEpoch  176 | Train Loss:     0.3391 | Validation Loss:     0.3416\nEpoch  177 | Train Loss:     0.3383 | Validation Loss:     0.3407\nEpoch  178 | Train Loss:     0.3375 | Validation Loss:     0.3399\nEpoch  179 | Train Loss:     0.3370 | Validation Loss:     0.3392\nEpoch  180 | Train Loss:     0.3364 | Validation Loss:     0.3388\nEpoch  181 | Train Loss:     0.3360 | Validation Loss:     0.3382\nEpoch  182 | Train Loss:     0.3356 | Validation Loss:     0.3379\nEpoch  183 | Train Loss:     0.3353 | Validation Loss:     0.3375\nEpoch  184 | Train Loss:     0.3350 | Validation Loss:     0.3372\nEpoch  185 | Train Loss:     0.3348 | Validation Loss:     0.3370\nEpoch  186 | Train Loss:     0.3346 | Validation Loss:     0.3369\nEpoch  187 | Train Loss:     0.3344 | Validation Loss:     0.3366\nEpoch  188 | Train Loss:     0.3344 | Validation Loss:     0.3367\nEpoch  189 | Train Loss:     0.3342 | Validation Loss:     0.3364\nEpoch  190 | Train Loss:     0.3341 | Validation Loss:     0.3364\nEpoch  191 | Train Loss:     0.3340 | Validation Loss:     0.3362\nEpoch  192 | Train Loss:     0.3339 | Validation Loss:     0.3361\nEpoch  193 | Train Loss:     0.3338 | Validation Loss:     0.3360\nEpoch  194 | Train Loss:     0.3338 | Validation Loss:     0.3360\nEpoch  195 | Train Loss:     0.3337 | Validation Loss:     0.3359\nEpoch  196 | Train Loss:     0.3337 | Validation Loss:     0.3359\nEpoch  207 | Train Loss:     0.3335 | Validation Loss:     0.3356\nEpoch  208 | Train Loss:     0.3335 | Validation Loss:     0.3356\nEpoch  209 | Train Loss:     0.3334 | Validation Loss:     0.3356\nEpoch  210 | Train Loss:     0.3334 | Validation Loss:     0.3356\nEpoch  211 | Train Loss:     0.3334 | Validation Loss:     0.3355\nEpoch  212 | Train Loss:     0.3334 | Validation Loss:     0.3355\nEpoch  213 | Train Loss:     0.3333 | Validation Loss:     0.3355\nEpoch  214 | Train Loss:     0.3334 | Validation Loss:     0.3355\nEpoch  215 | Train Loss:     0.3333 | Validation Loss:     0.3355\nEpoch  216 | Train Loss:     0.3333 | Validation Loss:     0.3355\nEpoch  217 | Train Loss:     0.3333 | Validation Loss:     0.3355\nEpoch  218 | Train Loss:     0.3334 | Validation Loss:     0.3355\nEpoch  219 | Train Loss:     0.3333 | Validation Loss:     0.3355\nEpoch  220 | Train Loss:     0.3333 | Validation Loss:     0.3355\nEpoch  221 | Train Loss:     0.3333 | Validation Loss:     0.3355\nEpoch  222 | Train Loss:     0.3333 | Validation Loss:     0.3355\nEpoch  223 | Train Loss:     0.3333 | Validation Loss:     0.3356\nEpoch  224 | Train Loss:     0.3333 | Validation Loss:     0.3355\nEpoch  225 | Train Loss:     0.3333 | Validation Loss:     0.3355\nEpoch  226 | Train Loss:     0.3333 | Validation Loss:     0.3355\nEpoch  227 | Train Loss:     0.3333 | Validation Loss:     0.3354\nEpoch  228 | Train Loss:     0.3333 | Validation Loss:     0.3355\nEpoch  229 | Train Loss:     0.3333 | Validation Loss:     0.3355\nEpoch  230 | Train Loss:     0.3333 | Validation Loss:     0.3354\nEpoch  231 | Train Loss:     0.3333 | Validation Loss:     0.3355\nEpoch  232 | Train Loss:     0.3333 | Validation Loss:     0.3354\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","Cell \u001b[0;32mIn[72], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mevaluate_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_prepu\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcustom_train_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m)\u001b[49m\n","Cell \u001b[0;32mIn[50], line 87\u001b[0m, in \u001b[0;36mevaluate_model\u001b[0;34m(model, custom_train_loader, criterion, optimizer)\u001b[0m\n\u001b[1;32m     85\u001b[0m parameters \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m     86\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m name, param \u001b[38;5;129;01min\u001b[39;00m model\u001b[38;5;241m.\u001b[39mnamed_parameters():\n\u001b[0;32m---> 87\u001b[0m     parameters\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mparam\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     89\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[1;32m     90\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/_tensor.py:934\u001b[0m, in \u001b[0;36mTensor.__format__\u001b[0;34m(self, format_spec)\u001b[0m\n\u001b[1;32m    932\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdim() \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mis_meta \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;129;01mis\u001b[39;00m Tensor:\n\u001b[1;32m    933\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mitem()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__format__\u001b[39m(format_spec)\n\u001b[0;32m--> 934\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mobject\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__format__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mformat_spec\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/parameter.py:63\u001b[0m, in \u001b[0;36mParameter.__repr__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     62\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__repr__\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m---> 63\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mParameter containing:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__repr__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/_tensor.py:431\u001b[0m, in \u001b[0;36mTensor.__repr__\u001b[0;34m(self, tensor_contents)\u001b[0m\n\u001b[1;32m    427\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    428\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__repr__\u001b[39m, (\u001b[38;5;28mself\u001b[39m,), \u001b[38;5;28mself\u001b[39m, tensor_contents\u001b[38;5;241m=\u001b[39mtensor_contents\n\u001b[1;32m    429\u001b[0m     )\n\u001b[1;32m    430\u001b[0m \u001b[38;5;66;03m# All strings are unicode in Python 3.\u001b[39;00m\n\u001b[0;32m--> 431\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_tensor_str\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_str\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtensor_contents\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtensor_contents\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/_tensor_str.py:664\u001b[0m, in \u001b[0;36m_str\u001b[0;34m(self, tensor_contents)\u001b[0m\n\u001b[1;32m    662\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad(), torch\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39m_python_dispatch\u001b[38;5;241m.\u001b[39m_disable_current_modes():\n\u001b[1;32m    663\u001b[0m     guard \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39m_C\u001b[38;5;241m.\u001b[39m_DisableFuncTorch()\n\u001b[0;32m--> 664\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_str_intern\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtensor_contents\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtensor_contents\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/_tensor_str.py:595\u001b[0m, in \u001b[0;36m_str_intern\u001b[0;34m(inp, tensor_contents)\u001b[0m\n\u001b[1;32m    593\u001b[0m                     tensor_str \u001b[38;5;241m=\u001b[39m _tensor_str(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mto_dense(), indent)\n\u001b[1;32m    594\u001b[0m                 \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 595\u001b[0m                     tensor_str \u001b[38;5;241m=\u001b[39m \u001b[43m_tensor_str\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindent\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    597\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayout \u001b[38;5;241m!=\u001b[39m torch\u001b[38;5;241m.\u001b[39mstrided:\n\u001b[1;32m    598\u001b[0m     suffixes\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlayout=\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayout))\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/_tensor_str.py:347\u001b[0m, in \u001b[0;36m_tensor_str\u001b[0;34m(self, indent)\u001b[0m\n\u001b[1;32m    343\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _tensor_str_with_formatter(\n\u001b[1;32m    344\u001b[0m         \u001b[38;5;28mself\u001b[39m, indent, summarize, real_formatter, imag_formatter\n\u001b[1;32m    345\u001b[0m     )\n\u001b[1;32m    346\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 347\u001b[0m     formatter \u001b[38;5;241m=\u001b[39m \u001b[43m_Formatter\u001b[49m\u001b[43m(\u001b[49m\u001b[43mget_summarized_data\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43msummarize\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    348\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _tensor_str_with_formatter(\u001b[38;5;28mself\u001b[39m, indent, summarize, formatter)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/_tensor_str.py:179\u001b[0m, in \u001b[0;36m_Formatter.__init__\u001b[0;34m(self, tensor)\u001b[0m\n\u001b[1;32m    177\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msci_mode \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    178\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m value \u001b[38;5;129;01min\u001b[39;00m nonzero_finite_vals:\n\u001b[0;32m--> 179\u001b[0m         value_str \u001b[38;5;241m=\u001b[39m \u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;130;43;01m{{\u001b[39;49;00m\u001b[38;5;124;43m:.\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mPRINT_OPTS\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprecision\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43me\u001b[39;49m\u001b[38;5;130;43;01m}}\u001b[39;49;00m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mformat\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    180\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmax_width \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmax\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmax_width, \u001b[38;5;28mlen\u001b[39m(value_str))\n\u001b[1;32m    181\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/_tensor.py:933\u001b[0m, in \u001b[0;36mTensor.__format__\u001b[0;34m(self, format_spec)\u001b[0m\n\u001b[1;32m    931\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(Tensor\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__format__\u001b[39m, (\u001b[38;5;28mself\u001b[39m,), \u001b[38;5;28mself\u001b[39m, format_spec)\n\u001b[1;32m    932\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdim() \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mis_meta \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;129;01mis\u001b[39;00m Tensor:\n\u001b[0;32m--> 933\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__format__\u001b[39m(format_spec)\n\u001b[1;32m    934\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mobject\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__format__\u001b[39m(\u001b[38;5;28mself\u001b[39m, format_spec)\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "],"ename":"KeyboardInterrupt","evalue":"","output_type":"error"}]},{"cell_type":"code","source":"evaluate_model(model_prepu, custom_train_loader, criterion, optimizer)","metadata":{"execution":{"iopub.status.busy":"2024-06-14T13:59:28.470498Z","iopub.execute_input":"2024-06-14T13:59:28.471375Z","iopub.status.idle":"2024-06-14T14:00:02.350844Z","shell.execute_reply.started":"2024-06-14T13:59:28.471340Z","shell.execute_reply":"2024-06-14T14:00:02.349477Z"},"collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":70,"outputs":[{"name":"stdout","text":"Epoch    1 | Train Loss:     1.6984 | Validation Loss:     1.6664\nEpoch    2 | Train Loss:     1.6680 | Validation Loss:     1.6462\nEpoch    3 | Train Loss:     1.6447 | Validation Loss:     1.6308\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","Cell \u001b[0;32mIn[70], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mevaluate_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_prepu\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcustom_train_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m)\u001b[49m\n","Cell \u001b[0;32mIn[50], line 100\u001b[0m, in \u001b[0;36mevaluate_model\u001b[0;34m(model, custom_train_loader, criterion, optimizer)\u001b[0m\n\u001b[1;32m     98\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m inputs, labels \u001b[38;5;129;01min\u001b[39;00m custom_train_loader\u001b[38;5;241m.\u001b[39mget_val_loader():\n\u001b[1;32m     99\u001b[0m     inputs, labels \u001b[38;5;241m=\u001b[39m inputs\u001b[38;5;241m.\u001b[39mto(device), labels\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m--> 100\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mview\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m4\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    101\u001b[0m     val_loss \u001b[38;5;241m=\u001b[39m criterion(outputs, labels\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m))\n\u001b[1;32m    102\u001b[0m     running_val_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m val_loss\u001b[38;5;241m.\u001b[39mitem()\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n","Cell \u001b[0;32mIn[69], line 25\u001b[0m, in \u001b[0;36mPairwiseCustomActivationNetwork.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     23\u001b[0m     concatenated_outputs \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat(outputs, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     24\u001b[0m     out \u001b[38;5;241m=\u001b[39m layer(concatenated_outputs)\n\u001b[0;32m---> 25\u001b[0m     out \u001b[38;5;241m=\u001b[39m \u001b[43mcustom_layer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconcatenated_outputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     26\u001b[0m     outputs\u001b[38;5;241m.\u001b[39mappend(out)\n\u001b[1;32m     28\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m outputs[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "],"ename":"KeyboardInterrupt","evalue":"","output_type":"error"}]},{"cell_type":"code","source":"evaluate_model(model_prepu, custom_train_loader, criterion, optimizer)","metadata":{"execution":{"iopub.status.busy":"2024-06-14T13:54:45.614119Z","iopub.execute_input":"2024-06-14T13:54:45.614860Z","iopub.status.idle":"2024-06-14T13:59:15.618248Z","shell.execute_reply.started":"2024-06-14T13:54:45.614831Z","shell.execute_reply":"2024-06-14T13:59:15.616895Z"},"collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":68,"outputs":[{"name":"stdout","text":"Epoch    1 | Train Loss:     1.6136 | Validation Loss:     1.5721\nEpoch    2 | Train Loss:     1.5250 | Validation Loss:     1.5402\nEpoch    3 | Train Loss:     1.4959 | Validation Loss:     1.5255\nEpoch    4 | Train Loss:     1.4831 | Validation Loss:     1.5175\nEpoch    5 | Train Loss:     1.4759 | Validation Loss:     1.5112\nEpoch    6 | Train Loss:     1.4714 | Validation Loss:     1.5089\nEpoch    7 | Train Loss:     1.4682 | Validation Loss:     1.5068\nEpoch    8 | Train Loss:     1.4660 | Validation Loss:     1.5044\nEpoch    9 | Train Loss:     1.4645 | Validation Loss:     1.5031\nEpoch   10 | Train Loss:     1.4631 | Validation Loss:     1.5005\nEpoch   11 | Train Loss:     1.4621 | Validation Loss:     1.4993\nEpoch   12 | Train Loss:     1.4611 | Validation Loss:     1.5010\nEpoch   13 | Train Loss:     1.4605 | Validation Loss:     1.4975\nEpoch   14 | Train Loss:     1.4598 | Validation Loss:     1.4964\nEpoch   15 | Train Loss:     1.4593 | Validation Loss:     1.4957\nEpoch   16 | Train Loss:     1.4588 | Validation Loss:     1.4961\nEpoch   17 | Train Loss:     1.4582 | Validation Loss:     1.4949\nEpoch   18 | Train Loss:     1.4578 | Validation Loss:     1.4944\nEpoch   19 | Train Loss:     1.4574 | Validation Loss:     1.4943\nEpoch   20 | Train Loss:     1.4571 | Validation Loss:     1.4939\nEpoch   21 | Train Loss:     1.4567 | Validation Loss:     1.4930\nEpoch   22 | Train Loss:     1.4565 | Validation Loss:     1.4925\nEpoch   23 | Train Loss:     1.4561 | Validation Loss:     1.4933\nEpoch   24 | Train Loss:     1.4559 | Validation Loss:     1.4920\nEpoch   25 | Train Loss:     1.4556 | Validation Loss:     1.4931\nEpoch   26 | Train Loss:     1.4555 | Validation Loss:     1.4925\nEpoch   27 | Train Loss:     1.4553 | Validation Loss:     1.4920\nEpoch   28 | Train Loss:     1.4550 | Validation Loss:     1.4910\nEpoch   29 | Train Loss:     1.4548 | Validation Loss:     1.4909\nEpoch   30 | Train Loss:     1.4547 | Validation Loss:     1.4918\nEpoch   31 | Train Loss:     1.4546 | Validation Loss:     1.4907\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","Cell \u001b[0;32mIn[68], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mevaluate_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_prepu\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcustom_train_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m)\u001b[49m\n","Cell \u001b[0;32mIn[50], line 87\u001b[0m, in \u001b[0;36mevaluate_model\u001b[0;34m(model, custom_train_loader, criterion, optimizer)\u001b[0m\n\u001b[1;32m     85\u001b[0m parameters \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m     86\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m name, param \u001b[38;5;129;01min\u001b[39;00m model\u001b[38;5;241m.\u001b[39mnamed_parameters():\n\u001b[0;32m---> 87\u001b[0m     parameters\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mparam\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     89\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[1;32m     90\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/_tensor.py:934\u001b[0m, in \u001b[0;36mTensor.__format__\u001b[0;34m(self, format_spec)\u001b[0m\n\u001b[1;32m    932\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdim() \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mis_meta \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;129;01mis\u001b[39;00m Tensor:\n\u001b[1;32m    933\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mitem()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__format__\u001b[39m(format_spec)\n\u001b[0;32m--> 934\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mobject\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__format__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mformat_spec\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/parameter.py:63\u001b[0m, in \u001b[0;36mParameter.__repr__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     62\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__repr__\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m---> 63\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mParameter containing:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__repr__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/_tensor.py:431\u001b[0m, in \u001b[0;36mTensor.__repr__\u001b[0;34m(self, tensor_contents)\u001b[0m\n\u001b[1;32m    427\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    428\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__repr__\u001b[39m, (\u001b[38;5;28mself\u001b[39m,), \u001b[38;5;28mself\u001b[39m, tensor_contents\u001b[38;5;241m=\u001b[39mtensor_contents\n\u001b[1;32m    429\u001b[0m     )\n\u001b[1;32m    430\u001b[0m \u001b[38;5;66;03m# All strings are unicode in Python 3.\u001b[39;00m\n\u001b[0;32m--> 431\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_tensor_str\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_str\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtensor_contents\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtensor_contents\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/_tensor_str.py:664\u001b[0m, in \u001b[0;36m_str\u001b[0;34m(self, tensor_contents)\u001b[0m\n\u001b[1;32m    662\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad(), torch\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39m_python_dispatch\u001b[38;5;241m.\u001b[39m_disable_current_modes():\n\u001b[1;32m    663\u001b[0m     guard \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39m_C\u001b[38;5;241m.\u001b[39m_DisableFuncTorch()\n\u001b[0;32m--> 664\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_str_intern\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtensor_contents\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtensor_contents\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/_tensor_str.py:595\u001b[0m, in \u001b[0;36m_str_intern\u001b[0;34m(inp, tensor_contents)\u001b[0m\n\u001b[1;32m    593\u001b[0m                     tensor_str \u001b[38;5;241m=\u001b[39m _tensor_str(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mto_dense(), indent)\n\u001b[1;32m    594\u001b[0m                 \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 595\u001b[0m                     tensor_str \u001b[38;5;241m=\u001b[39m \u001b[43m_tensor_str\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindent\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    597\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayout \u001b[38;5;241m!=\u001b[39m torch\u001b[38;5;241m.\u001b[39mstrided:\n\u001b[1;32m    598\u001b[0m     suffixes\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlayout=\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayout))\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/_tensor_str.py:347\u001b[0m, in \u001b[0;36m_tensor_str\u001b[0;34m(self, indent)\u001b[0m\n\u001b[1;32m    343\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _tensor_str_with_formatter(\n\u001b[1;32m    344\u001b[0m         \u001b[38;5;28mself\u001b[39m, indent, summarize, real_formatter, imag_formatter\n\u001b[1;32m    345\u001b[0m     )\n\u001b[1;32m    346\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 347\u001b[0m     formatter \u001b[38;5;241m=\u001b[39m \u001b[43m_Formatter\u001b[49m\u001b[43m(\u001b[49m\u001b[43mget_summarized_data\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43msummarize\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    348\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _tensor_str_with_formatter(\u001b[38;5;28mself\u001b[39m, indent, summarize, formatter)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/_tensor_str.py:147\u001b[0m, in \u001b[0;36m_Formatter.__init__\u001b[0;34m(self, tensor)\u001b[0m\n\u001b[1;32m    145\u001b[0m \u001b[38;5;66;03m# Convert to double for easy calculation. HalfTensor overflows with 1e8, and there's no div() on CPU.\u001b[39;00m\n\u001b[1;32m    146\u001b[0m nonzero_finite_abs \u001b[38;5;241m=\u001b[39m tensor_totype(nonzero_finite_vals\u001b[38;5;241m.\u001b[39mabs())\n\u001b[0;32m--> 147\u001b[0m nonzero_finite_min \u001b[38;5;241m=\u001b[39m \u001b[43mtensor_totype\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnonzero_finite_abs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmin\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    148\u001b[0m nonzero_finite_max \u001b[38;5;241m=\u001b[39m tensor_totype(nonzero_finite_abs\u001b[38;5;241m.\u001b[39mmax())\n\u001b[1;32m    150\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m value \u001b[38;5;129;01min\u001b[39;00m nonzero_finite_vals:\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/_tensor_str.py:118\u001b[0m, in \u001b[0;36mtensor_totype\u001b[0;34m(t)\u001b[0m\n\u001b[1;32m    116\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtensor_totype\u001b[39m(t):\n\u001b[1;32m    117\u001b[0m     dtype \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mfloat \u001b[38;5;28;01mif\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_mps \u001b[38;5;28;01melse\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mdouble\n\u001b[0;32m--> 118\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "],"ename":"KeyboardInterrupt","evalue":"","output_type":"error"}]},{"cell_type":"code","source":"evaluate_model(model_prepu, custom_train_loader, criterion, optimizer)","metadata":{"execution":{"iopub.status.busy":"2024-06-14T13:49:35.745589Z","iopub.execute_input":"2024-06-14T13:49:35.746447Z","iopub.status.idle":"2024-06-14T13:52:51.785420Z","shell.execute_reply.started":"2024-06-14T13:49:35.746415Z","shell.execute_reply":"2024-06-14T13:52:51.783842Z"},"collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":64,"outputs":[{"name":"stdout","text":"Epoch    1 | Train Loss:     1.6131 | Validation Loss:     1.5726\nEpoch    2 | Train Loss:     1.5250 | Validation Loss:     1.5402\nEpoch    3 | Train Loss:     1.4958 | Validation Loss:     1.5244\nEpoch    4 | Train Loss:     1.4829 | Validation Loss:     1.5184\nEpoch    5 | Train Loss:     1.4758 | Validation Loss:     1.5110\nEpoch    6 | Train Loss:     1.4712 | Validation Loss:     1.5084\nEpoch    7 | Train Loss:     1.4682 | Validation Loss:     1.5062\nEpoch    8 | Train Loss:     1.4660 | Validation Loss:     1.5046\nEpoch    9 | Train Loss:     1.4643 | Validation Loss:     1.5041\nEpoch   10 | Train Loss:     1.4630 | Validation Loss:     1.5014\nEpoch   11 | Train Loss:     1.4621 | Validation Loss:     1.4991\nEpoch   12 | Train Loss:     1.4612 | Validation Loss:     1.4988\nEpoch   13 | Train Loss:     1.4604 | Validation Loss:     1.4971\nEpoch   14 | Train Loss:     1.4599 | Validation Loss:     1.4974\nEpoch   15 | Train Loss:     1.4593 | Validation Loss:     1.4959\nEpoch   16 | Train Loss:     1.4587 | Validation Loss:     1.4944\nEpoch   17 | Train Loss:     1.4583 | Validation Loss:     1.4965\nEpoch   18 | Train Loss:     1.4578 | Validation Loss:     1.4944\nEpoch   19 | Train Loss:     1.4574 | Validation Loss:     1.4956\nEpoch   20 | Train Loss:     1.4570 | Validation Loss:     1.4932\nEpoch   21 | Train Loss:     1.4567 | Validation Loss:     1.4933\nEpoch   22 | Train Loss:     1.4565 | Validation Loss:     1.4929\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","Cell \u001b[0;32mIn[64], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mevaluate_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_prepu\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcustom_train_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m)\u001b[49m\n","Cell \u001b[0;32mIn[50], line 87\u001b[0m, in \u001b[0;36mevaluate_model\u001b[0;34m(model, custom_train_loader, criterion, optimizer)\u001b[0m\n\u001b[1;32m     85\u001b[0m parameters \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m     86\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m name, param \u001b[38;5;129;01min\u001b[39;00m model\u001b[38;5;241m.\u001b[39mnamed_parameters():\n\u001b[0;32m---> 87\u001b[0m     parameters\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mparam\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     89\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[1;32m     90\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/_tensor.py:934\u001b[0m, in \u001b[0;36mTensor.__format__\u001b[0;34m(self, format_spec)\u001b[0m\n\u001b[1;32m    932\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdim() \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mis_meta \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;129;01mis\u001b[39;00m Tensor:\n\u001b[1;32m    933\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mitem()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__format__\u001b[39m(format_spec)\n\u001b[0;32m--> 934\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mobject\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__format__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mformat_spec\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/parameter.py:63\u001b[0m, in \u001b[0;36mParameter.__repr__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     62\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__repr__\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m---> 63\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mParameter containing:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__repr__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/_tensor.py:431\u001b[0m, in \u001b[0;36mTensor.__repr__\u001b[0;34m(self, tensor_contents)\u001b[0m\n\u001b[1;32m    427\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    428\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__repr__\u001b[39m, (\u001b[38;5;28mself\u001b[39m,), \u001b[38;5;28mself\u001b[39m, tensor_contents\u001b[38;5;241m=\u001b[39mtensor_contents\n\u001b[1;32m    429\u001b[0m     )\n\u001b[1;32m    430\u001b[0m \u001b[38;5;66;03m# All strings are unicode in Python 3.\u001b[39;00m\n\u001b[0;32m--> 431\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_tensor_str\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_str\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtensor_contents\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtensor_contents\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/_tensor_str.py:664\u001b[0m, in \u001b[0;36m_str\u001b[0;34m(self, tensor_contents)\u001b[0m\n\u001b[1;32m    662\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad(), torch\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39m_python_dispatch\u001b[38;5;241m.\u001b[39m_disable_current_modes():\n\u001b[1;32m    663\u001b[0m     guard \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39m_C\u001b[38;5;241m.\u001b[39m_DisableFuncTorch()\n\u001b[0;32m--> 664\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_str_intern\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtensor_contents\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtensor_contents\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/_tensor_str.py:595\u001b[0m, in \u001b[0;36m_str_intern\u001b[0;34m(inp, tensor_contents)\u001b[0m\n\u001b[1;32m    593\u001b[0m                     tensor_str \u001b[38;5;241m=\u001b[39m _tensor_str(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mto_dense(), indent)\n\u001b[1;32m    594\u001b[0m                 \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 595\u001b[0m                     tensor_str \u001b[38;5;241m=\u001b[39m \u001b[43m_tensor_str\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindent\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    597\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayout \u001b[38;5;241m!=\u001b[39m torch\u001b[38;5;241m.\u001b[39mstrided:\n\u001b[1;32m    598\u001b[0m     suffixes\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlayout=\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayout))\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/_tensor_str.py:347\u001b[0m, in \u001b[0;36m_tensor_str\u001b[0;34m(self, indent)\u001b[0m\n\u001b[1;32m    343\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _tensor_str_with_formatter(\n\u001b[1;32m    344\u001b[0m         \u001b[38;5;28mself\u001b[39m, indent, summarize, real_formatter, imag_formatter\n\u001b[1;32m    345\u001b[0m     )\n\u001b[1;32m    346\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 347\u001b[0m     formatter \u001b[38;5;241m=\u001b[39m \u001b[43m_Formatter\u001b[49m\u001b[43m(\u001b[49m\u001b[43mget_summarized_data\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43msummarize\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    348\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _tensor_str_with_formatter(\u001b[38;5;28mself\u001b[39m, indent, summarize, formatter)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/_tensor_str.py:179\u001b[0m, in \u001b[0;36m_Formatter.__init__\u001b[0;34m(self, tensor)\u001b[0m\n\u001b[1;32m    177\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msci_mode \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    178\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m value \u001b[38;5;129;01min\u001b[39;00m nonzero_finite_vals:\n\u001b[0;32m--> 179\u001b[0m         value_str \u001b[38;5;241m=\u001b[39m \u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;130;43;01m{{\u001b[39;49;00m\u001b[38;5;124;43m:.\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mPRINT_OPTS\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprecision\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43me\u001b[39;49m\u001b[38;5;130;43;01m}}\u001b[39;49;00m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mformat\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    180\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmax_width \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmax\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmax_width, \u001b[38;5;28mlen\u001b[39m(value_str))\n\u001b[1;32m    181\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/_tensor.py:933\u001b[0m, in \u001b[0;36mTensor.__format__\u001b[0;34m(self, format_spec)\u001b[0m\n\u001b[1;32m    931\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(Tensor\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__format__\u001b[39m, (\u001b[38;5;28mself\u001b[39m,), \u001b[38;5;28mself\u001b[39m, format_spec)\n\u001b[1;32m    932\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdim() \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mis_meta \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;129;01mis\u001b[39;00m Tensor:\n\u001b[0;32m--> 933\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__format__\u001b[39m(format_spec)\n\u001b[1;32m    934\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mobject\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__format__\u001b[39m(\u001b[38;5;28mself\u001b[39m, format_spec)\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "],"ename":"KeyboardInterrupt","evalue":"","output_type":"error"}]},{"cell_type":"code","source":"# # custom_train_loader = CustomDataLoader(x_train, y_train, batch_size=128, validation_size=0.2, shuffle=True)\n\n# class PairwiseCustomActivationNetwork(nn.Module):\n#     def __init__(self, input_size, num_layers, output_size):\n#         super(PairwiseCustomActivationNetwork, self).__init__()\n# #         self.n = 255\n# #         self.in_features = 1\n\n# #         self.fc1 = nn.Linear(self.in_features, self.n)\n# #         self.fc_layers = nn.ModuleList([nn.Linear(self.n + self.in_features, self.n) for _ in range(64)])\n# #         self.fc32 = nn.Linear(self.n + self.in_features, 1)\n\n# #         self.custom_layer1 = CustomLayer1(self.in_features, self.n)\n# #         self.custom_layers = nn.ModuleList([CustomLayer1(self.n + self.in_features, self.n) if (i % 3 == 1) else\n# #                                             CustomLayer1(self.n + self.in_features, self.n) if (i % 3 == 2) else\n# #                                             CustomLayer1(self.n + self.in_features, self.n)\n# #                                             for i in range(64)])\n\n# #         self.relu = nn.ReLU()\n\n# #     def forward(self, x):\n# #         residual = x\n        \n# #         x_fc1 = self.fc1(x)\n# #         x = self.custom_layer1(x_fc1, x)\n# #         x = torch.cat([x, residual], dim=1)\n        \n# #         for i in range(64):\n# #             x_fc = self.fc_layers[i](x)\n# #             x = self.custom_layers[i](x_fc, x)\n# #             x = torch.cat([x, residual], dim=1)\n\n# #         x = self.fc32(x)\n# #         return x\n\n#         self.relu = nn.ReLU()\n#         self.sigmoid = nn.Sigmoid()\n#         self.num_layers = num_layers\n#         self.layers = nn.ModuleList()\n#         self.custom_layers = nn.ModuleList()\n   \n#         layer_size = input_size\n#         for i in range(1, num_layers):\n#             self.layers.append(nn.Linear(layer_size, layer_size))\n#             self.custom_layers.append(CustomLayer1(layer_size, layer_size))\n#             layer_size *= 2\n    \n#         self.layers.append(nn.Linear(layer_size, output_size))       \n#         self.custom_layers.append(CustomLayer1(layer_size, output_size))\n#         self._initialize_weights()\n\n#     def forward(self, x):\n#         outputs = [x]\n        \n#         for layer, custom_layer in zip(self.layers, self.custom_layers):\n#             concatenated_outputs = torch.cat(outputs, dim=1)\n#             out = layer(concatenated_outputs)\n#             out = custom_layer(out, concatenated_outputs)\n#             outputs.append(out)\n\n#         return outputs[-1]\n    \n#     def _initialize_weights(self):\n#         for i, layer in enumerate(self.layers):\n#             if isinstance(layer, nn.Linear):\n#                 if i == len(self.layers) - 1:\n#                     layer.weight.data.fill_(0)\n#                 else:\n#                     in_features = layer.weight.size(1)\n#                     eye_matrix = torch.eye(in_features)\n#                     layer.weight.data = eye_matrix\n#                 if layer.bias is not None:\n#                     nn.init.normal_(layer.bias, mean=0.0, std=1.0)\n#                     layer.bias.data.zero_()\n\n# model_prepu = PairwiseCustomActivationNetwork(32, 8, 1).to(device)\n# print(summary(model_prepu, input_size=(1, 32)))\n# criterion = nn.MSELoss()\n# optimizer = optim.Adam(model_prepu.parameters(), lr=0.00000001)","metadata":{"execution":{"iopub.status.busy":"2024-06-14T13:16:58.777969Z","iopub.execute_input":"2024-06-14T13:16:58.778436Z","iopub.status.idle":"2024-06-14T13:16:58.785494Z","shell.execute_reply.started":"2024-06-14T13:16:58.778407Z","shell.execute_reply":"2024-06-14T13:16:58.784543Z"},"trusted":true},"execution_count":20,"outputs":[]},{"cell_type":"code","source":"evaluate_model(model_prepu, custom_train_loader, criterion, optimizer)","metadata":{"execution":{"iopub.status.busy":"2024-06-14T06:08:24.790563Z","iopub.execute_input":"2024-06-14T06:08:24.790927Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"evaluate_model(model_prepu, custom_train_loader, criterion, optimizer)","metadata":{"execution":{"iopub.status.busy":"2024-06-14T05:21:08.899816Z","iopub.execute_input":"2024-06-14T05:21:08.900187Z","iopub.status.idle":"2024-06-14T05:40:16.643833Z","shell.execute_reply.started":"2024-06-14T05:21:08.900155Z","shell.execute_reply":"2024-06-14T05:40:16.642518Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"evaluate_model(model_prepu, custom_train_loader, criterion, optimizer)","metadata":{"execution":{"iopub.status.busy":"2024-06-13T15:49:36.475629Z","iopub.execute_input":"2024-06-13T15:49:36.476390Z","iopub.status.idle":"2024-06-13T16:34:07.807105Z","shell.execute_reply.started":"2024-06-13T15:49:36.476357Z","shell.execute_reply":"2024-06-13T16:34:07.805685Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"evaluate_model(model_prepu, custom_train_loader, criterion, optimizer)","metadata":{"execution":{"iopub.status.busy":"2024-06-13T14:31:25.900101Z","iopub.execute_input":"2024-06-13T14:31:25.900429Z","iopub.status.idle":"2024-06-13T15:35:30.219528Z","shell.execute_reply.started":"2024-06-13T14:31:25.900403Z","shell.execute_reply":"2024-06-13T15:35:30.217948Z"},"collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"evaluate_model(model_prepu, custom_train_loader, criterion, optimizer)","metadata":{"execution":{"iopub.status.busy":"2024-06-11T20:02:23.779202Z","iopub.execute_input":"2024-06-11T20:02:23.779975Z","iopub.status.idle":"2024-06-11T22:12:11.031578Z","shell.execute_reply.started":"2024-06-11T20:02:23.779936Z","shell.execute_reply":"2024-06-11T22:12:11.030216Z"},"collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"evaluate_model(model_prepu, custom_train_loader, criterion, optimizer)","metadata":{"execution":{"iopub.status.busy":"2024-06-11T04:35:37.173585Z","iopub.execute_input":"2024-06-11T04:35:37.174136Z","iopub.status.idle":"2024-06-11T05:49:42.627474Z","shell.execute_reply.started":"2024-06-11T04:35:37.174096Z","shell.execute_reply":"2024-06-11T05:49:42.626377Z"},"collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"evaluate_model(model_prepu, custom_train_loader, criterion, optimizer)","metadata":{"execution":{"iopub.status.busy":"2024-06-11T05:52:29.812473Z","iopub.execute_input":"2024-06-11T05:52:29.813023Z","iopub.status.idle":"2024-06-11T06:25:35.552554Z","shell.execute_reply.started":"2024-06-11T05:52:29.812957Z","shell.execute_reply":"2024-06-11T06:25:35.550380Z"},"collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import cv2\nimage_folder = 'training_images'\n\nnum_epochs = 58\nframe = cv2.imread(f\"{image_folder}/epoch_0001.png\")\nheight, width, layers = frame.shape\n\nvideo = cv2.VideoWriter('training_progress_GLU_58.mp4', cv2.VideoWriter_fourcc(*'mp4v'), 30, (width, height))\n\nfor epoch in range(num_epochs):\n    filename = f\"{image_folder}/epoch_{epoch+1:04d}.png\"\n    video.write(cv2.imread(filename))\n\nvideo.release()","metadata":{"execution":{"iopub.status.busy":"2024-06-15T18:16:05.625642Z","iopub.execute_input":"2024-06-15T18:16:05.626076Z","iopub.status.idle":"2024-06-15T18:16:06.378893Z","shell.execute_reply.started":"2024-06-15T18:16:05.626029Z","shell.execute_reply":"2024-06-15T18:16:06.377553Z"},"trusted":true},"execution_count":47,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}