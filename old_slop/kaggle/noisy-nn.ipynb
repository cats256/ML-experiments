{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport numpy as np\n\n# Set random seed for reproducibility\ntorch.manual_seed(42)\nnp.random.seed(42)\n\n# Generate data\nn_samples = 1000\n\n# Features A and B\nA = np.random.uniform(-5, 5, n_samples).astype(np.float32)\nB = np.random.uniform(-5, 5, n_samples).astype(np.float32)\n\n# Noisy A and B\nnoise_level = 1.0\nnoisy_A = np.random.normal(0, noise_level, n_samples).astype(np.float32) * 0\nnoisy_B = B + np.random.normal(0, noise_level, n_samples).astype(np.float32) * 0\n\n# Target y\ny = np.tanh(A + B).astype(np.float32) + np.random.normal(0, noise_level, n_samples).astype(np.float32) * 1\n\n# Combine features into a single dataset\nX = np.stack([A, B, noisy_A, noisy_B], axis=1)\n\n# Convert to PyTorch tensors\nX_tensor = torch.tensor(X)\ny_tensor = torch.tensor(y).unsqueeze(1)\n\n# Define the neural network model\nclass NeuralNetwork(nn.Module):\n    def __init__(self):\n        super(NeuralNetwork, self).__init__()\n        self.model = nn.Sequential(\n            nn.Linear(4, 32),  # Input layer with 4 features\n            nn.ReLU(),         # Hidden layer with ReLU activation\n            nn.Linear(32, 32), # Another hidden layer\n            nn.ReLU(),\n            nn.Linear(32, 1),  # Output layer\n        )\n    \n    def forward(self, x):\n        return self.model(x)\n\n# Initialize the model, loss function, and optimizer\nmodel = NeuralNetwork()\ncriterion = nn.MSELoss()\noptimizer = optim.Adam(model.parameters(), lr=0.01)\n\n# Split the data into training and testing sets\ntrain_size = int(0.8 * n_samples)\nX_train, X_test = X_tensor[:train_size], X_tensor[train_size:]\ny_train, y_test = y_tensor[:train_size], y_tensor[train_size:]\n\n# Training loop\nn_epochs = 1000\nfor epoch in range(n_epochs):\n    model.train()\n    optimizer.zero_grad()\n    y_pred = model(X_train)\n    loss = criterion(y_pred, y_train)\n    loss.backward()\n    optimizer.step()\n    \n    # Evaluate on the test set\n    model.eval()\n    with torch.no_grad():\n        test_loss = criterion(model(X_test), y_test)\n    \n    if epoch % 10 == 0:\n        print(f\"Epoch {epoch}, Train Loss: {loss.item():.4f}, Test Loss: {test_loss.item():.4f}\")\n\n# Evaluate the model on the test set\nmodel.eval()\nwith torch.no_grad():\n    predictions = model(X_test)\n\n# Compare predictions with the actual values\nprint(\"\\nSample Predictions:\")\nprint(f\"Predicted: {predictions[:5].squeeze()}\")\nprint(f\"Actual:    {y_test[:5].squeeze()}\")\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2024-11-18T23:46:43.422004Z","iopub.execute_input":"2024-11-18T23:46:43.422428Z","iopub.status.idle":"2024-11-18T23:46:45.632024Z","shell.execute_reply.started":"2024-11-18T23:46:43.422392Z","shell.execute_reply":"2024-11-18T23:46:45.630894Z"}},"outputs":[{"name":"stdout","text":"Epoch 0, Train Loss: 2.0188, Test Loss: 1.4802\nEpoch 10, Train Loss: 1.0961, Test Loss: 1.0199\nEpoch 20, Train Loss: 1.0477, Test Loss: 0.9268\nEpoch 30, Train Loss: 1.0155, Test Loss: 0.8996\nEpoch 40, Train Loss: 1.0009, Test Loss: 0.9097\nEpoch 50, Train Loss: 0.9874, Test Loss: 0.9084\nEpoch 60, Train Loss: 0.9744, Test Loss: 0.9060\nEpoch 70, Train Loss: 0.9643, Test Loss: 0.9125\nEpoch 80, Train Loss: 0.9566, Test Loss: 0.9214\nEpoch 90, Train Loss: 0.9507, Test Loss: 0.9272\nEpoch 100, Train Loss: 0.9457, Test Loss: 0.9279\nEpoch 110, Train Loss: 0.9404, Test Loss: 0.9381\nEpoch 120, Train Loss: 0.9442, Test Loss: 0.9371\nEpoch 130, Train Loss: 0.9329, Test Loss: 0.9434\nEpoch 140, Train Loss: 0.9261, Test Loss: 0.9425\nEpoch 150, Train Loss: 0.9224, Test Loss: 0.9589\nEpoch 160, Train Loss: 0.9210, Test Loss: 0.9554\nEpoch 170, Train Loss: 0.9147, Test Loss: 0.9782\nEpoch 180, Train Loss: 0.9138, Test Loss: 0.9802\nEpoch 190, Train Loss: 0.9141, Test Loss: 0.9773\nEpoch 200, Train Loss: 0.9071, Test Loss: 0.9863\nEpoch 210, Train Loss: 0.9042, Test Loss: 0.9797\nEpoch 220, Train Loss: 0.9008, Test Loss: 0.9909\nEpoch 230, Train Loss: 0.9161, Test Loss: 1.0093\nEpoch 240, Train Loss: 0.8981, Test Loss: 0.9796\nEpoch 250, Train Loss: 0.8945, Test Loss: 1.0141\nEpoch 260, Train Loss: 0.8908, Test Loss: 0.9992\nEpoch 270, Train Loss: 0.8882, Test Loss: 1.0020\nEpoch 280, Train Loss: 0.8882, Test Loss: 1.0026\nEpoch 290, Train Loss: 0.8949, Test Loss: 1.0302\nEpoch 300, Train Loss: 0.8828, Test Loss: 1.0331\nEpoch 310, Train Loss: 0.8798, Test Loss: 1.0297\nEpoch 320, Train Loss: 0.8789, Test Loss: 1.0362\nEpoch 330, Train Loss: 0.8797, Test Loss: 1.0557\nEpoch 340, Train Loss: 0.8767, Test Loss: 1.0532\nEpoch 350, Train Loss: 0.8732, Test Loss: 1.0366\nEpoch 360, Train Loss: 0.8717, Test Loss: 1.0438\nEpoch 370, Train Loss: 0.8692, Test Loss: 1.0594\nEpoch 380, Train Loss: 0.8692, Test Loss: 1.0610\nEpoch 390, Train Loss: 0.8672, Test Loss: 1.0531\nEpoch 400, Train Loss: 0.8645, Test Loss: 1.0476\nEpoch 410, Train Loss: 0.8773, Test Loss: 1.0638\nEpoch 420, Train Loss: 0.8753, Test Loss: 1.0244\nEpoch 430, Train Loss: 0.8646, Test Loss: 1.0655\nEpoch 440, Train Loss: 0.8621, Test Loss: 1.0358\nEpoch 450, Train Loss: 0.8599, Test Loss: 1.0557\nEpoch 460, Train Loss: 0.8586, Test Loss: 1.0576\nEpoch 470, Train Loss: 0.8581, Test Loss: 1.0537\nEpoch 480, Train Loss: 0.8592, Test Loss: 1.0584\nEpoch 490, Train Loss: 0.8592, Test Loss: 1.0592\nEpoch 500, Train Loss: 0.8574, Test Loss: 1.0498\nEpoch 510, Train Loss: 0.8595, Test Loss: 1.0528\nEpoch 520, Train Loss: 0.8561, Test Loss: 1.0555\nEpoch 530, Train Loss: 0.8550, Test Loss: 1.0572\nEpoch 540, Train Loss: 0.8580, Test Loss: 1.0426\nEpoch 550, Train Loss: 0.8572, Test Loss: 1.0655\nEpoch 560, Train Loss: 0.8523, Test Loss: 1.0604\nEpoch 570, Train Loss: 0.8518, Test Loss: 1.0674\nEpoch 580, Train Loss: 0.8540, Test Loss: 1.0773\nEpoch 590, Train Loss: 0.8497, Test Loss: 1.0602\nEpoch 600, Train Loss: 0.8511, Test Loss: 1.0820\nEpoch 610, Train Loss: 0.8759, Test Loss: 1.1034\nEpoch 620, Train Loss: 0.8631, Test Loss: 1.0889\nEpoch 630, Train Loss: 0.8506, Test Loss: 1.0398\nEpoch 640, Train Loss: 0.8472, Test Loss: 1.0751\nEpoch 650, Train Loss: 0.8470, Test Loss: 1.0746\nEpoch 660, Train Loss: 0.8463, Test Loss: 1.0755\nEpoch 670, Train Loss: 0.8454, Test Loss: 1.0779\nEpoch 680, Train Loss: 0.8558, Test Loss: 1.0835\nEpoch 690, Train Loss: 0.8507, Test Loss: 1.0702\nEpoch 700, Train Loss: 0.8504, Test Loss: 1.0755\nEpoch 710, Train Loss: 0.8483, Test Loss: 1.0657\nEpoch 720, Train Loss: 0.8485, Test Loss: 1.0767\nEpoch 730, Train Loss: 0.8430, Test Loss: 1.0821\nEpoch 740, Train Loss: 0.8503, Test Loss: 1.0791\nEpoch 750, Train Loss: 0.8527, Test Loss: 1.0873\nEpoch 760, Train Loss: 0.8430, Test Loss: 1.0897\nEpoch 770, Train Loss: 0.8430, Test Loss: 1.0823\nEpoch 780, Train Loss: 0.8416, Test Loss: 1.0835\nEpoch 790, Train Loss: 0.8410, Test Loss: 1.0735\nEpoch 800, Train Loss: 0.8462, Test Loss: 1.0678\nEpoch 810, Train Loss: 0.8458, Test Loss: 1.0652\nEpoch 820, Train Loss: 0.8435, Test Loss: 1.0573\nEpoch 830, Train Loss: 0.8453, Test Loss: 1.0679\nEpoch 840, Train Loss: 0.8490, Test Loss: 1.0648\nEpoch 850, Train Loss: 0.8392, Test Loss: 1.0730\nEpoch 860, Train Loss: 0.8426, Test Loss: 1.0890\nEpoch 870, Train Loss: 0.8480, Test Loss: 1.0949\nEpoch 880, Train Loss: 0.8363, Test Loss: 1.0801\nEpoch 890, Train Loss: 0.8459, Test Loss: 1.1011\nEpoch 900, Train Loss: 0.8487, Test Loss: 1.0971\nEpoch 910, Train Loss: 0.8406, Test Loss: 1.0856\nEpoch 920, Train Loss: 0.8393, Test Loss: 1.0769\nEpoch 930, Train Loss: 0.8357, Test Loss: 1.0647\nEpoch 940, Train Loss: 0.8343, Test Loss: 1.0771\nEpoch 950, Train Loss: 0.8336, Test Loss: 1.0878\nEpoch 960, Train Loss: 0.8376, Test Loss: 1.0798\nEpoch 970, Train Loss: 0.8383, Test Loss: 1.0979\nEpoch 980, Train Loss: 0.8332, Test Loss: 1.0982\nEpoch 990, Train Loss: 0.8339, Test Loss: 1.0832\n\nSample Predictions:\nPredicted: tensor([ 0.6868, -0.1414, -0.2316, -0.4299, -1.0486])\nActual:    tensor([ 0.7509, -2.4732, -0.2031, -0.0317, -0.7515])\n","output_type":"stream"}],"execution_count":13},{"cell_type":"code","source":"n_samples = 100\ntrain_size = int(0.8 * n_samples)\n\n# Shuffle indices for a representative train-test split\nindices = np.arange(n_samples)\nnp.random.shuffle(indices)\n\n# Apply the shuffled indices to split the data\ntrain_indices = indices[:train_size]\ntest_indices = indices[train_size:]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-19T01:04:33.584543Z","iopub.execute_input":"2024-11-19T01:04:33.585494Z","iopub.status.idle":"2024-11-19T01:04:33.590911Z","shell.execute_reply.started":"2024-11-19T01:04:33.585449Z","shell.execute_reply":"2024-11-19T01:04:33.589571Z"}},"outputs":[],"execution_count":96},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport numpy as np\n\n# Set random seed for reproducibility\ntorch.manual_seed(42)\nnp.random.seed(42)\n\n# Features A and B\nA = np.random.uniform(-5, 5, n_samples).astype(np.float32)\nB = np.random.uniform(-5, 5, n_samples).astype(np.float32)\n\n# Noisy A and B\nnoise_level = 1.0\nnoisy_A1 = np.random.normal(0, noise_level, n_samples).astype(np.float32) * 10\nnoisy_A2 = np.random.normal(0, noise_level, n_samples).astype(np.float32) * 10\nnoisy_A3 = np.random.normal(0, noise_level, n_samples).astype(np.float32) * 10\nnoisy_A4 = np.random.normal(0, noise_level, n_samples).astype(np.float32) * 10\nnoisy_A5 = np.random.normal(0, noise_level, n_samples).astype(np.float32) * 10\nnoisy_A6 = np.random.normal(0, noise_level, n_samples).astype(np.float32) * 10\nnoisy_A7 = np.random.normal(0, noise_level, n_samples).astype(np.float32) * 10\n\n# Target y\ny = (A).astype(np.float32) + np.random.normal(0, noise_level, n_samples).astype(np.float32) * 0\n\n# Combine features into a single dataset\nX = np.stack([A, noisy_A1, noisy_A2, noisy_A3, noisy_A4, noisy_A5, noisy_A6, noisy_A7], axis=1)\n\n# Convert to PyTorch tensors\nX_tensor = torch.tensor(X)\ny_tensor = torch.tensor(y).unsqueeze(1)\n\nprint(train_indices.shape)\nX_train, X_test = X_tensor[train_indices], X_tensor[test_indices]\ny_train, y_test = y_tensor[train_indices], y_tensor[test_indices]\n\n# Define the neural network model\nclass NeuralNetwork(nn.Module):\n    def __init__(self):\n        super(NeuralNetwork, self).__init__()\n        self.model = nn.Sequential(\n            nn.Linear(8, 512),  # Input layer with 4 features\n            nn.ReLU(),         # Hidden layer with ReLU activation\n            nn.Linear(512, 512), # Another hidden layer\n            nn.ReLU(),\n            nn.Linear(512, 512), # Another hidden layer\n            nn.ReLU(),\n            nn.Linear(512, 512), # Another hidden layer\n            nn.ReLU(),\n            nn.Linear(512, 1),  # Output layer\n        )\n    \n    def forward(self, x):\n        return self.model(x)\n\n# Initialize the model, loss function, and optimizer\nmodel = NeuralNetwork()\ncriterion = nn.MSELoss()\noptimizer = optim.Adam(model.parameters(), lr=0.00001)\n\n# Training loop\nn_epochs = 1000\nfor epoch in range(n_epochs):\n    model.train()\n    optimizer.zero_grad()\n    y_pred = model(X_train)\n    loss = criterion(y_pred, y_train)\n    loss.backward()\n    optimizer.step()\n    \n    # Evaluate on the test set\n    model.eval()\n    with torch.no_grad():\n        test_loss = criterion(model(X_test), y_test)\n    \n    if epoch % 10 == 0:\n        print(f\"Epoch {epoch}, Train Loss: {loss.item():.8f}, Test Loss: {test_loss.item():.8f}, Ratio {test_loss.item() / loss.item()}\")\n\n# Evaluate the model on the test set\nmodel.eval()\nwith torch.no_grad():\n    predictions = model(X_test)\n\n# Compare predictions with the actual values\nprint(\"\\nSample Predictions:\")\nprint(f\"Predicted: {predictions[:5].squeeze()}\")\nprint(f\"Actual:    {y_test[:5].squeeze()}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-19T02:14:58.552495Z","iopub.execute_input":"2024-11-19T02:14:58.554254Z","iopub.status.idle":"2024-11-19T02:15:08.070603Z","shell.execute_reply.started":"2024-11-19T02:14:58.554200Z","shell.execute_reply":"2024-11-19T02:15:08.069482Z"}},"outputs":[{"name":"stdout","text":"(80,)\nEpoch 0, Train Loss: 8.60751724, Test Loss: 9.19295311, Ratio 1.0680144867353392\nEpoch 10, Train Loss: 8.26245308, Test Loss: 9.08484173, Ratio 1.0995332307610624\nEpoch 20, Train Loss: 7.94275141, Test Loss: 9.00302887, Ratio 1.1334899466937844\nEpoch 30, Train Loss: 7.62965393, Test Loss: 8.93905163, Ratio 1.1716195399356422\nEpoch 40, Train Loss: 7.31105280, Test Loss: 8.87998772, Ratio 1.2145976729392922\nEpoch 50, Train Loss: 6.97510004, Test Loss: 8.81411457, Ratio 1.26365421564147\nEpoch 60, Train Loss: 6.61673450, Test Loss: 8.74108982, Ratio 1.321057965171979\nEpoch 70, Train Loss: 6.23148298, Test Loss: 8.64886761, Ratio 1.3879308715465368\nEpoch 80, Train Loss: 5.81755924, Test Loss: 8.54605675, Ratio 1.469010695305511\nEpoch 90, Train Loss: 5.38100195, Test Loss: 8.43030834, Ratio 1.5666800386609254\nEpoch 100, Train Loss: 4.92074966, Test Loss: 8.27936745, Ratio 1.6825418913209478\nEpoch 110, Train Loss: 4.43985844, Test Loss: 8.08988190, Ratio 1.822103567607462\nEpoch 120, Train Loss: 3.94586110, Test Loss: 7.85501385, Ratio 1.99069699768721\nEpoch 130, Train Loss: 3.44992638, Test Loss: 7.56601477, Ratio 2.193094559517458\nEpoch 140, Train Loss: 2.96004939, Test Loss: 7.22032785, Ratio 2.439259249056833\nEpoch 150, Train Loss: 2.48543882, Test Loss: 6.82998371, Ratio 2.7479991243860966\nEpoch 160, Train Loss: 2.03670311, Test Loss: 6.39279222, Ratio 3.1387943555977182\nEpoch 170, Train Loss: 1.62666285, Test Loss: 5.92651606, Ratio 3.6433585820666643\nEpoch 180, Train Loss: 1.27006435, Test Loss: 5.43326855, Ratio 4.277947436435571\nEpoch 190, Train Loss: 0.97390735, Test Loss: 4.94872046, Ratio 5.081305164787024\nEpoch 200, Train Loss: 0.73873585, Test Loss: 4.47956944, Ratio 6.063831079905157\nEpoch 210, Train Loss: 0.55605721, Test Loss: 4.05808926, Ratio 7.297970692110229\nEpoch 220, Train Loss: 0.41735777, Test Loss: 3.68171000, Ratio 8.82147223944126\nEpoch 230, Train Loss: 0.31295052, Test Loss: 3.34562373, Ratio 10.690583653155826\nEpoch 240, Train Loss: 0.23500705, Test Loss: 3.05658960, Ratio 13.006374208550143\nEpoch 250, Train Loss: 0.17734654, Test Loss: 2.81331754, Ratio 15.863390951999849\nEpoch 260, Train Loss: 0.13485737, Test Loss: 2.60831594, Ratio 19.341293076062485\nEpoch 270, Train Loss: 0.10387745, Test Loss: 2.43803835, Ratio 23.470333616765508\nEpoch 280, Train Loss: 0.08144776, Test Loss: 2.29645014, Ratio 28.195375792657945\nEpoch 290, Train Loss: 0.06507289, Test Loss: 2.18112755, Ratio 33.51821950607378\nEpoch 300, Train Loss: 0.05292023, Test Loss: 2.08563471, Ratio 39.410912842036915\nEpoch 310, Train Loss: 0.04374816, Test Loss: 2.00675583, Ratio 45.87063493624632\nEpoch 320, Train Loss: 0.03666872, Test Loss: 1.94199729, Ratio 52.960599740896846\nEpoch 330, Train Loss: 0.03102918, Test Loss: 1.88757968, Ratio 60.8324140318112\nEpoch 340, Train Loss: 0.02644451, Test Loss: 1.84093344, Ratio 69.61496838198512\nEpoch 350, Train Loss: 0.02264296, Test Loss: 1.80093884, Ratio 79.53636729849848\nEpoch 360, Train Loss: 0.01943074, Test Loss: 1.76543391, Ratio 90.85779870908148\nEpoch 370, Train Loss: 0.01671405, Test Loss: 1.73413682, Ratio 103.75323025406072\nEpoch 380, Train Loss: 0.01438171, Test Loss: 1.70605302, Ratio 118.62654193871255\nEpoch 390, Train Loss: 0.01237531, Test Loss: 1.68083286, Ratio 135.82149515731848\nEpoch 400, Train Loss: 0.01065969, Test Loss: 1.65834653, Ratio 155.57172737213688\nEpoch 410, Train Loss: 0.00919405, Test Loss: 1.63785481, Ratio 178.1429817083251\nEpoch 420, Train Loss: 0.00792964, Test Loss: 1.61940169, Ratio 204.22126919500624\nEpoch 430, Train Loss: 0.00683629, Test Loss: 1.60284162, Ratio 234.46086807317823\nEpoch 440, Train Loss: 0.00588498, Test Loss: 1.58730137, Ratio 269.7205909174676\nEpoch 450, Train Loss: 0.00506192, Test Loss: 1.57316029, Ratio 310.78350130041406\nEpoch 460, Train Loss: 0.00435371, Test Loss: 1.55998611, Ratio 358.3122490243341\nEpoch 470, Train Loss: 0.00374383, Test Loss: 1.54782259, Ratio 413.4330660260546\nEpoch 480, Train Loss: 0.00321768, Test Loss: 1.53672600, Ratio 477.58807406803874\nEpoch 490, Train Loss: 0.00276428, Test Loss: 1.52636421, Ratio 552.1743418608902\nEpoch 500, Train Loss: 0.00237347, Test Loss: 1.51711214, Ratio 639.1962118781986\nEpoch 510, Train Loss: 0.00203712, Test Loss: 1.50828683, Ratio 740.4027298027853\nEpoch 520, Train Loss: 0.00174726, Test Loss: 1.50016809, Ratio 858.5820642448142\nEpoch 530, Train Loss: 0.00149740, Test Loss: 1.49260652, Ratio 996.7995003171609\nEpoch 540, Train Loss: 0.00128163, Test Loss: 1.48552346, Ratio 1159.0892690122196\nEpoch 550, Train Loss: 0.00109547, Test Loss: 1.47897875, Ratio 1350.0902925891708\nEpoch 560, Train Loss: 0.00093509, Test Loss: 1.47268546, Ratio 1574.911384378995\nEpoch 570, Train Loss: 0.00079728, Test Loss: 1.46702754, Ratio 1840.0385679359863\nEpoch 580, Train Loss: 0.00067901, Test Loss: 1.46173930, Ratio 2152.7358194123744\nEpoch 590, Train Loss: 0.00057757, Test Loss: 1.45687687, Ratio 2522.421221634551\nEpoch 600, Train Loss: 0.00049041, Test Loss: 1.45237660, Ratio 2961.5578341764813\nEpoch 610, Train Loss: 0.00041570, Test Loss: 1.44816768, Ratio 3483.653180123152\nEpoch 620, Train Loss: 0.00035198, Test Loss: 1.44434214, Ratio 4103.454348061561\nEpoch 630, Train Loss: 0.00029778, Test Loss: 1.44083381, Ratio 4838.656715819816\nEpoch 640, Train Loss: 0.00025151, Test Loss: 1.43760145, Ratio 5715.81029506617\nEpoch 650, Train Loss: 0.00021210, Test Loss: 1.43463171, Ratio 6763.980064650404\nEpoch 660, Train Loss: 0.00017854, Test Loss: 1.43191123, Ratio 8019.898312450396\nEpoch 670, Train Loss: 0.00014999, Test Loss: 1.42945695, Ratio 9530.256181472338\nEpoch 680, Train Loss: 0.00012579, Test Loss: 1.42715096, Ratio 11345.875552120908\nEpoch 690, Train Loss: 0.00010531, Test Loss: 1.42502809, Ratio 13532.22593124304\nEpoch 700, Train Loss: 0.00008800, Test Loss: 1.42305493, Ratio 16171.974534722214\nEpoch 710, Train Loss: 0.00007337, Test Loss: 1.42122245, Ratio 19369.994626457054\nEpoch 720, Train Loss: 0.00006108, Test Loss: 1.41955888, Ratio 23241.645122716818\nEpoch 730, Train Loss: 0.00005079, Test Loss: 1.41802669, Ratio 27918.95092118006\nEpoch 740, Train Loss: 0.00004216, Test Loss: 1.41659248, Ratio 33596.84112124536\nEpoch 750, Train Loss: 0.00003493, Test Loss: 1.41528487, Ratio 40516.329492082805\nEpoch 760, Train Loss: 0.00002888, Test Loss: 1.41409564, Ratio 48963.219518997255\nEpoch 770, Train Loss: 0.00002384, Test Loss: 1.41299987, Ratio 59279.24075675487\nEpoch 780, Train Loss: 0.00001964, Test Loss: 1.41200471, Ratio 71882.1248840869\nEpoch 790, Train Loss: 0.00001617, Test Loss: 1.41109300, Ratio 87291.61631938232\nEpoch 800, Train Loss: 0.00001328, Test Loss: 1.41026258, Ratio 106195.32569931494\nEpoch 810, Train Loss: 0.00001089, Test Loss: 1.40950942, Ratio 129411.51494238169\nEpoch 820, Train Loss: 0.00000892, Test Loss: 1.40881777, Ratio 158006.90384458847\nEpoch 830, Train Loss: 0.00000729, Test Loss: 1.40819585, Ratio 193273.9322553477\nEpoch 840, Train Loss: 0.00000595, Test Loss: 1.40762448, Ratio 236655.7847151842\nEpoch 850, Train Loss: 0.00000485, Test Loss: 1.40711176, Ratio 290161.85298093106\nEpoch 860, Train Loss: 0.00000395, Test Loss: 1.40664816, Ratio 356416.93098058173\nEpoch 870, Train Loss: 0.00000321, Test Loss: 1.40622580, Ratio 438476.15090624057\nEpoch 880, Train Loss: 0.00000260, Test Loss: 1.40583968, Ratio 540323.3129256676\nEpoch 890, Train Loss: 0.00000211, Test Loss: 1.40549731, Ratio 666723.6756902512\nEpoch 900, Train Loss: 0.00000171, Test Loss: 1.40518463, Ratio 823981.0188286278\nEpoch 910, Train Loss: 0.00000138, Test Loss: 1.40490746, Ratio 1020247.2405521821\nEpoch 920, Train Loss: 0.00000111, Test Loss: 1.40465271, Ratio 1265448.9113091829\nEpoch 930, Train Loss: 0.00000089, Test Loss: 1.40442634, Ratio 1572278.9253607336\nEpoch 940, Train Loss: 0.00000072, Test Loss: 1.40421653, Ratio 1956695.405682972\nEpoch 950, Train Loss: 0.00000058, Test Loss: 1.40403402, Ratio 2439523.8436454623\nEpoch 960, Train Loss: 0.00000046, Test Loss: 1.40386701, Ratio 3046285.8816725067\nEpoch 970, Train Loss: 0.00000037, Test Loss: 1.40371728, Ratio 3811871.6190705528\nEpoch 980, Train Loss: 0.00000029, Test Loss: 1.40358329, Ratio 4778229.522211848\nEpoch 990, Train Loss: 0.00000023, Test Loss: 1.40346420, Ratio 5995321.09562888\n\nSample Predictions:\nPredicted: tensor([-0.8506, -2.6671,  0.7330, -2.9675, -1.6610])\nActual:    tensor([-0.7246, -4.4192, -1.8902, -4.3644, -2.2865])\n","output_type":"stream"}],"execution_count":115},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport numpy as np\n\n# Set random seed for reproducibility\ntorch.manual_seed(42)\nnp.random.seed(42)\n\n# Features A and B\nA = np.random.uniform(-5, 5, n_samples).astype(np.float32)\nB = np.random.uniform(-5, 5, n_samples).astype(np.float32)\n\n# Noisy A and B\nnoise_level = 1.0\nnoisy_A1 = np.random.normal(0, noise_level, n_samples).astype(np.float32) * 10\nnoisy_A2 = np.random.normal(0, noise_level, n_samples).astype(np.float32) * 0\nnoisy_A3 = np.random.normal(0, noise_level, n_samples).astype(np.float32) * 0\nnoisy_A4 = np.random.normal(0, noise_level, n_samples).astype(np.float32) * 0\nnoisy_A5 = np.random.normal(0, noise_level, n_samples).astype(np.float32) * 0\nnoisy_A6 = np.random.normal(0, noise_level, n_samples).astype(np.float32) * 0\nnoisy_A7 = np.random.normal(0, noise_level, n_samples).astype(np.float32) * 0\n\n# Target y\ny = (A).astype(np.float32) + np.random.normal(0, noise_level, n_samples).astype(np.float32) * 0\n\n# Combine features into a single dataset\nX = np.stack([A, noisy_A1, noisy_A2, noisy_A3, noisy_A4, noisy_A5, noisy_A6, noisy_A7], axis=1)\n\n# Convert to PyTorch tensors\nX_tensor = torch.tensor(X)\ny_tensor = torch.tensor(y).unsqueeze(1)\n\nprint(train_indices.shape)\nX_train, X_test = X_tensor[train_indices], X_tensor[test_indices]\ny_train, y_test = y_tensor[train_indices], y_tensor[test_indices]\n\n# Define the neural network model\nclass NeuralNetwork(nn.Module):\n    def __init__(self):\n        super(NeuralNetwork, self).__init__()\n        self.model = nn.Sequential(\n            nn.Linear(8, 512),  # Input layer with 4 features\n            nn.ReLU(),         # Hidden layer with ReLU activation\n            nn.Linear(512, 512), # Another hidden layer\n            nn.ReLU(),\n            nn.Linear(512, 512), # Another hidden layer\n            nn.ReLU(),\n            nn.Linear(512, 512), # Another hidden layer\n            nn.ReLU(),\n            nn.Linear(512, 1),  # Output layer\n        )\n    \n    def forward(self, x):\n        return self.model(x)\n\n# Initialize the model, loss function, and optimizer\nmodel = NeuralNetwork()\ncriterion = nn.MSELoss()\noptimizer = optim.Adam(model.parameters(), lr=0.00001)\n\n# Training loop\nn_epochs = 1000\nfor epoch in range(n_epochs):\n    model.train()\n    optimizer.zero_grad()\n    y_pred = model(X_train)\n    loss = criterion(y_pred, y_train)\n    loss.backward()\n    optimizer.step()\n    \n    # Evaluate on the test set\n    model.eval()\n    with torch.no_grad():\n        test_loss = criterion(model(X_test), y_test)\n    \n    if epoch % 10 == 0:\n        print(f\"Epoch {epoch}, Train Loss: {loss.item():.8f}, Test Loss: {test_loss.item():.8f}, Ratio {test_loss.item() / loss.item()}\")\n\n# Evaluate the model on the test set\nmodel.eval()\nwith torch.no_grad():\n    predictions = model(X_test)\n\n# Compare predictions with the actual values\nprint(\"\\nSample Predictions:\")\nprint(f\"Predicted: {predictions[:5].squeeze()}\")\nprint(f\"Actual:    {y_test[:5].squeeze()}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-19T02:15:16.962053Z","iopub.execute_input":"2024-11-19T02:15:16.962460Z","iopub.status.idle":"2024-11-19T02:15:27.023576Z","shell.execute_reply.started":"2024-11-19T02:15:16.962422Z","shell.execute_reply":"2024-11-19T02:15:27.022381Z"}},"outputs":[{"name":"stdout","text":"(80,)\nEpoch 0, Train Loss: 8.66910362, Test Loss: 9.36234283, Ratio 1.079966654250384\nEpoch 10, Train Loss: 8.43119240, Test Loss: 9.16146660, Ratio 1.0866157674929242\nEpoch 20, Train Loss: 8.20664692, Test Loss: 8.95676041, Ratio 1.091403163146217\nEpoch 30, Train Loss: 7.97741699, Test Loss: 8.73652172, Ratio 1.095156706668605\nEpoch 40, Train Loss: 7.73154736, Test Loss: 8.49347878, Ratio 1.098548373866659\nEpoch 50, Train Loss: 7.45937824, Test Loss: 8.21927452, Ratio 1.1018712624133424\nEpoch 60, Train Loss: 7.15457153, Test Loss: 7.91061020, Ratio 1.1056721094006594\nEpoch 70, Train Loss: 6.81586313, Test Loss: 7.56832123, Ratio 1.1103980641813596\nEpoch 80, Train Loss: 6.43583202, Test Loss: 7.18580914, Ratio 1.1165314925970506\nEpoch 90, Train Loss: 6.01337528, Test Loss: 6.75732708, Ratio 1.1237161764502979\nEpoch 100, Train Loss: 5.54554605, Test Loss: 6.27608395, Ratio 1.1317341672332402\nEpoch 110, Train Loss: 5.03363991, Test Loss: 5.73844624, Ratio 1.1400192188405265\nEpoch 120, Train Loss: 4.48081684, Test Loss: 5.15723324, Ratio 1.1509582786081531\nEpoch 130, Train Loss: 3.89946556, Test Loss: 4.54049015, Ratio 1.16438780636095\nEpoch 140, Train Loss: 3.30339098, Test Loss: 3.89435148, Ratio 1.1788951130048257\nEpoch 150, Train Loss: 2.71212602, Test Loss: 3.24761772, Ratio 1.1974435190916322\nEpoch 160, Train Loss: 2.14993215, Test Loss: 2.62737370, Ratio 1.2220728454957817\nEpoch 170, Train Loss: 1.63892019, Test Loss: 2.04961467, Ratio 1.250588456327452\nEpoch 180, Train Loss: 1.20599437, Test Loss: 1.53491211, Ratio 1.2727357196785407\nEpoch 190, Train Loss: 0.87173498, Test Loss: 1.11419511, Ratio 1.2781351421093576\nEpoch 200, Train Loss: 0.63845903, Test Loss: 0.79771680, Ratio 1.249440861349392\nEpoch 210, Train Loss: 0.48763782, Test Loss: 0.57768059, Ratio 1.184650916338119\nEpoch 220, Train Loss: 0.39008835, Test Loss: 0.43307528, Ratio 1.110197931988626\nEpoch 230, Train Loss: 0.32070822, Test Loss: 0.33837247, Ratio 1.0550788937497757\nEpoch 240, Train Loss: 0.26593581, Test Loss: 0.27249795, Ratio 1.0246756674221773\nEpoch 250, Train Loss: 0.22197554, Test Loss: 0.22376303, Ratio 1.0080526828872718\nEpoch 260, Train Loss: 0.18671235, Test Loss: 0.18593070, Ratio 0.9958135854801583\nEpoch 270, Train Loss: 0.15828724, Test Loss: 0.15521359, Ratio 0.9805818360444556\nEpoch 280, Train Loss: 0.13501832, Test Loss: 0.13001169, Ratio 0.9629189113783735\nEpoch 290, Train Loss: 0.11579472, Test Loss: 0.10946588, Ratio 0.9453443457680059\nEpoch 300, Train Loss: 0.09972998, Test Loss: 0.09268928, Ratio 0.9294023524356262\nEpoch 310, Train Loss: 0.08623978, Test Loss: 0.07880829, Ratio 0.9138276445763236\nEpoch 320, Train Loss: 0.07477580, Test Loss: 0.06710954, Ratio 0.8974767238481192\nEpoch 330, Train Loss: 0.06490599, Test Loss: 0.05721902, Ratio 0.8815676435401618\nEpoch 340, Train Loss: 0.05638349, Test Loss: 0.04874562, Ratio 0.8645371940403274\nEpoch 350, Train Loss: 0.04900439, Test Loss: 0.04152039, Ratio 0.8472789992973516\nEpoch 360, Train Loss: 0.04259112, Test Loss: 0.03538058, Ratio 0.8307031035160773\nEpoch 370, Train Loss: 0.03704584, Test Loss: 0.03022385, Ratio 0.8158501213143134\nEpoch 380, Train Loss: 0.03225660, Test Loss: 0.02589983, Ratio 0.8029313829815492\nEpoch 390, Train Loss: 0.02811938, Test Loss: 0.02222960, Ratio 0.7905437805124997\nEpoch 400, Train Loss: 0.02455483, Test Loss: 0.01910781, Ratio 0.7781691302732741\nEpoch 410, Train Loss: 0.02148832, Test Loss: 0.01647668, Ratio 0.7667738486389449\nEpoch 420, Train Loss: 0.01883580, Test Loss: 0.01426731, Ratio 0.7574568890413091\nEpoch 430, Train Loss: 0.01655002, Test Loss: 0.01243963, Ratio 0.7516383938911627\nEpoch 440, Train Loss: 0.01458064, Test Loss: 0.01089014, Ratio 0.746890648715195\nEpoch 450, Train Loss: 0.01287393, Test Loss: 0.00958944, Ratio 0.7448727244632369\nEpoch 460, Train Loss: 0.01139532, Test Loss: 0.00848004, Ratio 0.744168446372983\nEpoch 470, Train Loss: 0.01011448, Test Loss: 0.00755011, Ratio 0.7464650291003674\nEpoch 480, Train Loss: 0.00899814, Test Loss: 0.00676854, Ratio 0.752215039206432\nEpoch 490, Train Loss: 0.00802960, Test Loss: 0.00609556, Ratio 0.7591368143271744\nEpoch 500, Train Loss: 0.00719220, Test Loss: 0.00552436, Ratio 0.7681039969921928\nEpoch 510, Train Loss: 0.00645939, Test Loss: 0.00503448, Ratio 0.7794054885785339\nEpoch 520, Train Loss: 0.00581261, Test Loss: 0.00460880, Ratio 0.7928960350391516\nEpoch 530, Train Loss: 0.00524216, Test Loss: 0.00423881, Ratio 0.8086010635628245\nEpoch 540, Train Loss: 0.00473834, Test Loss: 0.00392894, Ratio 0.8291803052172241\nEpoch 550, Train Loss: 0.00428635, Test Loss: 0.00365293, Ratio 0.8522240585707921\nEpoch 560, Train Loss: 0.00386752, Test Loss: 0.00342637, Ratio 0.8859333761968059\nEpoch 570, Train Loss: 0.00351021, Test Loss: 0.00319197, Ratio 0.9093370759083276\nEpoch 580, Train Loss: 0.00319637, Test Loss: 0.00302042, Ratio 0.9449542634324603\nEpoch 590, Train Loss: 0.00291952, Test Loss: 0.00287124, Ratio 0.9834637651004833\nEpoch 600, Train Loss: 0.00267384, Test Loss: 0.00274234, Ratio 1.0256193322536922\nEpoch 610, Train Loss: 0.00245474, Test Loss: 0.00261901, Ratio 1.0669212363325464\nEpoch 620, Train Loss: 0.00225883, Test Loss: 0.00250257, Ratio 1.1079018175940998\nEpoch 630, Train Loss: 0.00208291, Test Loss: 0.00239727, Ratio 1.1509246907394999\nEpoch 640, Train Loss: 0.00192484, Test Loss: 0.00229509, Ratio 1.1923535621696437\nEpoch 650, Train Loss: 0.00178287, Test Loss: 0.00220517, Ratio 1.236865311331252\nEpoch 660, Train Loss: 0.00165493, Test Loss: 0.00212723, Ratio 1.285392037435669\nEpoch 670, Train Loss: 0.00153689, Test Loss: 0.00205485, Ratio 1.3370158104173961\nEpoch 680, Train Loss: 0.00143005, Test Loss: 0.00198744, Ratio 1.3897683389197029\nEpoch 690, Train Loss: 0.00133356, Test Loss: 0.00192761, Ratio 1.4454662138527163\nEpoch 700, Train Loss: 0.00124692, Test Loss: 0.00187204, Ratio 1.5013263490991162\nEpoch 710, Train Loss: 0.00116842, Test Loss: 0.00182153, Ratio 1.5589769178108193\nEpoch 720, Train Loss: 0.00109777, Test Loss: 0.00177723, Ratio 1.6189414713303751\nEpoch 730, Train Loss: 0.00103308, Test Loss: 0.00173708, Ratio 1.6814519532370271\nEpoch 740, Train Loss: 0.00097361, Test Loss: 0.00169601, Ratio 1.741983304633258\nEpoch 750, Train Loss: 0.00091899, Test Loss: 0.00165484, Ratio 1.8007190239212265\nEpoch 760, Train Loss: 0.00086859, Test Loss: 0.00161647, Ratio 1.8610208772858845\nEpoch 770, Train Loss: 0.00082257, Test Loss: 0.00158030, Ratio 1.9211600123183943\nEpoch 780, Train Loss: 0.00078030, Test Loss: 0.00154682, Ratio 1.9823340122071975\nEpoch 790, Train Loss: 0.00074171, Test Loss: 0.00151145, Ratio 2.0378034704218067\nEpoch 800, Train Loss: 0.00070593, Test Loss: 0.00147764, Ratio 2.0931762618312786\nEpoch 810, Train Loss: 0.00067270, Test Loss: 0.00144379, Ratio 2.1462660693755207\nEpoch 820, Train Loss: 0.00064168, Test Loss: 0.00141143, Ratio 2.199603588926252\nEpoch 830, Train Loss: 0.00061310, Test Loss: 0.00137979, Ratio 2.2505300994120345\nEpoch 840, Train Loss: 0.00058651, Test Loss: 0.00134884, Ratio 2.2997960917823184\nEpoch 850, Train Loss: 0.00056159, Test Loss: 0.00131841, Ratio 2.347649713261608\nEpoch 860, Train Loss: 0.00053852, Test Loss: 0.00128906, Ratio 2.393723795870636\nEpoch 870, Train Loss: 0.00051718, Test Loss: 0.00126134, Ratio 2.4388886237745715\nEpoch 880, Train Loss: 0.00049735, Test Loss: 0.00123605, Ratio 2.485273020202973\nEpoch 890, Train Loss: 0.00047891, Test Loss: 0.00121008, Ratio 2.5267642190848605\nEpoch 900, Train Loss: 0.00046159, Test Loss: 0.00118305, Ratio 2.563005188828398\nEpoch 910, Train Loss: 0.00044517, Test Loss: 0.00115673, Ratio 2.5983839673409572\nEpoch 920, Train Loss: 0.00042930, Test Loss: 0.00113178, Ratio 2.6363247782996453\nEpoch 930, Train Loss: 0.00041398, Test Loss: 0.00110724, Ratio 2.674619309074821\nEpoch 940, Train Loss: 0.00039985, Test Loss: 0.00108300, Ratio 2.708503799458468\nEpoch 950, Train Loss: 0.00038643, Test Loss: 0.00105984, Ratio 2.7426478197231945\nEpoch 960, Train Loss: 0.00037356, Test Loss: 0.00103701, Ratio 2.776036937303955\nEpoch 970, Train Loss: 0.00036127, Test Loss: 0.00101434, Ratio 2.807681373345528\nEpoch 980, Train Loss: 0.00034967, Test Loss: 0.00099155, Ratio 2.8356340354654423\nEpoch 990, Train Loss: 0.00033880, Test Loss: 0.00097090, Ratio 2.86574489314245\n\nSample Predictions:\nPredicted: tensor([-0.7177, -4.4261, -1.8075, -4.3609, -2.3039])\nActual:    tensor([-0.7246, -4.4192, -1.8902, -4.3644, -2.2865])\n","output_type":"stream"}],"execution_count":116},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}