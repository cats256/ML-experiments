{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":408,"sourceType":"datasetVersion","datasetId":180},{"sourceId":668,"sourceType":"datasetVersion","datasetId":308}],"dockerImageVersionId":30787,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np\nimport math\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom scipy.special import logit\nfrom scipy.stats import norm\n\nimport tensorflow as tf\nfrom keras import layers, models, datasets\n\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport torch.nn.functional as F\nfrom torch.utils.data import TensorDataset, DataLoader\nfrom torchinfo import summary\nfrom torch.optim.lr_scheduler import StepLR\nimport torch.autograd.profiler as profiler\n\nfrom sklearn.model_selection import train_test_split, LeaveOneOut, StratifiedKFold, cross_val_predict\nfrom sklearn.preprocessing import StandardScaler, MinMaxScaler, LabelEncoder, PowerTransformer\nfrom sklearn.metrics import f1_score, log_loss, accuracy_score\nfrom sklearn.linear_model import LogisticRegression\n\nimport sys\nimport time\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-10-25T22:28:52.148219Z","iopub.execute_input":"2024-10-25T22:28:52.148538Z","iopub.status.idle":"2024-10-25T22:29:09.816036Z","shell.execute_reply.started":"2024-10-25T22:28:52.148502Z","shell.execute_reply":"2024-10-25T22:29:09.815004Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"markdown","source":"An interpolation based learning technique, driven through explicit regularization","metadata":{}},{"cell_type":"code","source":"def calculate_metrics(model, data_tensor, labels_tensor, batch_size=1024):\n    model.eval()\n    all_preds = []\n    all_labels = []\n\n    with torch.no_grad():\n        for start_idx in range(0, len(data_tensor), batch_size):\n            end_idx = min(start_idx + batch_size, len(data_tensor))\n            inputs = data_tensor[start_idx:end_idx].view(-1, 54)\n            labels = labels_tensor[start_idx:end_idx]\n\n            outputs = model(inputs)\n            _, preds = torch.max(outputs, 1)\n            all_preds.extend(preds.cpu().numpy())\n            all_labels.extend(labels.cpu().numpy())\n\n    accuracy = accuracy_score(all_labels, all_preds)\n    f1 = f1_score(all_labels, all_preds, average='weighted')\n    return accuracy, f1","metadata":{"execution":{"iopub.status.busy":"2024-10-25T22:29:09.817861Z","iopub.execute_input":"2024-10-25T22:29:09.818414Z","iopub.status.idle":"2024-10-25T22:29:09.825923Z","shell.execute_reply.started":"2024-10-25T22:29:09.818379Z","shell.execute_reply":"2024-10-25T22:29:09.825057Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"class CustomDataLoader:\n    def __init__(self, features, labels, validation_size=0.2):\n        train_data, val_data, train_labels, val_labels = train_test_split(\n            features, labels, test_size=validation_size, stratify=labels, random_state=42\n        )\n        \n        self.train_data_tensor = torch.tensor(train_data).float().to(device)\n        self.train_labels_tensor = torch.tensor(train_labels).long().to(device)\n        \n        self.val_data_tensor = torch.tensor(val_data).float().to(device)\n        self.val_labels_tensor = torch.tensor(val_labels).long().to(device)","metadata":{"execution":{"iopub.status.busy":"2024-10-25T22:29:09.827044Z","iopub.execute_input":"2024-10-25T22:29:09.827436Z","iopub.status.idle":"2024-10-25T22:29:09.893868Z","shell.execute_reply.started":"2024-10-25T22:29:09.827393Z","shell.execute_reply":"2024-10-25T22:29:09.893005Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"def evaluate_model(model, custom_train_loader, criterion, optimizer, num_epochs, scheduler, batch_size=1024):\n    unregularized_criterion = nn.CrossEntropyLoss()\n\n    for epoch in range(num_epochs):\n        running_loss = 0.0\n        \n        model.train()\n        for start_idx in range(0, len(custom_train_loader.train_data_tensor), batch_size):\n            end_idx = min(start_idx + batch_size, len(custom_train_loader.train_data_tensor))\n            inputs = custom_train_loader.train_data_tensor[start_idx:end_idx].view(-1, 54)\n            labels = custom_train_loader.train_labels_tensor[start_idx:end_idx]\n\n            optimizer.zero_grad()\n            outputs = model(inputs)\n            loss = criterion(outputs, labels, model)\n            loss.backward()\n            optimizer.step()\n            scheduler.step()\n            running_loss += loss.item() * len(labels)\n                      \n        avg_train_loss = running_loss / len(custom_train_loader.train_data_tensor)\n\n        model.eval()\n        val_loss = 0.0\n        with torch.no_grad():\n            for start_idx in range(0, len(custom_train_loader.val_data_tensor), batch_size):\n                end_idx = min(start_idx + batch_size, len(custom_train_loader.val_data_tensor))\n                val_inputs = custom_train_loader.val_data_tensor[start_idx:end_idx].view(-1, 54)\n                val_labels = custom_train_loader.val_labels_tensor[start_idx:end_idx]\n\n                val_outputs = model(val_inputs)\n                val_loss += unregularized_criterion(val_outputs, val_labels).item() * len(val_labels)\n\n        avg_val_loss = val_loss / len(custom_train_loader.val_data_tensor)\n\n        train_accuracy, train_f1 = calculate_metrics(model, custom_train_loader.train_data_tensor, custom_train_loader.train_labels_tensor)\n        val_accuracy, val_f1 = calculate_metrics(model, custom_train_loader.val_data_tensor, custom_train_loader.val_labels_tensor)\n\n        print(f'Epoch {epoch + 1}, Training Loss: {avg_train_loss}, Validation Loss: {avg_val_loss}')\n        print(f'Training Accuracy: {train_accuracy}, Training F1 Score: {train_f1}')\n        print(f'Validation Accuracy: {val_accuracy}, Validation F1 Score: {val_f1}')\n        print()\n","metadata":{"execution":{"iopub.status.busy":"2024-10-26T00:50:20.758235Z","iopub.execute_input":"2024-10-26T00:50:20.758638Z","iopub.status.idle":"2024-10-26T00:50:20.771244Z","shell.execute_reply.started":"2024-10-26T00:50:20.758599Z","shell.execute_reply":"2024-10-26T00:50:20.770390Z"},"trusted":true},"execution_count":134,"outputs":[]},{"cell_type":"code","source":"class CustomLoss(nn.Module):\n    def __init__(self, criterion, l1_lambda, l2_lambda):\n        super(CustomLoss, self).__init__()\n        self.criterion = criterion\n        self.l1_lambda = l1_lambda\n        self.l2_lambda = l2_lambda\n\n    def forward(self, outputs, labels, model):\n        loss = self.criterion(outputs, labels)\n        \n        l1_norm = sum(p.abs().sum() for name, p in model.named_parameters() if 'bias' not in name)\n        l2_norm = sum(p.pow(2.0).sum() for name, p in model.named_parameters() if 'bias' not in name)\n        \n        loss += self.l1_lambda * l1_norm + self.l2_lambda * l2_norm\n        return loss","metadata":{"execution":{"iopub.status.busy":"2024-10-26T00:48:26.901933Z","iopub.execute_input":"2024-10-26T00:48:26.902920Z","iopub.status.idle":"2024-10-26T00:48:26.910213Z","shell.execute_reply.started":"2024-10-26T00:48:26.902877Z","shell.execute_reply":"2024-10-26T00:48:26.909029Z"},"trusted":true},"execution_count":132,"outputs":[]},{"cell_type":"code","source":"data = pd.read_csv('/kaggle/input/forest-cover-type-dataset/covtype.csv')\n# data = pd.read_csv('/kaggle/input/breast-cancer-wisconsin-data/data.csv')\n\n\"\"\"data = data.dropna()\"\"\"\n\n\"\"\"\nX = data[[\n    \"Elevation\",\n    \"Aspect\",\n    \"Slope\",\n    \"Horizontal_Distance_To_Hydrology\",\n    \"Vertical_Distance_To_Hydrology\",\n    \"Horizontal_Distance_To_Roadways\",\n    \"Hillshade_9am\",\n    \"Hillshade_Noon\",\n    \"Hillshade_3pm\",\n    \"Horizontal_Distance_To_Fire_Points\"\n]]\n\"\"\"\n\n# print(data.columns)\n# X = data[['radius_worst', 'concave points_worst']]\n# X = data.drop([\"id\", \"diagnosis\", \"Unnamed: 32\"], axis=1)\n# y = data[\"diagnosis\"]\nX = data.drop([\"Cover_Type\"], axis=1)\ny = data[\"Cover_Type\"]\n\nX = pd.get_dummies(X, drop_first=True)\nfor col in X.columns:\n    if (X[col] > 0).all():\n        X[col] = np.log(X[col])\n\nprint(X.shape, y.shape)\nprint(X.columns)","metadata":{"execution":{"iopub.status.busy":"2024-10-25T22:29:09.924430Z","iopub.execute_input":"2024-10-25T22:29:09.924718Z","iopub.status.idle":"2024-10-25T22:29:13.168050Z","shell.execute_reply.started":"2024-10-25T22:29:09.924688Z","shell.execute_reply":"2024-10-25T22:29:13.167119Z"},"trusted":true},"execution_count":6,"outputs":[{"name":"stdout","text":"(581012, 54) (581012,)\nIndex(['Elevation', 'Aspect', 'Slope', 'Horizontal_Distance_To_Hydrology',\n       'Vertical_Distance_To_Hydrology', 'Horizontal_Distance_To_Roadways',\n       'Hillshade_9am', 'Hillshade_Noon', 'Hillshade_3pm',\n       'Horizontal_Distance_To_Fire_Points', 'Wilderness_Area1',\n       'Wilderness_Area2', 'Wilderness_Area3', 'Wilderness_Area4',\n       'Soil_Type1', 'Soil_Type2', 'Soil_Type3', 'Soil_Type4', 'Soil_Type5',\n       'Soil_Type6', 'Soil_Type7', 'Soil_Type8', 'Soil_Type9', 'Soil_Type10',\n       'Soil_Type11', 'Soil_Type12', 'Soil_Type13', 'Soil_Type14',\n       'Soil_Type15', 'Soil_Type16', 'Soil_Type17', 'Soil_Type18',\n       'Soil_Type19', 'Soil_Type20', 'Soil_Type21', 'Soil_Type22',\n       'Soil_Type23', 'Soil_Type24', 'Soil_Type25', 'Soil_Type26',\n       'Soil_Type27', 'Soil_Type28', 'Soil_Type29', 'Soil_Type30',\n       'Soil_Type31', 'Soil_Type32', 'Soil_Type33', 'Soil_Type34',\n       'Soil_Type35', 'Soil_Type36', 'Soil_Type37', 'Soil_Type38',\n       'Soil_Type39', 'Soil_Type40'],\n      dtype='object')\n","output_type":"stream"}]},{"cell_type":"code","source":"x_scaler = StandardScaler()\nx_scaled = x_scaler.fit_transform(X)\n\nlabel_encoder = LabelEncoder()\ny_encoded = label_encoder.fit_transform(y)","metadata":{"execution":{"iopub.status.busy":"2024-10-26T00:13:28.409162Z","iopub.execute_input":"2024-10-26T00:13:28.410008Z","iopub.status.idle":"2024-10-26T00:13:28.989062Z","shell.execute_reply.started":"2024-10-26T00:13:28.409969Z","shell.execute_reply":"2024-10-26T00:13:28.988070Z"},"trusted":true},"execution_count":108,"outputs":[]},{"cell_type":"code","source":"class TestClass(torch.nn.Module):\n    def __init__(self, control_points, num_features, num_classes):\n        super(TestClass, self).__init__()\n        self.control_points = control_points\n        self.num_features = num_features\n        self.num_classes = num_classes\n        \n        self.copy_tensor = nn.Parameter(torch.zeros(self.num_features, self.num_classes, self.control_points + 2))\n        self.feature_idx = torch.arange(self.num_features).view(1, -1, 1).to(device)\n        self.class_idx = torch.arange(self.num_classes).view(1, 1, -1).to(device)\n        \n    def forward(self, x):        \n        scaled_x = x * self.control_points\n        \n        lower_idx = torch.floor(scaled_x).long()\n        upper_idx = lower_idx + 1\n\n        lower_value = self.copy_tensor[self.feature_idx, self.class_idx, lower_idx.unsqueeze(-1)]\n        upper_value = self.copy_tensor[self.feature_idx, self.class_idx, upper_idx.unsqueeze(-1)]\n\n        interp_factor = (scaled_x - lower_idx.float()).unsqueeze(-1)\n        interpolated_value = torch.lerp(lower_value, upper_value, interp_factor)\n        \"\"\"\n        interpolated_value = lower_value + (upper_value - lower_value) * interp_factor\n        \"\"\"\n\n        summed_tensor = interpolated_value.sum(dim=1)\n        return summed_tensor\n","metadata":{"execution":{"iopub.status.busy":"2024-10-26T00:13:29.944114Z","iopub.execute_input":"2024-10-26T00:13:29.944496Z","iopub.status.idle":"2024-10-26T00:13:29.953765Z","shell.execute_reply.started":"2024-10-26T00:13:29.944458Z","shell.execute_reply":"2024-10-26T00:13:29.952847Z"},"trusted":true},"execution_count":109,"outputs":[]},{"cell_type":"code","source":"class TestClass(torch.nn.Module):\n    def __init__(self, control_points, num_features, num_classes):\n        super(TestClass, self).__init__()\n        self.control_points = control_points\n        self.num_features = num_features\n        self.num_classes = num_classes\n        self.num_tensors = 1\n        \n        self.copy_tensor = nn.Parameter(torch.rand(self.num_tensors, self.num_features, self.num_features * 8, self.control_points + 2) * 2 - 1)\n        self.feature_idx = torch.arange(self.num_features).view(1, -1, 1).to(device)\n        self.class_idx = torch.arange(self.num_features * 8).view(1, 1, -1).to(device)\n\n        self.final_tensor = nn.Parameter(torch.zeros(self.num_features * 8, self.num_classes, self.control_points + 2))\n        self.final_feature_idx = torch.arange(self.num_features * 8).view(1, -1, 1).to(device)\n        self.final_class_idx = torch.arange(self.num_classes).view(1, 1, -1).to(device)\n        \n    def forward(self, x):        \n        scaled_x = x * self.control_points\n        \n        for i in range(self.num_tensors):\n            lower_idx = torch.floor(scaled_x).long()\n            upper_idx = lower_idx + 1        \n\n            copy_tensor = self.copy_tensor[i]\n            lower_value = copy_tensor[self.feature_idx, self.class_idx, lower_idx.unsqueeze(-1)]\n            upper_value = copy_tensor[self.feature_idx, self.class_idx, upper_idx.unsqueeze(-1)]\n\n            interp_factor = (scaled_x - lower_idx.float()).unsqueeze(-1)\n            interpolated_value = torch.lerp(lower_value, upper_value, interp_factor)\n\n            summed_tensor = interpolated_value.sum(dim=1)\n            scaled_x = torch.sigmoid(summed_tensor) * self.control_points\n            \n        lower_idx = torch.floor(scaled_x).long()\n        upper_idx = lower_idx + 1\n\n        lower_value = self.final_tensor[self.final_feature_idx, self.final_class_idx, lower_idx.unsqueeze(-1)]\n        upper_value = self.final_tensor[self.final_feature_idx, self.final_class_idx, upper_idx.unsqueeze(-1)]\n\n        interp_factor = (scaled_x - lower_idx.float()).unsqueeze(-1)\n        interpolated_value = torch.lerp(lower_value, upper_value, interp_factor)\n\n        summed_tensor = interpolated_value.sum(dim=1)\n        return summed_tensor","metadata":{"execution":{"iopub.status.busy":"2024-10-26T00:13:31.344417Z","iopub.execute_input":"2024-10-26T00:13:31.345102Z","iopub.status.idle":"2024-10-26T00:13:31.358985Z","shell.execute_reply.started":"2024-10-26T00:13:31.345049Z","shell.execute_reply":"2024-10-26T00:13:31.358024Z"},"trusted":true},"execution_count":110,"outputs":[]},{"cell_type":"code","source":"class CustomLayer(nn.Module):\n    def __init__(self, control_points, num_features, num_outputs):\n        super(CustomLayer, self).__init__()\n        \n        self.control_points = control_points\n        self.num_features = num_features\n        self.num_outputs = num_outputs\n        \n        self.copy_tensor = nn.Parameter(torch.zeros(self.num_features, self.num_outputs, self.control_points + 2))\n\n        self.feature_idx = torch.arange(self.num_features).view(1, -1, 1).to(device)\n        self.output_idx = torch.arange(self.num_outputs).view(1, 1, -1).to(device)\n\n    def forward(self, x):\n        scaled_x = x * self.control_points\n        \n        lower_idx = torch.floor(scaled_x).long()\n        upper_idx = lower_idx + 1\n\n        lower_value = self.copy_tensor[self.feature_idx, self.output_idx, lower_idx.unsqueeze(-1)]\n        upper_value = self.copy_tensor[self.feature_idx, self.output_idx, upper_idx.unsqueeze(-1)]\n\n        interp_factor = (scaled_x - lower_idx.float()).unsqueeze(-1)\n        interpolated_value = torch.lerp(lower_value, upper_value, interp_factor)\n        \n        return interpolated_value.sum(dim=1)","metadata":{"execution":{"iopub.status.busy":"2024-10-25T22:30:16.279959Z","iopub.execute_input":"2024-10-25T22:30:16.280910Z","iopub.status.idle":"2024-10-25T22:30:16.292437Z","shell.execute_reply.started":"2024-10-25T22:30:16.280856Z","shell.execute_reply":"2024-10-25T22:30:16.291402Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"class TestClass(torch.nn.Module):\n    def __init__(self, control_points, num_features, num_classes, num_layers=4):\n        super(TestClass, self).__init__()\n        self.activation = nn.Sigmoid()\n        self.copy_tensor = None\n        \n        self.first_layer = CustomLayer(control_points, num_features, num_features)\n        \n        self.layers = nn.ModuleList()\n        layer_size = num_features\n        for i in range(num_layers):\n            self.layers.append(CustomLayer(control_points, layer_size, layer_size))\n            layer_size *= 2\n            \n        self.last_layer = CustomLayer(control_points, layer_size, num_classes)\n\n    def forward(self, x):\n        outputs = [self.activation(self.first_layer(x))]\n        print(outputs)\n        return None\n        for layer in self.layers:\n            concatenated_outputs = torch.cat(outputs, dim=1)\n            outputs.append(self.activation(layer(concatenated_outputs)))\n\n        concatenated_outputs = torch.cat(outputs, dim=1)\n        return self.last_layer(concatenated_outputs)\n","metadata":{"execution":{"iopub.status.busy":"2024-10-26T00:09:56.118988Z","iopub.execute_input":"2024-10-26T00:09:56.119367Z","iopub.status.idle":"2024-10-26T00:09:56.128434Z","shell.execute_reply.started":"2024-10-26T00:09:56.119332Z","shell.execute_reply":"2024-10-26T00:09:56.127424Z"},"trusted":true},"execution_count":91,"outputs":[]},{"cell_type":"code","source":"num_epochs = 1\nnum_features = 54\nnum_classes = 7\n\nmodel = TestClass(50, num_features, num_classes).to(device)\ncriterion = CustomLoss(nn.CrossEntropyLoss(), first_order_weight=0.0, second_order_weight=0.0)\ncustom_train_loader = CustomDataLoader(x_scaled, y_encoded, validation_size=0.2)\n\nfor name, param in model.named_parameters():\n    break\n    print(f\"Layer: {name}\")\n    print(f\"Shape: {param.shape}\")\n    print(param)\n    \ntotal_params = sum(p.numel() for p in model.parameters())\nprint(f'Total number of parameters: {total_params}')","metadata":{"execution":{"iopub.status.busy":"2024-10-26T00:09:57.502648Z","iopub.execute_input":"2024-10-26T00:09:57.503530Z","iopub.status.idle":"2024-10-26T00:09:58.334248Z","shell.execute_reply.started":"2024-10-26T00:09:57.503487Z","shell.execute_reply":"2024-10-26T00:09:58.333199Z"},"trusted":true},"execution_count":92,"outputs":[{"name":"stdout","text":"Total number of parameters: 13354848\n","output_type":"stream"}]},{"cell_type":"code","source":"optimizer = optim.Adam(model.parameters(), lr=0.0001)\n\ntorch.cuda.synchronize()\nstart_time = time.time()\n\nevaluate_model(model, custom_train_loader, criterion, optimizer, 10, 1024 * 1)\n\nelapsed_time = time.time() - start_time\nprint(f\"Execution time: {elapsed_time:.6f} seconds\")","metadata":{"execution":{"iopub.status.busy":"2024-10-25T22:30:21.893578Z","iopub.execute_input":"2024-10-25T22:30:21.894011Z","iopub.status.idle":"2024-10-25T22:40:43.926183Z","shell.execute_reply.started":"2024-10-25T22:30:21.893969Z","shell.execute_reply":"2024-10-25T22:40:43.924940Z"},"collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":14,"outputs":[{"name":"stdout","text":"1.945911169052124\n1.8539522886276245\n1.8163913488388062\n1.715201497077942\n1.6285886764526367\n1.5590041875839233\n1.4995362758636475\n1.434444546699524\n1.4196561574935913\n1.3613539934158325\n1.342638611793518\n1.3134902715682983\n1.307121753692627\n1.256211280822754\n1.2345473766326904\n1.2597142457962036\n1.322450876235962\n1.2466729879379272\n1.2229632139205933\n1.2213397026062012\n1.2172123193740845\n1.246507167816162\n1.267657995223999\n1.1504957675933838\n1.2170944213867188\n1.2137398719787598\n1.1827181577682495\n1.1604377031326294\n1.243660569190979\n1.202210783958435\n1.1625198125839233\n1.206444501876831\n1.2000513076782227\n1.1536914110183716\n1.1840262413024902\n1.0890932083129883\n1.1023576259613037\n1.0915223360061646\n1.0756689310073853\n1.066365122795105\n1.0776621103286743\n1.059004783630371\n1.0676666498184204\n1.0391112565994263\n1.0311548709869385\n1.02267324924469\n1.0441426038742065\n1.0317800045013428\n0.9669236540794373\n1.0050534009933472\n1.0003108978271484\n0.972579300403595\n0.9390429258346558\n0.9791568517684937\n0.9634207487106323\n0.9791294932365417\n0.9351717233657837\n1.005050539970398\n0.9631199240684509\n0.9584965705871582\n0.9230943322181702\n0.9379952549934387\n0.908242404460907\n0.9722333550453186\n0.9358109831809998\n0.9148333668708801\n0.8906043767929077\n0.9364398121833801\n0.9156736731529236\n0.9146056771278381\n0.9091563820838928\n0.8867032527923584\n0.9202242493629456\n0.9285148978233337\n0.8906529545783997\n0.9071884751319885\n0.9120542407035828\n0.9207101464271545\n0.9243211150169373\n0.8838826417922974\n0.9001631736755371\n0.8971020579338074\n0.8974078297615051\n0.9037265181541443\n0.8795702457427979\n0.9016750454902649\n0.8911728858947754\n0.9045199155807495\n0.905388355255127\n0.8612877726554871\n0.9229925274848938\n0.8774603605270386\n0.8880602717399597\n0.8782229423522949\n0.8443280458450317\n0.8469269871711731\n0.8366394639015198\n0.8820289969444275\n0.8795858025550842\n0.8648414611816406\n0.856878399848938\n0.8823636174201965\n0.8592439889907837\n0.8418151140213013\n0.8359853625297546\n0.9069457650184631\n0.855623722076416\n0.8733200430870056\n0.8513335585594177\n0.9417465329170227\n0.878559410572052\n0.859660804271698\n0.814525306224823\n0.8190531134605408\n0.8495919704437256\n0.8129695653915405\n0.8843043446540833\n0.8372322916984558\n0.8426076173782349\n0.8276400566101074\n0.8741180896759033\n0.8062178492546082\n0.8659454584121704\n0.8362687826156616\n0.8279706835746765\n0.8514378070831299\n0.8463622331619263\n0.8250638842582703\n0.844689667224884\n0.835104763507843\n0.8063673377037048\n0.8315103650093079\n0.8245901465415955\n0.8709135055541992\n0.8298376798629761\n0.8064380288124084\n0.8869982957839966\n0.8104671239852905\n0.8002964854240417\n0.8391864895820618\n0.8413243889808655\n0.8581200838088989\n0.7906520962715149\n0.8256132006645203\n0.8252668380737305\n0.8580768704414368\n0.8812165260314941\n0.8324961066246033\n0.8192349076271057\n0.8117034435272217\n0.8132330179214478\n0.8225097060203552\n0.8200896978378296\n0.8294039368629456\n0.801386296749115\n0.8333284258842468\n0.8126593232154846\n0.7997696399688721\n0.8352392911911011\n0.8125798106193542\n0.7710001468658447\n0.8176700472831726\n0.8227382898330688\n0.8014302253723145\n0.7936318516731262\n0.8175134658813477\n0.8162907361984253\n0.8539597988128662\n0.8456635475158691\n0.7876113653182983\n0.7860226631164551\n0.817348301410675\n0.8082634210586548\n0.854112446308136\n0.8549636602401733\n0.7777242660522461\n0.7956171631813049\n0.8093209862709045\n0.821717381477356\n0.817050039768219\n0.8140982389450073\n0.8247317671775818\n0.8031589388847351\n0.8151313066482544\n0.8125351071357727\n0.7943525910377502\n0.8406657576560974\n0.8016443848609924\n0.7638715505599976\n0.8459386229515076\n0.8129270672798157\n0.7578145265579224\n0.8175333738327026\n0.8410303592681885\n0.7795150279998779\n0.7530336380004883\n0.7685321569442749\n0.7582896947860718\n0.8152076005935669\n0.7606654763221741\n0.8352653384208679\n0.8123151063919067\n0.7330055832862854\n0.847744882106781\n0.8009970784187317\n0.8326815366744995\n0.7701510787010193\n0.7753229141235352\n0.7991154789924622\n0.7984557747840881\n0.7850295305252075\n0.8366614580154419\n0.8036537766456604\n0.7604438066482544\n0.8113211989402771\n0.7749791741371155\n0.7529757618904114\n0.7346832752227783\n0.7661459445953369\n0.7197831273078918\n0.8184854984283447\n0.7648232579231262\n0.7846871614456177\n0.7313863635063171\n0.7602084875106812\n0.7489795684814453\n0.7283865213394165\n0.7597160339355469\n0.7606282234191895\n0.7655876874923706\n0.7391210794448853\n0.766880214214325\n0.7567527294158936\n0.707065761089325\n0.7420213222503662\n0.7989319562911987\n0.7433822154998779\n0.7839528918266296\n0.7311304807662964\n0.7792048454284668\n0.7798144817352295\n0.7448393106460571\n0.7075382471084595\n0.7589676380157471\n0.7738656401634216\n0.7127843499183655\n0.7599323987960815\n0.7131657600402832\n0.7726325988769531\n0.7141486406326294\n0.7670695185661316\n0.7787036299705505\n0.7252501845359802\n0.7252166867256165\n0.7350460290908813\n0.7617301940917969\n0.725007176399231\n0.7724388837814331\n0.7302626371383667\n0.7080588936805725\n0.7540996670722961\n0.729278028011322\n0.7841126918792725\n0.695656418800354\n0.7719150185585022\n0.7341253757476807\n0.7379425764083862\n0.7138524651527405\n0.7218484878540039\n0.774787425994873\n0.7430711984634399\n0.7347457408905029\n0.7054752707481384\n0.6940138936042786\n0.694941520690918\n0.6935109496116638\n0.683009147644043\n0.7110257148742676\n0.7115353345870972\n0.6881383061408997\n0.7326762080192566\n0.7015483975410461\n0.7452131509780884\n0.7239547967910767\n0.6833944320678711\n0.7006976008415222\n0.6675755977630615\n0.6850226521492004\n0.7136625647544861\n0.723403811454773\n0.6990900039672852\n0.7166821956634521\n0.6774905920028687\n0.6794390678405762\n0.6824817061424255\n0.699536919593811\n0.6647465229034424\n0.7019347548484802\n0.681816816329956\n0.6867773532867432\n0.708192765712738\n0.7071071863174438\n0.6856801509857178\n0.7152935266494751\n0.6885113716125488\n0.6744605898857117\n0.6734246015548706\n0.7193668484687805\n0.7029830813407898\n0.7391001582145691\n0.6656379699707031\n0.6571235060691833\n0.7019691467285156\n0.6987849473953247\n0.7565891742706299\n0.6841771006584167\n0.7181691527366638\n0.6963784098625183\n0.6877467632293701\n0.6763890981674194\n0.69855797290802\n0.7272307276725769\n0.6839767694473267\n0.7703147530555725\n0.7209352850914001\n0.7360941767692566\n0.7018846273422241\n0.7249958515167236\n0.6915627717971802\n0.6576994061470032\n0.7023499608039856\n0.6785081028938293\n0.6970094442367554\n0.6680492162704468\n0.648033857345581\n0.6740310192108154\n0.6847168207168579\n0.7000852823257446\n0.6727181673049927\n0.701505184173584\n0.7111537456512451\n0.6825075745582581\n0.7030436992645264\n0.6577497124671936\n0.6616673469543457\n0.6998439431190491\n0.691017210483551\n0.6701408624649048\n0.6969867944717407\n0.6760584115982056\n0.6585784554481506\n0.6868233680725098\n0.6898147463798523\n0.6859768033027649\n0.7117825150489807\n0.6898642778396606\n0.6976630091667175\n0.680872917175293\n0.6801261901855469\n0.6736471652984619\n0.672346830368042\n0.7207903861999512\n0.6770881414413452\n0.7062545418739319\n0.6691394448280334\n0.6716052293777466\n0.743735134601593\n0.6496461033821106\n0.6574037671089172\n0.6584364771842957\n0.6605256795883179\n0.7080008387565613\n0.6776394844055176\n0.6697494387626648\n0.682252049446106\n0.7154034376144409\n0.654119610786438\n0.6731154322624207\n0.7027682662010193\n0.7298459410667419\n0.6747728586196899\n0.6491166949272156\n0.6576361656188965\n0.6336808204650879\n0.6546182632446289\n0.6763656735420227\n0.685638964176178\n0.6746010184288025\n0.6444292068481445\n0.6777545809745789\n0.7319328188896179\n0.7319127917289734\n0.7008119821548462\n0.6899442076683044\n0.681169331073761\n0.6448588967323303\n0.6602593660354614\n0.6317156553268433\n0.664351224899292\n0.6621474623680115\n0.6969469785690308\n0.6664055585861206\n0.6444072127342224\n0.6824924349784851\n0.6838952898979187\n0.6855757832527161\n0.7156120538711548\n0.6699908375740051\n0.6859595775604248\n0.6710336208343506\n0.6602016687393188\n0.6715532541275024\n0.6831156015396118\n0.6550442576408386\n0.6446437239646912\n0.6872392892837524\n0.6656369566917419\n0.7112116813659668\n0.6403927206993103\n0.6903684735298157\n0.6252633929252625\n0.645000696182251\n0.7199010848999023\n0.7063155770301819\n0.6701204180717468\n0.665422260761261\n0.6561087965965271\n0.6831251382827759\n0.7112225890159607\n0.6909390091896057\n0.6601241230964661\n0.631305992603302\n0.6498958468437195\n0.7099085450172424\n0.6674528121948242\n0.7305779457092285\n0.6612175703048706\n0.6598528623580933\n0.6758033633232117\n0.6480567455291748\n0.6303557753562927\n0.6857258677482605\n0.6934777498245239\n0.6671667695045471\n0.6969985365867615\n0.6838088035583496\n0.6678000092506409\n0.7110821008682251\n0.687682032585144\n0.6848893761634827\n0.6632381081581116\n0.692904531955719\n0.706535279750824\n0.7100374698638916\nEpoch 1, Training Loss: 0.8162772270813723, Validation Loss: 0.6796758010284977\nTraining Accuracy: 0.7124388727412765, Training F1 Score: 0.6932436013798989\nValidation Accuracy: 0.7115995284114868, Validation F1 Score: 0.6923432734150791\n\n0.6655221581459045\n0.6953747868537903\n0.7137449383735657\n0.6891246438026428\n0.6391295194625854\n0.6976150870323181\n0.6713640689849854\n0.7336257100105286\n0.6434841752052307\n0.7076207995414734\n0.6703879237174988\n0.683961033821106\n0.6820425391197205\n0.6657644510269165\n0.6744499206542969\n0.6323316097259521\n0.7215200066566467\n0.6965123414993286\n0.6498559713363647\n0.6662318706512451\n0.6622243523597717\n0.7736323475837708\n0.6785959601402283\n0.6774518489837646\n0.6901065707206726\n0.694222629070282\n0.6773737668991089\n0.6144750118255615\n0.6781290173530579\n0.6660934686660767\n0.6439775228500366\n0.6850652098655701\n0.7017162442207336\n0.669266402721405\n0.7090206742286682\n0.6415011286735535\n0.681833028793335\n0.6429591178894043\n0.649594783782959\n0.6745637655258179\n0.6865224838256836\n0.6840713024139404\n0.653985857963562\n0.6435381174087524\n0.6536770462989807\n0.7187737226486206\n0.702976644039154\n0.6728252172470093\n0.6802211403846741\n0.6712284088134766\n0.6976239681243896\n0.6663673520088196\n0.6752086877822876\n0.6925041675567627\n0.6822721362113953\n0.6878503561019897\n0.6621330976486206\n0.6905900835990906\n0.6691911220550537\n0.6896938681602478\n0.6989048719406128\n0.6476462483406067\n0.6783720850944519\n0.6932051777839661\n0.6847100853919983\n0.6829112768173218\n0.6637732982635498\n0.7086086869239807\n0.6508098244667053\n0.6629388928413391\n0.6635426878929138\n0.6737496256828308\n0.700049877166748\n0.6966070532798767\n0.6575220227241516\n0.6508268117904663\n0.6852580904960632\n0.7146923542022705\n0.6958252787590027\n0.679990291595459\n0.664623498916626\n0.662372350692749\n0.694508969783783\n0.6667199730873108\n0.6381693482398987\n0.659895658493042\n0.6845928430557251\n0.7086726427078247\n0.6921170353889465\n0.6256387233734131\n0.6746071577072144\n0.658419668674469\n0.6785993576049805\n0.669421911239624\n0.655613124370575\n0.6631150841712952\n0.6358250379562378\n0.6735789775848389\n0.6754230856895447\n0.7052167057991028\n0.6772088408470154\n0.6880057454109192\n0.6789360046386719\n0.657026469707489\n0.6618945002555847\n0.7029233574867249\n0.7000139355659485\n0.6581109762191772\n0.6774256229400635\n0.726793110370636\n0.6939579248428345\n0.6822556257247925\n0.6447388529777527\n0.6576524972915649\n0.7034448981285095\n0.6483745574951172\n0.7057192325592041\n0.6698145866394043\n0.6716630458831787\n0.6886985301971436\n0.7120176553726196\n0.6613411903381348\n0.6982343196868896\n0.6791759133338928\n0.6627140045166016\n0.7225216031074524\n0.6860294342041016\n0.665959894657135\n0.7097775340080261\n0.6574816703796387\n0.6441076993942261\n0.6971055269241333\n0.6517495512962341\n0.6942645311355591\n0.6586489081382751\n0.6349501609802246\n0.6993365287780762\n0.6656073331832886\n0.6672852039337158\n0.709403395652771\n0.6955874562263489\n0.6953598260879517\n0.6378132700920105\n0.6536039710044861\n0.6809787750244141\n0.7356476783752441\n0.7225887775421143\n0.7315561175346375\n0.648364782333374\n0.6918790340423584\n0.7291354537010193\n0.6428318023681641\n0.706192672252655\n0.7101380228996277\n0.676750898361206\n0.7205962538719177\n0.665925920009613\n0.6768124103546143\n0.7414814829826355\n0.6651451587677002\n0.6601628661155701\n0.7188758850097656\n0.7176392674446106\n0.6839138269424438\n0.7091245651245117\n0.6814801096916199\n0.6974226832389832\n0.7039121985435486\n0.6858583092689514\n0.6645075678825378\n0.6465450525283813\n0.7036769390106201\n0.6756879091262817\n0.7046504020690918\n0.696260392665863\n0.6534827351570129\n0.6619494557380676\n0.6671671867370605\n0.6832727789878845\n0.6944796442985535\n0.6725436449050903\n0.6833463311195374\n0.6794934868812561\n0.6983436346054077\n0.6755077242851257\n0.6786171197891235\n0.7117162942886353\n0.6812727451324463\n0.6585345268249512\n0.7282969951629639\n0.6818485260009766\n0.6617276668548584\n0.6790995597839355\n0.6918138265609741\n0.6871994137763977\n0.6761695146560669\n0.6641731858253479\n0.6612299680709839\n0.7221135497093201\n0.6520594954490662\n0.7174882888793945\n0.6707412600517273\n0.6376439332962036\n0.7003328204154968\n0.6603522896766663\n0.6959320902824402\n0.6673908233642578\n0.6851763725280762\n0.6933193206787109\n0.6845239996910095\n0.6819489598274231\n0.7071326971054077\n0.6898418068885803\n0.6561803817749023\n0.7024322748184204\n0.6597784757614136\n0.6515881419181824\n0.6437461376190186\n0.6521362662315369\n0.6517273187637329\n0.7041119933128357\n0.6523115038871765\n0.6915391683578491\n0.6436156034469604\n0.6580680012702942\n0.6770256757736206\n0.6291050910949707\n0.6815455555915833\n0.6714910268783569\n0.6669344902038574\n0.6686686277389526\n0.6772701144218445\n0.6879944205284119\n0.640123188495636\n0.6422306299209595\n0.6839068531990051\n0.6810691952705383\n0.6956518292427063\n0.6442145705223083\n0.6910146474838257\n0.6733108162879944\n0.6712656617164612\n0.6856803297996521\n0.6777971982955933\n0.6770817637443542\n0.672078549861908\n0.6528199315071106\n0.6461068987846375\n0.6823019981384277\n0.6360591053962708\n0.6973972320556641\n0.7057431936264038\n0.6504014134407043\n0.6641990542411804\n0.6756721138954163\n0.7033207416534424\n0.6872397065162659\n0.6542537212371826\n0.6842455267906189\n0.6374194025993347\n0.6601116061210632\n0.6744270920753479\n0.7133162617683411\n0.6731560826301575\n0.6527438759803772\n0.6830596327781677\n0.6620435118675232\n0.663192093372345\n0.6760517358779907\n0.7368194460868835\n0.7083941102027893\n0.6956834197044373\n0.676688551902771\n0.6488497853279114\n0.6647071242332458\n0.6574754118919373\n0.6335130929946899\n0.6918700337409973\n0.66910320520401\n0.645939826965332\n0.707158625125885\n0.6802223920822144\n0.7196269631385803\n0.6938194632530212\n0.6483548879623413\n0.6742728352546692\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","Cell \u001b[0;32mIn[14], line 6\u001b[0m\n\u001b[1;32m      3\u001b[0m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39msynchronize()\n\u001b[1;32m      4\u001b[0m start_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[0;32m----> 6\u001b[0m \u001b[43mevaluate_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcustom_train_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1024\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      8\u001b[0m elapsed_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;241m-\u001b[39m start_time\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExecution time: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00melapsed_time\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.6f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m seconds\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n","Cell \u001b[0;32mIn[4], line 29\u001b[0m, in \u001b[0;36mevaluate_model\u001b[0;34m(model, custom_train_loader, criterion, optimizer, num_epochs, batch_size)\u001b[0m\n\u001b[1;32m     27\u001b[0m         loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[1;32m     28\u001b[0m         optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[0;32m---> 29\u001b[0m         running_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mlen\u001b[39m(labels)\n\u001b[1;32m     30\u001b[0m         \u001b[38;5;28mprint\u001b[39m(loss\u001b[38;5;241m.\u001b[39mitem())\n\u001b[1;32m     32\u001b[0m avg_train_loss \u001b[38;5;241m=\u001b[39m running_loss \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mlen\u001b[39m(custom_train_loader\u001b[38;5;241m.\u001b[39mtrain_data_tensor)\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "],"ename":"KeyboardInterrupt","evalue":"","output_type":"error"}]},{"cell_type":"code","source":"class CustomLinearLayer(nn.Module):\n    def __init__(self, input_size, output_size, init=\"default\"):\n        super(CustomLinearLayer, self).__init__()\n        self.linear = nn.Linear(input_size, output_size, bias=True)\n        nn.init.zeros_(self.linear.bias)\n        \n        if init == \"zero\":\n            nn.init.zeros_(self.linear.weight)\n        elif init == \"splits_inputs\":\n            self.splits_inputs_init()\n        elif init == \"looks_linear\":\n            self.looks_linear_init()\n\n    def looks_linear_init(self):\n        with torch.no_grad():\n            size = self.linear.weight.size(0)\n            weight = torch.zeros(size, size)\n\n            indices = torch.arange(0, size, step=2)\n\n            weight[indices, indices] = 1\n            weight[indices, indices + 1] = -1\n            weight[indices + 1, indices] = -1\n            weight[indices + 1, indices + 1] = 1\n\n            self.linear.weight.copy_(weight)\n        \n        \"\"\" Example matrix: [\n            [1, -1, 0, 0],\n            [-1, 1, 0, 0],\n            [0, 0, 1, -1],\n            [0, 0, -1, 1]\n        ] \"\"\"\n            \n    def splits_inputs_init(self):\n        with torch.no_grad():\n            weight = torch.zeros(self.linear.out_features, self.linear.in_features)\n\n            for i in range(self.linear.in_features):\n                weight[2 * i, i] = 1\n                weight[2 * i + 1, i] = -1\n\n            self.linear.weight.copy_(weight)\n            \n        \"\"\" Example matrix: [\n            [1, 0, 0],\n            [-1, 0, 0],\n            [0, 1, 0],\n            [0, -1, 0],\n            [0, 0, 1],\n            [0, 0, -1]\n        ] \"\"\"\n        self.linear.weight.requires_grad = False\n\n    def forward(self, x):\n        return self.linear(x)","metadata":{"execution":{"iopub.status.busy":"2024-10-26T00:23:17.343005Z","iopub.execute_input":"2024-10-26T00:23:17.343420Z","iopub.status.idle":"2024-10-26T00:23:17.356331Z","shell.execute_reply.started":"2024-10-26T00:23:17.343382Z","shell.execute_reply":"2024-10-26T00:23:17.355485Z"},"trusted":true},"execution_count":121,"outputs":[]},{"cell_type":"code","source":"class customReLU(torch.autograd.Function):\n    @staticmethod\n    def forward(ctx, input):\n        ctx.save_for_backward(input)\n        return input.clamp(min=0)\n\n    @staticmethod\n    def backward(ctx, grad_output):\n        (input,) = ctx.saved_tensors\n        grad_input = torch.zeros_like(input)\n        \n        grad_input[input > 0] = grad_output[input > 0]\n        grad_input[input == 0] = grad_output[input == 0] * 0.5\n        return grad_input\n\ndef custom_relu(x):\n    return customReLU.apply(x)","metadata":{"execution":{"iopub.status.busy":"2024-10-26T00:23:19.101901Z","iopub.execute_input":"2024-10-26T00:23:19.102314Z","iopub.status.idle":"2024-10-26T00:23:19.109195Z","shell.execute_reply.started":"2024-10-26T00:23:19.102276Z","shell.execute_reply":"2024-10-26T00:23:19.108247Z"},"trusted":true},"execution_count":122,"outputs":[]},{"cell_type":"code","source":"class TabularDenseNet(nn.Module):\n    def __init__(self, input_size, num_layers, output_size):\n        super(TabularDenseNet, self).__init__()\n        self.copy_tensor = None\n        self.activation = nn.GELU()\n        \n        layer_size = input_size * 2\n        self.first_layer = CustomLinearLayer(input_size, layer_size, init=\"splits_inputs\")\n        \n        self.layers = nn.ModuleList()\n        for i in range(num_layers):\n            self.layers.append(CustomLinearLayer(layer_size, layer_size, init=\"looks_linear\"))\n            layer_size *= 2\n            \n        self.last_layer = CustomLinearLayer(layer_size, output_size, init=\"zero\")\n\n    def forward(self, x):\n        outputs = [custom_relu(self.first_layer(x))]\n    \n        for layer in self.layers:\n            concatenated_outputs = torch.cat(outputs, dim=1)\n            outputs.append(custom_relu(layer(concatenated_outputs)))\n\n        concatenated_outputs = torch.cat(outputs, dim=1)\n        return self.last_layer(concatenated_outputs)","metadata":{"execution":{"iopub.status.busy":"2024-10-26T00:23:19.483463Z","iopub.execute_input":"2024-10-26T00:23:19.483845Z","iopub.status.idle":"2024-10-26T00:23:19.492591Z","shell.execute_reply.started":"2024-10-26T00:23:19.483808Z","shell.execute_reply":"2024-10-26T00:23:19.491569Z"},"trusted":true},"execution_count":123,"outputs":[]},{"cell_type":"code","source":"num_epochs = 1\nnum_features = 54\nnum_classes = 7\n\nmodel = TabularDenseNet(num_features, 6, num_classes).to(device)\ncriterion = CustomLoss(nn.CrossEntropyLoss(), first_order_weight=0.0, second_order_weight=0.0)\ncustom_train_loader = CustomDataLoader(x_scaled, y_encoded, validation_size=0.2)\n\nfor name, param in model.named_parameters():\n    break\n    print(f\"Layer: {name}\")\n    print(f\"Shape: {param.shape}\")\n    print(param)\n    \ntotal_params = sum(p.numel() for p in model.parameters())\nprint(f'Total number of parameters: {total_params}')","metadata":{"execution":{"iopub.status.busy":"2024-10-26T00:26:29.778394Z","iopub.execute_input":"2024-10-26T00:26:29.779076Z","iopub.status.idle":"2024-10-26T00:26:30.729866Z","shell.execute_reply.started":"2024-10-26T00:26:29.779031Z","shell.execute_reply":"2024-10-26T00:26:30.728865Z"},"trusted":true},"execution_count":129,"outputs":[{"name":"stdout","text":"Total number of parameters: 15982495\n","output_type":"stream"}]},{"cell_type":"code","source":"for name, param in model.named_parameters():\n    print(name)\n    print(param)","metadata":{"execution":{"iopub.status.busy":"2024-10-26T00:13:58.206535Z","iopub.execute_input":"2024-10-26T00:13:58.206920Z","iopub.status.idle":"2024-10-26T00:13:58.256492Z","shell.execute_reply.started":"2024-10-26T00:13:58.206883Z","shell.execute_reply":"2024-10-26T00:13:58.255590Z"},"collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":115,"outputs":[{"name":"stdout","text":"first_layer.linear.weight\nParameter containing:\ntensor([[ 1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  ...,  0.,  0.,  0.,  0.,  0.,\n          0.,  0.,  0.,  0.,  0.],\n        [-1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  ...,  0.,  0.,  0.,  0.,  0.,\n          0.,  0.,  0.,  0.,  0.],\n        [ 0.,  1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  ...,  0.,  0.,  0.,  0.,  0.,\n          0.,  0.,  0.,  0.,  0.],\n        [ 0., -1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  ...,  0.,  0.,  0.,  0.,  0.,\n          0.,  0.,  0.,  0.,  0.],\n        [ 0.,  0.,  1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  ...,  0.,  0.,  0.,  0.,  0.,\n          0.,  0.,  0.,  0.,  0.],\n        [ 0.,  0., -1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  ...,  0.,  0.,  0.,  0.,  0.,\n          0.,  0.,  0.,  0.,  0.],\n        [ 0.,  0.,  0.,  1.,  0.,  0.,  0.,  0.,  0.,  0.,  ...,  0.,  0.,  0.,  0.,  0.,\n          0.,  0.,  0.,  0.,  0.],\n        [ 0.,  0.,  0., -1.,  0.,  0.,  0.,  0.,  0.,  0.,  ...,  0.,  0.,  0.,  0.,  0.,\n          0.,  0.,  0.,  0.,  0.],\n        [ 0.,  0.,  0.,  0.,  1.,  0.,  0.,  0.,  0.,  0.,  ...,  0.,  0.,  0.,  0.,  0.,\n          0.,  0.,  0.,  0.,  0.],\n        [ 0.,  0.,  0.,  0., -1.,  0.,  0.,  0.,  0.,  0.,  ...,  0.,  0.,  0.,  0.,  0.,\n          0.,  0.,  0.,  0.,  0.],\n        ...,\n        [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  ...,  0.,  0.,  0.,  0.,  0.,\n          1.,  0.,  0.,  0.,  0.],\n        [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  ...,  0.,  0.,  0.,  0.,  0.,\n         -1.,  0.,  0.,  0.,  0.],\n        [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  ...,  0.,  0.,  0.,  0.,  0.,\n          0.,  1.,  0.,  0.,  0.],\n        [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  ...,  0.,  0.,  0.,  0.,  0.,\n          0., -1.,  0.,  0.,  0.],\n        [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  ...,  0.,  0.,  0.,  0.,  0.,\n          0.,  0.,  1.,  0.,  0.],\n        [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  ...,  0.,  0.,  0.,  0.,  0.,\n          0.,  0., -1.,  0.,  0.],\n        [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  ...,  0.,  0.,  0.,  0.,  0.,\n          0.,  0.,  0.,  1.,  0.],\n        [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  ...,  0.,  0.,  0.,  0.,  0.,\n          0.,  0.,  0., -1.,  0.],\n        [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  ...,  0.,  0.,  0.,  0.,  0.,\n          0.,  0.,  0.,  0.,  1.],\n        [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  ...,  0.,  0.,  0.,  0.,  0.,\n          0.,  0.,  0.,  0., -1.]], device='cuda:0', requires_grad=True)\nfirst_layer.linear.bias\nParameter containing:\ntensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,  ..., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n       device='cuda:0', requires_grad=True)\nlayers.0.linear.weight\nParameter containing:\ntensor([[ 1., -1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  ...,  0.,  0.,  0.,  0.,  0.,\n          0.,  0.,  0.,  0.,  0.],\n        [-1.,  1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  ...,  0.,  0.,  0.,  0.,  0.,\n          0.,  0.,  0.,  0.,  0.],\n        [ 0.,  0.,  1., -1.,  0.,  0.,  0.,  0.,  0.,  0.,  ...,  0.,  0.,  0.,  0.,  0.,\n          0.,  0.,  0.,  0.,  0.],\n        [ 0.,  0., -1.,  1.,  0.,  0.,  0.,  0.,  0.,  0.,  ...,  0.,  0.,  0.,  0.,  0.,\n          0.,  0.,  0.,  0.,  0.],\n        [ 0.,  0.,  0.,  0.,  1., -1.,  0.,  0.,  0.,  0.,  ...,  0.,  0.,  0.,  0.,  0.,\n          0.,  0.,  0.,  0.,  0.],\n        [ 0.,  0.,  0.,  0., -1.,  1.,  0.,  0.,  0.,  0.,  ...,  0.,  0.,  0.,  0.,  0.,\n          0.,  0.,  0.,  0.,  0.],\n        [ 0.,  0.,  0.,  0.,  0.,  0.,  1., -1.,  0.,  0.,  ...,  0.,  0.,  0.,  0.,  0.,\n          0.,  0.,  0.,  0.,  0.],\n        [ 0.,  0.,  0.,  0.,  0.,  0., -1.,  1.,  0.,  0.,  ...,  0.,  0.,  0.,  0.,  0.,\n          0.,  0.,  0.,  0.,  0.],\n        [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  1., -1.,  ...,  0.,  0.,  0.,  0.,  0.,\n          0.,  0.,  0.,  0.,  0.],\n        [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -1.,  1.,  ...,  0.,  0.,  0.,  0.,  0.,\n          0.,  0.,  0.,  0.,  0.],\n        ...,\n        [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  ...,  1., -1.,  0.,  0.,  0.,\n          0.,  0.,  0.,  0.,  0.],\n        [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  ..., -1.,  1.,  0.,  0.,  0.,\n          0.,  0.,  0.,  0.,  0.],\n        [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  ...,  0.,  0.,  1., -1.,  0.,\n          0.,  0.,  0.,  0.,  0.],\n        [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  ...,  0.,  0., -1.,  1.,  0.,\n          0.,  0.,  0.,  0.,  0.],\n        [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  ...,  0.,  0.,  0.,  0.,  1.,\n         -1.,  0.,  0.,  0.,  0.],\n        [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  ...,  0.,  0.,  0.,  0., -1.,\n          1.,  0.,  0.,  0.,  0.],\n        [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  ...,  0.,  0.,  0.,  0.,  0.,\n          0.,  1., -1.,  0.,  0.],\n        [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  ...,  0.,  0.,  0.,  0.,  0.,\n          0., -1.,  1.,  0.,  0.],\n        [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  ...,  0.,  0.,  0.,  0.,  0.,\n          0.,  0.,  0.,  1., -1.],\n        [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  ...,  0.,  0.,  0.,  0.,  0.,\n          0.,  0.,  0., -1.,  1.]], device='cuda:0', requires_grad=True)\nlayers.0.linear.bias\nParameter containing:\ntensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,  ..., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n       device='cuda:0', requires_grad=True)\nlayers.1.linear.weight\nParameter containing:\ntensor([[ 1., -1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  ...,  0.,  0.,  0.,  0.,  0.,\n          0.,  0.,  0.,  0.,  0.],\n        [-1.,  1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  ...,  0.,  0.,  0.,  0.,  0.,\n          0.,  0.,  0.,  0.,  0.],\n        [ 0.,  0.,  1., -1.,  0.,  0.,  0.,  0.,  0.,  0.,  ...,  0.,  0.,  0.,  0.,  0.,\n          0.,  0.,  0.,  0.,  0.],\n        [ 0.,  0., -1.,  1.,  0.,  0.,  0.,  0.,  0.,  0.,  ...,  0.,  0.,  0.,  0.,  0.,\n          0.,  0.,  0.,  0.,  0.],\n        [ 0.,  0.,  0.,  0.,  1., -1.,  0.,  0.,  0.,  0.,  ...,  0.,  0.,  0.,  0.,  0.,\n          0.,  0.,  0.,  0.,  0.],\n        [ 0.,  0.,  0.,  0., -1.,  1.,  0.,  0.,  0.,  0.,  ...,  0.,  0.,  0.,  0.,  0.,\n          0.,  0.,  0.,  0.,  0.],\n        [ 0.,  0.,  0.,  0.,  0.,  0.,  1., -1.,  0.,  0.,  ...,  0.,  0.,  0.,  0.,  0.,\n          0.,  0.,  0.,  0.,  0.],\n        [ 0.,  0.,  0.,  0.,  0.,  0., -1.,  1.,  0.,  0.,  ...,  0.,  0.,  0.,  0.,  0.,\n          0.,  0.,  0.,  0.,  0.],\n        [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  1., -1.,  ...,  0.,  0.,  0.,  0.,  0.,\n          0.,  0.,  0.,  0.,  0.],\n        [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -1.,  1.,  ...,  0.,  0.,  0.,  0.,  0.,\n          0.,  0.,  0.,  0.,  0.],\n        ...,\n        [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  ...,  1., -1.,  0.,  0.,  0.,\n          0.,  0.,  0.,  0.,  0.],\n        [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  ..., -1.,  1.,  0.,  0.,  0.,\n          0.,  0.,  0.,  0.,  0.],\n        [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  ...,  0.,  0.,  1., -1.,  0.,\n          0.,  0.,  0.,  0.,  0.],\n        [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  ...,  0.,  0., -1.,  1.,  0.,\n          0.,  0.,  0.,  0.,  0.],\n        [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  ...,  0.,  0.,  0.,  0.,  1.,\n         -1.,  0.,  0.,  0.,  0.],\n        [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  ...,  0.,  0.,  0.,  0., -1.,\n          1.,  0.,  0.,  0.,  0.],\n        [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  ...,  0.,  0.,  0.,  0.,  0.,\n          0.,  1., -1.,  0.,  0.],\n        [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  ...,  0.,  0.,  0.,  0.,  0.,\n          0., -1.,  1.,  0.,  0.],\n        [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  ...,  0.,  0.,  0.,  0.,  0.,\n          0.,  0.,  0.,  1., -1.],\n        [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  ...,  0.,  0.,  0.,  0.,  0.,\n          0.,  0.,  0., -1.,  1.]], device='cuda:0', requires_grad=True)\nlayers.1.linear.bias\nParameter containing:\ntensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,  ..., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n       device='cuda:0', requires_grad=True)\nlayers.2.linear.weight\nParameter containing:\ntensor([[ 1., -1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  ...,  0.,  0.,  0.,  0.,  0.,\n          0.,  0.,  0.,  0.,  0.],\n        [-1.,  1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  ...,  0.,  0.,  0.,  0.,  0.,\n          0.,  0.,  0.,  0.,  0.],\n        [ 0.,  0.,  1., -1.,  0.,  0.,  0.,  0.,  0.,  0.,  ...,  0.,  0.,  0.,  0.,  0.,\n          0.,  0.,  0.,  0.,  0.],\n        [ 0.,  0., -1.,  1.,  0.,  0.,  0.,  0.,  0.,  0.,  ...,  0.,  0.,  0.,  0.,  0.,\n          0.,  0.,  0.,  0.,  0.],\n        [ 0.,  0.,  0.,  0.,  1., -1.,  0.,  0.,  0.,  0.,  ...,  0.,  0.,  0.,  0.,  0.,\n          0.,  0.,  0.,  0.,  0.],\n        [ 0.,  0.,  0.,  0., -1.,  1.,  0.,  0.,  0.,  0.,  ...,  0.,  0.,  0.,  0.,  0.,\n          0.,  0.,  0.,  0.,  0.],\n        [ 0.,  0.,  0.,  0.,  0.,  0.,  1., -1.,  0.,  0.,  ...,  0.,  0.,  0.,  0.,  0.,\n          0.,  0.,  0.,  0.,  0.],\n        [ 0.,  0.,  0.,  0.,  0.,  0., -1.,  1.,  0.,  0.,  ...,  0.,  0.,  0.,  0.,  0.,\n          0.,  0.,  0.,  0.,  0.],\n        [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  1., -1.,  ...,  0.,  0.,  0.,  0.,  0.,\n          0.,  0.,  0.,  0.,  0.],\n        [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -1.,  1.,  ...,  0.,  0.,  0.,  0.,  0.,\n          0.,  0.,  0.,  0.,  0.],\n        ...,\n        [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  ...,  1., -1.,  0.,  0.,  0.,\n          0.,  0.,  0.,  0.,  0.],\n        [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  ..., -1.,  1.,  0.,  0.,  0.,\n          0.,  0.,  0.,  0.,  0.],\n        [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  ...,  0.,  0.,  1., -1.,  0.,\n          0.,  0.,  0.,  0.,  0.],\n        [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  ...,  0.,  0., -1.,  1.,  0.,\n          0.,  0.,  0.,  0.,  0.],\n        [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  ...,  0.,  0.,  0.,  0.,  1.,\n         -1.,  0.,  0.,  0.,  0.],\n        [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  ...,  0.,  0.,  0.,  0., -1.,\n          1.,  0.,  0.,  0.,  0.],\n        [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  ...,  0.,  0.,  0.,  0.,  0.,\n          0.,  1., -1.,  0.,  0.],\n        [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  ...,  0.,  0.,  0.,  0.,  0.,\n          0., -1.,  1.,  0.,  0.],\n        [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  ...,  0.,  0.,  0.,  0.,  0.,\n          0.,  0.,  0.,  1., -1.],\n        [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  ...,  0.,  0.,  0.,  0.,  0.,\n          0.,  0.,  0., -1.,  1.]], device='cuda:0', requires_grad=True)\nlayers.2.linear.bias\nParameter containing:\ntensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,  ..., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n       device='cuda:0', requires_grad=True)\nlayers.3.linear.weight\nParameter containing:\ntensor([[ 1., -1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  ...,  0.,  0.,  0.,  0.,  0.,\n          0.,  0.,  0.,  0.,  0.],\n        [-1.,  1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  ...,  0.,  0.,  0.,  0.,  0.,\n          0.,  0.,  0.,  0.,  0.],\n        [ 0.,  0.,  1., -1.,  0.,  0.,  0.,  0.,  0.,  0.,  ...,  0.,  0.,  0.,  0.,  0.,\n          0.,  0.,  0.,  0.,  0.],\n        [ 0.,  0., -1.,  1.,  0.,  0.,  0.,  0.,  0.,  0.,  ...,  0.,  0.,  0.,  0.,  0.,\n          0.,  0.,  0.,  0.,  0.],\n        [ 0.,  0.,  0.,  0.,  1., -1.,  0.,  0.,  0.,  0.,  ...,  0.,  0.,  0.,  0.,  0.,\n          0.,  0.,  0.,  0.,  0.],\n        [ 0.,  0.,  0.,  0., -1.,  1.,  0.,  0.,  0.,  0.,  ...,  0.,  0.,  0.,  0.,  0.,\n          0.,  0.,  0.,  0.,  0.],\n        [ 0.,  0.,  0.,  0.,  0.,  0.,  1., -1.,  0.,  0.,  ...,  0.,  0.,  0.,  0.,  0.,\n          0.,  0.,  0.,  0.,  0.],\n        [ 0.,  0.,  0.,  0.,  0.,  0., -1.,  1.,  0.,  0.,  ...,  0.,  0.,  0.,  0.,  0.,\n          0.,  0.,  0.,  0.,  0.],\n        [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  1., -1.,  ...,  0.,  0.,  0.,  0.,  0.,\n          0.,  0.,  0.,  0.,  0.],\n        [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -1.,  1.,  ...,  0.,  0.,  0.,  0.,  0.,\n          0.,  0.,  0.,  0.,  0.],\n        ...,\n        [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  ...,  1., -1.,  0.,  0.,  0.,\n          0.,  0.,  0.,  0.,  0.],\n        [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  ..., -1.,  1.,  0.,  0.,  0.,\n          0.,  0.,  0.,  0.,  0.],\n        [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  ...,  0.,  0.,  1., -1.,  0.,\n          0.,  0.,  0.,  0.,  0.],\n        [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  ...,  0.,  0., -1.,  1.,  0.,\n          0.,  0.,  0.,  0.,  0.],\n        [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  ...,  0.,  0.,  0.,  0.,  1.,\n         -1.,  0.,  0.,  0.,  0.],\n        [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  ...,  0.,  0.,  0.,  0., -1.,\n          1.,  0.,  0.,  0.,  0.],\n        [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  ...,  0.,  0.,  0.,  0.,  0.,\n          0.,  1., -1.,  0.,  0.],\n        [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  ...,  0.,  0.,  0.,  0.,  0.,\n          0., -1.,  1.,  0.,  0.],\n        [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  ...,  0.,  0.,  0.,  0.,  0.,\n          0.,  0.,  0.,  1., -1.],\n        [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  ...,  0.,  0.,  0.,  0.,  0.,\n          0.,  0.,  0., -1.,  1.]], device='cuda:0', requires_grad=True)\nlayers.3.linear.bias\nParameter containing:\ntensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,  ..., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n       device='cuda:0', requires_grad=True)\nlayers.4.linear.weight\nParameter containing:\ntensor([[ 1., -1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  ...,  0.,  0.,  0.,  0.,  0.,\n          0.,  0.,  0.,  0.,  0.],\n        [-1.,  1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  ...,  0.,  0.,  0.,  0.,  0.,\n          0.,  0.,  0.,  0.,  0.],\n        [ 0.,  0.,  1., -1.,  0.,  0.,  0.,  0.,  0.,  0.,  ...,  0.,  0.,  0.,  0.,  0.,\n          0.,  0.,  0.,  0.,  0.],\n        [ 0.,  0., -1.,  1.,  0.,  0.,  0.,  0.,  0.,  0.,  ...,  0.,  0.,  0.,  0.,  0.,\n          0.,  0.,  0.,  0.,  0.],\n        [ 0.,  0.,  0.,  0.,  1., -1.,  0.,  0.,  0.,  0.,  ...,  0.,  0.,  0.,  0.,  0.,\n          0.,  0.,  0.,  0.,  0.],\n        [ 0.,  0.,  0.,  0., -1.,  1.,  0.,  0.,  0.,  0.,  ...,  0.,  0.,  0.,  0.,  0.,\n          0.,  0.,  0.,  0.,  0.],\n        [ 0.,  0.,  0.,  0.,  0.,  0.,  1., -1.,  0.,  0.,  ...,  0.,  0.,  0.,  0.,  0.,\n          0.,  0.,  0.,  0.,  0.],\n        [ 0.,  0.,  0.,  0.,  0.,  0., -1.,  1.,  0.,  0.,  ...,  0.,  0.,  0.,  0.,  0.,\n          0.,  0.,  0.,  0.,  0.],\n        [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  1., -1.,  ...,  0.,  0.,  0.,  0.,  0.,\n          0.,  0.,  0.,  0.,  0.],\n        [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -1.,  1.,  ...,  0.,  0.,  0.,  0.,  0.,\n          0.,  0.,  0.,  0.,  0.],\n        ...,\n        [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  ...,  1., -1.,  0.,  0.,  0.,\n          0.,  0.,  0.,  0.,  0.],\n        [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  ..., -1.,  1.,  0.,  0.,  0.,\n          0.,  0.,  0.,  0.,  0.],\n        [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  ...,  0.,  0.,  1., -1.,  0.,\n          0.,  0.,  0.,  0.,  0.],\n        [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  ...,  0.,  0., -1.,  1.,  0.,\n          0.,  0.,  0.,  0.,  0.],\n        [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  ...,  0.,  0.,  0.,  0.,  1.,\n         -1.,  0.,  0.,  0.,  0.],\n        [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  ...,  0.,  0.,  0.,  0., -1.,\n          1.,  0.,  0.,  0.,  0.],\n        [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  ...,  0.,  0.,  0.,  0.,  0.,\n          0.,  1., -1.,  0.,  0.],\n        [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  ...,  0.,  0.,  0.,  0.,  0.,\n          0., -1.,  1.,  0.,  0.],\n        [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  ...,  0.,  0.,  0.,  0.,  0.,\n          0.,  0.,  0.,  1., -1.],\n        [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  ...,  0.,  0.,  0.,  0.,  0.,\n          0.,  0.,  0., -1.,  1.]], device='cuda:0', requires_grad=True)\nlayers.4.linear.bias\nParameter containing:\ntensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,  ..., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n       device='cuda:0', requires_grad=True)\nlayers.5.linear.weight\nParameter containing:\ntensor([[ 1., -1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  ...,  0.,  0.,  0.,  0.,  0.,\n          0.,  0.,  0.,  0.,  0.],\n        [-1.,  1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  ...,  0.,  0.,  0.,  0.,  0.,\n          0.,  0.,  0.,  0.,  0.],\n        [ 0.,  0.,  1., -1.,  0.,  0.,  0.,  0.,  0.,  0.,  ...,  0.,  0.,  0.,  0.,  0.,\n          0.,  0.,  0.,  0.,  0.],\n        [ 0.,  0., -1.,  1.,  0.,  0.,  0.,  0.,  0.,  0.,  ...,  0.,  0.,  0.,  0.,  0.,\n          0.,  0.,  0.,  0.,  0.],\n        [ 0.,  0.,  0.,  0.,  1., -1.,  0.,  0.,  0.,  0.,  ...,  0.,  0.,  0.,  0.,  0.,\n          0.,  0.,  0.,  0.,  0.],\n        [ 0.,  0.,  0.,  0., -1.,  1.,  0.,  0.,  0.,  0.,  ...,  0.,  0.,  0.,  0.,  0.,\n          0.,  0.,  0.,  0.,  0.],\n        [ 0.,  0.,  0.,  0.,  0.,  0.,  1., -1.,  0.,  0.,  ...,  0.,  0.,  0.,  0.,  0.,\n          0.,  0.,  0.,  0.,  0.],\n        [ 0.,  0.,  0.,  0.,  0.,  0., -1.,  1.,  0.,  0.,  ...,  0.,  0.,  0.,  0.,  0.,\n          0.,  0.,  0.,  0.,  0.],\n        [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  1., -1.,  ...,  0.,  0.,  0.,  0.,  0.,\n          0.,  0.,  0.,  0.,  0.],\n        [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -1.,  1.,  ...,  0.,  0.,  0.,  0.,  0.,\n          0.,  0.,  0.,  0.,  0.],\n        ...,\n        [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  ...,  1., -1.,  0.,  0.,  0.,\n          0.,  0.,  0.,  0.,  0.],\n        [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  ..., -1.,  1.,  0.,  0.,  0.,\n          0.,  0.,  0.,  0.,  0.],\n        [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  ...,  0.,  0.,  1., -1.,  0.,\n          0.,  0.,  0.,  0.,  0.],\n        [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  ...,  0.,  0., -1.,  1.,  0.,\n          0.,  0.,  0.,  0.,  0.],\n        [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  ...,  0.,  0.,  0.,  0.,  1.,\n         -1.,  0.,  0.,  0.,  0.],\n        [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  ...,  0.,  0.,  0.,  0., -1.,\n          1.,  0.,  0.,  0.,  0.],\n        [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  ...,  0.,  0.,  0.,  0.,  0.,\n          0.,  1., -1.,  0.,  0.],\n        [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  ...,  0.,  0.,  0.,  0.,  0.,\n          0., -1.,  1.,  0.,  0.],\n        [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  ...,  0.,  0.,  0.,  0.,  0.,\n          0.,  0.,  0.,  1., -1.],\n        [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  ...,  0.,  0.,  0.,  0.,  0.,\n          0.,  0.,  0., -1.,  1.]], device='cuda:0', requires_grad=True)\nlayers.5.linear.bias\nParameter containing:\ntensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,  ..., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n       device='cuda:0', requires_grad=True)\nlast_layer.linear.weight\nParameter containing:\ntensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,  ..., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,  ..., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,  ..., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,  ..., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,  ..., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,  ..., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,  ..., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]],\n       device='cuda:0', requires_grad=True)\nlast_layer.linear.bias\nParameter containing:\ntensor([0., 0., 0., 0., 0., 0., 0.], device='cuda:0', requires_grad=True)\n","output_type":"stream"}]},{"cell_type":"code","source":"import torch.optim as optim\nimport torch.optim.lr_scheduler as lr_scheduler\n\noptimizer = optim.Adam(model.parameters(), lr=0.001)\nscheduler = lr_scheduler.StepLR(optimizer, step_size=1, gamma=0.9995)\n\ntorch.cuda.synchronize()\nstart_time = time.time()\n\nevaluate_model(model, custom_train_loader, criterion, optimizer, 100, scheduler, 1024 * 16)\n\nelapsed_time = time.time() - start_time\nprint(f\"Execution time: {elapsed_time:.6f} seconds\")","metadata":{"execution":{"iopub.status.busy":"2024-10-26T00:26:33.493559Z","iopub.execute_input":"2024-10-26T00:26:33.494313Z","iopub.status.idle":"2024-10-26T00:46:21.864513Z","shell.execute_reply.started":"2024-10-26T00:26:33.494264Z","shell.execute_reply":"2024-10-26T00:46:21.863537Z"},"trusted":true},"execution_count":130,"outputs":[{"name":"stdout","text":"Epoch 1, Training Loss: 0.7631811161969053, Validation Loss: 0.6049645809301719\nTraining Accuracy: 0.7458439918332046, Training F1 Score: 0.7342838693278908\nValidation Accuracy: 0.746899821863463, Validation F1 Score: 0.7357112266037402\n\nEpoch 2, Training Loss: 0.5659122141144055, Validation Loss: 0.5301073068942687\nTraining Accuracy: 0.7727044872194816, Training F1 Score: 0.7662282535716993\nValidation Accuracy: 0.7730867533540442, Validation F1 Score: 0.7670259138198642\n\nEpoch 3, Training Loss: 0.49747702820954093, Validation Loss: 0.47047423482296175\nTraining Accuracy: 0.7997349448913424, Training F1 Score: 0.7967041193827643\nValidation Accuracy: 0.7977849108887034, Validation F1 Score: 0.7948715407853291\n\nEpoch 4, Training Loss: 0.44008046138619716, Validation Loss: 0.4296236481197065\nTraining Accuracy: 0.819790494590251, Training F1 Score: 0.8154495921973002\nValidation Accuracy: 0.8181028028536268, Validation F1 Score: 0.8137423959897123\n\nEpoch 5, Training Loss: 0.39515685453323335, Validation Loss: 0.38987902115951945\nTraining Accuracy: 0.8401128205348863, Training F1 Score: 0.8368565382514629\nValidation Accuracy: 0.8378785401409602, Validation F1 Score: 0.8347145337286291\n\nEpoch 6, Training Loss: 0.36181370118500844, Validation Loss: 0.35156786393792216\nTraining Accuracy: 0.8586731324049233, Training F1 Score: 0.8567546860610455\nValidation Accuracy: 0.8556233487947815, Validation F1 Score: 0.8537965709894767\n\nEpoch 7, Training Loss: 0.32805550570354963, Validation Loss: 0.32220195820043035\nTraining Accuracy: 0.8718677994617143, Training F1 Score: 0.8707289929675391\nValidation Accuracy: 0.8682822302350197, Validation F1 Score: 0.8671202116195229\n\nEpoch 8, Training Loss: 0.3039946273060004, Validation Loss: 0.3070862860260271\nTraining Accuracy: 0.8781348898149562, Training F1 Score: 0.8769412678044347\nValidation Accuracy: 0.8733681574486029, Validation F1 Score: 0.8721537020898789\n\nEpoch 9, Training Loss: 0.27961654252096274, Validation Loss: 0.2909451360302035\nTraining Accuracy: 0.8854561766230861, Training F1 Score: 0.8843590715180367\nValidation Accuracy: 0.8801838162525925, Validation F1 Score: 0.8790348358725345\n\nEpoch 10, Training Loss: 0.2608027164450092, Validation Loss: 0.2640777143730536\nTraining Accuracy: 0.8981280482951062, Training F1 Score: 0.8975570669397352\nValidation Accuracy: 0.8924898668709069, Validation F1 Score: 0.8918916111216193\n\nEpoch 11, Training Loss: 0.2460200990582497, Validation Loss: 0.2586807070532519\nTraining Accuracy: 0.9004537347598691, Training F1 Score: 0.8994798126216195\nValidation Accuracy: 0.8943744997977677, Validation F1 Score: 0.8933043174116339\n\nEpoch 12, Training Loss: 0.23161287720939688, Validation Loss: 0.2355831835549129\nTraining Accuracy: 0.9122435236839218, Training F1 Score: 0.9120245915508084\nValidation Accuracy: 0.9044430866672978, Validation F1 Score: 0.9041818319113292\n\nEpoch 13, Training Loss: 0.21512868563295592, Validation Loss: 0.22977630298616725\nTraining Accuracy: 0.9147370210129322, Training F1 Score: 0.9141027009710263\nValidation Accuracy: 0.9071968882042633, Validation F1 Score: 0.9064620132348804\n\nEpoch 14, Training Loss: 0.203115673128286, Validation Loss: 0.21974653322376123\nTraining Accuracy: 0.9192205830782106, Training F1 Score: 0.9186446610115716\nValidation Accuracy: 0.9112587454712873, Validation F1 Score: 0.9105820694747422\n\nEpoch 15, Training Loss: 0.19447549717686036, Validation Loss: 0.2104633367799832\nTraining Accuracy: 0.9238160190529874, Training F1 Score: 0.923231014233968\nValidation Accuracy: 0.9158627574159015, Validation F1 Score: 0.9151842439651351\n\nEpoch 16, Training Loss: 0.18565927214604405, Validation Loss: 0.1998774895668553\nTraining Accuracy: 0.9287148054362114, Training F1 Score: 0.9285396337217362\nValidation Accuracy: 0.9205442200287428, Validation F1 Score: 0.9203261818027572\n\nEpoch 17, Training Loss: 0.17663735818399884, Validation Loss: 0.19034894869883937\nTraining Accuracy: 0.9331703990241152, Training F1 Score: 0.932851551084817\nValidation Accuracy: 0.9241241620267979, Validation F1 Score: 0.9237395674511284\n\nEpoch 18, Training Loss: 0.16544428943097364, Validation Loss: 0.18564623058009985\nTraining Accuracy: 0.935379908736707, Training F1 Score: 0.9350961095159395\nValidation Accuracy: 0.9259485555450375, Validation F1 Score: 0.9255938850532176\n\nEpoch 19, Training Loss: 0.1595990505341424, Validation Loss: 0.18043701619031144\nTraining Accuracy: 0.9377507750495365, Training F1 Score: 0.9374936319938474\nValidation Accuracy: 0.9278503997315044, Validation F1 Score: 0.9275156658376708\n\nEpoch 20, Training Loss: 0.15464850766577806, Validation Loss: 0.18123509204435984\nTraining Accuracy: 0.937382882000994, Training F1 Score: 0.9370473199860124\nValidation Accuracy: 0.9274459351307626, Validation F1 Score: 0.9269983139254923\n\nEpoch 21, Training Loss: 0.1507325568736556, Validation Loss: 0.18218245212633608\nTraining Accuracy: 0.9370085346884419, Training F1 Score: 0.9367054517308226\nValidation Accuracy: 0.9270844986790359, Validation F1 Score: 0.926680121385504\n\nEpoch 22, Training Loss: 0.1498672653985585, Validation Loss: 0.1653208711932397\nTraining Accuracy: 0.9452119042445392, Training F1 Score: 0.9452898854012405\nValidation Accuracy: 0.9348381711315542, Validation F1 Score: 0.9349238680809651\n\nEpoch 23, Training Loss: 0.13621351123776224, Validation Loss: 0.1599125841204072\nTraining Accuracy: 0.9476516160401369, Training F1 Score: 0.9475937294975794\nValidation Accuracy: 0.9368346772458542, Validation F1 Score: 0.93673909421198\n\nEpoch 24, Training Loss: 0.13047233053499194, Validation Loss: 0.15733751485702452\nTraining Accuracy: 0.9488499577245707, Training F1 Score: 0.94879877473286\nValidation Accuracy: 0.9372391418465961, Validation F1 Score: 0.9371466797593841\n\nEpoch 25, Training Loss: 0.12571544204960305, Validation Loss: 0.15475831555991193\nTraining Accuracy: 0.950041845144995, Training F1 Score: 0.9500103949884446\nValidation Accuracy: 0.9385041694276396, Validation F1 Score: 0.9384369695091156\n\nEpoch 26, Training Loss: 0.12217074765242544, Validation Loss: 0.1523921837224253\nTraining Accuracy: 0.9512208240374003, Training F1 Score: 0.9511746385103653\nValidation Accuracy: 0.9394507887059714, Validation F1 Score: 0.9393719969453612\n\nEpoch 27, Training Loss: 0.11801693702447819, Validation Loss: 0.15114017918203393\nTraining Accuracy: 0.9519437016064664, Training F1 Score: 0.9518269591732536\nValidation Accuracy: 0.9400445771623797, Validation F1 Score: 0.9398711825897857\n\nEpoch 28, Training Loss: 0.12007948747985288, Validation Loss: 0.1475046234799553\nTraining Accuracy: 0.9539617348201089, Training F1 Score: 0.9539177603671605\nValidation Accuracy: 0.9415763792673166, Validation F1 Score: 0.9415037533757021\n\nEpoch 29, Training Loss: 0.11774332389776038, Validation Loss: 0.153693263275038\nTraining Accuracy: 0.9515306287098572, Training F1 Score: 0.9513405687824756\nValidation Accuracy: 0.9394077605569564, Validation F1 Score: 0.9391295782498519\n\nEpoch 30, Training Loss: 0.11563342137005952, Validation Loss: 0.15445483146407357\nTraining Accuracy: 0.9517264080514792, Training F1 Score: 0.9517397673806559\nValidation Accuracy: 0.9388311833601543, Validation F1 Score: 0.9388202443229408\n\nEpoch 31, Training Loss: 0.11292084225931076, Validation Loss: 0.1478051107644534\nTraining Accuracy: 0.954688915231848, Training F1 Score: 0.9545879367010729\nValidation Accuracy: 0.9416366186759378, Validation F1 Score: 0.9414861432797204\n\nEpoch 32, Training Loss: 0.1136401172650061, Validation Loss: 0.16609436639041378\nTraining Accuracy: 0.9469330413137439, Training F1 Score: 0.9465896475862461\nValidation Accuracy: 0.9354749877369776, Validation F1 Score: 0.9350181484356399\n\nEpoch 33, Training Loss: 0.1100442871211611, Validation Loss: 0.14867916838126222\nTraining Accuracy: 0.9552805560993871, Training F1 Score: 0.9552346293822579\nValidation Accuracy: 0.9416021961567257, Validation F1 Score: 0.9415284140376873\n\nEpoch 34, Training Loss: 0.10274348721846234, Validation Loss: 0.14423395050414436\nTraining Accuracy: 0.9572749236783281, Training F1 Score: 0.9571828674745001\nValidation Accuracy: 0.9434954347133895, Validation F1 Score: 0.9433555321528017\n\nEpoch 35, Training Loss: 0.10016344888144066, Validation Loss: 0.1399812733870936\nTraining Accuracy: 0.9590369377529264, Training F1 Score: 0.9589831806789866\nValidation Accuracy: 0.9454058845296593, Validation F1 Score: 0.9453075907236537\n\nEpoch 36, Training Loss: 0.09875131764259358, Validation Loss: 0.13587644517144223\nTraining Accuracy: 0.96122063040948, Training F1 Score: 0.9611806307860726\nValidation Accuracy: 0.9472388836777019, Validation F1 Score: 0.9471508928282478\n\nEpoch 37, Training Loss: 0.09712557287810596, Validation Loss: 0.13328809146559883\nTraining Accuracy: 0.9626061457501899, Training F1 Score: 0.9625794314997359\nValidation Accuracy: 0.948280164883867, Validation F1 Score: 0.9482155750664476\n\nEpoch 38, Training Loss: 0.09466457832170853, Validation Loss: 0.13586949595234776\nTraining Accuracy: 0.9615067694472353, Training F1 Score: 0.9615010200788257\nValidation Accuracy: 0.9471270104902627, Validation F1 Score: 0.9470803946746186\n\nEpoch 39, Training Loss: 0.0915555744706225, Validation Loss: 0.1335347312412663\nTraining Accuracy: 0.9628815276812627, Training F1 Score: 0.962855656330389\nValidation Accuracy: 0.9479273340619433, Validation F1 Score: 0.9478634402649676\n\nEpoch 40, Training Loss: 0.08746828578047368, Validation Loss: 0.1290138511435388\nTraining Accuracy: 0.9648758952602037, Training F1 Score: 0.9648209998002536\nValidation Accuracy: 0.9499324458060463, Validation F1 Score: 0.9498270966039792\n\nEpoch 41, Training Loss: 0.08667144076669855, Validation Loss: 0.1279649454252191\nTraining Accuracy: 0.9655019588691269, Training F1 Score: 0.9654332225435366\nValidation Accuracy: 0.9505692624114696, Validation F1 Score: 0.9504431236880575\n\nEpoch 42, Training Loss: 0.08505361296381243, Validation Loss: 0.12769468804101836\nTraining Accuracy: 0.9660032400405327, Training F1 Score: 0.9659805083541254\nValidation Accuracy: 0.9505176286326514, Validation F1 Score: 0.9504614044965047\n\nEpoch 43, Training Loss: 0.08426398775807344, Validation Loss: 0.1358686991974418\nTraining Accuracy: 0.963023521489472, Training F1 Score: 0.963045837073211\nValidation Accuracy: 0.9474712356823835, Validation F1 Score: 0.947481313968723\n\nEpoch 44, Training Loss: 0.08423333609874321, Validation Loss: 0.1268481827163953\nTraining Accuracy: 0.9670983135008143, Training F1 Score: 0.9670467633242079\nValidation Accuracy: 0.9514384310215743, Validation F1 Score: 0.9513368842732477\n\nEpoch 45, Training Loss: 0.09318709589415754, Validation Loss: 0.15605189120061239\nTraining Accuracy: 0.9551622279258792, Training F1 Score: 0.95488256303021\nValidation Accuracy: 0.9419119988296344, Validation F1 Score: 0.9414987089702922\n\nEpoch 46, Training Loss: 0.08447256736829821, Validation Loss: 0.1231611017727802\nTraining Accuracy: 0.9689248702155079, Training F1 Score: 0.9688506153027765\nValidation Accuracy: 0.9530218669053294, Validation F1 Score: 0.9528913002798423\n\nEpoch 47, Training Loss: 0.07787197398920578, Validation Loss: 0.12480647724554951\nTraining Accuracy: 0.9683784091960354, Training F1 Score: 0.9682907313469443\nValidation Accuracy: 0.9528153317900571, Validation F1 Score: 0.9526545863952001\n\nEpoch 48, Training Loss: 0.07588624382916384, Validation Loss: 0.12242204445069672\nTraining Accuracy: 0.9698091043848118, Training F1 Score: 0.9697421565627908\nValidation Accuracy: 0.9535984441021316, Validation F1 Score: 0.9534776572309812\n\nEpoch 49, Training Loss: 0.0743359812458547, Validation Loss: 0.12181193864239476\nTraining Accuracy: 0.9702522971801321, Training F1 Score: 0.9701939060021264\nValidation Accuracy: 0.9539512749240553, Validation F1 Score: 0.9538550866267614\n\nEpoch 50, Training Loss: 0.07356666715693355, Validation Loss: 0.12115136901833712\nTraining Accuracy: 0.97077078972223, Training F1 Score: 0.9707219210520788\nValidation Accuracy: 0.9539082467750403, Validation F1 Score: 0.953822797889319\n\nEpoch 51, Training Loss: 0.07282999998182715, Validation Loss: 0.12153838480878067\nTraining Accuracy: 0.9707277612954999, Training F1 Score: 0.9706853287422994\nValidation Accuracy: 0.9540373312220854, Validation F1 Score: 0.9539635645426662\n\nEpoch 52, Training Loss: 0.07264045820841014, Validation Loss: 0.1200756639745875\nTraining Accuracy: 0.9712914336856644, Training F1 Score: 0.9712651638776005\nValidation Accuracy: 0.9542610775969639, Validation F1 Score: 0.9542191506430258\n\nEpoch 53, Training Loss: 0.07295450375904806, Validation Loss: 0.11771360504270276\nTraining Accuracy: 0.9724618068927237, Training F1 Score: 0.9724528285929772\nValidation Accuracy: 0.9551130349474626, Validation F1 Score: 0.955093373856717\n\nEpoch 54, Training Loss: 0.07450412077723677, Validation Loss: 0.1159192988433722\nTraining Accuracy: 0.9729114539520535, Training F1 Score: 0.972918532172753\nValidation Accuracy: 0.9555261051780074, Validation F1 Score: 0.9555348369161146\n\nEpoch 55, Training Loss: 0.07790472346881594, Validation Loss: 0.12181359882305105\nTraining Accuracy: 0.9706072817006556, Training F1 Score: 0.9706108100966938\nValidation Accuracy: 0.9541061762605096, Validation F1 Score: 0.9540926129680152\n\nEpoch 56, Training Loss: 0.08196095839575464, Validation Loss: 0.15169579432385463\nTraining Accuracy: 0.9592499284652406, Training F1 Score: 0.9591142992251527\nValidation Accuracy: 0.9448465185924632, Validation F1 Score: 0.9446397468514404\n\nEpoch 57, Training Loss: 0.07493359519887526, Validation Loss: 0.1150760584022396\nTraining Accuracy: 0.9737505082732908, Training F1 Score: 0.9737385521351853\nValidation Accuracy: 0.956842766537869, Validation F1 Score: 0.9568154438295958\n\nEpoch 58, Training Loss: 0.07772564074333656, Validation Loss: 0.11863448883910704\nTraining Accuracy: 0.9724639583140602, Training F1 Score: 0.9725138329389499\nValidation Accuracy: 0.9551130349474626, Validation F1 Score: 0.9552022214895298\n\nEpoch 59, Training Loss: 0.07538579284429801, Validation Loss: 0.11434253990074664\nTraining Accuracy: 0.9746024711225472, Training F1 Score: 0.9746529436224574\nValidation Accuracy: 0.9567050764610208, Validation F1 Score: 0.9567945941171758\n\nEpoch 60, Training Loss: 0.06366948065571858, Validation Loss: 0.10913731269022582\nTraining Accuracy: 0.9773670475399573, Training F1 Score: 0.9773994698234441\nValidation Accuracy: 0.958779033243548, Validation F1 Score: 0.958842381487827\n\nEpoch 61, Training Loss: 0.05987611628179848, Validation Loss: 0.11039007158871059\nTraining Accuracy: 0.9771970852543733, Training F1 Score: 0.9772459976312974\nValidation Accuracy: 0.9582282729361549, Validation F1 Score: 0.9583250456820316\n\nEpoch 62, Training Loss: 0.05921524066957956, Validation Loss: 0.11099747611186203\nTraining Accuracy: 0.977111028400913, Training F1 Score: 0.9771626942057989\nValidation Accuracy: 0.9581163997487157, Validation F1 Score: 0.9582205041608703\n\nEpoch 63, Training Loss: 0.05820486728513274, Validation Loss: 0.11130449445215561\nTraining Accuracy: 0.9772982020571891, Training F1 Score: 0.9773441558027497\nValidation Accuracy: 0.957789385816201, Validation F1 Score: 0.9578851091169875\n\nEpoch 64, Training Loss: 0.05705728506848117, Validation Loss: 0.11142119789774348\nTraining Accuracy: 0.9774724671854461, Training F1 Score: 0.9775111807236427\nValidation Accuracy: 0.9580303434506854, Validation F1 Score: 0.9581112064933021\n\nEpoch 65, Training Loss: 0.0560138874127524, Validation Loss: 0.11115542429715651\nTraining Accuracy: 0.9777155777964712, Training F1 Score: 0.9777459682044802\nValidation Accuracy: 0.9580905828593066, Validation F1 Score: 0.9581537726438152\n\nEpoch 66, Training Loss: 0.05499958566829408, Validation Loss: 0.11136963253080105\nTraining Accuracy: 0.9777672119085474, Training F1 Score: 0.9777897497707245\nValidation Accuracy: 0.9583573573832, Validation F1 Score: 0.9583984026985268\n\nEpoch 67, Training Loss: 0.054155940549804236, Validation Loss: 0.111113922375177\nTraining Accuracy: 0.9779350227727949, Training F1 Score: 0.9779508950467185\nValidation Accuracy: 0.9586585544263057, Validation F1 Score: 0.9586819241006134\n\nEpoch 68, Training Loss: 0.05339867454037437, Validation Loss: 0.11129274859482151\nTraining Accuracy: 0.9782061018611946, Training F1 Score: 0.9782135785308078\nValidation Accuracy: 0.958787638873351, Validation F1 Score: 0.9587880400273234\n\nEpoch 69, Training Loss: 0.05266270962767192, Validation Loss: 0.11131394452947986\nTraining Accuracy: 0.9783265814560389, Training F1 Score: 0.9783279987071148\nValidation Accuracy: 0.9590199908780324, Validation F1 Score: 0.9590060157483836\n\nEpoch 70, Training Loss: 0.0520261921018743, Validation Loss: 0.11140085539723646\nTraining Accuracy: 0.9785395721683531, Training F1 Score: 0.9785340182584042\nValidation Accuracy: 0.9590802302866536, Validation F1 Score: 0.9590525410073196\n\nEpoch 71, Training Loss: 0.051378256359654796, Validation Loss: 0.11151419628096108\nTraining Accuracy: 0.9786923230832449, Training F1 Score: 0.9786765204081185\nValidation Accuracy: 0.9592351316231078, Validation F1 Score: 0.9591808876962882\n\nEpoch 72, Training Loss: 0.0507954312052058, Validation Loss: 0.1104570237079548\nTraining Accuracy: 0.9791785443052953, Training F1 Score: 0.9791592500462264\nValidation Accuracy: 0.9596826243728648, Validation F1 Score: 0.9596212222847036\n\nEpoch 73, Training Loss: 0.05002118769525416, Validation Loss: 0.10811485944515242\nTraining Accuracy: 0.9802822234509229, Training F1 Score: 0.9802721106006748\nValidation Accuracy: 0.9607497224684388, Validation F1 Score: 0.9607126381942481\n\nEpoch 74, Training Loss: 0.04904511314873427, Validation Loss: 0.10683644755342545\nTraining Accuracy: 0.9811707604628999, Training F1 Score: 0.9811667117548664\nValidation Accuracy: 0.9611800039585897, Validation F1 Score: 0.9611556349746142\n\nEpoch 75, Training Loss: 0.04796441658994016, Validation Loss: 0.10657269313641528\nTraining Accuracy: 0.9814418395512996, Training F1 Score: 0.9814342085655475\nValidation Accuracy: 0.9614037503334681, Validation F1 Score: 0.9613728361265722\n\nEpoch 76, Training Loss: 0.046971203526578666, Validation Loss: 0.10680934107214728\nTraining Accuracy: 0.9815472591967883, Training F1 Score: 0.9815343355454736\nValidation Accuracy: 0.9612230321076048, Validation F1 Score: 0.9611773318477088\n\nEpoch 77, Training Loss: 0.04607436412650045, Validation Loss: 0.10718222973806875\nTraining Accuracy: 0.9816935558476708, Training F1 Score: 0.9816771227326662\nValidation Accuracy: 0.9611025532903625, Validation F1 Score: 0.9610503433379177\n\nEpoch 78, Training Loss: 0.045209966284479874, Validation Loss: 0.10749532791325528\nTraining Accuracy: 0.9818355496558802, Training F1 Score: 0.9818186695734483\nValidation Accuracy: 0.9610939476605596, Validation F1 Score: 0.9610409921713379\n\nEpoch 79, Training Loss: 0.04448921391986632, Validation Loss: 0.10776862611346935\nTraining Accuracy: 0.9820506917895307, Training F1 Score: 0.9820351491611222\nValidation Accuracy: 0.9610853420307566, Validation F1 Score: 0.9610353052420328\n\nEpoch 80, Training Loss: 0.04390866493414763, Validation Loss: 0.10825756760328292\nTraining Accuracy: 0.9821087801656164, Training F1 Score: 0.9820941981188592\nValidation Accuracy: 0.9610767364009535, Validation F1 Score: 0.9610324734338466\n\nEpoch 81, Training Loss: 0.04342122861890495, Validation Loss: 0.10842571078001882\nTraining Accuracy: 0.9824508561581209, Training F1 Score: 0.9824414093004574\nValidation Accuracy: 0.9609476519539083, Validation F1 Score: 0.9609215896413715\n\nEpoch 82, Training Loss: 0.043268578587870025, Validation Loss: 0.10845612078409098\nTraining Accuracy: 0.9827047238758285, Training F1 Score: 0.9826994307322378\nValidation Accuracy: 0.9610767364009535, Validation F1 Score: 0.9610664014284952\n\nEpoch 83, Training Loss: 0.04330120698065823, Validation Loss: 0.1089856486869829\nTraining Accuracy: 0.9827391466172126, Training F1 Score: 0.9827411648441379\nValidation Accuracy: 0.9609906801029233, Validation F1 Score: 0.960998808253367\n\nEpoch 84, Training Loss: 0.04383600381273883, Validation Loss: 0.11074940543738952\nTraining Accuracy: 0.9820679031602229, Training F1 Score: 0.9820785400156712\nValidation Accuracy: 0.9606894830598177, Validation F1 Score: 0.9607209242692808\n\nEpoch 85, Training Loss: 0.04472458063114844, Validation Loss: 0.11502332779438312\nTraining Accuracy: 0.9803897945177481, Training F1 Score: 0.9804119133764246\nValidation Accuracy: 0.9589597514694113, Validation F1 Score: 0.9590210437640502\n\nEpoch 86, Training Loss: 0.0459318719434971, Validation Loss: 0.11905696359112196\nTraining Accuracy: 0.9786170233364673, Training F1 Score: 0.9786507478622654\nValidation Accuracy: 0.9575570338115195, Validation F1 Score: 0.9576364694529447\n\nEpoch 87, Training Loss: 0.046613522588891496, Validation Loss: 0.11307264110314126\nTraining Accuracy: 0.981226697417649, Training F1 Score: 0.9812461897312726\nValidation Accuracy: 0.9598633425987281, Validation F1 Score: 0.959908633769839\n\nEpoch 88, Training Loss: 0.04477771522056383, Validation Loss: 0.12023222236029094\nTraining Accuracy: 0.9792516926307365, Training F1 Score: 0.9792259509457611\nValidation Accuracy: 0.9580819772295035, Validation F1 Score: 0.9580166798106892\n\nEpoch 89, Training Loss: 0.04396557788894337, Validation Loss: 0.1138576770060156\nTraining Accuracy: 0.9818914866106293, Training F1 Score: 0.9818832719456252\nValidation Accuracy: 0.959820314449713, Validation F1 Score: 0.9597994901420046\n\nEpoch 90, Training Loss: 0.04190853404759596, Validation Loss: 0.11306322274927523\nTraining Accuracy: 0.9821926855977402, Training F1 Score: 0.982191089587316\nValidation Accuracy: 0.960293624088879, Validation F1 Score: 0.9602919716522067\n\nEpoch 91, Training Loss: 0.040405843175896836, Validation Loss: 0.11528768001528632\nTraining Accuracy: 0.9813708426471949, Training F1 Score: 0.9813621418989553\nValidation Accuracy: 0.9598891594881371, Validation F1 Score: 0.9598679120189149\n\nEpoch 92, Training Loss: 0.03988228306402247, Validation Loss: 0.11744371897860698\nTraining Accuracy: 0.9808050188356938, Training F1 Score: 0.9807957425189524\nValidation Accuracy: 0.9593556104403501, Validation F1 Score: 0.9593327190858741\n\nEpoch 93, Training Loss: 0.03985452696089075, Validation Loss: 0.1185437041458289\nTraining Accuracy: 0.9806565707634749, Training F1 Score: 0.9806504664026999\nValidation Accuracy: 0.9591662865846837, Validation F1 Score: 0.9591471054194095\n\nEpoch 94, Training Loss: 0.03968943661495866, Validation Loss: 0.12030572234839476\nTraining Accuracy: 0.9801144125866754, Training F1 Score: 0.9801072711881248\nValidation Accuracy: 0.9589425402098053, Validation F1 Score: 0.9589220751023294\n\nEpoch 95, Training Loss: 0.03967430791557179, Validation Loss: 0.12230495662816643\nTraining Accuracy: 0.9796798254767012, Training F1 Score: 0.9796689949240729\nValidation Accuracy: 0.958288512344776, Validation F1 Score: 0.9582647544952129\n\nEpoch 96, Training Loss: 0.039825136581282825, Validation Loss: 0.12337049336972732\nTraining Accuracy: 0.9794453205510221, Training F1 Score: 0.979432152531096\nValidation Accuracy: 0.9580905828593066, Validation F1 Score: 0.9580619475474156\n\nEpoch 97, Training Loss: 0.04000861589515949, Validation Loss: 0.12453033775931104\nTraining Accuracy: 0.9792022099399968, Training F1 Score: 0.9791892650112461\nValidation Accuracy: 0.9575742450711255, Validation F1 Score: 0.9575528068154247\n\nEpoch 98, Training Loss: 0.04031829676454693, Validation Loss: 0.1258985712973857\nTraining Accuracy: 0.9786514460778514, Training F1 Score: 0.9786408577316722\nValidation Accuracy: 0.957298864917429, Validation F1 Score: 0.9572752772447942\n\nEpoch 99, Training Loss: 0.04082410225410643, Validation Loss: 0.1269633921885888\nTraining Accuracy: 0.9784083354668262, Training F1 Score: 0.9783941327955705\nValidation Accuracy: 0.9572300198790048, Validation F1 Score: 0.9572005117391204\n\nEpoch 100, Training Loss: 0.041483922242413346, Validation Loss: 0.12659928757150843\nTraining Accuracy: 0.9787826827793782, Training F1 Score: 0.9787554232318213\nValidation Accuracy: 0.9576603013691557, Validation F1 Score: 0.9575920187247056\n\nExecution time: 1188.363789 seconds\n","output_type":"stream"}]},{"cell_type":"code","source":"criterion = CustomLoss(nn.CrossEntropyLoss(), l1_lambda=0.001 * 0.001, l2_lambda=0.000)\nevaluate_model(model, custom_train_loader, criterion, optimizer, 5, scheduler, 1024 * 16)","metadata":{"execution":{"iopub.status.busy":"2024-10-26T00:51:00.759185Z","iopub.execute_input":"2024-10-26T00:51:00.760059Z","iopub.status.idle":"2024-10-26T00:52:01.762552Z","shell.execute_reply.started":"2024-10-26T00:51:00.760003Z","shell.execute_reply":"2024-10-26T00:52:01.761543Z"},"trusted":true},"execution_count":136,"outputs":[{"name":"stdout","text":"Epoch 1, Training Loss: 0.4068671660701097, Validation Loss: 0.3232626430720744\nTraining Accuracy: 0.8779971988494198, Training F1 Score: 0.8766222488126303\nValidation Accuracy: 0.8740910303520563, Validation F1 Score: 0.8727003746293566\n\nEpoch 2, Training Loss: 0.34153788010821173, Validation Loss: 0.2800840114670676\nTraining Accuracy: 0.8974869247368273, Training F1 Score: 0.8963194646122477\nValidation Accuracy: 0.8922661204960285, Validation F1 Score: 0.8910005880572152\n\nEpoch 3, Training Loss: 0.30741332398555826, Validation Loss: 0.25716636766968054\nTraining Accuracy: 0.9073167688233231, Training F1 Score: 0.9063392478887443\nValidation Accuracy: 0.9018183695773775, Validation F1 Score: 0.9007810831017664\n\nEpoch 4, Training Loss: 0.2856579917746917, Validation Loss: 0.24097073410499256\nTraining Accuracy: 0.9144207620764658, Training F1 Score: 0.9136148467649793\nValidation Accuracy: 0.9088061409774274, Validation F1 Score: 0.907951415023494\n\nEpoch 5, Training Loss: 0.27030155916667314, Validation Loss: 0.22899679397435\nTraining Accuracy: 0.9194787536385913, Training F1 Score: 0.9187679585980915\nValidation Accuracy: 0.9127991532060273, Validation F1 Score: 0.9120260830977992\n\n","output_type":"stream"}]},{"cell_type":"code","source":"evaluate_model(model, custom_train_loader, criterion, optimizer, 5, scheduler, 1024 * 16)","metadata":{"execution":{"iopub.status.busy":"2024-10-26T00:52:31.051557Z","iopub.execute_input":"2024-10-26T00:52:31.051960Z","iopub.status.idle":"2024-10-26T00:53:32.336508Z","shell.execute_reply.started":"2024-10-26T00:52:31.051922Z","shell.execute_reply":"2024-10-26T00:53:32.335553Z"},"trusted":true},"execution_count":137,"outputs":[{"name":"stdout","text":"Epoch 1, Training Loss: 0.25826393838581835, Validation Loss: 0.218076343175137\nTraining Accuracy: 0.9245991364194756, Training F1 Score: 0.9240301797851345\nValidation Accuracy: 0.917136390626748, Validation F1 Score: 0.916487911701331\n\nEpoch 2, Training Loss: 0.24857534973569392, Validation Loss: 0.21029824574821068\nTraining Accuracy: 0.9275272208584602, Training F1 Score: 0.9270130136884623\nValidation Accuracy: 0.9194857275629716, Validation F1 Score: 0.9188864608172448\n\nEpoch 3, Training Loss: 0.24054947699908305, Validation Loss: 0.20218904001611157\nTraining Accuracy: 0.9313761136294693, Training F1 Score: 0.9309510736961516\nValidation Accuracy: 0.9231086977100419, Validation F1 Score: 0.922613686187858\n\nEpoch 4, Training Loss: 0.23273020475105996, Validation Loss: 0.1960009202417145\nTraining Accuracy: 0.9340481789294097, Training F1 Score: 0.9336565368028746\nValidation Accuracy: 0.9251912601223721, Validation F1 Score: 0.924724325807495\n\nEpoch 5, Training Loss: 0.22615871474063906, Validation Loss: 0.19129639902194415\nTraining Accuracy: 0.9358833413294493, Training F1 Score: 0.9355070926285494\nValidation Accuracy: 0.9264390764438095, Validation F1 Score: 0.925976714807189\n\n","output_type":"stream"}]},{"cell_type":"code","source":"for name, param in model.named_parameters():\n    print(name)\n    print(param)","metadata":{"execution":{"iopub.status.busy":"2024-10-26T00:54:01.025589Z","iopub.execute_input":"2024-10-26T00:54:01.026428Z","iopub.status.idle":"2024-10-26T00:54:01.119945Z","shell.execute_reply.started":"2024-10-26T00:54:01.026386Z","shell.execute_reply":"2024-10-26T00:54:01.118912Z"},"trusted":true},"execution_count":138,"outputs":[{"name":"stdout","text":"first_layer.linear.weight\nParameter containing:\ntensor([[ 1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  ...,  0.,  0.,  0.,  0.,  0.,\n          0.,  0.,  0.,  0.,  0.],\n        [-1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  ...,  0.,  0.,  0.,  0.,  0.,\n          0.,  0.,  0.,  0.,  0.],\n        [ 0.,  1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  ...,  0.,  0.,  0.,  0.,  0.,\n          0.,  0.,  0.,  0.,  0.],\n        [ 0., -1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  ...,  0.,  0.,  0.,  0.,  0.,\n          0.,  0.,  0.,  0.,  0.],\n        [ 0.,  0.,  1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  ...,  0.,  0.,  0.,  0.,  0.,\n          0.,  0.,  0.,  0.,  0.],\n        [ 0.,  0., -1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  ...,  0.,  0.,  0.,  0.,  0.,\n          0.,  0.,  0.,  0.,  0.],\n        [ 0.,  0.,  0.,  1.,  0.,  0.,  0.,  0.,  0.,  0.,  ...,  0.,  0.,  0.,  0.,  0.,\n          0.,  0.,  0.,  0.,  0.],\n        [ 0.,  0.,  0., -1.,  0.,  0.,  0.,  0.,  0.,  0.,  ...,  0.,  0.,  0.,  0.,  0.,\n          0.,  0.,  0.,  0.,  0.],\n        [ 0.,  0.,  0.,  0.,  1.,  0.,  0.,  0.,  0.,  0.,  ...,  0.,  0.,  0.,  0.,  0.,\n          0.,  0.,  0.,  0.,  0.],\n        [ 0.,  0.,  0.,  0., -1.,  0.,  0.,  0.,  0.,  0.,  ...,  0.,  0.,  0.,  0.,  0.,\n          0.,  0.,  0.,  0.,  0.],\n        ...,\n        [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  ...,  0.,  0.,  0.,  0.,  0.,\n          1.,  0.,  0.,  0.,  0.],\n        [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  ...,  0.,  0.,  0.,  0.,  0.,\n         -1.,  0.,  0.,  0.,  0.],\n        [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  ...,  0.,  0.,  0.,  0.,  0.,\n          0.,  1.,  0.,  0.,  0.],\n        [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  ...,  0.,  0.,  0.,  0.,  0.,\n          0., -1.,  0.,  0.,  0.],\n        [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  ...,  0.,  0.,  0.,  0.,  0.,\n          0.,  0.,  1.,  0.,  0.],\n        [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  ...,  0.,  0.,  0.,  0.,  0.,\n          0.,  0., -1.,  0.,  0.],\n        [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  ...,  0.,  0.,  0.,  0.,  0.,\n          0.,  0.,  0.,  1.,  0.],\n        [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  ...,  0.,  0.,  0.,  0.,  0.,\n          0.,  0.,  0., -1.,  0.],\n        [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  ...,  0.,  0.,  0.,  0.,  0.,\n          0.,  0.,  0.,  0.,  1.],\n        [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  ...,  0.,  0.,  0.,  0.,  0.,\n          0.,  0.,  0.,  0., -1.]], device='cuda:0')\nfirst_layer.linear.bias\nParameter containing:\ntensor([ 0.0017,  0.0078,  0.0334,  0.0391,  0.0207,  0.0195,  0.0209,  0.0072,  0.0233,\n        -0.0045,  ..., -0.2966, -0.0146, -0.0061, -0.0274, -0.0103, -0.0054, -0.0050,\n         0.0016, -0.0027, -0.0028], device='cuda:0', requires_grad=True)\nlayers.0.linear.weight\nParameter containing:\ntensor([[ 1.0947e+00, -1.1645e+00,  4.3873e-03, -3.1576e-03, -8.8359e-04,  1.1082e-02,\n         -2.1352e-02,  2.6249e-03, -1.0775e-02,  1.1946e-02,  ..., -3.4769e-03,\n          1.8287e-03,  1.4546e-02,  1.1297e-03, -2.4711e-02,  5.3220e-04, -2.4750e-02,\n          6.4324e-04, -2.0803e-02,  8.3178e-04],\n        [-1.0158e+00,  1.1550e+00, -2.5539e-03,  1.1584e-02, -2.5932e-02,  4.9432e-03,\n          7.0448e-02, -4.9512e-02,  5.0203e-02, -3.8622e-02,  ...,  0.0000e+00,\n          1.2497e-07,  0.0000e+00,  1.2817e-04,  0.0000e+00,  1.2300e-03,  1.4715e-03,\n          1.3219e-03,  0.0000e+00,  1.8698e-03],\n        [-2.7748e-02, -7.7955e-03,  1.0647e+00, -1.0275e+00, -4.5975e-02, -3.7187e-03,\n         -4.7017e-02, -1.6701e-02,  6.4769e-03,  1.7295e-02,  ...,  2.4425e-02,\n          1.5622e-03,  2.1156e-03, -1.0925e-08,  7.4536e-04,  4.4724e-03, -1.6622e-02,\n          3.6726e-03,  4.0532e-03,  4.0997e-03],\n        [-2.1895e-02, -5.5253e-03, -9.6881e-01,  1.0679e+00, -9.5793e-02,  4.8448e-02,\n          1.1931e-02, -4.6401e-02, -2.8178e-02, -7.1730e-03,  ..., -6.5796e-04,\n         -9.3273e-04,  2.1364e-06, -7.5856e-04,  8.2287e-03,  6.9031e-03, -3.6756e-03,\n          7.8050e-03,  1.9187e-03,  6.9633e-03],\n        [ 7.3788e-03,  5.5386e-03, -1.2798e-02, -4.1386e-02,  1.1170e+00, -1.2060e+00,\n          5.0235e-02,  4.8488e-02,  1.3653e-02,  2.8203e-02,  ..., -9.0801e-04,\n          1.5477e-07,  1.3508e-04, -6.8757e-04,  3.9663e-02,  3.4944e-03, -1.0669e-02,\n          3.5702e-03, -4.6885e-02,  3.1707e-03],\n        [-7.1807e-04, -8.2963e-03, -2.6172e-03,  7.2858e-03, -1.0217e+00,  1.1091e+00,\n          4.6297e-02, -3.1470e-02, -1.5384e-02,  1.0575e-02,  ..., -4.3057e-03,\n         -1.4538e-07, -9.8969e-04,  1.6695e-03, -1.1257e-03,  2.0446e-03,  9.2063e-03,\n          3.2915e-03,  4.6213e-03,  2.6178e-03],\n        [ 2.0790e-02,  1.2241e-01,  2.4133e-02,  8.5830e-03,  4.3916e-02,  3.1542e-02,\n          1.1942e+00, -1.2139e+00, -4.3304e-02,  3.4882e-03,  ...,  5.4504e-03,\n          5.7772e-06, -3.6413e-03,  2.5750e-04, -1.9110e-02,  4.5431e-03, -6.1594e-03,\n          3.9247e-03,  4.4250e-02,  2.3319e-03],\n        [ 1.3680e-02, -5.2626e-04,  3.5958e-03,  1.7271e-02, -1.1242e-02, -8.0113e-03,\n         -1.0146e+00,  1.1352e+00,  1.1280e-01, -1.3430e-03,  ...,  0.0000e+00,\n         -8.1166e-04, -2.8621e-04,  9.3880e-07, -4.6041e-03,  1.9541e-03,  3.2488e-04,\n          2.5406e-03, -3.6032e-03,  3.5887e-03],\n        [-1.2002e-02,  1.7416e-03,  1.3133e-02,  3.6781e-02,  1.5766e-02,  5.9631e-02,\n         -8.5275e-02,  1.6812e-01,  1.2400e+00, -1.1389e+00,  ...,  9.8067e-03,\n          4.5845e-08,  8.3590e-04,  1.7829e-03, -5.2447e-03,  1.9808e-02,  3.6506e-02,\n          1.3695e-02,  7.1916e-04,  9.4478e-03],\n        [-9.5897e-03, -2.6485e-02,  1.6839e-03, -9.8068e-03, -1.4335e-02,  1.5678e-02,\n          2.7017e-03, -9.2345e-03, -1.0404e+00,  1.2473e+00,  ...,  0.0000e+00,\n          1.3655e-03, -9.4504e-04,  2.0142e-05, -2.2706e-02,  2.2159e-03,  2.0055e-02,\n          3.5191e-03, -3.3713e-04,  2.3519e-03],\n        ...,\n        [-1.1865e-01,  0.0000e+00, -1.9338e-01,  3.7034e-03, -9.1975e-02, -2.7073e-02,\n          2.8959e-03,  0.0000e+00, -1.3739e-01,  0.0000e+00,  ...,  8.0337e-01,\n         -9.2099e-01,  0.0000e+00, -4.6327e-02,  0.0000e+00, -7.5371e-02,  0.0000e+00,\n         -7.5722e-02,  0.0000e+00, -7.4391e-02],\n        [ 5.2975e-04,  8.2397e-04, -9.5974e-05,  4.9403e-04,  1.0573e-03, -3.5443e-04,\n         -5.0727e-04, -1.4363e-07,  6.6518e-04, -1.4236e-03,  ..., -9.2099e-01,\n          9.1584e-01,  8.3928e-04, -7.9957e-04, -1.7464e-03,  6.1997e-04, -5.3470e-06,\n         -5.5995e-04, -4.5067e-04, -3.3747e-04],\n        [ 6.6478e-03, -6.2609e-03, -1.5697e-03,  2.6400e-03,  9.7951e-04,  8.6872e-03,\n         -4.1124e-03,  6.2192e-03, -4.2223e-03,  8.7878e-03,  ..., -1.9957e-03,\n         -4.0253e-04,  9.3816e-01, -9.2559e-01,  1.0510e-03,  2.0404e-03, -2.4885e-03,\n          2.1710e-03,  5.8220e-04,  1.5562e-03],\n        [ 1.7997e-04,  2.2501e-03,  5.1178e-03, -4.1940e-03, -1.5096e-04, -9.9519e-04,\n          2.5452e-03,  2.0267e-04, -2.3913e-03,  2.4507e-03,  ..., -1.4976e-03,\n         -6.9078e-04, -9.2099e-01,  9.1625e-01,  4.6396e-06,  1.3401e-03, -1.3099e-04,\n         -1.3302e-05,  2.0328e-04, -7.5768e-04],\n        [-1.9166e-02,  4.2481e-03,  1.2084e-02,  3.6148e-03,  5.8649e-06, -4.4087e-03,\n         -1.4334e-02,  5.9689e-03,  1.6265e-04, -8.6883e-04,  ...,  0.0000e+00,\n          2.9565e-04,  0.0000e+00,  9.8576e-04,  9.8565e-01, -9.1635e-01, -5.0855e-04,\n          2.2962e-03,  7.0384e-04,  1.1907e-03],\n        [-1.2608e-01,  4.3710e-02, -2.0125e-02, -5.3588e-03, -5.5673e-03, -9.5867e-03,\n          8.5486e-03, -3.4191e-02, -6.1107e-02, -5.3023e-02,  ...,  4.1753e-03,\n         -3.5879e-04, -9.5745e-04, -1.9856e-03, -9.2099e-01,  9.9529e-01,  1.2437e-02,\n          7.5820e-04,  1.3727e-02,  4.7461e-04],\n        [ 7.5226e-03, -6.5476e-03, -1.0245e-03, -9.4573e-04, -6.0183e-03, -1.6854e-02,\n          4.6925e-03,  9.6981e-04, -5.3478e-02, -4.5161e-02,  ...,  0.0000e+00,\n         -2.2509e-03,  0.0000e+00,  4.3002e-04,  0.0000e+00,  1.7194e-03,  9.8766e-01,\n         -8.8935e-01, -5.4439e-04,  1.3795e-03],\n        [-1.0622e-01,  3.4917e-03,  1.9878e-03,  1.0398e-02,  1.2966e-03,  3.3818e-03,\n          2.8362e-03, -7.0665e-03, -5.3329e-03, -1.3134e-02,  ...,  2.5971e-04,\n         -2.1911e-07, -5.0219e-04,  8.2229e-05,  8.6785e-03,  1.9452e-03, -9.2099e-01,\n          1.0027e+00, -7.4227e-04,  1.0542e-04],\n        [ 6.8539e-03,  1.4515e-01,  3.9147e-02, -3.4254e-03,  4.3945e-03,  2.0271e-03,\n          3.7289e-04, -1.5908e-01,  2.3963e-02, -1.0199e-01,  ...,  0.0000e+00,\n          4.4184e-04,  0.0000e+00, -2.7872e-04, -3.3757e-03,  1.4796e-03, -5.8427e-03,\n         -1.2736e-03,  9.8992e-01, -9.3201e-01],\n        [-9.5749e-02,  1.6594e-03, -3.9207e-03, -5.3861e-03, -2.0326e-03,  2.1966e-03,\n         -6.0205e-03, -6.6251e-02, -2.0789e-02,  6.5513e-04,  ..., -1.1254e-03,\n         -8.4452e-05,  5.4465e-05,  2.5535e-07,  8.0524e-03,  1.1147e-03,  5.9392e-03,\n          3.7478e-04, -9.2099e-01,  9.9119e-01]], device='cuda:0', requires_grad=True)\nlayers.0.linear.bias\nParameter containing:\ntensor([ 5.0275e-04,  7.2084e-04,  2.3781e-02,  4.1722e-02,  1.1566e-02,  2.0959e-02,\n         1.5409e-02,  6.3009e-03,  4.1465e-02, -4.7533e-03,  ..., -1.7567e-01,\n        -1.3695e-02,  1.1519e-02, -8.5002e-03, -8.6187e-03,  3.8702e-03,  5.2356e-05,\n         1.2268e-02,  1.3153e-03,  1.6505e-02], device='cuda:0', requires_grad=True)\nlayers.1.linear.weight\nParameter containing:\ntensor([[ 1.1022e+00, -1.1470e+00, -6.8584e-03,  6.0237e-04, -1.5622e-03, -5.1572e-04,\n         -1.1812e-02, -1.5997e-02, -9.5862e-03, -1.0167e-02,  ..., -3.1346e-03,\n         -1.6982e-03,  9.3658e-03, -3.0286e-04, -2.1410e-02, -5.5343e-03, -2.2352e-02,\n         -9.8091e-03, -2.3873e-02, -2.6159e-03],\n        [-1.0486e+00,  1.1421e+00,  1.1373e-03, -5.3635e-03,  6.2695e-03, -1.0761e-02,\n          2.9479e-02, -2.1264e-03,  2.6465e-02, -2.2585e-02,  ...,  0.0000e+00,\n          2.0409e-03,  2.0361e-03, -1.3422e-03,  4.1878e-04,  4.0994e-03, -2.8906e-03,\n          1.8098e-03,  9.4416e-02,  3.9504e-03],\n        [ 4.1860e-03, -1.2798e-03,  1.0500e+00, -9.9132e-01, -2.7237e-02, -4.2125e-03,\n          1.4674e-03,  6.9805e-04,  1.2182e-03, -1.3832e-03,  ..., -5.6844e-04,\n          4.8489e-04, -4.5867e-04, -6.1128e-04,  1.5652e-03,  8.8128e-04,  5.2509e-03,\n          1.5225e-03, -3.8731e-03,  2.0795e-03],\n        [-4.3966e-03, -6.2990e-03, -9.2147e-01,  1.0588e+00, -6.3023e-02,  2.4913e-02,\n          2.2572e-03, -4.6745e-02, -9.1341e-03,  3.0091e-03,  ...,  3.6215e-04,\n          6.7321e-04, -2.1984e-03, -3.0992e-04, -2.6381e-03,  1.0332e-03, -2.9856e-03,\n          2.3565e-03, -8.7646e-04,  1.6409e-03],\n        [ 3.3396e-03,  2.1875e-03,  1.8116e-03,  5.2048e-03,  1.0788e+00, -1.0617e+00,\n          3.3643e-02,  2.5673e-02,  6.4316e-03,  4.4933e-02,  ..., -1.5030e-03,\n         -1.5108e-04,  4.0540e-04,  9.1871e-04,  7.7395e-02,  1.0805e-04, -6.5476e-03,\n          5.3751e-04,  2.6871e-03,  1.1750e-03],\n        [-1.1910e-02,  3.4724e-03, -1.0504e-02, -1.2027e-03, -8.9659e-01,  1.0799e+00,\n          2.3156e-03, -9.3053e-04, -4.8710e-02,  4.5040e-03,  ..., -2.6665e-03,\n          5.0438e-04, -1.0357e-03, -3.7996e-04,  2.8332e-03,  3.3053e-03,  1.2758e-02,\n          1.9070e-03,  5.8297e-03,  1.7238e-03],\n        [ 1.3256e-02,  9.0245e-02,  1.8062e-02,  4.5955e-02,  4.8366e-02,  6.9682e-03,\n          1.1402e+00, -1.1262e+00, -4.5314e-02,  4.4640e-02,  ...,  2.5815e-03,\n         -2.5360e-03,  2.4175e-03, -3.7290e-04, -1.5842e-02,  5.0804e-03,  1.6671e-02,\n          4.4745e-03,  3.4973e-03,  6.5187e-03],\n        [ 3.1668e-02, -1.4752e-02,  5.5844e-03,  2.2697e-02, -2.4674e-02, -7.4135e-03,\n         -9.9898e-01,  1.1130e+00,  1.3873e-01,  9.0883e-03,  ...,  5.2040e-03,\n         -7.4676e-04,  5.1783e-03,  7.4701e-04,  1.8011e-03,  5.9298e-04,  1.4620e-04,\n          4.7950e-03, -1.2479e-02,  6.6859e-03],\n        [-6.5910e-02,  1.7609e-02,  1.1205e-02,  8.7504e-03,  3.2928e-02,  5.5715e-03,\n         -1.1527e-01,  1.6788e-01,  1.1718e+00, -1.1267e+00,  ..., -6.4732e-04,\n         -8.7016e-05,  8.8742e-04, -4.3539e-04, -2.1369e-03,  3.1718e-03, -2.3275e-03,\n          3.3434e-02,  5.9875e-03,  4.4322e-03],\n        [-7.9844e-03, -2.5685e-03, -5.3361e-03, -1.0167e-03,  2.7236e-03, -9.0561e-03,\n         -4.0264e-02, -4.0862e-02, -9.7291e-01,  1.2193e+00,  ...,  0.0000e+00,\n         -6.5672e-04,  3.3862e-03, -7.4292e-04, -3.4099e-03,  5.0086e-03,  3.3327e-02,\n          1.8603e-03,  1.4874e-02,  5.6047e-03],\n        ...,\n        [-5.1301e-02, -2.3476e-03, -1.6009e-01,  2.7314e-03, -6.5347e-03,  2.0795e-03,\n          3.8705e-03, -2.6640e-03, -9.1930e-02,  1.9735e-03,  ...,  8.5993e-01,\n         -9.2139e-01,  3.8561e-04,  5.8964e-04, -1.3110e-07,  3.6943e-04, -1.0692e-07,\n         -4.8415e-04,  1.3751e-07,  6.7638e-05],\n        [ 6.7721e-04, -8.4325e-07, -1.4916e-03, -2.3420e-04, -2.1833e-06, -1.0393e-03,\n         -9.6649e-07, -1.2655e-03, -6.8881e-04,  2.4642e-04,  ..., -9.2099e-01,\n          9.1585e-01,  7.2153e-04,  7.0675e-04, -5.8255e-04, -2.2160e-04, -7.4322e-09,\n          5.3224e-04,  1.7476e-04,  3.8184e-04],\n        [ 5.6917e-02, -6.6222e-02,  6.2475e-03,  3.4390e-03,  2.0186e-05,  2.0681e-03,\n         -3.2679e-02,  3.3998e-02, -2.2064e-02,  3.0086e-03,  ..., -1.6387e-03,\n          7.8790e-04,  9.2850e-01, -9.1606e-01, -4.3913e-03, -1.3307e-03, -1.7663e-03,\n         -7.5518e-04, -8.8320e-04,  2.1594e-03],\n        [ 9.7665e-04, -5.7358e-04, -1.6746e-04,  6.1074e-04, -1.6268e-06, -1.5835e-03,\n          2.1073e-04, -1.2331e-04, -9.6092e-05,  5.7926e-04,  ...,  1.6733e-03,\n          3.0973e-04, -9.1981e-01,  9.1544e-01,  1.5205e-05, -2.0486e-03, -1.6621e-03,\n         -4.0806e-04, -7.2996e-04, -2.1146e-03],\n        [ 1.3231e-01, -1.2933e-02,  5.7721e-02, -4.6069e-02, -6.0419e-04,  1.0623e-02,\n         -1.1248e-02, -2.1233e-02,  1.6718e-02,  7.9575e-03,  ..., -2.4781e-04,\n          5.5098e-07,  2.4592e-05, -1.6008e-03,  9.6946e-01, -9.7861e-01,  2.4451e-03,\n         -1.9595e-03,  3.5677e-03, -1.5952e-03],\n        [-9.2635e-02,  2.6555e-02, -6.2173e-04, -7.0046e-04, -6.8775e-03, -4.9482e-04,\n          8.1166e-03, -3.9070e-02, -5.5784e-02, -5.4382e-02,  ...,  9.5236e-04,\n          1.3555e-06, -4.9287e-04,  1.1350e-03, -9.4150e-01,  1.0228e+00, -3.2089e-03,\n          1.4781e-02,  1.1455e-02,  6.9333e-03],\n        [ 6.8407e-02,  5.2782e-03, -2.2618e-02,  1.3357e-02, -1.4697e-02,  5.2885e-03,\n          3.6544e-03,  2.0106e-02,  6.0374e-02,  1.5585e-02,  ..., -2.0719e-03,\n          2.7007e-06,  4.3828e-04, -1.6504e-03,  5.2250e-03, -1.1674e-03,  9.6187e-01,\n         -9.8410e-01,  3.0761e-04, -1.3138e-02],\n        [-9.6155e-02,  3.3337e-03,  4.7721e-03,  9.1033e-03, -2.9192e-03,  1.2747e-02,\n          9.0797e-03, -2.5208e-04, -1.2068e-03, -1.3491e-02,  ..., -3.1019e-03,\n          1.5157e-03,  1.2301e-03,  1.0288e-03,  1.2761e-02,  8.6490e-03, -1.0208e+00,\n          1.0077e+00, -6.9733e-04,  8.1809e-03],\n        [ 1.9643e-03, -1.2483e-03,  2.6376e-02,  2.5580e-03,  7.0471e-05, -2.4269e-02,\n          4.0317e-03,  1.0226e-01,  2.2417e-02,  3.5427e-03,  ..., -2.1641e-03,\n         -9.4064e-05,  2.6373e-06, -1.5096e-03, -1.0837e-02, -1.3843e-04, -8.4355e-04,\n          1.1759e-03,  9.5462e-01, -9.6432e-01],\n        [-8.4663e-02, -2.0923e-02, -1.8927e-03, -2.3214e-03, -1.0224e-02,  2.8713e-02,\n          3.2595e-04, -1.0503e-01, -3.2793e-02,  2.4045e-02,  ..., -3.9940e-04,\n          1.4830e-03,  9.0217e-04,  3.2232e-06, -2.7136e-03,  9.0910e-03, -2.9937e-03,\n          5.7026e-03, -1.0077e+00,  1.0070e+00]], device='cuda:0', requires_grad=True)\nlayers.1.linear.bias\nParameter containing:\ntensor([-0.0029,  0.0013,  0.0175,  0.0375,  0.0187,  0.0237,  0.0247,  0.0113,  0.0300,\n         0.0035,  ..., -0.1037, -0.0147,  0.0105, -0.0133,  0.0005,  0.0102, -0.0142,\n         0.0153, -0.0015,  0.0152], device='cuda:0', requires_grad=True)\nlayers.2.linear.weight\nParameter containing:\ntensor([[ 1.0694e+00, -9.8606e-01, -3.5153e-03, -1.2544e-03, -1.2764e-03, -1.1806e-03,\n         -1.5728e-02,  6.1951e-03, -2.2950e-03, -5.5433e-03,  ..., -4.9463e-04,\n         -2.0895e-03,  2.5447e-03,  8.5934e-04, -1.1668e-02, -1.1064e-02, -6.8746e-03,\n         -8.6802e-03, -1.6085e-02, -9.6139e-03],\n        [-1.0133e+00,  1.1255e+00,  3.8570e-03,  5.9961e-03, -1.2074e-02,  1.0758e-02,\n          2.3487e-02, -5.8445e-03, -3.0312e-03,  1.3909e-03,  ...,  1.2039e-04,\n          4.2244e-07,  4.8076e-03, -2.3718e-03, -1.0214e-03,  3.6180e-02, -2.0455e-03,\n          2.6417e-02, -4.3407e-02,  1.3120e-02],\n        [-1.8654e-03, -9.5263e-04,  1.0233e+00, -1.1174e+00,  5.9925e-04,  2.7783e-02,\n         -2.7773e-02, -3.7131e-04, -2.2312e-03, -1.7089e-04,  ..., -3.7121e-04,\n          2.5511e-03,  6.9423e-03,  6.9837e-04,  1.5501e-03, -1.0767e-03, -5.0771e-03,\n          6.9028e-04, -4.3804e-03,  4.0581e-03],\n        [-7.0082e-03, -2.2279e-04, -9.6359e-01,  1.0333e+00, -1.0411e-02,  1.6201e-03,\n          2.2328e-03,  7.5592e-03, -9.2297e-03,  3.5549e-03,  ..., -1.9339e-04,\n         -6.1806e-04, -2.1994e-03,  8.1630e-04, -8.2559e-04,  2.7943e-03, -5.6293e-04,\n          1.0958e-03, -2.7410e-04,  3.4675e-03],\n        [-4.2507e-03, -6.2933e-04, -2.5581e-02, -4.1764e-03,  1.0582e+00, -9.5856e-01,\n          1.9057e-03, -2.8871e-04, -2.8538e-03,  3.3898e-03,  ..., -6.8127e-04,\n          1.1532e-07,  3.2479e-04,  6.4077e-07,  2.1062e-02,  1.4246e-02,  9.2438e-04,\n          6.5736e-04,  2.9451e-04,  1.9239e-02],\n        [ 2.5920e-03, -3.7059e-03,  2.6444e-02, -1.4243e-03, -9.4721e-01,  1.0352e+00,\n         -2.9115e-04, -6.0338e-03, -2.0027e-02, -3.7781e-04,  ..., -1.2504e-03,\n         -7.6209e-04,  7.5914e-03,  7.5369e-04,  2.1996e-03, -1.0145e-03, -5.6758e-03,\n         -1.4644e-04,  2.8083e-03,  3.5050e-03],\n        [ 6.8433e-03, -4.6044e-03, -3.5411e-03, -3.3538e-04,  1.4285e-03,  2.8956e-03,\n          1.1032e+00, -1.0353e+00,  1.0344e-03,  4.3522e-03,  ..., -2.4778e-04,\n          9.9793e-04, -2.6457e-04, -3.8498e-04,  2.6605e-03,  1.7117e-03,  5.5297e-03,\n          1.6992e-03,  4.6948e-06, -4.1325e-04],\n        [-7.7564e-03, -1.6393e-02,  3.7733e-03, -1.9744e-03,  1.5835e-03, -4.9412e-03,\n         -9.7670e-01,  1.0882e+00, -6.0934e-03, -1.7885e-03,  ...,  6.6024e-04,\n         -7.1352e-04, -1.8073e-03,  6.9048e-04, -1.7257e-03, -1.4320e-03, -2.1311e-03,\n          2.3800e-03,  2.0918e-03, -1.3127e-03],\n        [-4.2543e-02, -9.7761e-06,  2.1230e-03, -3.5070e-03, -2.2031e-04,  2.9081e-02,\n         -4.4912e-02,  6.7695e-02,  1.0395e+00, -1.0492e+00,  ...,  9.8338e-04,\n          1.4699e-03, -1.9010e-03,  1.6956e-03, -7.3017e-03,  2.3339e-03, -9.0789e-04,\n          1.4358e-02, -4.9263e-04, -1.1587e-03],\n        [-8.2371e-02, -2.9621e-02,  4.5534e-04, -4.3180e-03,  9.6599e-03,  2.3805e-03,\n         -1.2497e-02, -1.8625e-02, -1.0822e+00,  1.1022e+00,  ..., -9.5499e-04,\n          2.5175e-03, -3.5553e-03,  6.8155e-04, -1.2817e-04,  2.1193e-03, -2.5242e-02,\n          1.6820e-03, -3.5688e-02,  9.8437e-03],\n        ...,\n        [ 8.0372e-02,  1.1321e-03, -5.2210e-03,  1.1474e-02,  9.4124e-05,  1.1069e-02,\n         -2.2466e-02, -3.2246e-03,  6.7498e-03,  5.5424e-02,  ...,  8.9233e-01,\n         -9.2147e-01,  1.4023e-03,  5.1003e-04,  2.1725e-02, -3.3650e-02,  1.3593e-02,\n          1.6231e-04,  5.2207e-03,  3.7802e-03],\n        [ 5.1223e-04,  1.0470e-05, -1.6952e-05,  2.7233e-04,  1.1341e-07,  1.6927e-03,\n          5.7804e-04,  5.7941e-04, -4.2371e-04,  8.7663e-05,  ..., -9.2099e-01,\n          9.1581e-01, -1.8626e-06,  6.6646e-04, -3.3374e-08,  6.1509e-04, -1.1637e-03,\n          9.0686e-04, -6.6354e-05, -3.9715e-04],\n        [ 3.6934e-03, -4.6133e-02, -2.1831e-03, -2.3657e-04,  2.2345e-03, -8.6570e-04,\n         -1.0699e-03,  4.2825e-02, -2.3309e-03,  1.6826e-03,  ...,  4.6850e-07,\n         -8.1951e-07,  9.6173e-01, -9.1346e-01,  3.9330e-03, -2.3299e-03,  8.8994e-04,\n         -1.4193e-04,  9.7065e-04,  1.1469e-03],\n        [-1.8504e-03,  5.8881e-03,  1.9429e-03, -2.0009e-03,  5.2216e-03,  2.6249e-03,\n          1.5009e-03, -2.4028e-03,  3.4502e-04,  4.4663e-03,  ..., -8.0451e-05,\n          2.9010e-04, -9.2928e-01,  9.1552e-01,  7.8141e-04,  2.0504e-03,  3.1210e-04,\n          1.2545e-03,  4.3428e-04,  1.3737e-03],\n        [ 1.1922e-01, -2.9425e-02,  8.3535e-03,  2.0809e-03, -8.7591e-04,  2.1373e-02,\n         -5.3171e-03,  3.5135e-02,  1.0936e-02,  3.8965e-02,  ..., -1.4680e-03,\n         -7.3655e-06,  7.2929e-03, -2.7549e-04,  9.8027e-01, -1.0459e+00,  7.6022e-03,\n         -3.0994e-02,  3.2667e-04,  2.2388e-03],\n        [-9.2917e-02,  3.1405e-02, -7.1073e-03, -1.3720e-03, -2.5181e-03,  4.0239e-03,\n         -1.4862e-02,  2.8313e-02, -1.7628e-02, -3.2399e-02,  ..., -9.0959e-04,\n          1.1168e-05, -5.4715e-04, -1.7945e-03, -1.1419e+00,  1.0334e+00, -9.0791e-03,\n          1.7498e-02,  3.5527e-02,  2.2465e-03],\n        [ 8.9434e-02, -8.4564e-03, -4.1545e-03,  4.6599e-03,  3.4617e-03,  1.0446e-03,\n         -3.4541e-04,  5.2223e-03,  5.4543e-03, -3.0820e-04,  ..., -7.7438e-04,\n          1.2933e-03,  2.4257e-02, -4.6071e-05,  6.7453e-03, -1.5782e-02,  9.8770e-01,\n         -1.0317e+00, -6.2921e-04, -7.1531e-03],\n        [-7.3354e-02,  1.3283e-03,  6.1772e-03,  1.0363e-02, -7.9687e-03,  7.7977e-03,\n         -1.7462e-04,  1.6890e-02,  1.3835e-03,  4.1895e-03,  ..., -4.1276e-03,\n          6.5735e-07, -2.1499e-03, -8.8507e-08, -6.6028e-05,  7.6176e-03, -1.0776e+00,\n          1.0017e+00,  1.2106e-02,  2.0366e-03],\n        [ 3.0105e-02,  2.9189e-02,  4.8170e-03,  2.1918e-02,  3.2452e-03, -5.7744e-05,\n          1.0682e-02,  3.0924e-02,  4.0201e-03, -2.7982e-02,  ..., -1.3750e-03,\n         -5.2786e-04, -6.7779e-04,  1.1171e-03, -7.9304e-04,  4.7478e-03,  4.0838e-03,\n          1.3677e-03,  9.5581e-01, -9.7694e-01],\n        [-3.8034e-02, -1.3363e-04,  1.7154e-03, -3.1676e-03, -2.6102e-03,  2.2086e-02,\n          4.4206e-03, -6.5460e-02, -8.3861e-03, -2.2963e-03,  ..., -1.6525e-04,\n         -1.6678e-03, -3.8319e-03,  5.1415e-08, -1.3336e-03,  6.8079e-03, -5.0419e-03,\n          7.0753e-03, -1.1168e+00,  1.0675e+00]], device='cuda:0', requires_grad=True)\nlayers.2.linear.bias\nParameter containing:\ntensor([-3.2845e-03,  1.0551e-02,  2.3466e-02,  4.2044e-02, -9.6167e-03,  1.6788e-02,\n        -1.1977e-02, -6.3691e-03, -6.8904e-03, -1.1689e-03,  ..., -6.8251e-03,\n        -1.6651e-02,  1.5695e-02, -1.1419e-02,  8.5395e-03,  5.4005e-03, -8.3195e-05,\n         1.1228e-02, -5.5139e-04,  2.3635e-02], device='cuda:0', requires_grad=True)\nlayers.3.linear.weight\nParameter containing:\ntensor([[ 1.0513e+00, -9.9414e-01,  8.0445e-04,  4.1210e-03,  3.5292e-03,  3.3383e-03,\n          3.3893e-03,  1.9894e-03,  1.1498e-03,  5.1253e-04,  ...,  4.7108e-04,\n          7.7763e-04,  6.3984e-03, -9.1409e-04,  9.8218e-04, -1.0280e-02,  2.6818e-03,\n         -7.6893e-04,  1.8593e-03, -2.1990e-03],\n        [-1.0071e+00,  1.0948e+00,  1.5084e-04,  3.1189e-03,  4.0707e-04,  4.7929e-03,\n          7.7484e-03, -3.7100e-03,  7.2564e-04,  2.3208e-03,  ..., -1.6799e-02,\n         -8.9734e-04,  2.2886e-03,  6.3160e-04, -1.0459e-03,  7.1329e-03,  5.6270e-04,\n          1.9491e-02, -1.1413e-03,  1.0011e-02],\n        [ 1.6803e-03, -3.1363e-03,  9.8444e-01, -1.0544e+00, -1.3125e-03,  7.0339e-04,\n         -2.5965e-03,  5.3519e-04, -3.7162e-03,  3.9367e-05,  ...,  4.3924e-04,\n          5.7288e-04,  2.1003e-03,  7.1781e-04,  1.6623e-03, -4.2447e-03,  5.5750e-04,\n          2.7863e-04,  2.4688e-05,  2.6557e-03],\n        [-5.8790e-03,  3.0086e-04, -1.0600e+00,  9.7355e-01, -3.9463e-03, -1.1421e-04,\n          3.1160e-03,  1.1422e-04, -2.9426e-03, -3.1030e-03,  ...,  8.5106e-05,\n          4.3288e-04, -4.5742e-04,  8.2868e-04, -1.4167e-03,  3.6385e-03, -6.1315e-04,\n          1.1337e-03, -1.6241e-03,  1.0698e-03],\n        [ 4.3948e-04, -1.6128e-04,  4.7958e-03,  2.6728e-03,  9.7115e-01, -1.0731e+00,\n         -7.4273e-02,  8.9858e-03,  4.5764e-03, -9.7546e-03,  ...,  9.7899e-04,\n         -1.1116e-03, -5.3850e-03,  1.6353e-05,  7.7656e-04,  3.1446e-03, -4.6908e-03,\n          1.1723e-03, -7.5035e-03,  4.7853e-04],\n        [ 2.2659e-03,  3.0571e-04,  2.5301e-03,  2.4230e-03, -1.1118e+00,  9.8736e-01,\n         -2.3460e-05, -6.8021e-04,  6.9669e-04, -1.9973e-03,  ...,  5.6000e-04,\n         -9.7762e-04, -3.0323e-04,  1.1435e-03, -2.6122e-03, -3.2753e-03,  3.5680e-03,\n         -3.9464e-03, -1.7721e-03,  4.8268e-03],\n        [ 2.5758e-04, -3.8217e-03,  5.4736e-04,  1.2684e-03,  1.8599e-02,  6.5313e-04,\n          1.0048e+00, -1.0530e+00,  1.8449e-03,  4.5080e-03,  ..., -4.7150e-03,\n         -3.7785e-04,  1.5988e-03, -7.4466e-04,  1.9216e-03, -2.9981e-03,  4.2697e-03,\n          1.4246e-04,  1.7809e-03, -5.7903e-04],\n        [-3.5378e-05,  2.8537e-03,  3.4129e-04, -1.7351e-04, -2.8167e-03, -3.7981e-03,\n         -1.0786e+00,  1.0286e+00,  2.0207e-03,  1.6168e-03,  ...,  1.3928e-03,\n          4.8670e-04,  2.9777e-04,  2.3099e-04,  1.6042e-03, -7.4995e-04,  1.8061e-03,\n         -3.5774e-04,  1.1629e-03,  4.6452e-06],\n        [-2.4404e-03,  3.7550e-03,  2.8857e-03,  3.1139e-03,  2.7808e-04, -9.1152e-03,\n         -2.2765e-02,  3.4373e-03,  1.0511e+00, -1.1011e+00,  ...,  1.3371e-03,\n          1.2740e-03, -7.3728e-03, -6.0797e-06, -3.8277e-03,  2.7193e-04, -2.1367e-02,\n         -1.5042e-03,  1.4275e-03, -2.1705e-03],\n        [-8.0401e-04, -2.7273e-02,  5.5662e-04, -5.5667e-04,  6.8710e-04, -1.1486e-03,\n         -3.4360e-03, -4.5673e-04, -1.1642e+00,  9.9703e-01,  ...,  1.9107e-03,\n         -1.2018e-03,  1.1790e-03,  9.7004e-04, -3.3716e-03, -2.7273e-04, -5.4159e-03,\n         -7.2709e-04, -4.8688e-04, -3.7212e-03],\n        ...,\n        [ 3.9801e-02,  4.9791e-03, -8.2137e-05,  7.2383e-03, -1.2973e-03,  6.2428e-03,\n         -3.1044e-03, -2.7458e-03,  3.7306e-03,  5.4380e-03,  ...,  9.9634e-01,\n         -9.2183e-01, -2.7764e-03, -1.2685e-04,  1.2584e-02, -4.4824e-03,  1.1932e-02,\n         -1.8216e-03,  5.8901e-03,  2.9970e-04],\n        [-1.2141e-03, -9.0674e-04, -1.2076e-04,  1.4005e-03, -4.5600e-04, -5.4675e-04,\n         -3.2751e-04,  7.5727e-04,  6.0921e-04, -1.9585e-04,  ..., -9.2101e-01,\n          9.1560e-01,  6.6118e-05, -8.9318e-04, -5.2877e-05, -5.3952e-05, -4.7661e-04,\n          3.3000e-04, -1.2100e-03,  6.6777e-04],\n        [ 9.9618e-04, -1.5972e-02, -8.0842e-04, -1.0736e-03,  2.1012e-03,  2.5437e-04,\n         -2.4493e-03, -3.8845e-04, -7.3633e-04, -1.9942e-03,  ..., -7.6592e-04,\n          1.3455e-03,  9.6635e-01, -9.1470e-01,  6.6249e-04, -3.3839e-03,  1.3439e-03,\n         -8.7517e-05,  1.5785e-03, -4.5950e-03],\n        [-1.3758e-03,  1.6246e-04,  1.4981e-03, -2.8291e-04,  3.1335e-03,  1.1978e-03,\n         -7.5044e-04,  2.4132e-04,  1.9992e-03,  1.1935e-03,  ..., -7.6299e-04,\n          3.0251e-04, -9.2833e-01,  9.1673e-01, -2.1420e-04,  3.5375e-03,  7.9616e-04,\n         -1.5844e-04,  8.4702e-04, -3.7494e-04],\n        [ 5.2137e-02, -3.0724e-03,  4.7006e-03, -4.3626e-04,  7.7648e-06,  2.2337e-02,\n          3.6721e-03,  2.1973e-03,  3.5948e-04,  2.4906e-03,  ...,  5.2599e-03,\n          1.3628e-03,  2.9804e-03,  5.3607e-04,  1.0105e+00, -1.0846e+00,  1.1961e-02,\n         -1.5857e-03,  7.5742e-03, -5.2180e-04],\n        [-2.2939e-02, -3.4620e-04,  5.6600e-04,  1.8429e-03, -6.3214e-03,  8.3725e-03,\n         -1.1450e-03,  5.0326e-03, -9.2600e-03,  6.4923e-03,  ..., -6.1017e-03,\n         -1.5811e-03,  1.3722e-03,  9.0622e-04, -1.0969e+00,  1.0354e+00, -2.6151e-02,\n          1.6175e-02,  1.0759e-02,  1.7396e-04],\n        [ 6.0896e-02,  2.5642e-03,  7.2913e-04,  9.2373e-04, -3.1041e-03,  1.6469e-02,\n         -1.8029e-04,  1.9627e-03, -1.2668e-02, -2.0080e-05,  ...,  1.3424e-02,\n         -3.1370e-04,  5.5743e-03,  2.1053e-03,  6.8854e-03, -5.2561e-03,  1.0072e+00,\n         -1.0429e+00,  7.2052e-04,  3.7638e-03],\n        [-2.9150e-02,  4.8583e-04,  6.3706e-03,  2.6714e-03,  6.8078e-04,  1.4530e-03,\n         -1.1730e-03,  5.5441e-03, -3.9073e-03,  7.7743e-03,  ...,  3.5764e-03,\n          1.2836e-03,  2.4791e-03, -1.9416e-04, -1.6777e-02,  9.3507e-03, -1.0265e+00,\n          9.9222e-01,  9.3117e-03, -1.1040e-05],\n        [ 5.0914e-02,  6.7689e-03,  8.1773e-03, -7.8487e-04,  7.6622e-06,  3.4396e-03,\n          5.5030e-03, -4.5024e-04,  1.4152e-03, -1.0479e-02,  ..., -1.0873e-03,\n         -3.2047e-04, -6.0772e-04,  1.4442e-06, -3.3447e-04, -1.3305e-02,  2.6723e-03,\n          3.6085e-03,  9.9757e-01, -1.0697e+00],\n        [-3.9952e-03, -1.0793e-03, -2.7950e-03,  1.0597e-03, -1.3181e-03, -1.3747e-03,\n          2.0374e-03, -3.4072e-02, -3.7260e-03, -3.5461e-03,  ..., -1.3366e-02,\n          1.4577e-03,  3.7511e-04, -2.0556e-03, -3.1433e-03,  5.5539e-04, -1.6465e-02,\n         -2.2155e-03, -1.1692e+00,  1.0092e+00]], device='cuda:0', requires_grad=True)\nlayers.3.linear.bias\nParameter containing:\ntensor([ 0.0006,  0.0089,  0.0057,  0.0216, -0.0017,  0.0163,  0.0054, -0.0017,  0.0058,\n         0.0007,  ..., -0.0061, -0.0124,  0.0192, -0.0089,  0.0083,  0.0070,  0.0020,\n         0.0016, -0.0022, -0.0090], device='cuda:0', requires_grad=True)\nlayers.4.linear.weight\nParameter containing:\ntensor([[ 9.7867e-01, -9.8695e-01,  2.0574e-04,  5.2677e-07, -2.9429e-04, -3.4187e-04,\n          1.7257e-03, -8.8908e-04, -2.0589e-03,  4.0271e-04,  ...,  1.6186e-03,\n         -5.5202e-04,  2.6394e-03, -1.1056e-03, -4.1106e-04,  3.2032e-04,  9.5551e-04,\n         -4.5233e-03,  1.0684e-03,  3.3266e-03],\n        [-9.4058e-01,  1.0212e+00, -1.1168e-03, -2.1418e-04,  1.8686e-04, -5.2919e-04,\n         -1.6285e-04,  7.3080e-04,  2.9269e-03, -1.9822e-05,  ...,  2.3537e-03,\n          1.6842e-03, -3.8518e-05, -1.4941e-03, -1.5336e-04, -6.8679e-04,  4.1366e-04,\n         -1.2659e-03,  4.3718e-04, -2.6807e-04],\n        [ 1.3054e-04, -5.9867e-04,  9.6287e-01, -9.9625e-01, -9.7535e-04, -5.8032e-04,\n         -1.1298e-03,  3.4833e-04, -7.2331e-04, -2.7938e-04,  ..., -2.8811e-03,\n          4.7427e-04,  5.5542e-04, -6.4634e-04,  9.4899e-04, -2.2201e-03,  1.6800e-03,\n         -1.2578e-03,  2.9121e-03, -1.2288e-06],\n        [-2.3823e-03, -1.2642e-03, -1.0325e+00,  9.4533e-01, -5.4937e-04,  2.3063e-03,\n          1.2728e-03,  5.3877e-04,  7.6845e-04,  1.1797e-04,  ...,  2.8141e-04,\n         -7.5512e-04,  3.6553e-04,  8.4947e-04,  6.1248e-04,  9.6561e-04, -4.6647e-03,\n          3.8231e-03,  9.3442e-04,  3.9017e-03],\n        [ 3.2205e-04,  1.7155e-03,  6.3618e-04, -5.1744e-04,  9.7830e-01, -9.5737e-01,\n         -8.5645e-02,  3.2005e-03, -8.8809e-04, -6.7998e-05,  ...,  3.9814e-03,\n         -1.3320e-03,  4.3610e-03,  1.2340e-04, -8.6779e-04,  6.7047e-04,  2.6825e-03,\n         -2.9809e-03, -1.1871e-03, -3.0188e-04],\n        [ 1.4323e-04, -3.5897e-04,  3.5372e-04,  9.6152e-04, -1.1095e+00,  9.3994e-01,\n          1.7865e-03,  8.0975e-05, -2.8207e-03, -3.1581e-04,  ...,  9.6890e-04,\n         -1.0403e-03,  4.0669e-05,  7.0248e-04, -7.9962e-05,  1.5509e-03, -2.0655e-04,\n         -1.8985e-03,  3.2478e-03,  1.3388e-03],\n        [ 1.2092e-03, -1.0624e-02,  7.0877e-04,  4.9453e-04, -9.9390e-04,  7.1986e-04,\n          9.8773e-01, -1.0281e+00, -1.8003e-03,  5.6037e-04,  ...,  2.3680e-03,\n         -4.2564e-04,  3.6780e-03, -8.1242e-04,  2.0778e-03,  4.3580e-04, -7.9568e-05,\n         -2.0915e-03,  2.5827e-04,  3.2871e-04],\n        [-1.8077e-03,  2.8818e-03,  2.2271e-03,  2.2073e-04, -6.2930e-04, -5.8766e-04,\n         -9.8374e-01,  9.9338e-01, -4.4380e-04,  4.4598e-04,  ..., -5.6172e-04,\n          1.3063e-04,  1.8598e-03, -1.2021e-03, -3.2783e-03,  1.8209e-03,  4.2617e-04,\n          2.8331e-03, -3.7376e-03, -9.9441e-04],\n        [ 2.0870e-03, -5.7384e-04, -1.8232e-03,  2.6435e-03, -5.0912e-03,  8.8320e-04,\n         -1.7115e-03,  2.1022e-03,  9.8135e-01, -9.9060e-01,  ...,  4.1381e-04,\n         -1.2694e-03,  9.0306e-04, -1.5949e-03, -5.5903e-04, -1.0661e-02, -4.8010e-03,\n         -3.0943e-03, -2.9611e-03, -7.5924e-04],\n        [-1.6744e-03,  1.0150e-04, -2.7914e-03,  4.2174e-03, -8.4777e-04,  2.3502e-04,\n         -3.1216e-06,  7.4688e-04, -1.0119e+00,  9.5937e-01,  ...,  1.0503e-03,\n          4.5150e-05,  2.4724e-03,  4.7985e-04, -5.9732e-04,  4.8385e-03, -7.8577e-04,\n          1.5093e-03,  1.5085e-03,  1.7597e-03],\n        ...,\n        [ 1.5388e-02,  3.5471e-03, -1.2357e-03, -2.1515e-03,  8.1182e-04,  1.6157e-03,\n          6.1864e-03, -1.3905e-03,  2.5061e-03,  4.1001e-03,  ...,  1.0054e+00,\n         -9.2151e-01,  1.3493e-04, -6.8306e-04,  1.2293e-02, -8.6418e-03,  2.7820e-03,\n          6.3967e-03, -4.9884e-04, -4.2058e-04],\n        [ 1.1908e-03,  1.4627e-03,  3.8765e-04, -8.2994e-04, -2.4605e-04,  1.0043e-04,\n          1.1821e-03, -6.8164e-04, -1.0405e-03, -1.6176e-03,  ..., -9.2112e-01,\n          9.1499e-01, -1.4474e-03, -6.6773e-04,  2.7064e-05, -1.0229e-03,  2.7170e-03,\n          3.4293e-04,  2.4940e-03,  5.1565e-04],\n        [-1.6994e-03, -1.2379e-02, -1.9485e-03,  9.6598e-05,  1.0484e-03,  2.9937e-04,\n         -4.9546e-04, -1.3684e-03, -2.2176e-04, -1.1016e-03,  ..., -1.6761e-04,\n          1.3286e-03,  9.5749e-01, -9.1504e-01, -1.3506e-03,  4.2483e-03,  3.3540e-03,\n         -1.7491e-03,  1.8606e-03, -1.1442e-03],\n        [-2.4291e-03, -2.0705e-03,  6.2850e-06,  3.8454e-05, -1.6118e-04, -5.3043e-04,\n         -9.3500e-04, -2.5481e-04, -1.7810e-03, -5.7101e-06,  ...,  9.6553e-04,\n         -8.9412e-04, -9.2838e-01,  9.1653e-01,  1.0076e-03,  7.6132e-04,  8.6005e-06,\n          1.2013e-04,  6.5549e-06, -3.9055e-04],\n        [ 3.5060e-02, -1.3505e-03,  1.5513e-03, -5.4356e-04, -5.8361e-04,  2.5380e-03,\n         -9.2187e-04, -2.7895e-04, -4.9602e-04, -2.1386e-03,  ...,  1.8389e-03,\n         -3.0607e-04, -1.1723e-03,  1.6809e-03,  1.0089e+00, -1.1507e+00,  3.4638e-02,\n         -3.0357e-02,  1.6508e-02,  3.7942e-03],\n        [-1.4308e-03,  1.4677e-03, -5.0427e-04,  9.6130e-04, -2.1835e-03,  1.3071e-04,\n          1.8432e-03, -3.2608e-04,  4.1474e-04,  1.5917e-03,  ..., -3.2744e-03,\n          1.3701e-03,  1.6739e-03, -8.2142e-04, -1.1557e+00,  9.9888e-01, -1.3853e-03,\n          1.6919e-03, -1.8498e-03, -2.5847e-03],\n        [ 3.6172e-02,  4.1412e-03, -1.1734e-03,  2.3294e-03,  3.9956e-03, -1.9074e-04,\n          2.6828e-03, -1.3326e-03,  4.0157e-03, -2.0077e-03,  ...,  2.6643e-03,\n          7.5223e-04,  1.6842e-03,  1.7479e-03,  9.6121e-03, -6.9668e-02,  1.0170e+00,\n         -1.0520e+00, -9.8138e-05, -3.5584e-02],\n        [-6.6613e-03,  3.8397e-03,  7.4198e-04,  3.7145e-04, -1.7559e-05, -1.1552e-04,\n          1.5307e-04, -1.3011e-03,  1.7163e-03, -2.3162e-03,  ..., -4.0123e-03,\n          5.9892e-04,  1.5097e-04, -1.7771e-03, -1.0385e-02,  2.3474e-03, -1.0624e+00,\n          9.9764e-01,  5.2577e-03, -4.5173e-03],\n        [ 1.0639e-02,  5.5014e-04,  1.7843e-03,  9.3518e-04, -5.0909e-05, -2.2383e-03,\n          6.1975e-04,  6.5060e-04, -1.2185e-03, -5.4940e-04,  ...,  7.7443e-03,\n         -2.1485e-03,  1.8871e-04, -1.3507e-04,  5.3724e-03, -2.6450e-02,  2.3555e-02,\n         -1.0255e-02,  1.0085e+00, -9.5691e-01],\n        [ 3.1264e-03,  1.3721e-04, -2.1878e-03, -2.3903e-04,  5.4952e-04,  4.1232e-04,\n          5.2253e-03,  1.5170e-03,  3.6915e-03, -2.1128e-03,  ...,  8.6678e-04,\n         -3.2362e-04,  3.1269e-03, -5.4529e-04,  5.7658e-03, -5.1858e-03,  3.2739e-03,\n         -5.1492e-03, -1.2235e+00,  9.9639e-01]], device='cuda:0', requires_grad=True)\nlayers.4.linear.bias\nParameter containing:\ntensor([ 0.0008, -0.0014,  0.0093,  0.0142,  0.0071,  0.0022,  0.0072,  0.0117, -0.0003,\n        -0.0041,  ...,  0.0061, -0.0110,  0.0141, -0.0103, -0.0007,  0.0012,  0.0026,\n        -0.0050,  0.0012,  0.0061], device='cuda:0', requires_grad=True)\nlayers.5.linear.weight\nParameter containing:\ntensor([[ 9.4542e-01, -9.6714e-01,  8.9649e-05,  1.3546e-03,  5.2047e-05,  1.9669e-03,\n          1.2768e-05,  1.1024e-06,  2.4626e-03,  1.0393e-03,  ...,  1.0739e-03,\n         -6.3559e-04, -1.1817e-03, -3.8996e-04,  4.5510e-03, -3.7563e-04,  2.8429e-03,\n         -7.6336e-04,  1.8210e-03,  1.5048e-03],\n        [-9.2962e-01,  9.4509e-01, -1.1281e-03, -1.6395e-04, -1.0666e-06,  5.2357e-04,\n         -1.6504e-03,  1.7889e-03,  4.6543e-04,  2.7277e-03,  ..., -4.4256e-07,\n         -1.1889e-04, -1.5032e-03,  1.3998e-04,  9.3036e-07, -5.6910e-04, -6.9027e-04,\n          1.2539e-04,  8.7864e-04,  2.4087e-04],\n        [-7.0058e-05, -4.0761e-04,  9.4610e-01, -1.0915e+00,  3.5682e-04,  7.0921e-04,\n          2.4661e-03,  2.4868e-03, -4.2110e-03, -8.6764e-03,  ..., -1.7504e-03,\n          8.2344e-04,  4.4114e-03, -3.4978e-04, -2.5376e-05, -7.9732e-04,  4.6349e-03,\n         -5.8458e-03,  9.0309e-04,  4.9168e-06],\n        [-7.2642e-05, -1.3591e-04, -9.9857e-01,  9.4907e-01,  1.7752e-04, -4.6627e-04,\n         -3.0637e-04,  1.5687e-03,  3.1848e-04,  5.9481e-05,  ..., -1.1503e-05,\n          3.7837e-04, -3.5300e-04,  8.9521e-04,  7.5625e-04,  2.0298e-03, -1.1976e-03,\n          2.1736e-03, -1.6518e-03, -1.4076e-04],\n        [ 1.4333e-03,  1.0791e-03,  1.6260e-04,  3.4163e-03,  9.3127e-01, -9.7016e-01,\n         -9.1089e-04,  1.2520e-03,  3.5123e-03,  1.1688e-03,  ...,  1.3107e-03,\n         -3.0470e-04, -1.6156e-04,  6.5331e-04, -5.6720e-02,  7.8985e-03,  5.5590e-04,\n          2.4841e-03, -1.6953e-02,  3.3250e-03],\n        [ 1.3027e-03, -1.4088e-03, -5.5664e-04,  1.5932e-03, -9.2836e-01,  9.2262e-01,\n          4.7161e-04, -1.6289e-05, -6.8308e-05,  4.1588e-04,  ...,  4.6944e-03,\n         -7.5162e-04, -5.4482e-06,  9.0649e-04, -4.8725e-03,  5.7083e-04, -1.6659e-05,\n          1.7422e-03, -4.7061e-03, -3.0150e-06],\n        [-9.8733e-04,  3.1847e-03, -2.5189e-04, -1.0164e-04,  3.4821e-03,  3.9779e-04,\n          9.5174e-01, -9.1436e-01,  1.5894e-03, -1.0039e-03,  ...,  5.5903e-04,\n          1.3074e-03, -2.1492e-03,  6.1033e-04,  2.3626e-04,  7.4877e-04, -3.2917e-04,\n          1.7106e-03,  7.9518e-04,  3.7384e-03],\n        [-7.7988e-04, -1.2861e-03, -5.2161e-05,  2.0379e-04, -1.0602e-04,  1.6282e-03,\n         -1.0031e+00,  9.5628e-01,  1.8302e-03,  1.9604e-03,  ..., -4.5804e-03,\n          1.5951e-04,  9.1137e-05, -8.0700e-04,  2.8856e-03,  4.4477e-03,  4.0792e-03,\n          3.3291e-03,  4.3628e-03,  3.7483e-04],\n        [-8.2536e-04,  2.2543e-03, -5.1290e-04,  3.2960e-03, -8.2574e-04, -1.4000e-03,\n          2.4412e-04,  4.7837e-04,  9.4355e-01, -9.3740e-01,  ..., -7.1632e-03,\n         -4.8761e-04,  1.3944e-04,  5.1014e-04,  1.6191e-03, -2.3989e-03, -6.1206e-03,\n          2.0362e-03, -2.7791e-05, -9.4067e-04],\n        [-1.1738e-03,  6.3266e-04, -2.1308e-04, -7.2261e-04,  5.5157e-06, -1.2260e-03,\n         -1.4044e-03,  6.5192e-05, -1.0266e+00,  9.3139e-01,  ..., -1.8822e-04,\n          2.9234e-04,  3.7755e-04, -5.9559e-04, -6.9414e-04,  4.0704e-03, -6.2881e-05,\n          6.9816e-04,  1.5198e-03, -2.2699e-03],\n        ...,\n        [ 3.2370e-03, -2.8897e-04,  3.8603e-04,  1.8217e-03,  8.0506e-04,  3.7332e-03,\n          5.3216e-03,  2.4699e-03,  2.9186e-03, -5.0910e-04,  ...,  1.0024e+00,\n         -9.2164e-01,  1.6729e-03, -2.2281e-03,  6.1301e-03, -6.4159e-02,  1.3627e-02,\n         -7.8054e-02,  5.2131e-03,  1.2439e-03],\n        [-2.3343e-03, -1.0449e-07, -4.7562e-04, -2.0600e-04,  1.4172e-03, -1.2183e-04,\n         -4.1798e-04, -6.7392e-04,  1.4296e-03, -2.0446e-03,  ..., -9.2099e-01,\n          9.1459e-01, -1.5556e-03,  5.9760e-04,  1.3956e-03,  5.2223e-04,  1.3218e-03,\n         -8.9490e-04, -2.0355e-03, -7.6069e-04],\n        [-1.6104e-03,  5.3537e-04,  1.4917e-03,  1.5846e-03,  5.6156e-04, -6.9645e-04,\n          1.0478e-03, -2.7383e-04, -1.0375e-03,  3.2895e-03,  ..., -6.4208e-03,\n         -4.2844e-04,  9.5446e-01, -9.1560e-01,  8.6138e-04,  2.1576e-03,  3.5420e-03,\n          1.2504e-03,  2.2953e-03, -8.1774e-04],\n        [ 1.2275e-03,  2.7688e-05,  3.0031e-03,  2.7850e-07,  1.5625e-03, -1.5606e-03,\n          7.5359e-07, -2.0989e-03, -1.5073e-03,  1.5055e-03,  ..., -4.8406e-04,\n         -7.5253e-04, -9.2843e-01,  9.1630e-01,  9.6994e-07, -2.3366e-04,  1.3369e-03,\n         -4.8983e-04,  2.0908e-07, -4.0907e-04],\n        [ 2.2335e-03,  1.1442e-03,  1.9885e-03, -1.2332e-03,  2.4470e-03, -1.9950e-04,\n         -1.9256e-03, -1.3152e-03,  6.9014e-05, -3.7038e-03,  ...,  5.5130e-04,\n         -3.1749e-04, -8.8993e-04, -1.2138e-06,  9.9419e-01, -9.6691e-01,  4.8599e-03,\n          2.3342e-03,  9.1704e-03, -1.6331e-03],\n        [-2.0380e-05, -1.5245e-03,  8.2880e-04, -4.9218e-04, -8.2061e-05, -4.5385e-04,\n          7.2715e-04, -4.3298e-04,  6.6658e-04, -2.2630e-04,  ...,  4.4825e-03,\n          3.9810e-04,  1.5977e-03, -7.1985e-04, -9.3336e-01,  9.0076e-01, -1.1497e-03,\n          2.7661e-03, -3.4793e-05, -6.0441e-06],\n        [ 1.6956e-03,  1.0637e-03, -4.1166e-03,  1.3115e-03, -1.9640e-04, -3.5476e-04,\n         -1.3556e-04, -1.0003e-03,  2.5444e-04, -2.5955e-03,  ...,  5.0695e-03,\n          3.0863e-03, -6.9493e-05, -8.2827e-06,  8.2730e-03,  1.4646e-03,  9.9571e-01,\n         -9.3193e-01,  5.2903e-03, -1.2130e-03],\n        [ 6.4791e-03,  6.5675e-04,  1.9627e-03,  1.6645e-03,  3.7330e-03,  2.6729e-04,\n          5.1464e-04,  2.2742e-03,  1.3772e-03,  2.4221e-03,  ...,  6.5622e-03,\n          2.0925e-04,  4.4047e-03, -8.3015e-04,  1.2202e-02,  8.5351e-03, -9.0089e-01,\n          9.8978e-01,  6.3457e-03, -9.5217e-04],\n        [ 1.7663e-03,  6.5183e-04,  2.5556e-03,  2.1345e-03,  1.5515e-03,  1.9415e-03,\n          6.7914e-04,  5.3860e-04,  1.9850e-04,  1.3316e-03,  ..., -1.8011e-03,\n         -3.8864e-05,  4.4598e-04,  3.4368e-04,  4.6139e-03,  7.0014e-03,  5.2841e-03,\n          1.5643e-03,  9.4837e-01, -9.4405e-01],\n        [ 2.3375e-04,  1.8126e-03, -4.7997e-03, -2.4276e-04,  1.0762e-04,  3.4779e-03,\n          2.0366e-04,  1.9161e-04,  3.0121e-04,  2.5724e-03,  ..., -1.7376e-02,\n          4.0149e-04,  6.3389e-03,  5.7078e-04,  4.3629e-03,  1.7477e-03,  2.7161e-03,\n          4.2629e-03, -9.6306e-01,  9.5955e-01]], device='cuda:0', requires_grad=True)\nlayers.5.linear.bias\nParameter containing:\ntensor([ 0.0134,  0.0025,  0.0122,  0.0093,  0.0152,  0.0064,  0.0252,  0.0050,  0.0058,\n        -0.0036,  ...,  0.0096, -0.0209,  0.0028, -0.0075, -0.0020,  0.0045,  0.0004,\n         0.0103,  0.0068,  0.0099], device='cuda:0', requires_grad=True)\nlast_layer.linear.weight\nParameter containing:\ntensor([[ 7.5338e-04,  1.4149e-03,  8.7634e-04,  3.0748e-03, -1.7477e-03,  7.3430e-03,\n          4.7924e-03,  3.8354e-03,  8.1513e-03, -1.1162e-03,  ..., -1.0403e-01,\n          1.2881e-04,  1.1581e-02, -5.5866e-04,  2.9184e-02,  3.0908e-02,  2.5944e-02,\n          7.7557e-02,  1.4921e-02,  9.0851e-02],\n        [ 7.9753e-05, -1.1058e-02,  7.9792e-04,  4.1176e-03, -8.2396e-04, -2.2776e-03,\n         -1.0810e-03,  2.6709e-03, -4.5676e-03,  2.2689e-03,  ...,  8.0070e-02,\n          7.5682e-04, -1.7393e-02,  2.4773e-04, -5.0789e-02, -1.5081e-02, -4.4628e-02,\n         -5.8734e-02, -2.3948e-02, -6.5381e-02],\n        [-9.1583e-04,  1.2552e-02, -7.8637e-04,  2.0921e-03, -1.2725e-03,  2.1435e-03,\n         -4.1267e-03,  3.4325e-03, -1.5313e-02, -2.5131e-04,  ...,  2.1149e-03,\n         -2.1894e-04, -2.3608e-02, -5.0048e-04, -4.7727e-03,  5.5081e-03, -1.7811e-02,\n          4.1308e-03, -3.4981e-02,  6.8350e-04],\n        [-3.3633e-03,  2.1754e-03, -9.8829e-03,  8.7324e-04,  1.0941e-03, -2.9363e-03,\n          2.1427e-03, -3.0262e-03, -2.2920e-04, -7.3518e-04,  ...,  6.1439e-08,\n          4.1103e-04, -6.5285e-03,  6.9954e-04, -1.0224e-02, -2.2459e-04, -7.2360e-03,\n         -7.8016e-03, -1.4682e-03, -3.4676e-04],\n        [-6.6677e-03,  1.0228e-03,  3.2508e-03,  5.2353e-03,  1.2763e-03,  1.4510e-03,\n         -9.9789e-04,  1.3745e-03, -1.9830e-03,  6.2583e-03,  ..., -1.2393e-04,\n          1.6847e-05,  3.3022e-02, -6.5180e-04, -1.4161e-02, -4.3080e-03, -4.1686e-03,\n         -4.0400e-03,  1.3633e-02, -2.2736e-04],\n        [-1.7843e-03,  7.4953e-03,  5.3952e-04, -9.2252e-04,  1.0983e-03,  1.0574e-03,\n         -1.6614e-03,  1.3543e-03,  2.2482e-02, -2.9969e-03,  ...,  9.3194e-04,\n          6.4082e-04,  1.9294e-03,  6.2010e-04,  1.1920e-02, -2.1365e-03, -6.5672e-03,\n          5.6697e-03, -6.7101e-03, -1.6285e-03],\n        [ 2.4771e-02, -4.7330e-03,  2.3702e-03,  1.8061e-03, -1.7603e-03,  2.1845e-03,\n          5.2987e-03, -2.7412e-04,  2.0871e-03,  3.3908e-03,  ...,  4.4650e-02,\n         -1.9006e-04,  5.9051e-03, -4.8451e-04,  2.1991e-02, -2.8449e-02,  2.0896e-02,\n         -1.3858e-02,  1.6908e-02, -1.1382e-02]], device='cuda:0', requires_grad=True)\nlast_layer.linear.bias\nParameter containing:\ntensor([ 0.0083,  0.0027, -0.0080, -0.0221,  0.0027, -0.0068, -0.0025], device='cuda:0',\n       requires_grad=True)\n","output_type":"stream"}]},{"cell_type":"code","source":"optimizer = optim.Adam(model.parameters(), lr=0.001)\n\ntorch.cuda.synchronize()\nstart_time = time.time()\n\nevaluate_model(model, custom_train_loader, criterion, optimizer, 10, 1024 * 16)\n\nelapsed_time = time.time() - start_time\nprint(f\"Execution time: {elapsed_time:.6f} seconds\")","metadata":{"execution":{"iopub.status.busy":"2024-10-25T22:48:47.164463Z","iopub.execute_input":"2024-10-25T22:48:47.164826Z","iopub.status.idle":"2024-10-25T22:50:40.093986Z","shell.execute_reply.started":"2024-10-25T22:48:47.164793Z","shell.execute_reply":"2024-10-25T22:50:40.093003Z"},"collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":37,"outputs":[{"name":"stdout","text":"Epoch 1, Training Loss: 1.0655988254591775, Validation Loss: 0.7915980608723059\nTraining Accuracy: 0.6703183458151628, Training F1 Score: 0.6367499655522032\nValidation Accuracy: 0.6688811820693097, Validation F1 Score: 0.6353570301802609\n\nEpoch 2, Training Loss: 0.7162294657670648, Validation Loss: 0.6859404292588623\nTraining Accuracy: 0.7093472802807175, Training F1 Score: 0.6910844651502446\nValidation Accuracy: 0.7091125013984149, Validation F1 Score: 0.6907912643993278\n\nEpoch 3, Training Loss: 0.6556414300141757, Validation Loss: 0.6401570571434966\nTraining Accuracy: 0.7271847145816884, Training F1 Score: 0.7134759129666042\nValidation Accuracy: 0.7268831269416453, Validation F1 Score: 0.7132260566642362\n\nEpoch 4, Training Loss: 0.6234223745597189, Validation Loss: 0.6195389865843056\nTraining Accuracy: 0.7335378617883905, Training F1 Score: 0.7173529678995823\nValidation Accuracy: 0.7315645895544866, Validation F1 Score: 0.715376273662036\n\nEpoch 5, Training Loss: 0.5997171407423667, Validation Loss: 0.5835306468625923\nTraining Accuracy: 0.750293131157099, Training F1 Score: 0.7406082393140092\nValidation Accuracy: 0.7503248625250639, Validation F1 Score: 0.740978244952949\n\nEpoch 6, Training Loss: 0.5770762019396962, Validation Loss: 0.5654811407506534\nTraining Accuracy: 0.7574229414662796, Training F1 Score: 0.7523952573360658\nValidation Accuracy: 0.7578461829729009, Validation F1 Score: 0.753144001998483\n\nEpoch 7, Training Loss: 0.5471220143783213, Validation Loss: 0.5411619325077585\nTraining Accuracy: 0.7696645288709987, Training F1 Score: 0.7638277564833559\nValidation Accuracy: 0.7690593186062322, Validation F1 Score: 0.7635756432303746\n\nEpoch 8, Training Loss: 0.5272593170424887, Validation Loss: 0.5086972146389479\nTraining Accuracy: 0.7849955573149401, Training F1 Score: 0.7792836373594972\nValidation Accuracy: 0.7849280999629958, Validation F1 Score: 0.779574873944845\n\nEpoch 9, Training Loss: 0.5025995179818988, Validation Loss: 0.4941518757346021\nTraining Accuracy: 0.7900513974557292, Training F1 Score: 0.7828241465787614\nValidation Accuracy: 0.7896870132440643, Validation F1 Score: 0.7828416761411184\n\nEpoch 10, Training Loss: 0.4805782851586737, Validation Loss: 0.46509801263450273\nTraining Accuracy: 0.8054233029050643, Training F1 Score: 0.8008236472480985\nValidation Accuracy: 0.8038260630104214, Validation F1 Score: 0.7994842837480287\n\nExecution time: 112.922238 seconds\n","output_type":"stream"}]},{"cell_type":"code","source":"for name, param in model.named_parameters():\n    print(f\"Parameter Name: {name}\")\n    print(param)","metadata":{"execution":{"iopub.status.busy":"2024-10-26T00:25:47.641029Z","iopub.execute_input":"2024-10-26T00:25:47.641967Z","iopub.status.idle":"2024-10-26T00:25:47.734591Z","shell.execute_reply.started":"2024-10-26T00:25:47.641912Z","shell.execute_reply":"2024-10-26T00:25:47.733670Z"},"trusted":true},"execution_count":128,"outputs":[{"name":"stdout","text":"Parameter Name: first_layer.linear.weight\nParameter containing:\ntensor([[ 1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  ...,  0.,  0.,  0.,  0.,  0.,\n          0.,  0.,  0.,  0.,  0.],\n        [-1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  ...,  0.,  0.,  0.,  0.,  0.,\n          0.,  0.,  0.,  0.,  0.],\n        [ 0.,  1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  ...,  0.,  0.,  0.,  0.,  0.,\n          0.,  0.,  0.,  0.,  0.],\n        [ 0., -1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  ...,  0.,  0.,  0.,  0.,  0.,\n          0.,  0.,  0.,  0.,  0.],\n        [ 0.,  0.,  1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  ...,  0.,  0.,  0.,  0.,  0.,\n          0.,  0.,  0.,  0.,  0.],\n        [ 0.,  0., -1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  ...,  0.,  0.,  0.,  0.,  0.,\n          0.,  0.,  0.,  0.,  0.],\n        [ 0.,  0.,  0.,  1.,  0.,  0.,  0.,  0.,  0.,  0.,  ...,  0.,  0.,  0.,  0.,  0.,\n          0.,  0.,  0.,  0.,  0.],\n        [ 0.,  0.,  0., -1.,  0.,  0.,  0.,  0.,  0.,  0.,  ...,  0.,  0.,  0.,  0.,  0.,\n          0.,  0.,  0.,  0.,  0.],\n        [ 0.,  0.,  0.,  0.,  1.,  0.,  0.,  0.,  0.,  0.,  ...,  0.,  0.,  0.,  0.,  0.,\n          0.,  0.,  0.,  0.,  0.],\n        [ 0.,  0.,  0.,  0., -1.,  0.,  0.,  0.,  0.,  0.,  ...,  0.,  0.,  0.,  0.,  0.,\n          0.,  0.,  0.,  0.,  0.],\n        ...,\n        [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  ...,  0.,  0.,  0.,  0.,  0.,\n          1.,  0.,  0.,  0.,  0.],\n        [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  ...,  0.,  0.,  0.,  0.,  0.,\n         -1.,  0.,  0.,  0.,  0.],\n        [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  ...,  0.,  0.,  0.,  0.,  0.,\n          0.,  1.,  0.,  0.,  0.],\n        [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  ...,  0.,  0.,  0.,  0.,  0.,\n          0., -1.,  0.,  0.,  0.],\n        [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  ...,  0.,  0.,  0.,  0.,  0.,\n          0.,  0.,  1.,  0.,  0.],\n        [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  ...,  0.,  0.,  0.,  0.,  0.,\n          0.,  0., -1.,  0.,  0.],\n        [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  ...,  0.,  0.,  0.,  0.,  0.,\n          0.,  0.,  0.,  1.,  0.],\n        [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  ...,  0.,  0.,  0.,  0.,  0.,\n          0.,  0.,  0., -1.,  0.],\n        [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  ...,  0.,  0.,  0.,  0.,  0.,\n          0.,  0.,  0.,  0.,  1.],\n        [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  ...,  0.,  0.,  0.,  0.,  0.,\n          0.,  0.,  0.,  0., -1.]], device='cuda:0')\nParameter Name: first_layer.linear.bias\nParameter containing:\ntensor([-0.0003,  0.0053, -0.0043, -0.0018, -0.0023, -0.0038, -0.0009, -0.0027,  0.0068,\n        -0.0040,  ..., -0.0337, -0.0048,  0.0144, -0.0056,  0.0065, -0.0031,  0.0080,\n        -0.0006,  0.0051, -0.0034], device='cuda:0', requires_grad=True)\nParameter Name: layers.0.linear.weight\nParameter containing:\ntensor([[ 1.0135e+00, -1.0074e+00, -1.8072e-03, -4.9150e-05,  1.2793e-03,  2.5622e-03,\n         -2.2274e-03,  1.0139e-02,  9.4383e-03, -5.9988e-03,  ..., -1.3432e-02,\n         -6.1006e-04,  1.4000e-03, -4.3493e-04, -6.6325e-03,  8.2900e-05, -7.0903e-03,\n          1.1815e-04, -7.8752e-03, -6.1508e-05],\n        [-1.0066e+00,  1.0349e+00,  2.5364e-03, -1.5976e-03, -8.5460e-03,  3.2531e-03,\n          2.5515e-02, -1.8875e-02,  2.2917e-02, -1.3516e-02,  ...,  0.0000e+00,\n          5.0063e-03,  0.0000e+00,  4.7178e-03,  0.0000e+00,  4.0746e-03, -9.2577e-03,\n          4.0396e-03,  0.0000e+00,  4.1011e-03],\n        [-4.6696e-03, -1.0558e-02,  9.9740e-01, -1.0037e+00, -1.0027e-02, -6.5063e-03,\n         -4.5067e-03, -6.3769e-03, -8.1957e-03, -8.6597e-03,  ...,  4.0982e-03,\n         -4.7955e-03, -4.8003e-03, -4.6018e-03, -9.1823e-03, -3.7029e-03, -7.1518e-03,\n         -4.0478e-03, -1.3018e-02, -3.8808e-03],\n        [-8.2071e-03, -1.1209e-02, -1.0064e+00,  9.9953e-01, -9.3774e-03, -3.9492e-03,\n          6.2875e-03, -5.4681e-03, -1.0573e-02, -3.8483e-04,  ..., -3.0115e-03,\n         -2.8865e-03, -1.0208e-02, -2.5538e-03, -8.5890e-03, -1.5353e-03, -7.1506e-03,\n         -1.4855e-03, -8.0457e-03, -1.7225e-03],\n        [-6.4444e-04, -1.1649e-03, -2.0022e-04, -6.1638e-03,  9.9507e-01, -9.9997e-01,\n         -1.3923e-03,  2.2120e-03, -7.5982e-03,  2.8045e-03,  ..., -5.3947e-03,\n         -9.7302e-04, -9.8720e-03, -1.2573e-03, -3.3163e-03, -1.5376e-03, -2.8786e-03,\n         -1.5505e-03, -4.5189e-03, -1.4762e-03],\n        [-5.7450e-03, -8.9885e-03, -3.5706e-03, -2.3907e-03, -1.0014e+00,  9.9549e-01,\n         -6.6889e-03, -5.8210e-03, -9.5475e-03, -4.4016e-03,  ..., -7.7582e-03,\n         -4.3841e-03, -7.9967e-03, -4.2396e-03, -7.1760e-03, -3.6059e-03, -7.2790e-03,\n         -3.7879e-03, -1.0136e-02, -3.7853e-03],\n        [-5.8343e-03,  2.3765e-03, -3.2045e-03, -3.0538e-03,  2.9117e-03, -8.9049e-03,\n          9.9734e-01, -1.0096e+00, -8.3063e-03,  7.2660e-03,  ..., -1.4029e-03,\n         -3.0040e-03, -8.4412e-03, -2.6653e-03, -4.2858e-03, -1.7630e-03, -6.6255e-03,\n         -1.6248e-03, -4.0220e-03, -1.8352e-03],\n        [-2.2651e-03, -3.1583e-03, -3.5511e-03, -1.2858e-02, -4.7677e-03, -4.5827e-03,\n         -9.7993e-01,  9.9807e-01, -7.0771e-03, -2.0500e-04,  ...,  0.0000e+00,\n         -4.3010e-03, -9.8043e-03, -4.0188e-03, -9.0386e-03, -2.9947e-03,  2.1406e-03,\n         -3.7665e-03,  8.6019e-03, -3.5768e-03],\n        [-1.9825e-03,  3.8852e-03, -1.2870e-03,  4.1702e-04,  5.8186e-04,  7.5030e-03,\n          4.9018e-03,  8.9604e-03,  9.9554e-01, -9.8304e-01,  ...,  5.3273e-03,\n          6.5846e-03, -7.4620e-03,  6.5089e-03, -7.5312e-03,  7.2238e-03, -4.6182e-03,\n          7.1111e-03, -3.7879e-03,  6.9904e-03],\n        [-6.8622e-03, -1.0329e-02, -5.3344e-03, -3.5689e-03, -7.4443e-03, -4.8406e-03,\n         -4.7290e-03, -3.2129e-03, -1.0154e+00,  9.9697e-01,  ...,  0.0000e+00,\n         -4.5240e-03, -1.0667e-02, -4.3658e-03, -8.4144e-03, -3.7199e-03, -8.1771e-03,\n         -3.8132e-03, -1.0200e-02, -4.0265e-03],\n        ...,\n        [-8.5932e-03, -4.1554e-03, -6.2822e-03, -6.6638e-03, -4.9684e-03, -7.3403e-03,\n         -4.7872e-03, -7.1624e-03, -5.3328e-03, -6.3353e-03,  ...,  9.6679e-01,\n         -1.0054e+00, -5.7312e-03, -5.4174e-03, -8.4890e-03, -4.8015e-03, -9.1716e-03,\n         -4.7744e-03, -8.4755e-03, -5.0183e-03],\n        [-6.4437e-03, -1.0625e-02, -7.2807e-03, -6.8175e-03, -9.2817e-03, -7.0029e-03,\n         -7.2634e-03, -7.0941e-03, -9.1758e-03, -6.9640e-03,  ..., -1.0000e+00,\n          9.9289e-01, -5.9758e-03, -7.1201e-03, -6.9140e-03, -7.0384e-03, -7.0225e-03,\n         -7.0539e-03, -7.0720e-03, -7.0830e-03],\n        [-3.9163e-03,  2.4463e-03, -7.6852e-03, -3.9895e-03, -4.0810e-03,  1.0372e-03,\n         -4.1038e-03, -4.8882e-03, -4.1647e-03, -4.8071e-03,  ...,  0.0000e+00,\n         -5.0873e-03,  1.0145e+00, -1.0057e+00, -5.1892e-03, -4.5779e-03, -5.1895e-03,\n         -4.6610e-03, -5.1896e-03, -4.8047e-03],\n        [-5.9801e-03, -1.0126e-02, -6.3218e-03, -5.7202e-03, -9.0851e-03, -5.7019e-03,\n         -6.2185e-03, -6.1163e-03, -8.7138e-03, -5.7885e-03,  ..., -5.9758e-03,\n         -6.0391e-03, -1.0000e+00,  9.9399e-01, -6.8248e-03, -5.8102e-03, -7.2080e-03,\n         -5.8334e-03, -6.1673e-03, -5.8933e-03],\n        [ 8.4381e-03, -3.7088e-02,  1.0012e-02,  5.3640e-03,  7.2925e-03,  5.8274e-03,\n          8.6395e-03,  6.1422e-03,  7.7901e-03,  1.3157e-02,  ...,  0.0000e+00,\n          6.6052e-03,  0.0000e+00,  6.7079e-03,  1.0065e+00, -9.8631e-01,  5.3761e-03,\n          6.8603e-03, -8.8447e-03,  6.8818e-03],\n        [-5.5708e-03, -7.3907e-03, -2.1814e-03, -2.9250e-03, -4.0685e-03, -3.5054e-03,\n         -1.7382e-03, -6.5251e-03, -4.1493e-03, -4.1656e-03,  ..., -6.8609e-03,\n         -2.9566e-03, -8.1500e-03, -2.8914e-03, -1.0000e+00,  9.9722e-01, -5.9844e-03,\n         -2.6162e-03, -5.0969e-03, -2.7405e-03],\n        [ 1.7712e-02, -4.9693e-03, -9.0817e-03,  1.2044e-02,  8.1508e-03,  1.5543e-02,\n          1.3907e-02,  5.7908e-03,  3.6373e-02,  9.0693e-03,  ...,  2.3597e-03,\n          6.9281e-03,  3.5556e-04,  7.0198e-03,  5.4906e-03,  6.8772e-03,  1.0080e+00,\n         -9.9767e-01, -2.8603e-04,  7.1079e-03],\n        [-5.5511e-03, -1.0746e-03, -1.2586e-03, -2.3756e-05, -3.7018e-03, -4.3788e-04,\n         -7.6315e-04, -3.3327e-03, -3.8351e-03, -1.9669e-03,  ..., -6.8129e-03,\n         -6.0146e-04, -8.5812e-03, -5.6591e-04, -6.4060e-03, -1.4055e-04, -1.0000e+00,\n          9.9951e-01, -6.6398e-03, -3.2098e-04],\n        [ 1.1247e-02, -3.9220e-02,  5.1098e-03,  1.4246e-02,  8.3763e-03, -9.4842e-04,\n          9.8552e-03, -8.8830e-03,  1.3932e-02, -3.5177e-03,  ..., -7.0880e-03,\n          5.2500e-03,  1.7045e-07,  5.3561e-03, -1.5655e-02,  5.5278e-03,  5.3153e-04,\n          5.4573e-03,  1.0050e+00, -9.8835e-01],\n        [-5.5691e-03, -8.0809e-03, -2.9301e-03, -3.2519e-03, -4.8410e-03, -3.7405e-03,\n         -2.7171e-03, -5.7830e-03, -4.2675e-03, -4.2650e-03,  ..., -6.1543e-03,\n         -3.3608e-03, -6.3935e-03, -3.3060e-03, -5.5324e-03, -3.1331e-03, -5.7339e-03,\n         -3.1272e-03, -1.0000e+00,  9.9678e-01]], device='cuda:0', requires_grad=True)\nParameter Name: layers.0.linear.bias\nParameter containing:\ntensor([-0.0003,  0.0040, -0.0042, -0.0018, -0.0016, -0.0039, -0.0018, -0.0033,  0.0067,\n        -0.0040,  ..., -0.0049, -0.0071, -0.0044, -0.0060,  0.0068, -0.0028,  0.0070,\n        -0.0005,  0.0055, -0.0032], device='cuda:0', requires_grad=True)\nParameter Name: layers.1.linear.weight\nParameter containing:\ntensor([[ 1.0142e+00, -1.0079e+00, -1.2719e-03, -1.2900e-03, -2.3029e-03,  9.2441e-04,\n          1.7657e-03,  4.1334e-03,  8.3574e-03, -1.1918e-02,  ..., -4.2445e-03,\n         -2.8179e-03,  1.3773e-03, -3.0695e-03, -8.2887e-03, -8.0843e-04, -4.6869e-03,\n         -8.9474e-04, -8.1478e-03, -7.9664e-04],\n        [-9.7802e-01,  1.0346e+00,  1.0593e-02,  1.9012e-03, -6.2167e-03,  9.9733e-03,\n          2.5876e-02, -1.5567e-02,  7.7756e-03, -8.5932e-03,  ..., -9.1077e-03,\n          8.0892e-03, -6.3918e-03,  8.1795e-03, -2.0657e-02,  8.8309e-03, -3.1624e-02,\n          8.9111e-03, -3.5914e-02,  9.0085e-03],\n        [-4.6317e-03, -1.0476e-02,  9.9757e-01, -1.0002e+00, -9.5927e-03, -6.5279e-03,\n         -4.6239e-03, -6.0407e-03, -8.5024e-03, -8.0317e-03,  ..., -6.5857e-03,\n         -6.7244e-03, -7.8779e-03, -6.7112e-03, -9.2034e-03, -4.2386e-03, -6.9970e-03,\n         -4.4375e-03, -1.2830e-02, -4.7868e-03],\n        [-8.5729e-03, -1.1377e-02, -1.0221e+00,  9.9989e-01, -8.9921e-03, -4.0506e-03,\n          6.6095e-03, -5.3669e-03, -1.0731e-02, -5.9881e-05,  ..., -5.1796e-03,\n         -6.5272e-03, -1.0230e-02, -6.3222e-03, -8.9472e-03, -2.4707e-03, -6.8907e-03,\n         -1.8604e-03, -7.8275e-03, -3.0708e-03],\n        [-7.1004e-04, -2.0761e-03, -4.2227e-04, -6.6378e-03,  9.9498e-01, -1.0054e+00,\n         -7.0311e-04,  8.5791e-04, -7.5335e-03,  1.7373e-03,  ..., -7.4554e-03,\n          7.3538e-03, -1.0482e-02,  6.3546e-03, -9.9713e-04, -1.3532e-03, -2.5434e-03,\n         -1.4641e-03, -3.9527e-03, -4.6283e-04],\n        [-6.3023e-03, -1.0130e-02, -3.7178e-03, -2.1620e-03, -1.0037e+00,  9.9553e-01,\n         -7.4113e-03, -5.7874e-03, -9.7971e-03, -4.2889e-03,  ..., -5.7880e-03,\n         -6.4123e-03, -7.9671e-03, -6.1483e-03, -7.3181e-03, -3.9900e-03, -7.4656e-03,\n         -3.9300e-03, -1.0608e-02, -4.3495e-03],\n        [-5.8915e-03, -1.8055e-03, -3.9842e-03, -2.9487e-03,  3.2539e-03, -1.0158e-02,\n          9.9716e-01, -1.0150e+00, -7.9498e-03,  6.6905e-03,  ..., -7.6935e-06,\n         -5.4419e-03, -8.6429e-03, -5.4097e-03, -4.4142e-03, -3.2463e-03, -6.8381e-03,\n         -2.5480e-03, -4.4042e-03, -3.6487e-03],\n        [-3.3362e-03, -3.6925e-03, -3.3079e-03, -1.2472e-02, -5.6305e-03, -4.2947e-03,\n         -1.0048e+00,  9.9809e-01, -8.7108e-03, -2.3291e-04,  ...,  3.3489e-03,\n         -6.6368e-03, -9.9330e-03, -6.5756e-03, -9.3500e-03, -4.4269e-03,  1.8370e-03,\n         -4.2508e-03,  7.2719e-03, -5.2090e-03],\n        [-2.4446e-03,  3.0480e-03, -3.0283e-03, -4.9858e-04,  1.6876e-04,  5.9025e-03,\n          4.4169e-03,  6.7648e-03,  9.9404e-01, -9.8619e-01,  ...,  4.4545e-03,\n          7.7373e-03, -7.6763e-03,  7.7566e-03, -7.1295e-03,  5.9843e-03, -5.3758e-03,\n          5.7628e-03, -4.6344e-03,  6.3567e-03],\n        [-7.1359e-03, -1.0500e-02, -5.1750e-03, -3.6805e-03, -6.8490e-03, -4.5341e-03,\n         -4.4630e-03, -3.3401e-03, -1.0190e+00,  9.9700e-01,  ...,  2.6322e-03,\n         -6.4631e-03, -1.0638e-02, -6.2785e-03, -8.6838e-03, -4.2207e-03, -8.2401e-03,\n         -4.0253e-03, -1.0999e-02, -4.7017e-03],\n        ...,\n        [-6.9075e-03, -5.2080e-03, -6.4246e-03, -6.1184e-03, -6.1152e-03, -2.6372e-03,\n         -3.2086e-03, -7.9877e-03, -4.3536e-03, -6.4763e-03,  ...,  9.6877e-01,\n         -9.9436e-01,  6.9426e-03,  5.7577e-03, -8.4773e-03, -3.6599e-03, -9.0561e-03,\n         -3.9200e-03, -8.3446e-03, -3.6022e-03],\n        [-6.1048e-03, -1.0368e-02, -6.6503e-03, -6.2822e-03, -9.0341e-03, -6.0546e-03,\n         -6.5418e-03, -6.6417e-03, -8.7384e-03, -6.2888e-03,  ..., -1.0005e+00,\n          9.9340e-01, -5.9755e-03, -6.5758e-03, -6.9098e-03, -6.3715e-03, -6.9980e-03,\n         -6.3968e-03, -7.1410e-03, -6.4391e-03],\n        [-7.5617e-03, -1.3613e-03, -1.0416e-02, -9.2455e-03, -9.6139e-03, -4.4268e-03,\n         -1.0216e-02, -6.4733e-03, -1.0956e-02, -6.4656e-03,  ..., -1.2209e-02,\n          3.2661e-03,  9.9632e-01, -1.0052e+00, -6.1488e-03, -9.5937e-03, -5.6860e-03,\n         -9.6521e-03, -7.6099e-03, -7.3876e-03],\n        [-6.6742e-03, -9.8831e-03, -6.2946e-03, -6.0667e-03, -9.4800e-03, -5.7686e-03,\n         -6.3625e-03, -6.3337e-03, -9.0542e-03, -5.9661e-03,  ..., -5.9762e-03,\n         -6.4831e-03, -1.0006e+00,  9.9360e-01, -6.9887e-03, -6.0290e-03, -7.0833e-03,\n         -6.0699e-03, -7.1356e-03, -6.1350e-03],\n        [ 1.3028e-02,  2.9166e-02,  5.5561e-03,  6.1833e-03, -1.2888e-02,  8.9130e-03,\n          1.1164e-02,  1.9509e-02,  1.1811e-02,  1.2008e-02,  ...,  2.0669e-03,\n          6.9602e-03,  9.9938e-03,  7.0295e-03,  1.0064e+00, -1.0083e+00,  1.2232e-02,\n          3.0787e-03,  8.0008e-03,  3.9292e-03],\n        [-4.9415e-03, -6.5834e-03, -2.1397e-03, -2.4699e-03, -3.6027e-03, -3.1693e-03,\n         -1.9115e-03, -4.8377e-03, -3.4703e-03, -3.2861e-03,  ..., -6.0773e-03,\n         -5.3457e-03, -6.2446e-03, -4.8197e-03, -1.0074e+00,  9.9701e-01, -5.6607e-03,\n         -2.7647e-03, -4.4898e-03, -3.0589e-03],\n        [ 2.3662e-02,  1.6689e-02, -3.5710e-03,  2.0191e-03,  8.2850e-03,  1.7182e-02,\n          1.4130e-02,  2.2231e-02,  2.3683e-02,  1.5919e-02,  ...,  4.4064e-03,\n          7.1890e-03,  1.0166e-02,  7.3225e-03,  1.1687e-04,  4.5619e-03,  1.0081e+00,\n         -9.9885e-01,  5.2350e-03,  4.1964e-03],\n        [-4.6784e-03, -6.9703e-04, -9.0224e-04,  9.8464e-05, -2.7808e-03,  4.4569e-06,\n         -1.1243e-04, -2.5123e-03, -2.3635e-03, -1.3881e-03,  ..., -7.3477e-03,\n         -2.2591e-03, -6.2830e-03, -1.6890e-03, -6.3115e-03, -8.0241e-05, -1.0160e+00,\n          9.9956e-01, -6.4539e-03, -3.6505e-04],\n        [ 1.5889e-02,  3.3137e-02, -1.4635e-03,  1.4035e-02, -7.0876e-03,  2.0319e-02,\n          1.9133e-02,  6.4496e-03,  1.2454e-02, -1.0481e-02,  ...,  3.3415e-03,\n          6.9359e-03,  1.0290e-02,  6.7666e-03, -1.6233e-02,  2.3806e-03, -1.1290e-02,\n          2.8773e-03,  1.0047e+00, -1.0072e+00],\n        [-4.7001e-03, -6.9046e-03, -2.8842e-03, -2.6072e-03, -4.3552e-03, -3.0947e-03,\n         -1.5337e-03, -4.8136e-03, -3.3627e-03, -3.7978e-03,  ..., -6.0064e-03,\n         -5.4007e-03, -6.5473e-03, -4.9824e-03, -5.5867e-03, -3.2196e-03, -6.0171e-03,\n         -3.0536e-03, -1.0016e+00,  9.9654e-01]], device='cuda:0', requires_grad=True)\nParameter Name: layers.1.linear.bias\nParameter containing:\ntensor([-0.0014,  0.0081, -0.0041, -0.0016, -0.0021, -0.0039, -0.0021, -0.0033,  0.0060,\n        -0.0039,  ..., -0.0050, -0.0065, -0.0085, -0.0061,  0.0031, -0.0026,  0.0051,\n        -0.0001,  0.0014, -0.0029], device='cuda:0', requires_grad=True)\nParameter Name: layers.2.linear.weight\nParameter containing:\ntensor([[ 1.0141e+00, -9.9441e-01, -1.3238e-03,  3.6355e-03,  9.6277e-03, -2.0627e-03,\n          4.5647e-03,  8.9473e-03,  1.2903e-02, -6.5093e-03,  ..., -8.1081e-03,\n         -1.9351e-03,  1.9440e-03, -2.1672e-03, -6.3833e-03,  1.0600e-04, -4.5490e-03,\n          8.2872e-04, -8.5577e-03, -1.1403e-03],\n        [-9.9850e-01,  1.0345e+00,  8.2834e-03,  4.1131e-03, -5.0721e-03,  1.0502e-02,\n          6.9464e-03, -1.2162e-02, -4.3102e-03, -1.2856e-03,  ..., -5.0664e-03,\n          7.7064e-03, -7.4104e-03,  7.8056e-03, -6.2127e-03,  9.7009e-03, -1.3917e-02,\n          9.0811e-03, -3.2158e-03,  8.8527e-03],\n        [-3.9749e-03, -1.1087e-02,  9.9815e-01, -9.9491e-01, -8.8011e-03, -6.0404e-03,\n         -3.9666e-03, -5.5876e-03, -7.2066e-03, -7.4325e-03,  ..., -4.5015e-03,\n         -6.7370e-03,  3.4007e-03, -6.7256e-03, -9.1465e-03, -5.0900e-03, -5.2196e-03,\n         -4.5948e-03, -1.2212e-02, -5.7699e-03],\n        [-9.0754e-03, -1.2100e-02, -1.0159e+00,  1.0004e+00, -8.8954e-03, -4.0485e-03,\n          7.0513e-03, -4.9376e-03, -1.1470e-02,  3.7795e-04,  ..., -4.2627e-03,\n         -6.6313e-03, -8.5282e-03, -6.6005e-03, -8.6258e-03, -3.8682e-03, -6.8727e-03,\n         -2.4586e-03, -7.7255e-03, -4.7196e-03],\n        [-1.6648e-03, -4.0881e-03, -1.0727e-03, -7.3261e-03,  9.9457e-01, -1.0065e+00,\n         -1.0235e-03, -8.6593e-04, -8.2867e-03,  2.0495e-04,  ..., -9.9862e-03,\n          7.5643e-03, -1.1227e-02,  7.4397e-03, -1.0713e-03, -7.7814e-05, -2.0927e-03,\n         -4.2418e-04, -3.9225e-03,  1.6947e-03],\n        [-6.8320e-03, -1.2100e-02, -3.6615e-03, -2.0143e-03, -1.0085e+00,  9.9561e-01,\n         -8.0461e-03, -5.3794e-03, -9.8397e-03, -4.2329e-03,  ..., -4.9143e-04,\n         -6.6346e-03, -6.6212e-03, -6.5346e-03, -7.6098e-03, -4.3236e-03, -7.4984e-03,\n         -3.8211e-03, -1.1278e-02, -4.9759e-03],\n        [-5.5958e-03, -6.5096e-03, -4.1492e-03, -2.4013e-03,  3.0583e-03, -1.0348e-02,\n          9.9738e-01, -1.0205e+00, -7.1720e-03,  4.6814e-03,  ...,  2.2253e-04,\n         -5.3230e-03, -7.0165e-03, -5.3857e-03, -4.6124e-03, -3.8590e-03, -5.6360e-03,\n         -2.1709e-03, -5.7975e-03, -4.7769e-03],\n        [-6.0075e-03, -4.1925e-03, -1.6098e-03, -1.1867e-02, -5.6339e-03, -3.5896e-03,\n         -1.0268e+00,  9.9876e-01, -9.6199e-03,  2.6108e-05,  ...,  1.9148e-02,\n         -6.7126e-03, -9.5488e-03, -6.6497e-03, -1.0389e-02, -5.0789e-03,  1.1187e-03,\n         -4.2745e-03,  3.5179e-03, -5.8588e-03],\n        [-3.6656e-03, -1.7741e-03, -5.1749e-03, -2.8284e-03, -2.9647e-03,  3.1278e-03,\n          2.9652e-03,  4.4600e-03,  9.9086e-01, -1.0045e+00,  ..., -9.5221e-03,\n          4.8949e-03, -7.9901e-03,  5.2142e-03, -7.2167e-03,  5.1529e-03, -6.1420e-03,\n          3.3310e-03, -6.9872e-03,  5.3724e-03],\n        [-6.9090e-03, -1.1292e-02, -4.8635e-03, -3.0893e-03, -5.9579e-03, -4.5979e-03,\n         -4.4856e-03, -3.1644e-03, -1.0047e+00,  9.9727e-01,  ...,  2.5051e-04,\n         -6.6414e-03, -8.9866e-03, -6.5471e-03, -9.1053e-03, -4.7282e-03, -8.2226e-03,\n         -4.0550e-03, -9.8667e-03, -5.2374e-03],\n        ...,\n        [-7.0537e-03, -5.3839e-03, -6.2988e-03, -4.8845e-03, -6.6869e-03, -1.2877e-03,\n         -3.7418e-03, -5.4633e-03, -6.3504e-03, -2.5945e-03,  ...,  9.7679e-01,\n         -9.9503e-01, -5.5174e-03,  5.6950e-03, -1.0051e-02, -1.0105e-03, -8.1555e-03,\n         -2.5228e-03, -9.6852e-03,  4.8364e-04],\n        [-6.1410e-03, -1.0344e-02, -6.5757e-03, -6.1827e-03, -9.0424e-03, -5.8615e-03,\n         -6.4759e-03, -6.5960e-03, -8.7853e-03, -6.1771e-03,  ..., -1.0016e+00,\n          9.9339e-01, -5.9810e-03, -6.6025e-03, -6.9349e-03, -6.3377e-03, -7.0359e-03,\n         -6.3651e-03, -7.1861e-03, -6.4191e-03],\n        [-5.1680e-03, -3.3179e-03, -2.6959e-04, -2.2654e-03, -7.5352e-03,  1.3368e-03,\n         -7.6106e-03,  2.9486e-03, -1.0264e-02,  1.0966e-03,  ..., -1.2274e-02,\n          5.3197e-03,  9.9088e-01, -9.9466e-01, -7.9590e-03,  2.8427e-03, -7.9099e-03,\n          1.6935e-03, -1.0234e-02,  3.7264e-03],\n        [-6.6166e-03, -1.0164e-02, -6.3940e-03, -6.0677e-03, -9.4151e-03, -5.6645e-03,\n         -6.3639e-03, -6.4238e-03, -9.0326e-03, -5.9793e-03,  ..., -5.9767e-03,\n         -6.7149e-03, -1.0046e+00,  9.9336e-01, -6.9338e-03, -6.1500e-03, -7.0328e-03,\n         -6.2014e-03, -7.1791e-03, -6.2801e-03],\n        [ 2.6183e-03, -2.5131e-03, -1.4099e-03,  1.4210e-03, -6.1991e-03,  1.4383e-03,\n         -8.8761e-04,  7.9664e-04, -3.5401e-03,  1.8650e-03,  ..., -7.5446e-03,\n          6.7864e-03,  7.8854e-03,  6.8704e-03,  1.0062e+00, -1.0078e+00, -5.9434e-03,\n         -2.4624e-04, -8.4197e-03,  2.0632e-03],\n        [-4.9607e-03, -6.7438e-03, -2.9722e-03, -2.7845e-03, -4.4628e-03, -2.9733e-03,\n         -1.6568e-03, -4.5483e-03, -3.8373e-03, -3.5681e-03,  ..., -5.2110e-03,\n         -5.5749e-03, -3.8452e-03, -5.5338e-03, -9.9297e-01,  9.9629e-01, -6.9967e-03,\n         -3.1994e-03, -6.2667e-03, -3.8929e-03],\n        [ 1.3330e-02, -8.0079e-03, -9.4115e-03,  6.0046e-03,  2.2180e-03, -9.4145e-04,\n          1.9079e-03,  1.1550e-02,  5.2159e-03,  1.2297e-02,  ..., -5.9695e-03,\n          6.4815e-03,  1.0089e-02,  6.2368e-03, -1.6909e-02,  5.0892e-04,  1.0083e+00,\n         -1.0100e+00, -4.9322e-03,  2.2400e-03],\n        [-4.1236e-03, -3.7548e-04, -9.3324e-04,  6.8012e-04, -3.2219e-03,  4.9633e-04,\n          1.9619e-04, -1.6421e-03, -2.5143e-03, -6.7695e-04,  ..., -8.0718e-03,\n         -2.9593e-03, -4.1184e-03, -2.6584e-03, -6.0881e-03, -3.5478e-04, -1.0063e+00,\n          9.9939e-01, -6.4764e-03, -7.8983e-04],\n        [ 1.5349e-03, -8.1930e-03, -4.4385e-03, -2.1024e-03, -5.8594e-03, -2.4744e-03,\n         -2.0447e-03, -4.6999e-03, -2.3249e-03, -4.8015e-03,  ..., -8.0896e-03,\n          8.9481e-03,  9.9453e-03,  8.0994e-03, -6.3986e-03, -3.4845e-03, -4.8274e-03,\n         -3.4964e-03,  1.0039e+00, -1.0075e+00],\n        [-4.7963e-03, -8.1317e-03, -3.9659e-03, -3.7228e-03, -5.5865e-03, -4.0218e-03,\n         -3.3823e-03, -4.8156e-03, -5.3089e-03, -4.2978e-03,  ..., -4.3679e-03,\n         -5.6567e-03, -4.6219e-03, -5.6429e-03, -6.9064e-03, -4.3740e-03, -7.0602e-03,\n         -4.0936e-03, -9.9878e-01,  9.9536e-01]], device='cuda:0', requires_grad=True)\nParameter Name: layers.2.linear.bias\nParameter containing:\ntensor([ 0.0023,  0.0065, -0.0034, -0.0011, -0.0029, -0.0038, -0.0021, -0.0030,  0.0042,\n        -0.0035,  ..., -0.0044, -0.0064, -0.0003, -0.0062, -0.0019, -0.0028,  0.0009,\n         0.0002, -0.0040, -0.0039], device='cuda:0', requires_grad=True)\nParameter Name: layers.3.linear.weight\nParameter containing:\ntensor([[ 1.0158e+00, -1.0145e+00, -1.8477e-03,  6.1837e-03,  1.1450e-02,  8.0234e-04,\n          4.1581e-03,  1.6378e-02,  1.0833e-02,  6.2968e-04,  ..., -3.5795e-03,\n         -8.5314e-04,  3.7371e-03, -1.0795e-03, -3.4696e-03, -1.2446e-03, -4.1895e-03,\n          1.1000e-03, -5.0133e-03, -1.9271e-03],\n        [-9.9553e-01,  1.0366e+00,  7.4898e-03,  2.5265e-03,  4.1592e-03,  6.7054e-03,\n          9.2722e-03, -1.3375e-02, -9.4768e-04,  5.7686e-04,  ..., -6.2978e-03,\n          5.9441e-03, -1.4167e-03,  6.2937e-03, -2.9684e-03,  7.5447e-03, -9.4671e-03,\n          9.4528e-03,  1.6401e-03,  7.4846e-03],\n        [-3.0763e-03, -1.1762e-02,  9.9939e-01, -1.0088e+00, -7.6934e-03, -5.7950e-03,\n         -3.2785e-03, -3.8753e-03, -6.7163e-03, -6.6939e-03,  ...,  7.7742e-04,\n         -6.7701e-03,  6.1101e-03, -6.7595e-03, -1.1018e-02, -6.0005e-03, -5.0513e-03,\n         -4.9590e-03, -1.6460e-02, -6.4887e-03],\n        [-8.9950e-03, -1.3137e-02, -1.0359e+00,  1.0021e+00, -7.2130e-03, -3.9768e-03,\n          9.1652e-03, -4.3666e-03, -1.0924e-02,  1.5440e-03,  ..., -3.4950e-03,\n         -6.7082e-03,  3.4162e-03, -6.6790e-03, -9.0665e-03, -5.2996e-03, -5.6337e-03,\n         -2.8464e-03, -5.4582e-03, -6.0815e-03],\n        [-2.7711e-03, -8.3834e-03, -1.7445e-03, -7.5173e-03,  9.9421e-01, -1.0001e+00,\n         -1.4075e-03, -3.5966e-03, -8.1615e-03, -1.0925e-03,  ..., -1.2045e-02,\n          7.5984e-03, -1.1815e-02,  7.5552e-03, -1.3307e-03,  2.5516e-03, -1.1661e-03,\n          5.0995e-04, -2.4959e-03,  4.7927e-03],\n        [-5.8923e-03, -1.3586e-02, -2.8465e-03, -1.8328e-03, -1.0075e+00,  9.9634e-01,\n         -6.7161e-03, -4.5612e-03, -8.2233e-03, -3.6783e-03,  ...,  2.1233e-03,\n         -6.7384e-03,  2.5299e-03, -6.6672e-03, -6.8902e-03, -5.1137e-03, -7.1453e-03,\n         -3.5409e-03, -1.1849e-02, -5.9057e-03],\n        [-5.4609e-03, -1.0648e-02, -3.6703e-03, -1.0426e-03,  2.8713e-03, -9.9043e-03,\n          9.9806e-01, -1.0150e+00, -5.2671e-03,  4.3839e-03,  ...,  2.5679e-03,\n         -5.2503e-03, -1.7096e-03, -5.2802e-03, -5.4929e-03, -5.0364e-03, -8.7089e-03,\n         -3.1450e-03, -6.2244e-03, -5.3026e-03],\n        [-1.0907e-02, -5.0556e-03,  1.2294e-03, -1.1165e-02, -7.4490e-03, -2.9678e-03,\n         -1.0421e+00,  9.9991e-01, -1.5567e-02,  1.4357e-03,  ...,  1.5102e-02,\n         -6.7596e-03,  2.7957e-03, -6.6929e-03, -1.2792e-02, -5.6913e-03, -7.5753e-03,\n         -3.7841e-03,  1.5862e-03, -6.1423e-03],\n        [-9.2247e-03, -3.9046e-03, -8.6955e-03, -5.4580e-03, -3.0241e-03, -2.2043e-03,\n         -5.0977e-04, -1.0371e-02,  9.9046e-01, -1.0350e+00,  ..., -9.8466e-03,\n          7.7899e-04, -1.1357e-02,  1.1847e-03, -4.4644e-03,  2.3792e-03, -6.5099e-03,\n         -1.9899e-03, -1.0765e-02,  2.3130e-03],\n        [-7.2979e-03, -1.0999e-02, -2.2324e-03, -2.7421e-03, -6.2020e-03, -4.1465e-03,\n         -4.6044e-03, -1.3600e-03, -1.0046e+00,  9.9812e-01,  ..., -1.7962e-03,\n         -6.7167e-03,  2.9732e-03, -6.6327e-03, -9.9559e-03, -5.2028e-03, -8.7671e-03,\n         -3.8032e-03, -1.1580e-02, -5.9374e-03],\n        ...,\n        [-8.1202e-03, -5.9081e-03, -6.0811e-03, -5.0604e-03, -7.0720e-03, -1.8440e-03,\n         -3.6837e-03, -5.7931e-03, -4.8682e-03, -4.4752e-03,  ...,  9.8515e-01,\n         -9.9465e-01, -5.3004e-03,  5.8160e-03, -8.9686e-03,  5.0008e-03, -8.2463e-03,\n         -1.8467e-03, -9.8299e-03,  7.2290e-03],\n        [-6.1576e-03, -1.0331e-02, -6.5065e-03, -6.0296e-03, -9.1517e-03, -5.6317e-03,\n         -6.3878e-03, -6.5321e-03, -8.7611e-03, -6.0110e-03,  ..., -1.0000e+00,\n          9.9333e-01, -7.1831e-03, -6.6582e-03, -6.9946e-03, -6.3371e-03, -7.1079e-03,\n         -6.3659e-03, -7.2902e-03, -6.4349e-03],\n        [-4.8435e-03, -4.6887e-03, -1.6734e-03, -1.3575e-03, -9.6940e-03,  1.5322e-03,\n         -7.8642e-04,  9.0665e-04, -6.0153e-03, -5.3521e-04,  ..., -1.2613e-02,\n          5.3110e-03,  9.8893e-01, -9.9469e-01, -8.4050e-03,  5.2241e-03, -7.6395e-03,\n          2.8160e-03, -9.2977e-03,  5.9258e-03],\n        [-6.3690e-03, -1.0333e-02, -6.4917e-03, -6.0255e-03, -9.0733e-03, -5.5894e-03,\n         -6.3936e-03, -6.5066e-03, -8.8139e-03, -5.9782e-03,  ..., -5.9734e-03,\n         -6.7657e-03, -1.0053e+00,  9.9327e-01, -6.9931e-03, -6.3180e-03, -7.1040e-03,\n         -6.3639e-03, -7.2818e-03, -6.4417e-03],\n        [ 2.1644e-03, -9.1021e-03, -3.7694e-03, -3.0498e-03, -6.2403e-03, -5.4236e-04,\n         -3.9548e-03, -7.1073e-04, -5.2192e-03, -3.4332e-04,  ..., -6.7144e-03,\n          8.9404e-03, -5.5516e-04,  8.9969e-03,  1.0056e+00, -1.0038e+00, -5.1059e-03,\n         -1.7097e-03, -6.8910e-03,  1.1541e-03],\n        [-4.7551e-03, -8.8386e-03, -4.5895e-03, -4.0340e-03, -6.7487e-03, -3.8914e-03,\n         -4.1150e-03, -4.9879e-03, -6.4726e-03, -4.3734e-03,  ..., -4.2924e-03,\n         -5.4865e-03, -8.0689e-04, -5.4893e-03, -9.9394e-01,  9.9507e-01, -6.9912e-03,\n         -4.5745e-03, -6.9310e-03, -5.0587e-03],\n        [ 9.4983e-04, -6.8404e-03, -5.6143e-03, -3.4599e-03, -2.8620e-03, -4.2905e-03,\n         -4.3479e-03, -3.0740e-03, -3.3244e-03, -3.7435e-03,  ..., -7.9088e-03,\n         -2.9267e-03,  5.4071e-03, -2.9022e-03, -5.3075e-03, -3.2318e-03,  1.0081e+00,\n         -1.0052e+00, -4.7668e-03, -3.0800e-03],\n        [-3.2742e-03, -1.7858e-03, -1.0910e-03,  5.3839e-04, -4.4212e-03,  1.1875e-03,\n         -4.6893e-04, -7.7906e-04, -3.6423e-03, -4.7101e-04,  ..., -3.0316e-03,\n         -3.4532e-03,  2.4629e-03, -3.3232e-03, -6.8010e-03, -1.5463e-03, -9.9282e-01,\n          9.9850e-01, -6.8946e-03, -2.1121e-03],\n        [ 5.6865e-03, -7.9070e-03, -2.2943e-03, -4.3591e-04, -4.4173e-03,  1.2371e-03,\n         -1.5195e-03,  1.6904e-03, -2.1264e-03, -8.0201e-04,  ..., -6.2545e-03,\n          7.9709e-03,  5.5997e-04,  8.0486e-03, -3.2884e-03,  2.0976e-03, -6.4980e-04,\n          8.1330e-04,  1.0025e+00, -9.9708e-01],\n        [-4.9976e-03, -9.2670e-03, -5.2246e-03, -4.8674e-03, -7.1710e-03, -5.0887e-03,\n         -4.9043e-03, -5.5126e-03, -6.8434e-03, -5.2077e-03,  ..., -5.6105e-03,\n         -5.4361e-03, -4.3974e-03, -5.4407e-03, -6.9299e-03, -5.2872e-03, -6.8935e-03,\n         -5.1598e-03, -9.9761e-01,  9.9472e-01]], device='cuda:0', requires_grad=True)\nParameter Name: layers.3.linear.bias\nParameter containing:\ntensor([ 0.0033,  0.0081, -0.0024, -0.0002, -0.0038, -0.0032, -0.0019, -0.0029,  0.0005,\n        -0.0025,  ..., -0.0047, -0.0062, -0.0002, -0.0062, -0.0036, -0.0043, -0.0041,\n         0.0002, -0.0010, -0.0051], device='cuda:0', requires_grad=True)\nParameter Name: layers.4.linear.weight\nParameter containing:\ntensor([[ 1.0171e+00, -1.0228e+00,  4.7176e-03,  8.1384e-03,  7.2114e-03,  2.7423e-03,\n          5.3460e-03,  2.1437e-02,  2.3460e-04,  8.9404e-03,  ...,  7.1811e-04,\n          5.3159e-04,  3.8966e-03,  4.9977e-04, -2.5702e-03, -7.7144e-04, -2.5309e-03,\n         -3.8779e-04, -1.2496e-03, -3.8851e-04],\n        [-1.0051e+00,  1.0355e+00,  5.3689e-03,  3.6525e-04,  5.7310e-03,  6.8024e-03,\n          2.2044e-03, -1.9586e-02,  1.6021e-03, -3.8566e-03,  ..., -2.9205e-03,\n          3.3729e-03,  2.3738e-04,  3.5296e-03,  2.3425e-03,  5.5324e-03,  2.3719e-02,\n          7.4294e-03,  4.4674e-03,  5.0503e-03],\n        [-2.2741e-03, -1.4076e-02,  1.0023e+00, -1.0131e+00, -6.1890e-03, -4.7213e-03,\n         -4.0408e-03,  4.2172e-04, -6.1633e-03, -5.3191e-03,  ...,  6.2581e-03,\n         -6.7910e-03,  5.1848e-03, -6.7574e-03, -1.4433e-02, -6.4214e-03, -5.5414e-03,\n         -5.0806e-03, -1.3761e-02, -6.5627e-03],\n        [-8.8273e-03, -1.4640e-02, -1.0221e+00,  1.0059e+00, -6.4554e-03, -2.7683e-03,\n          8.1910e-03, -1.5442e-03, -1.2516e-02,  4.1315e-03,  ...,  2.5848e-03,\n         -6.7510e-03,  4.9517e-03, -6.7091e-03, -8.5476e-03, -6.2462e-03, -2.8932e-03,\n         -3.3299e-03, -8.8808e-03, -6.4137e-03],\n        [-6.9847e-03, -8.1653e-03, -1.8904e-03, -7.9219e-03,  9.9508e-01, -1.0065e+00,\n         -1.8607e-03, -4.6194e-03, -1.1090e-02, -2.6830e-03,  ..., -1.2720e-02,\n          4.5547e-03, -1.0967e-02,  4.7042e-03, -1.1182e-03,  2.9301e-04, -1.0244e-02,\n         -1.1907e-03, -1.0943e-02,  1.7806e-03],\n        [-3.0049e-03, -1.5777e-02, -8.1502e-04, -7.7243e-04, -1.0167e+00,  9.9835e-01,\n         -3.7571e-03, -9.9306e-04, -9.5253e-03, -1.5518e-03,  ...,  2.9218e-03,\n         -6.7875e-03,  5.7486e-03, -6.7316e-03, -6.1279e-03, -5.9206e-03, -6.9357e-03,\n         -2.9543e-03, -8.3648e-03, -6.2364e-03],\n        [-3.6960e-03, -1.0398e-02, -3.9765e-03,  4.5252e-03,  1.0687e-03, -3.1619e-03,\n          1.0017e+00, -1.0144e+00, -3.2250e-03,  9.7912e-03,  ...,  4.7192e-03,\n         -4.9319e-03,  3.1433e-03, -4.9161e-03, -1.0751e-02, -4.7746e-03, -5.6912e-03,\n         -2.8329e-03, -7.3152e-03, -4.7952e-03],\n        [-1.1412e-02, -6.4403e-03,  6.8669e-03, -7.2599e-03, -5.7152e-03,  5.4997e-04,\n         -1.0484e+00,  1.0038e+00, -2.5233e-02,  6.1445e-03,  ...,  9.4146e-03,\n         -6.7675e-03,  6.8525e-03, -6.7315e-03, -1.0226e-02, -6.1073e-03, -9.4084e-03,\n         -2.5167e-03, -6.4184e-03, -6.3751e-03],\n        [-5.7918e-03, -7.6878e-03, -1.0844e-02, -4.3825e-03, -5.1757e-04, -3.9991e-03,\n          8.8440e-04, -1.5329e-02,  9.9491e-01, -1.0265e+00,  ..., -9.6249e-03,\n         -3.4349e-03, -7.0808e-03, -3.3778e-03, -1.4269e-02, -2.6459e-03, -6.7281e-03,\n         -6.4709e-03, -2.1983e-02, -2.8451e-03],\n        [-7.8930e-03, -1.7105e-02,  2.5405e-04, -8.1793e-04, -3.2543e-03, -3.6970e-03,\n         -5.2358e-03,  1.1440e-03, -1.0001e+00,  1.0004e+00,  ...,  1.2130e-03,\n         -6.7404e-03,  5.1772e-03, -6.6873e-03, -7.0127e-03, -5.8843e-03, -8.0615e-03,\n         -3.2824e-03, -5.1002e-03, -6.2120e-03],\n        ...,\n        [-9.2731e-03, -6.2979e-03, -6.4136e-03, -5.8608e-03, -7.5173e-03, -2.5036e-03,\n         -4.8499e-03, -6.4755e-03, -5.7516e-03, -5.1248e-03,  ...,  9.8949e-01,\n         -9.9514e-01, -5.2084e-03,  5.0058e-03, -9.5863e-03,  7.2073e-03, -8.2403e-03,\n         -1.7433e-03, -1.1219e-02,  7.6852e-03],\n        [-6.0648e-03, -1.0150e-02, -6.3392e-03, -5.6578e-03, -9.3591e-03, -5.1673e-03,\n         -6.1615e-03, -6.3727e-03, -8.7825e-03, -5.6436e-03,  ..., -1.0043e+00,\n          9.9327e-01, -8.4424e-03, -6.6839e-03, -7.4140e-03, -6.3065e-03, -7.7422e-03,\n         -6.3220e-03, -8.5368e-03, -6.4135e-03],\n        [-7.9846e-03, -5.7263e-03, -6.0475e-03, -5.5313e-03, -1.1061e-02, -2.2660e-03,\n         -5.9401e-03, -4.4198e-03, -1.0406e-02, -3.5476e-03,  ..., -1.2835e-02,\n          5.3096e-03,  9.8951e-01, -9.9469e-01, -7.7848e-03,  6.0889e-03, -7.1505e-03,\n          1.0141e-03, -9.9276e-03,  6.0250e-03],\n        [-5.9127e-03, -1.0054e-02, -6.3144e-03, -5.6261e-03, -8.8301e-03, -5.1695e-03,\n         -6.1179e-03, -6.3649e-03, -8.4311e-03, -5.6416e-03,  ..., -5.9782e-03,\n         -6.7045e-03, -1.0053e+00,  9.9333e-01, -7.3392e-03, -6.2900e-03, -7.6392e-03,\n         -6.2840e-03, -8.3318e-03, -6.3777e-03],\n        [ 2.1072e-04, -5.9715e-03, -4.2864e-03, -4.7370e-03, -3.7454e-03, -5.1264e-03,\n         -4.5080e-03, -3.0996e-03, -4.1096e-03, -3.5908e-03,  ..., -9.4785e-03,\n          7.9045e-04, -7.4484e-03,  9.2137e-04,  1.0033e+00, -1.0029e+00,  3.4488e-04,\n         -3.2772e-03, -3.3194e-03, -4.6141e-04],\n        [-4.9293e-03, -9.4481e-03, -5.3612e-03, -5.0106e-03, -7.8431e-03, -4.9276e-03,\n         -5.0502e-03, -5.6589e-03, -7.3849e-03, -5.1519e-03,  ..., -5.9737e-03,\n         -5.4958e-03, -7.9831e-03, -5.4735e-03, -1.0061e+00,  9.9462e-01, -7.2390e-03,\n         -5.3037e-03, -7.5761e-03, -5.3621e-03],\n        [ 2.4139e-03, -5.4706e-03, -3.0911e-03, -3.8022e-03, -1.0593e-03, -4.4288e-03,\n         -3.8850e-03, -2.1096e-03, -1.8303e-03, -3.6880e-03,  ..., -7.6354e-03,\n         -7.4831e-04, -3.4550e-03, -6.9931e-04,  4.0344e-03, -2.1489e-03,  1.0059e+00,\n         -1.0038e+00, -1.7816e-03, -1.4446e-03],\n        [-2.9510e-03, -4.1130e-03, -2.3620e-03, -1.0241e-03, -6.8235e-03,  2.1300e-04,\n         -1.7483e-03, -1.3275e-03, -4.4375e-03, -1.4384e-03,  ...,  4.5130e-03,\n         -3.7662e-03,  6.9570e-03, -3.7382e-03, -6.6894e-03, -3.0336e-03, -9.9548e-01,\n          9.9697e-01, -6.0829e-03, -3.3105e-03],\n        [ 1.8653e-03, -5.4129e-03, -3.0044e-03, -4.0433e-03, -2.9237e-03, -4.4736e-03,\n         -3.3775e-03, -3.2275e-03, -2.2792e-03, -4.2829e-03,  ..., -1.0376e-02,\n          3.7812e-03, -5.8448e-03,  3.8557e-03,  1.1750e-03,  5.6852e-04,  2.3226e-03,\n         -9.0667e-04,  9.9976e-01, -9.9875e-01],\n        [-4.7680e-03, -9.2409e-03, -5.0334e-03, -4.7989e-03, -7.7482e-03, -4.8825e-03,\n         -4.7624e-03, -5.4038e-03, -7.0639e-03, -4.9835e-03,  ..., -5.9738e-03,\n         -5.3642e-03, -7.4864e-03, -5.3445e-03, -7.0468e-03, -5.2151e-03, -7.2247e-03,\n         -5.1279e-03, -1.0062e+00,  9.9480e-01]], device='cuda:0', requires_grad=True)\nParameter Name: layers.4.linear.bias\nParameter containing:\ntensor([ 0.0055,  0.0053, -0.0004,  0.0014, -0.0046, -0.0011,  0.0005,  0.0001, -0.0021,\n        -0.0008,  ..., -0.0053, -0.0060, -0.0047, -0.0059, -0.0049, -0.0053, -0.0038,\n        -0.0014, -0.0040, -0.0051], device='cuda:0', requires_grad=True)\nParameter Name: layers.5.linear.weight\nParameter containing:\ntensor([[ 1.0175e+00, -1.0263e+00,  9.5151e-03,  8.5351e-03,  5.1748e-03,  4.5772e-03,\n          3.6993e-03,  1.9742e-02, -2.8476e-03,  9.7679e-03,  ...,  3.5306e-03,\n          2.2822e-03,  4.8493e-03,  2.7395e-03, -1.7893e-03,  2.1432e-03, -3.4320e-03,\n          1.1807e-03, -3.1462e-03,  3.4595e-03],\n        [-1.0102e+00,  1.0275e+00, -5.4212e-03,  3.1185e-03,  1.2666e-04, -2.5469e-03,\n         -3.9384e-03, -2.3601e-02,  2.7700e-04, -1.3859e-02,  ...,  1.1284e-02,\n         -3.5256e-03,  2.2816e-03, -3.5647e-03, -5.9656e-03, -2.9108e-03, -3.1511e-03,\n         -2.9936e-03, -6.3291e-03, -3.3115e-03],\n        [ 1.9481e-04, -1.1557e-02,  1.0117e+00, -1.0497e+00, -8.9998e-03, -4.1812e-03,\n         -9.1427e-03,  4.8950e-03,  3.2297e-03, -1.5435e-02,  ...,  2.3746e-03,\n         -6.8513e-03,  5.3677e-03, -6.7795e-03, -1.1921e-02, -6.2449e-03, -1.2394e-02,\n         -6.3975e-03, -1.0428e-02, -6.2520e-03],\n        [-8.3850e-03, -1.3523e-02, -1.0344e+00,  1.0125e+00, -7.8349e-03,  2.8773e-03,\n          1.0851e-02,  1.1260e-02, -6.1801e-03,  1.1797e-02,  ...,  6.3649e-03,\n         -6.7985e-03,  5.8661e-03, -6.7291e-03, -2.1760e-03, -5.9079e-03, -9.4166e-03,\n         -3.4912e-03, -6.7624e-03, -5.9825e-03],\n        [-7.1967e-03, -6.7811e-03, -4.8800e-03, -5.5796e-03,  9.9277e-01, -1.0064e+00,\n         -5.0857e-03, -6.3612e-03, -6.0328e-03, -4.8618e-03,  ..., -4.7807e-03,\n         -6.0698e-03, -8.5869e-03, -6.1475e-03, -8.9895e-03, -5.2656e-03, -1.0061e-02,\n         -5.2803e-03, -9.3940e-03, -5.5575e-03],\n        [ 2.1370e-03, -2.2250e-02,  8.3857e-03,  6.8082e-03, -1.0139e+00,  1.0031e+00,\n         -1.5041e-03,  1.6964e-02, -1.2932e-02,  3.4895e-03,  ...,  4.1102e-03,\n         -6.7343e-03,  6.6272e-03, -6.6856e-03, -6.2712e-03, -5.4602e-03, -7.3374e-03,\n         -2.4521e-03, -6.3217e-03, -5.7821e-03],\n        [ 1.5396e-03, -1.5764e-02, -3.0884e-03,  9.0247e-03, -8.5145e-04,  2.1896e-03,\n          1.0082e+00, -1.0062e+00, -2.1570e-03, -2.1141e-04,  ...,  1.3403e-03,\n         -4.1560e-03,  7.2492e-03, -4.1866e-03, -4.8206e-03, -3.9067e-03, -1.6290e-03,\n         -3.6224e-03, -4.4414e-03, -4.1007e-03],\n        [ 9.3913e-05, -1.4400e-02,  1.3942e-02, -1.1519e-02,  9.1300e-03,  5.2966e-04,\n         -1.0515e+00,  1.0195e+00,  3.7933e-03,  1.4650e-02,  ...,  3.7982e-03,\n         -6.8354e-03,  6.5148e-03, -6.7773e-03, -1.3645e-02, -6.2422e-03, -1.3613e-02,\n         -3.0974e-03, -6.4475e-03, -6.2596e-03],\n        [-1.4245e-03, -1.0769e-03,  2.6358e-03, -6.8236e-03,  1.9280e-02, -1.0672e-02,\n          1.5487e-02, -1.0157e-02,  1.0092e+00, -1.0017e+00,  ...,  1.6544e-03,\n         -4.3895e-03,  9.2446e-03, -4.3969e-03, -4.8359e-03, -4.2715e-03, -7.6363e-03,\n         -4.9056e-03, -9.5203e-03, -4.3545e-03],\n        [-2.0844e-03, -1.9190e-02,  1.0758e-02,  4.5740e-03, -5.6337e-03,  1.1278e-03,\n         -2.1141e-03,  1.3224e-02, -1.0273e+00,  1.0072e+00,  ...,  4.4451e-03,\n         -6.7583e-03,  6.6356e-03, -6.7167e-03, -8.0603e-03, -5.7417e-03, -8.1843e-03,\n         -5.2151e-03, -3.4564e-03, -5.9805e-03],\n        ...,\n        [-9.2806e-03, -6.3425e-03, -6.9967e-03, -6.7499e-03, -8.4428e-03, -2.8964e-03,\n         -6.9357e-03, -6.6739e-03, -6.8070e-03, -5.4009e-03,  ...,  9.9286e-01,\n         -1.0000e+00, -4.5919e-03, -1.6400e-04, -8.4087e-03, -4.1550e-03, -7.8469e-03,\n         -4.3452e-03, -5.9420e-03, -4.0540e-03],\n        [-5.5616e-03, -9.1319e-03, -5.4232e-03, -3.9959e-03, -9.9577e-03, -3.3001e-03,\n         -5.0951e-03, -5.5678e-03, -9.4234e-03, -3.9995e-03,  ..., -1.0054e+00,\n          9.9322e-01, -8.0265e-03, -6.7245e-03, -9.3710e-03, -5.7582e-03, -9.6299e-03,\n         -5.7411e-03, -9.9495e-03, -5.9246e-03],\n        [-8.5525e-03, -6.0369e-03, -1.0889e-02, -1.2849e-02, -1.2065e-02, -6.9340e-03,\n         -1.0900e-02, -7.0821e-03, -1.1251e-02, -7.5249e-03,  ..., -1.1737e-02,\n          5.2479e-03,  9.8856e-01, -9.9476e-01, -7.5680e-03,  5.9669e-03, -7.0930e-03,\n         -5.2382e-03, -8.1285e-03,  5.5022e-03],\n        [-5.2591e-03, -8.8522e-03, -5.3439e-03, -3.7429e-03, -9.2644e-03, -3.1227e-03,\n         -4.9397e-03, -5.5269e-03, -8.9212e-03, -3.8081e-03,  ..., -5.9987e-03,\n         -6.7843e-03, -1.0054e+00,  9.9328e-01, -9.0902e-03, -5.7332e-03, -9.3970e-03,\n         -5.6914e-03, -9.9290e-03, -5.8925e-03],\n        [ 1.0928e-03, -7.3746e-03, -2.6877e-03, -2.9530e-03, -3.7780e-03, -2.8629e-03,\n         -4.2206e-03, -7.6582e-04, -4.4815e-03, -1.6992e-03,  ..., -8.4149e-03,\n          2.8339e-03, -7.3108e-03,  3.2568e-03,  9.9776e-01, -9.9865e-01, -6.0779e-03,\n          3.1115e-03, -6.5678e-03,  3.7419e-03],\n        [-4.1025e-03, -8.9863e-03, -4.4660e-03, -3.4403e-03, -9.1967e-03, -2.4907e-03,\n         -3.5058e-03, -5.2283e-03, -9.0272e-03, -3.3000e-03,  ..., -5.9995e-03,\n         -6.7201e-03, -8.4131e-03, -6.6697e-03, -1.0063e+00,  9.9426e-01, -9.3569e-03,\n         -5.4411e-03, -9.9804e-03, -5.7301e-03],\n        [ 4.4559e-03, -7.4895e-03, -1.1826e-03, -1.5941e-03, -1.1202e-03, -1.9569e-03,\n         -2.9737e-03,  3.4425e-04, -1.8737e-03, -9.5964e-04,  ..., -2.2898e-03,\n          3.3558e-03, -5.3600e-03,  3.7345e-03, -3.2533e-03,  2.9654e-03,  9.9877e-01,\n         -9.9736e-01, -5.3093e-03,  4.1546e-03],\n        [-3.3310e-03, -2.7196e-03, -3.6567e-03, -4.9366e-04, -8.2401e-03,  1.1483e-04,\n         -1.5171e-03, -1.4526e-03, -1.5069e-03, -2.5889e-03,  ...,  2.3905e-03,\n         -4.2520e-03,  6.4167e-03, -4.2336e-03, -7.2077e-03, -3.9272e-03, -9.9850e-01,\n          9.9531e-01, -7.4309e-03, -4.0686e-03],\n        [ 1.5740e-03, -7.3517e-03, -2.2213e-03, -2.9728e-03, -3.8699e-03, -2.6687e-03,\n         -3.9391e-03, -1.9038e-03, -3.8302e-03, -2.4668e-03,  ..., -6.6849e-03,\n          2.8999e-03, -6.7946e-03,  3.3261e-03, -5.3829e-03,  2.4741e-03, -5.8500e-03,\n          3.2161e-03,  9.9647e-01, -9.9720e-01],\n        [-3.8475e-03, -8.6106e-03, -4.1971e-03, -3.0310e-03, -8.8297e-03, -2.5856e-03,\n         -3.1247e-03, -5.3512e-03, -7.9125e-03, -3.3638e-03,  ..., -6.0016e-03,\n         -6.7698e-03, -8.5566e-03, -6.6910e-03, -8.7221e-03, -5.5052e-03, -9.1551e-03,\n         -5.4467e-03, -1.0065e+00,  9.9421e-01]], device='cuda:0', requires_grad=True)\nParameter Name: layers.5.linear.bias\nParameter containing:\ntensor([ 0.0068,  0.0005,  0.0033,  0.0049, -0.0053,  0.0047,  0.0028,  0.0074, -0.0010,\n         0.0053,  ..., -0.0057, -0.0046, -0.0112, -0.0045, -0.0032, -0.0040, -0.0018,\n        -0.0023, -0.0030, -0.0038], device='cuda:0', requires_grad=True)\nParameter Name: last_layer.linear.weight\nParameter containing:\ntensor([[ 0.0100, -0.0141,  0.0056,  0.0061,  0.0003,  0.0033,  0.0037,  0.0071,  0.0029,\n          0.0045,  ...,  0.0055,  0.0051, -0.0044,  0.0056, -0.0021,  0.0060, -0.0007,\n          0.0050, -0.0032,  0.0066],\n        [-0.0096, -0.0014,  0.0024,  0.0033,  0.0011,  0.0025,  0.0016, -0.0005,  0.0006,\n          0.0031,  ..., -0.0034, -0.0014,  0.0048, -0.0018,  0.0016, -0.0020,  0.0015,\n         -0.0028,  0.0024, -0.0026],\n        [-0.0144,  0.0123, -0.0111, -0.0195,  0.0103, -0.0126,  0.0024, -0.0220,  0.0038,\n         -0.0180,  ..., -0.0057,  0.0039, -0.0094,  0.0049, -0.0115,  0.0063, -0.0114,\n          0.0043, -0.0115,  0.0064],\n        [-0.0110,  0.0091, -0.0205, -0.0134, -0.0155, -0.0056, -0.0128,  0.0036, -0.0079,\n          0.0006,  ..., -0.0127, -0.0069, -0.0116, -0.0069, -0.0121, -0.0070, -0.0121,\n         -0.0071, -0.0109, -0.0069],\n        [-0.0224, -0.0182,  0.0013, -0.0114, -0.0021, -0.0016, -0.0080, -0.0003,  0.0046,\n         -0.0084,  ..., -0.0108,  0.0037, -0.0085,  0.0035, -0.0073,  0.0025, -0.0065,\n          0.0010, -0.0059,  0.0027],\n        [-0.0146,  0.0137,  0.0006,  0.0044, -0.0073, -0.0146, -0.0205, -0.0023, -0.0104,\n         -0.0120,  ..., -0.0062,  0.0032, -0.0108,  0.0031, -0.0108,  0.0011, -0.0108,\n          0.0006, -0.0099,  0.0014],\n        [ 0.0142, -0.0171, -0.0054, -0.0076, -0.0036,  0.0007, -0.0021,  0.0065, -0.0014,\n         -0.0006,  ..., -0.0036,  0.0041, -0.0031,  0.0039,  0.0023,  0.0033,  0.0041,\n          0.0026,  0.0016,  0.0035]], device='cuda:0', requires_grad=True)\nParameter Name: last_layer.linear.bias\nParameter containing:\ntensor([ 0.0040,  0.0038, -0.0074, -0.0131, -0.0061, -0.0080, -0.0050], device='cuda:0',\n       requires_grad=True)\n","output_type":"stream"}]},{"cell_type":"code","source":"torch.cuda.synchronize()\nstart_time = time.time()\n\nevaluate_model(model, custom_train_loader, criterion, optimizer, 20, 1024 * 16)\n\nelapsed_time = time.time() - start_time\nprint(f\"Execution time: {elapsed_time:.6f} seconds\")","metadata":{"execution":{"iopub.status.busy":"2024-10-25T22:51:49.057522Z","iopub.execute_input":"2024-10-25T22:51:49.057978Z","iopub.status.idle":"2024-10-25T22:55:33.712971Z","shell.execute_reply.started":"2024-10-25T22:51:49.057935Z","shell.execute_reply":"2024-10-25T22:55:33.711976Z"},"jupyter":{"source_hidden":true,"outputs_hidden":true},"collapsed":true,"trusted":true},"execution_count":38,"outputs":[{"name":"stdout","text":"Epoch 1, Training Loss: 0.46187055407704974, Validation Loss: 0.45659633310192227\nTraining Accuracy: 0.8091452618172196, Training F1 Score: 0.8031132165885962\nValidation Accuracy: 0.8077416245707942, Validation F1 Score: 0.8019075643506941\n\nEpoch 2, Training Loss: 0.4436647745006696, Validation Loss: 0.43283464763916135\nTraining Accuracy: 0.8202681101269553, Training F1 Score: 0.8187171411119616\nValidation Accuracy: 0.8173369018011584, Validation F1 Score: 0.8159239445570963\n\nEpoch 3, Training Loss: 0.4340482068153286, Validation Loss: 0.42634240369568477\nTraining Accuracy: 0.8242869651835485, Training F1 Score: 0.8208560143068604\nValidation Accuracy: 0.8210373226164557, Validation F1 Score: 0.8177703077557913\n\nEpoch 4, Training Loss: 0.4044414708486058, Validation Loss: 0.41213445558972317\nTraining Accuracy: 0.8296741242101594, Training F1 Score: 0.8255909437985843\nValidation Accuracy: 0.8271387141467949, Validation F1 Score: 0.8231474386760982\n\nEpoch 5, Training Loss: 0.39152673554778145, Validation Loss: 0.38753773548625153\nTraining Accuracy: 0.8429505452777377, Training F1 Score: 0.8401836739787552\nValidation Accuracy: 0.8396512998803818, Validation F1 Score: 0.836931792806343\n\nEpoch 6, Training Loss: 0.37562058007636934, Validation Loss: 0.3712424588530375\nTraining Accuracy: 0.8500846584295915, Training F1 Score: 0.8483867253674369\nValidation Accuracy: 0.8460797053432355, Validation F1 Score: 0.844448424985027\n\nEpoch 7, Training Loss: 0.366577626670493, Validation Loss: 0.35185768131393064\nTraining Accuracy: 0.8605620803383756, Training F1 Score: 0.8588777320921036\nValidation Accuracy: 0.8565527568135074, Validation F1 Score: 0.8548941476225438\n\nEpoch 8, Training Loss: 0.34313461069013473, Validation Loss: 0.35519015320952685\nTraining Accuracy: 0.8566464935059347, Training F1 Score: 0.8547400501429251\nValidation Accuracy: 0.8537387158679208, Validation F1 Score: 0.8518925380250906\n\nEpoch 9, Training Loss: 0.3380954175318533, Validation Loss: 0.3467813337436421\nTraining Accuracy: 0.8599790451561824, Training F1 Score: 0.8568012382193109\nValidation Accuracy: 0.8569486157844461, Validation F1 Score: 0.8536772088630276\n\nEpoch 10, Training Loss: 0.3188023780204604, Validation Loss: 0.3342499435966145\nTraining Accuracy: 0.865206999003892, Training F1 Score: 0.8623786645411583\nValidation Accuracy: 0.8621550218152716, Validation F1 Score: 0.8592561605754859\n\nEpoch 11, Training Loss: 0.33093679143852717, Validation Loss: 0.3128697756748811\nTraining Accuracy: 0.8765019610205482, Training F1 Score: 0.8763303137691696\nValidation Accuracy: 0.8726022563961343, Validation F1 Score: 0.8724346497952808\n\nEpoch 12, Training Loss: 0.2953630841573611, Validation Loss: 0.330736304460388\nTraining Accuracy: 0.8676381051141436, Training F1 Score: 0.865206807790262\nValidation Accuracy: 0.8651325697271155, Validation F1 Score: 0.8626750746897304\n\nEpoch 13, Training Loss: 0.29809110373360986, Validation Loss: 0.3288269168363529\nTraining Accuracy: 0.8686342131929459, Training F1 Score: 0.8652027548630241\nValidation Accuracy: 0.8653046823231758, Validation F1 Score: 0.8618180737730864\n\nEpoch 14, Training Loss: 0.284415074868246, Validation Loss: 0.3070021766483218\nTraining Accuracy: 0.8781865239270324, Training F1 Score: 0.8754228900599448\nValidation Accuracy: 0.8745815512508283, Validation F1 Score: 0.8717795482981242\n\nEpoch 15, Training Loss: 0.2717549630688359, Validation Loss: 0.28027315948459497\nTraining Accuracy: 0.8907830958522748, Training F1 Score: 0.8890861019880868\nValidation Accuracy: 0.8863196303021437, Validation F1 Score: 0.8845459608831943\n\nEpoch 16, Training Loss: 0.26162344899935425, Validation Loss: 0.2805904627267434\nTraining Accuracy: 0.8902430890968118, Training F1 Score: 0.8883318992042271\nValidation Accuracy: 0.8853471941344028, Validation F1 Score: 0.8833190547805939\n\nEpoch 17, Training Loss: 0.2583410183148477, Validation Loss: 0.27734912906310794\nTraining Accuracy: 0.8916931470776168, Training F1 Score: 0.890120311634265\nValidation Accuracy: 0.8868273624605216, Validation F1 Score: 0.8851657243711905\n\nEpoch 18, Training Loss: 0.25092540111695505, Validation Loss: 0.252815516240691\nTraining Accuracy: 0.9040638197625261, Training F1 Score: 0.9031836765928019\nValidation Accuracy: 0.8985826527714431, Validation F1 Score: 0.8976300684113904\n\nEpoch 19, Training Loss: 0.2535389331380042, Validation Loss: 0.2654120029962977\nTraining Accuracy: 0.8972717826031767, Training F1 Score: 0.8956709664270185\nValidation Accuracy: 0.8920767966403621, Validation F1 Score: 0.8903280084083862\n\nEpoch 20, Training Loss: 0.23696291164790645, Validation Loss: 0.2635610618494405\nTraining Accuracy: 0.8976181614383543, Training F1 Score: 0.8960663162192549\nValidation Accuracy: 0.8925328950199221, Validation F1 Score: 0.89080514559692\n\nExecution time: 224.649432 seconds\n","output_type":"stream"}]},{"cell_type":"code","source":"optimizer = optim.Adam(model.parameters(), lr=0.0005)\n\ntorch.cuda.synchronize()\nstart_time = time.time()\n\nevaluate_model(model, custom_train_loader, criterion, optimizer, 10, 1024 * 16)\n\nelapsed_time = time.time() - start_time\nprint(f\"Execution time: {elapsed_time:.6f} seconds\")","metadata":{"execution":{"iopub.status.busy":"2024-10-25T22:55:38.867414Z","iopub.execute_input":"2024-10-25T22:55:38.867771Z","iopub.status.idle":"2024-10-25T22:57:31.244064Z","shell.execute_reply.started":"2024-10-25T22:55:38.867738Z","shell.execute_reply":"2024-10-25T22:57:31.243144Z"},"jupyter":{"source_hidden":true,"outputs_hidden":true},"collapsed":true,"trusted":true},"execution_count":39,"outputs":[{"name":"stdout","text":"Epoch 1, Training Loss: 0.4832981421382457, Validation Loss: 0.28052039835777887\nTraining Accuracy: 0.8896428425439267, Training F1 Score: 0.8880123027005108\nValidation Accuracy: 0.8851406590191303, Validation F1 Score: 0.8834208061592084\n\nEpoch 2, Training Loss: 0.24218053046919955, Validation Loss: 0.2405916653315699\nTraining Accuracy: 0.9111936300717068, Training F1 Score: 0.9106527102036942\nValidation Accuracy: 0.9046496217825701, Validation F1 Score: 0.9040364159201201\n\nEpoch 3, Training Loss: 0.22201471403067238, Validation Loss: 0.23294533299483985\nTraining Accuracy: 0.9152576649763666, Training F1 Score: 0.914926472659158\nValidation Accuracy: 0.9086770565303822, Validation F1 Score: 0.9082882072967442\n\nEpoch 4, Training Loss: 0.21588015815661285, Validation Loss: 0.22925650259786215\nTraining Accuracy: 0.9167980826533049, Training F1 Score: 0.9165233187522076\nValidation Accuracy: 0.9102690980439403, Validation F1 Score: 0.9099442322053461\n\nEpoch 5, Training Loss: 0.21227046516738346, Validation Loss: 0.22658846008433234\nTraining Accuracy: 0.9182266264207448, Training F1 Score: 0.9178822668569483\nValidation Accuracy: 0.9113792242885296, Validation F1 Score: 0.9109839309213866\n\nEpoch 6, Training Loss: 0.2093392990490121, Validation Loss: 0.22416670851185475\nTraining Accuracy: 0.9193862425211216, Training F1 Score: 0.9191307703272702\nValidation Accuracy: 0.9123860829754825, Validation F1 Score: 0.9120742293495582\n\nEpoch 7, Training Loss: 0.2064722035626723, Validation Loss: 0.22128235069813193\nTraining Accuracy: 0.9206104012615934, Training F1 Score: 0.9204261649085484\nValidation Accuracy: 0.9137285612247532, Validation F1 Score: 0.9135025563401304\n\nEpoch 8, Training Loss: 0.20371461937334878, Validation Loss: 0.21901740032637124\nTraining Accuracy: 0.9215591780709926, Training F1 Score: 0.9213878003019266\nValidation Accuracy: 0.9144342228686007, Validation F1 Score: 0.9142376396519132\n\nEpoch 9, Training Loss: 0.20089769035875857, Validation Loss: 0.21670714716795372\nTraining Accuracy: 0.9225940117338519, Training F1 Score: 0.922444426913862\nValidation Accuracy: 0.9152775745892964, Validation F1 Score: 0.9151085195104041\n\nEpoch 10, Training Loss: 0.1981835825907416, Validation Loss: 0.21498435305490682\nTraining Accuracy: 0.923682630930124, Training F1 Score: 0.9235806762003408\nValidation Accuracy: 0.916155348829204, Validation F1 Score: 0.9160482231251197\n\nExecution time: 112.369303 seconds\n","output_type":"stream"}]},{"cell_type":"code","source":"evaluate_model(model, custom_train_loader, criterion, optimizer, 20, 1024 * 16)","metadata":{"execution":{"iopub.status.busy":"2024-10-25T22:58:02.022481Z","iopub.execute_input":"2024-10-25T22:58:02.023360Z","iopub.status.idle":"2024-10-25T23:01:47.349356Z","shell.execute_reply.started":"2024-10-25T22:58:02.023319Z","shell.execute_reply":"2024-10-25T23:01:47.348303Z"},"collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":40,"outputs":[{"name":"stdout","text":"Epoch 1, Training Loss: 0.19556368032406754, Validation Loss: 0.21258746848952584\nTraining Accuracy: 0.9249562723613355, Training F1 Score: 0.9248662991308148\nValidation Accuracy: 0.9169987005498997, Validation F1 Score: 0.9168945693979441\n\nEpoch 2, Training Loss: 0.19315933476198208, Validation Loss: 0.20994883616611287\nTraining Accuracy: 0.9261180398830487, Training F1 Score: 0.9260122439050491\nValidation Accuracy: 0.9180657986454739, Validation F1 Score: 0.9179444012113159\n\nEpoch 3, Training Loss: 0.1908943934996064, Validation Loss: 0.20769453433510274\nTraining Accuracy: 0.9269484885189401, Training F1 Score: 0.9267984641835668\nValidation Accuracy: 0.9187198265105032, Validation F1 Score: 0.9185286275904471\n\nEpoch 4, Training Loss: 0.18837908832293598, Validation Loss: 0.20535351621438136\nTraining Accuracy: 0.9280371077152121, Training F1 Score: 0.92787846666878\nValidation Accuracy: 0.9194254881543505, Validation F1 Score: 0.9192230292636936\n\nEpoch 5, Training Loss: 0.18504032078930036, Validation Loss: 0.2030258539696995\nTraining Accuracy: 0.9293279605171156, Training F1 Score: 0.9292593973790586\nValidation Accuracy: 0.9203032623942583, Validation F1 Score: 0.9202115532367595\n\nEpoch 6, Training Loss: 0.18245983362901758, Validation Loss: 0.20137340345449925\nTraining Accuracy: 0.930354188494629, Training F1 Score: 0.9303601929941316\nValidation Accuracy: 0.9210347409275148, Validation F1 Score: 0.9210267070048561\n\nEpoch 7, Training Loss: 0.1800496442607391, Validation Loss: 0.19862662893278726\nTraining Accuracy: 0.9315439244937168, Training F1 Score: 0.9315784321093821\nValidation Accuracy: 0.9217576138309682, Validation F1 Score: 0.9217867016050397\n\nEpoch 8, Training Loss: 0.17698334941173285, Validation Loss: 0.19622362284887343\nTraining Accuracy: 0.9322366821640717, Training F1 Score: 0.9322120964547044\nValidation Accuracy: 0.9224718811046186, Validation F1 Score: 0.9224361623231851\n\nEpoch 9, Training Loss: 0.173378191478895, Validation Loss: 0.19361362322973485\nTraining Accuracy: 0.9333166956749976, Training F1 Score: 0.933242518630526\nValidation Accuracy: 0.92361642986842, Validation F1 Score: 0.9235101288698335\n\nEpoch 10, Training Loss: 0.17010397520609286, Validation Loss: 0.19094507399709124\nTraining Accuracy: 0.9344763117753744, Training F1 Score: 0.9343759817499281\nValidation Accuracy: 0.9247523730024182, Validation F1 Score: 0.9246168172874486\n\nEpoch 11, Training Loss: 0.1676683409278332, Validation Loss: 0.19001152793822512\nTraining Accuracy: 0.9345903371062092, Training F1 Score: 0.9343977030231335\nValidation Accuracy: 0.9247609786322212, Validation F1 Score: 0.9245205794842651\n\nEpoch 12, Training Loss: 0.16546787644577932, Validation Loss: 0.19291580038031264\nTraining Accuracy: 0.9324066444496557, Training F1 Score: 0.9321103646333114\nValidation Accuracy: 0.9230914864504359, Validation F1 Score: 0.9227309014713566\n\nEpoch 13, Training Loss: 0.1630144901559472, Validation Loss: 0.19218509840576772\nTraining Accuracy: 0.9325744553139032, Training F1 Score: 0.93224688893293\nValidation Accuracy: 0.9232291765272842, Validation F1 Score: 0.922833428912658\n\nEpoch 14, Training Loss: 0.16037089155213774, Validation Loss: 0.19213393974551743\nTraining Accuracy: 0.9324776413537603, Training F1 Score: 0.9321036551066099\nValidation Accuracy: 0.9229882188927997, Validation F1 Score: 0.9225334222385432\n\nEpoch 15, Training Loss: 0.15874445215846503, Validation Loss: 0.18949969589056756\nTraining Accuracy: 0.9339061851212003, Training F1 Score: 0.9335509463613629\nValidation Accuracy: 0.9240725282479798, Validation F1 Score: 0.9236387299277691\n\nEpoch 16, Training Loss: 0.1566956314341191, Validation Loss: 0.18789457062582354\nTraining Accuracy: 0.9347280280717456, Training F1 Score: 0.9343983524001782\nValidation Accuracy: 0.9247437673726152, Validation F1 Score: 0.924334825462654\n\nEpoch 17, Training Loss: 0.15507912016051306, Validation Loss: 0.18704341319606876\nTraining Accuracy: 0.9351841293950849, Training F1 Score: 0.9348662634219643\nValidation Accuracy: 0.9250965981945388, Validation F1 Score: 0.9246950448561164\n\nEpoch 18, Training Loss: 0.15430030533666075, Validation Loss: 0.19400894722753326\nTraining Accuracy: 0.9315891043417834, Training F1 Score: 0.9311528273756138\nValidation Accuracy: 0.92216207843171, Validation F1 Score: 0.92162394022197\n\nEpoch 19, Training Loss: 0.1561922878453022, Validation Loss: 0.19516309984595454\nTraining Accuracy: 0.9312104541865583, Training F1 Score: 0.9307472357341794\nValidation Accuracy: 0.9216629519031351, Validation F1 Score: 0.9210823967060118\n\nEpoch 20, Training Loss: 0.15751662939124772, Validation Loss: 0.1875815324471645\nTraining Accuracy: 0.9358274043747001, Training F1 Score: 0.9358270804409144\nValidation Accuracy: 0.9254924571654777, Validation F1 Score: 0.9254465291163604\n\n","output_type":"stream"}]},{"cell_type":"code","source":"for param_group in optimizer.param_groups:\n    param_group['lr'] = 0.0002\n\ntorch.cuda.synchronize()\nstart_time = time.time()\n\nevaluate_model(model, custom_train_loader, criterion, optimizer, 10, 1024 * 16)\n\nelapsed_time = time.time() - start_time\nprint(f\"Execution time: {elapsed_time:.6f} seconds\")","metadata":{"execution":{"iopub.status.busy":"2024-10-25T23:02:15.156831Z","iopub.execute_input":"2024-10-25T23:02:15.157806Z","iopub.status.idle":"2024-10-25T23:04:07.650823Z","shell.execute_reply.started":"2024-10-25T23:02:15.157751Z","shell.execute_reply":"2024-10-25T23:04:07.649805Z"},"jupyter":{"source_hidden":true,"outputs_hidden":true},"collapsed":true,"trusted":true},"execution_count":41,"outputs":[{"name":"stdout","text":"Epoch 1, Training Loss: 0.14379763308671362, Validation Loss: 0.16577850440881653\nTraining Accuracy: 0.9467265048654393, Training F1 Score: 0.9466305292780136\nValidation Accuracy: 0.9354577764773715, Validation F1 Score: 0.9352948471862124\n\nEpoch 2, Training Loss: 0.13807196640882904, Validation Loss: 0.16360953252927946\nTraining Accuracy: 0.9480066005606604, Training F1 Score: 0.9479556433405829\nValidation Accuracy: 0.9364388182749155, Validation F1 Score: 0.9363334707795365\n\nEpoch 3, Training Loss: 0.13655829844459322, Validation Loss: 0.1626027895133869\nTraining Accuracy: 0.9486670869109677, Training F1 Score: 0.9486290947130912\nValidation Accuracy: 0.9370153954717176, Validation F1 Score: 0.9369339159095087\n\nEpoch 4, Training Loss: 0.13536562498523103, Validation Loss: 0.1618144296230665\nTraining Accuracy: 0.9490930683355959, Training F1 Score: 0.9490589821208834\nValidation Accuracy: 0.9374542825916715, Validation F1 Score: 0.9373767362009776\n\nEpoch 5, Training Loss: 0.13428953160366017, Validation Loss: 0.16095492699240693\nTraining Accuracy: 0.9495212011815606, Training F1 Score: 0.9494961617445289\nValidation Accuracy: 0.9377554796347771, Validation F1 Score: 0.9376915159022683\n\nEpoch 6, Training Loss: 0.13326321683516765, Validation Loss: 0.16029658169727645\nTraining Accuracy: 0.949871882859411, Training F1 Score: 0.9498527555886175\nValidation Accuracy: 0.9380566766778826, Validation F1 Score: 0.9379993594897507\n\nEpoch 7, Training Loss: 0.1323338833369611, Validation Loss: 0.15975184601810472\nTraining Accuracy: 0.950233321643944, Training F1 Score: 0.9502199772604866\nValidation Accuracy: 0.9380824935672917, Validation F1 Score: 0.9380338322781507\n\nEpoch 8, Training Loss: 0.131485847382787, Validation Loss: 0.15913658232563974\nTraining Accuracy: 0.9505517320017469, Training F1 Score: 0.9505445475349636\nValidation Accuracy: 0.9384439300190185, Validation F1 Score: 0.9384070721079516\n\nEpoch 9, Training Loss: 0.13070145493775695, Validation Loss: 0.1585574459381713\nTraining Accuracy: 0.9507797826634166, Training F1 Score: 0.9507791712976903\nValidation Accuracy: 0.9387881552111391, Validation F1 Score: 0.9387603288224861\n\nEpoch 10, Training Loss: 0.12999197148511737, Validation Loss: 0.15803070180853826\nTraining Accuracy: 0.9510228932744418, Training F1 Score: 0.9510303781677086\nValidation Accuracy: 0.9388914227687754, Validation F1 Score: 0.9388711598528336\n\nExecution time: 112.486648 seconds\n","output_type":"stream"}]},{"cell_type":"code","source":"for param_group in optimizer.param_groups:\n    param_group['lr'] = 0.0001\n\ntorch.cuda.synchronize()\nstart_time = time.time()\n\nevaluate_model(model, custom_train_loader, criterion, optimizer, 5, 1024 * 16)\n\nelapsed_time = time.time() - start_time\nprint(f\"Execution time: {elapsed_time:.6f} seconds\")","metadata":{"execution":{"iopub.status.busy":"2024-10-25T23:04:31.986060Z","iopub.execute_input":"2024-10-25T23:04:31.986948Z","iopub.status.idle":"2024-10-25T23:05:28.147765Z","shell.execute_reply.started":"2024-10-25T23:04:31.986908Z","shell.execute_reply":"2024-10-25T23:05:28.146853Z"},"jupyter":{"source_hidden":true,"outputs_hidden":true},"collapsed":true,"trusted":true},"execution_count":42,"outputs":[{"name":"stdout","text":"Epoch 1, Training Loss: 0.12786425165094453, Validation Loss: 0.15829317533588666\nTraining Accuracy: 0.9507819340847531, Training F1 Score: 0.9506746247958024\nValidation Accuracy: 0.938693493283306, Validation F1 Score: 0.9385165565295784\n\nEpoch 2, Training Loss: 0.1276408696758141, Validation Loss: 0.1581428434846313\nTraining Accuracy: 0.9508550824101943, Training F1 Score: 0.9507536351713198\nValidation Accuracy: 0.9387451270621241, Validation F1 Score: 0.9385709520551978\n\nEpoch 3, Training Loss: 0.12732115726626095, Validation Loss: 0.1573824483101115\nTraining Accuracy: 0.9513305465255621, Training F1 Score: 0.9512293792093213\nValidation Accuracy: 0.9389430565475935, Validation F1 Score: 0.9387658279429677\n\nEpoch 4, Training Loss: 0.12688898075054048, Validation Loss: 0.15602797028228543\nTraining Accuracy: 0.9520469698306187, Training F1 Score: 0.9519512816516009\nValidation Accuracy: 0.9395540562636077, Validation F1 Score: 0.9393879345355352\n\nEpoch 5, Training Loss: 0.12625972611936717, Validation Loss: 0.15467938702536296\nTraining Accuracy: 0.9527676959783481, Training F1 Score: 0.9526802708356195\nValidation Accuracy: 0.9399499152345464, Validation F1 Score: 0.9397924157667802\n\nExecution time: 56.155166 seconds\n","output_type":"stream"}]},{"cell_type":"code","source":"for param_group in optimizer.param_groups:\n    param_group['lr'] = 0.0005\n\ntorch.cuda.synchronize()\nstart_time = time.time()\n\nevaluate_model(model, custom_train_loader, criterion, optimizer, 5, 1024 * 16)\n\nelapsed_time = time.time() - start_time\nprint(f\"Execution time: {elapsed_time:.6f} seconds\")","metadata":{"execution":{"iopub.status.busy":"2024-10-25T23:06:44.830243Z","iopub.execute_input":"2024-10-25T23:06:44.830628Z","iopub.status.idle":"2024-10-25T23:07:40.931041Z","shell.execute_reply.started":"2024-10-25T23:06:44.830590Z","shell.execute_reply":"2024-10-25T23:07:40.930060Z"},"jupyter":{"source_hidden":true,"outputs_hidden":true},"collapsed":true,"trusted":true},"execution_count":43,"outputs":[{"name":"stdout","text":"Epoch 1, Training Loss: 0.5471089795353995, Validation Loss: 0.29028088985177347\nTraining Accuracy: 0.8848925042329214, Training F1 Score: 0.8839382038144373\nValidation Accuracy: 0.8807173653003795, Validation F1 Score: 0.8797414446247134\n\nEpoch 2, Training Loss: 0.22933137161242187, Validation Loss: 0.20954105018485564\nTraining Accuracy: 0.9252832884044844, Training F1 Score: 0.9251237091191479\nValidation Accuracy: 0.918151854943504, Validation F1 Score: 0.917944418889338\n\nEpoch 3, Training Loss: 0.17783364648579775, Validation Loss: 0.18467929444810044\nTraining Accuracy: 0.9369891718964134, Training F1 Score: 0.9368706531628563\nValidation Accuracy: 0.9280999629957919, Validation F1 Score: 0.9279164487819294\n\nEpoch 4, Training Loss: 0.15746817645092853, Validation Loss: 0.17289367829075616\nTraining Accuracy: 0.9430066973746205, Training F1 Score: 0.9429727279587908\nValidation Accuracy: 0.933237523988193, Validation F1 Score: 0.9331618370836294\n\nEpoch 5, Training Loss: 0.14754376587778018, Validation Loss: 0.16616644381478635\nTraining Accuracy: 0.9463198862328397, Training F1 Score: 0.9462788686159215\nValidation Accuracy: 0.9351910019534779, Validation F1 Score: 0.9351095348331793\n\nExecution time: 56.094677 seconds\n","output_type":"stream"}]},{"cell_type":"code","source":"evaluate_model(model, custom_train_loader, criterion, optimizer, 10, 1024 * 16)","metadata":{"execution":{"iopub.status.busy":"2024-10-25T23:07:52.894711Z","iopub.execute_input":"2024-10-25T23:07:52.895336Z","iopub.status.idle":"2024-10-25T23:09:45.391471Z","shell.execute_reply.started":"2024-10-25T23:07:52.895293Z","shell.execute_reply":"2024-10-25T23:09:45.390504Z"},"collapsed":true,"jupyter":{"outputs_hidden":true,"source_hidden":true},"trusted":true},"execution_count":44,"outputs":[{"name":"stdout","text":"Epoch 1, Training Loss: 0.1420469445767831, Validation Loss: 0.16379475793291157\nTraining Accuracy: 0.9470664294366073, Training F1 Score: 0.9469446100653659\nValidation Accuracy: 0.9356815228522499, Validation F1 Score: 0.9354948313063487\n\nEpoch 2, Training Loss: 0.1374254807310955, Validation Loss: 0.16244263718689622\nTraining Accuracy: 0.9478237297470574, Training F1 Score: 0.947676116671323\nValidation Accuracy: 0.9362753113086582, Validation F1 Score: 0.9360504329009303\n\nEpoch 3, Training Loss: 0.13343840026647394, Validation Loss: 0.16008564715595616\nTraining Accuracy: 0.949467415648148, Training F1 Score: 0.9493709220407435\nValidation Accuracy: 0.9375231276300956, Validation F1 Score: 0.9373637995839212\n\nEpoch 4, Training Loss: 0.1308913204877495, Validation Loss: 0.15804413020032218\nTraining Accuracy: 0.9506894229672833, Training F1 Score: 0.9506387203690648\nValidation Accuracy: 0.9384869581680335, Validation F1 Score: 0.9383802786884938\n\nEpoch 5, Training Loss: 0.12862007784104787, Validation Loss: 0.15582451692175564\nTraining Accuracy: 0.9518640990170156, Training F1 Score: 0.9518309820166156\nValidation Accuracy: 0.939726168859668, Validation F1 Score: 0.9396524447179109\n\nEpoch 6, Training Loss: 0.12652397845499327, Validation Loss: 0.15459263165766013\nTraining Accuracy: 0.9525912794287547, Training F1 Score: 0.9525937988789266\nValidation Accuracy: 0.9398294364173042, Validation F1 Score: 0.939802139616695\n\nEpoch 7, Training Loss: 0.1250565920941852, Validation Loss: 0.15357806593941156\nTraining Accuracy: 0.9533378226325222, Training F1 Score: 0.9533672684216418\nValidation Accuracy: 0.9403801967246973, Validation F1 Score: 0.9403987431605322\n\nEpoch 8, Training Loss: 0.12368700374692636, Validation Loss: 0.15205335428048694\nTraining Accuracy: 0.9541037286283183, Training F1 Score: 0.9541305506433567\nValidation Accuracy: 0.9407588444360301, Validation F1 Score: 0.9407721738479233\n\nEpoch 9, Training Loss: 0.12188333816381418, Validation Loss: 0.15043680009209018\nTraining Accuracy: 0.9547857291919907, Training F1 Score: 0.9547889125725695\nValidation Accuracy: 0.9416710411951499, Validation F1 Score: 0.9416558829317142\n\nEpoch 10, Training Loss: 0.1197417590047484, Validation Loss: 0.14963988522050115\nTraining Accuracy: 0.9551837421392443, Training F1 Score: 0.9551647418689867\nValidation Accuracy: 0.9418345481614072, Validation F1 Score: 0.9417876223826777\n\n","output_type":"stream"}]},{"cell_type":"code","source":"evaluate_model(model, custom_train_loader, criterion, optimizer, 10, 1024 * 16)","metadata":{"execution":{"iopub.status.busy":"2024-10-25T23:10:12.108814Z","iopub.execute_input":"2024-10-25T23:10:12.109229Z","iopub.status.idle":"2024-10-25T23:12:04.443343Z","shell.execute_reply.started":"2024-10-25T23:10:12.109188Z","shell.execute_reply":"2024-10-25T23:12:04.442310Z"},"collapsed":true,"jupyter":{"outputs_hidden":true,"source_hidden":true},"trusted":true},"execution_count":45,"outputs":[{"name":"stdout","text":"Epoch 1, Training Loss: 0.11787714887081889, Validation Loss: 0.1487572648437567\nTraining Accuracy: 0.9555537866091233, Training F1 Score: 0.9555322846812451\nValidation Accuracy: 0.9419722382382555, Validation F1 Score: 0.9419201109443353\n\nEpoch 2, Training Loss: 0.116244681601608, Validation Loss: 0.14796671430039227\nTraining Accuracy: 0.956007736511126, Training F1 Score: 0.9559881340958213\nValidation Accuracy: 0.942247618391952, Validation F1 Score: 0.9421971464043155\n\nEpoch 3, Training Loss: 0.11478185112258762, Validation Loss: 0.1473187245781526\nTraining Accuracy: 0.9563863866663511, Training F1 Score: 0.956367202321877\nValidation Accuracy: 0.9426778998821028, Validation F1 Score: 0.9426240585922683\n\nEpoch 4, Training Loss: 0.11352190938833032, Validation Loss: 0.14653194336188227\nTraining Accuracy: 0.9568381851470174, Training F1 Score: 0.9568183377553612\nValidation Accuracy: 0.9429446744059964, Validation F1 Score: 0.9428883096294064\n\nEpoch 5, Training Loss: 0.11249595320158029, Validation Loss: 0.14563029574866562\nTraining Accuracy: 0.9573416177397598, Training F1 Score: 0.9573073759864245\nValidation Accuracy: 0.9435384628624046, Validation F1 Score: 0.9434649476077069\n\nEpoch 6, Training Loss: 0.11127574699990966, Validation Loss: 0.14609728876940556\nTraining Accuracy: 0.9570812957580426, Training F1 Score: 0.9570203162524124\nValidation Accuracy: 0.9435212516027985, Validation F1 Score: 0.9434084559369483\n\nEpoch 7, Training Loss: 0.11018193231800028, Validation Loss: 0.14601690946332704\nTraining Accuracy: 0.9569995417472553, Training F1 Score: 0.956928729066544\nValidation Accuracy: 0.9434954347133895, Validation F1 Score: 0.943375636581332\n\nEpoch 8, Training Loss: 0.10964092945535714, Validation Loss: 0.14612292270484956\nTraining Accuracy: 0.9570016931685918, Training F1 Score: 0.9569183592110605\nValidation Accuracy: 0.9431512095212689, Validation F1 Score: 0.9430067080580187\n\nEpoch 9, Training Loss: 0.10924681656861521, Validation Loss: 0.14615894149705716\nTraining Accuracy: 0.9570339644886394, Training F1 Score: 0.9569564123551834\nValidation Accuracy: 0.943237265819299, Validation F1 Score: 0.9430964189512716\n\nEpoch 10, Training Loss: 0.11085141812497065, Validation Loss: 0.1465094519919935\nTraining Accuracy: 0.9570189045392838, Training F1 Score: 0.9569579101903505\nValidation Accuracy: 0.9426778998821028, Validation F1 Score: 0.9425607404671952\n\n","output_type":"stream"}]},{"cell_type":"code","source":"for param_group in optimizer.param_groups:\n    param_group['lr'] = 0.0002\n\ntorch.cuda.synchronize()\nstart_time = time.time()\n\nevaluate_model(model, custom_train_loader, criterion, optimizer, 20, 1024 * 16)\n\nelapsed_time = time.time() - start_time\nprint(f\"Execution time: {elapsed_time:.6f} seconds\")","metadata":{"execution":{"iopub.status.busy":"2024-10-25T23:12:53.040970Z","iopub.execute_input":"2024-10-25T23:12:53.041788Z","iopub.status.idle":"2024-10-25T23:16:38.441587Z","shell.execute_reply.started":"2024-10-25T23:12:53.041745Z","shell.execute_reply":"2024-10-25T23:16:38.440623Z"},"collapsed":true,"jupyter":{"outputs_hidden":true,"source_hidden":true},"trusted":true},"execution_count":46,"outputs":[{"name":"stdout","text":"Epoch 1, Training Loss: 0.10438274481478599, Validation Loss: 0.13789183967236784\nTraining Accuracy: 0.9617283658448954, Training F1 Score: 0.9617088318731036\nValidation Accuracy: 0.9468172078173541, Validation F1 Score: 0.9467564429382519\n\nEpoch 2, Training Loss: 0.10207500785411677, Validation Loss: 0.13706161828342908\nTraining Accuracy: 0.9620575333093808, Training F1 Score: 0.9620447250449953\nValidation Accuracy: 0.9470065316730205, Validation F1 Score: 0.9469585256873705\n\nEpoch 3, Training Loss: 0.1012514452895042, Validation Loss: 0.13660896747498288\nTraining Accuracy: 0.9623544294538187, Training F1 Score: 0.9623528044203848\nValidation Accuracy: 0.9472044611584899, Validation F1 Score: 0.9471772112805764\n\nEpoch 4, Training Loss: 0.10063510835852954, Validation Loss: 0.13628075455353952\nTraining Accuracy: 0.9624985746833645, Training F1 Score: 0.9624993464718562\nValidation Accuracy: 0.9475745032400196, Validation F1 Score: 0.9475515934829855\n\nEpoch 5, Training Loss: 0.10012291269752341, Validation Loss: 0.13607339028430274\nTraining Accuracy: 0.9625996914861803, Training F1 Score: 0.9626019742510821\nValidation Accuracy: 0.9473937850141563, Validation F1 Score: 0.9473725303074159\n\nEpoch 6, Training Loss: 0.09961575971037137, Validation Loss: 0.13586453858070163\nTraining Accuracy: 0.9627739566144373, Training F1 Score: 0.9627768527740477\nValidation Accuracy: 0.9475142638313985, Validation F1 Score: 0.9474929904296315\n\nEpoch 7, Training Loss: 0.09915838071798888, Validation Loss: 0.13573072335858\nTraining Accuracy: 0.9628643163105706, Training F1 Score: 0.9628679096418803\nValidation Accuracy: 0.9476089257592317, Validation F1 Score: 0.9475877491652738\n\nEpoch 8, Training Loss: 0.09873765578196216, Validation Loss: 0.13561648181582187\nTraining Accuracy: 0.9629137990013102, Training F1 Score: 0.9629186653064975\nValidation Accuracy: 0.9474884469419894, Validation F1 Score: 0.9474687000607568\n\nEpoch 9, Training Loss: 0.09838622844424652, Validation Loss: 0.1354250462558365\nTraining Accuracy: 0.9631289411349608, Training F1 Score: 0.9631351613013812\nValidation Accuracy: 0.9476175313890347, Validation F1 Score: 0.947603121715293\n\nEpoch 10, Training Loss: 0.098027415191714, Validation Loss: 0.13540965015111753\nTraining Accuracy: 0.9631612124550084, Training F1 Score: 0.9631693754720672\nValidation Accuracy: 0.9473163343459291, Validation F1 Score: 0.9473050388991547\n\nEpoch 11, Training Loss: 0.09770212115598889, Validation Loss: 0.13541070952874687\nTraining Accuracy: 0.9631052755002593, Training F1 Score: 0.9631152105673044\nValidation Accuracy: 0.94729051745652, Validation F1 Score: 0.9472798898885691\n\nEpoch 12, Training Loss: 0.09741534821164344, Validation Loss: 0.1350595741718001\nTraining Accuracy: 0.9632859948925258, Training F1 Score: 0.9632974723533099\nValidation Accuracy: 0.9475142638313985, Validation F1 Score: 0.9475059651693772\n\nEpoch 13, Training Loss: 0.09708599525710057, Validation Loss: 0.1346941678820012\nTraining Accuracy: 0.9634796228128113, Training F1 Score: 0.9634893688380233\nValidation Accuracy: 0.9476949820572619, Validation F1 Score: 0.9476835517888936\n\nEpoch 14, Training Loss: 0.09678951357976272, Validation Loss: 0.13456267183019738\nTraining Accuracy: 0.9636108595143381, Training F1 Score: 0.9636189463602901\nValidation Accuracy: 0.9476261370188377, Validation F1 Score: 0.9476094188930781\n\nEpoch 15, Training Loss: 0.09654713634556139, Validation Loss: 0.1342446699065403\nTraining Accuracy: 0.9638001845919507, Training F1 Score: 0.9638115457240827\nValidation Accuracy: 0.9477294045764739, Validation F1 Score: 0.947718392956303\n\nEpoch 16, Training Loss: 0.0963988327514729, Validation Loss: 0.13361430828261384\nTraining Accuracy: 0.9642025003818773, Training F1 Score: 0.9642147300415542\nValidation Accuracy: 0.9482973761434731, Validation F1 Score: 0.9482915522491588\n\nEpoch 17, Training Loss: 0.09625216284230592, Validation Loss: 0.1331060687567715\nTraining Accuracy: 0.9644778823129501, Training F1 Score: 0.9644856710518533\nValidation Accuracy: 0.9485899675567756, Validation F1 Score: 0.9485782118965312\n\nEpoch 18, Training Loss: 0.09597418854749165, Validation Loss: 0.13316596634735042\nTraining Accuracy: 0.9643595541394422, Training F1 Score: 0.9643681677371007\nValidation Accuracy: 0.9488223195614571, Validation F1 Score: 0.9488092916913501\n\nEpoch 19, Training Loss: 0.09556819171686341, Validation Loss: 0.1341567164938383\nTraining Accuracy: 0.9639593897708522, Training F1 Score: 0.9639630022179431\nValidation Accuracy: 0.9484264605905183, Validation F1 Score: 0.9484101777969263\n\nEpoch 20, Training Loss: 0.09483112329955021, Validation Loss: 0.13409516986240821\nTraining Accuracy: 0.9639895096695632, Training F1 Score: 0.963983151582595\nValidation Accuracy: 0.9483231930328822, Validation F1 Score: 0.9482930475152966\n\nExecution time: 225.394504 seconds\n","output_type":"stream"}]},{"cell_type":"code","source":"torch.cuda.synchronize()\nstart_time = time.time()\n\nevaluate_model(model, custom_train_loader, criterion, optimizer, 20, 1024 * 16)\n\nelapsed_time = time.time() - start_time\nprint(f\"Execution time: {elapsed_time:.6f} seconds\")","metadata":{"execution":{"iopub.status.busy":"2024-10-25T23:18:06.791283Z","iopub.execute_input":"2024-10-25T23:18:06.791988Z","iopub.status.idle":"2024-10-25T23:21:51.966223Z","shell.execute_reply.started":"2024-10-25T23:18:06.791947Z","shell.execute_reply":"2024-10-25T23:21:51.965214Z"},"collapsed":true,"jupyter":{"outputs_hidden":true,"source_hidden":true},"trusted":true},"execution_count":47,"outputs":[{"name":"stdout","text":"Epoch 1, Training Loss: 0.09408655950850442, Validation Loss: 0.13329234114371585\nTraining Accuracy: 0.9644757308916135, Training F1 Score: 0.964462610451983\nValidation Accuracy: 0.9488395308210631, Validation F1 Score: 0.9487945787756927\n\nEpoch 2, Training Loss: 0.09331592123334488, Validation Loss: 0.13265810175312787\nTraining Accuracy: 0.9648500782041656, Training F1 Score: 0.9648325634773427\nValidation Accuracy: 0.9491063053449567, Validation F1 Score: 0.9490494404039059\n\nEpoch 3, Training Loss: 0.09254076163053716, Validation Loss: 0.1325647020476304\nTraining Accuracy: 0.9649189236869338, Training F1 Score: 0.9648995429491626\nValidation Accuracy: 0.9490890940853506, Validation F1 Score: 0.9490304819780228\n\nEpoch 4, Training Loss: 0.09195766459005449, Validation Loss: 0.13264460826264696\nTraining Accuracy: 0.964992072012375, Training F1 Score: 0.9649722544813696\nValidation Accuracy: 0.9490546715661385, Validation F1 Score: 0.9489931553773624\n\nEpoch 5, Training Loss: 0.09149378942082254, Validation Loss: 0.13246157573448022\nTraining Accuracy: 0.9650307975964321, Training F1 Score: 0.9650114658044703\nValidation Accuracy: 0.9490976997151537, Validation F1 Score: 0.9490368965180745\n\nEpoch 6, Training Loss: 0.09101662736425468, Validation Loss: 0.13232137562093418\nTraining Accuracy: 0.9651448229272669, Training F1 Score: 0.9651234337342954\nValidation Accuracy: 0.9490718828257446, Validation F1 Score: 0.9490043925685463\n\nEpoch 7, Training Loss: 0.09056992826504091, Validation Loss: 0.13248718254974162\nTraining Accuracy: 0.9649361350576258, Training F1 Score: 0.9649107416628842\nValidation Accuracy: 0.9489600096383054, Validation F1 Score: 0.9488857832881424\n\nEpoch 8, Training Loss: 0.09020613020554251, Validation Loss: 0.13297629630240546\nTraining Accuracy: 0.9646284818065055, Training F1 Score: 0.9645989784704329\nValidation Accuracy: 0.9489341927488963, Validation F1 Score: 0.9488533292499871\n\nEpoch 9, Training Loss: 0.08998754720014106, Validation Loss: 0.13324669887800272\nTraining Accuracy: 0.9644886394196326, Training F1 Score: 0.9644600808565402\nValidation Accuracy: 0.9487104463740179, Validation F1 Score: 0.9486307062372453\n\nEpoch 10, Training Loss: 0.09007448985330463, Validation Loss: 0.13318416999495206\nTraining Accuracy: 0.9645467277957183, Training F1 Score: 0.9645129941002633\nValidation Accuracy: 0.9487276576336239, Validation F1 Score: 0.9486381076920146\n\nEpoch 11, Training Loss: 0.09019560487315824, Validation Loss: 0.13422136050120542\nTraining Accuracy: 0.9638969985520934, Training F1 Score: 0.9638475941987229\nValidation Accuracy: 0.9482113198454429, Validation F1 Score: 0.9481023701143098\n\nEpoch 12, Training Loss: 0.09088618914594615, Validation Loss: 0.1315141779495642\nTraining Accuracy: 0.9654675361277428, Training F1 Score: 0.9654098033483182\nValidation Accuracy: 0.9496312487629407, Validation F1 Score: 0.9494994254682295\n\nEpoch 13, Training Loss: 0.09151271199862686, Validation Loss: 0.13152010025189645\nTraining Accuracy: 0.9657214038454505, Training F1 Score: 0.96565807006026\nValidation Accuracy: 0.949269812311214, Validation F1 Score: 0.9491274099076127\n\nEpoch 14, Training Loss: 0.09086608891976374, Validation Loss: 0.12902495304592818\nTraining Accuracy: 0.9670402251247286, Training F1 Score: 0.9670526088752078\nValidation Accuracy: 0.9501906147001369, Validation F1 Score: 0.9501911830876097\n\nEpoch 15, Training Loss: 0.08930786274295197, Validation Loss: 0.12996758788329868\nTraining Accuracy: 0.9665690638520339, Training F1 Score: 0.96661390392366\nValidation Accuracy: 0.9500787415126976, Validation F1 Score: 0.9501408265269417\n\nEpoch 16, Training Loss: 0.08867271930349954, Validation Loss: 0.13024648827478466\nTraining Accuracy: 0.9664959155265926, Training F1 Score: 0.9665469962839909\nValidation Accuracy: 0.9501303752915157, Validation F1 Score: 0.9502057945289389\n\nEpoch 17, Training Loss: 0.0879560285072852, Validation Loss: 0.13084180254159655\nTraining Accuracy: 0.9662441992302214, Training F1 Score: 0.9663016868860902\nValidation Accuracy: 0.9500012908444705, Validation F1 Score: 0.9500858867411689\n\nEpoch 18, Training Loss: 0.08734847398599035, Validation Loss: 0.1317413268677693\nTraining Accuracy: 0.9658440348616313, Training F1 Score: 0.965904036458681\nValidation Accuracy: 0.9494763474264865, Validation F1 Score: 0.9495636811938332\n\nEpoch 19, Training Loss: 0.08691314678711538, Validation Loss: 0.13234862868142\nTraining Accuracy: 0.9656439526773363, Training F1 Score: 0.9657036284279076\nValidation Accuracy: 0.949278417941017, Validation F1 Score: 0.9493610454276605\n\nEpoch 20, Training Loss: 0.08661625084896712, Validation Loss: 0.13271657175121993\nTraining Accuracy: 0.9654008420663112, Training F1 Score: 0.9654553086229934\nValidation Accuracy: 0.9490804884555476, Validation F1 Score: 0.9491516265848312\n\nExecution time: 225.169023 seconds\n","output_type":"stream"}]},{"cell_type":"code","source":"for param_group in optimizer.param_groups:\n    param_group['lr'] = 0.0001\n\ntorch.cuda.synchronize()\nstart_time = time.time()\n\nevaluate_model(model, custom_train_loader, criterion, optimizer, 20, 1024 * 16)\n\nelapsed_time = time.time() - start_time\nprint(f\"Execution time: {elapsed_time:.6f} seconds\")","metadata":{"execution":{"iopub.status.busy":"2024-10-25T23:22:09.564490Z","iopub.execute_input":"2024-10-25T23:22:09.565402Z","iopub.status.idle":"2024-10-25T23:25:54.746583Z","shell.execute_reply.started":"2024-10-25T23:22:09.565358Z","shell.execute_reply":"2024-10-25T23:25:54.745562Z"},"collapsed":true,"jupyter":{"outputs_hidden":true,"source_hidden":true},"trusted":true},"execution_count":48,"outputs":[{"name":"stdout","text":"Epoch 1, Training Loss: 0.08455128031209447, Validation Loss: 0.1281377704352258\nTraining Accuracy: 0.9681051786862991, Training F1 Score: 0.9680794581539828\nValidation Accuracy: 0.9510683889400445, Validation F1 Score: 0.9509997139809068\n\nEpoch 2, Training Loss: 0.08343363505337842, Validation Loss: 0.12799011852927272\nTraining Accuracy: 0.9682708381292101, Training F1 Score: 0.9682341672148342\nValidation Accuracy: 0.9511544452380748, Validation F1 Score: 0.9510604025717083\n\nEpoch 3, Training Loss: 0.0833648867990448, Validation Loss: 0.12691932296694558\nTraining Accuracy: 0.9689463844288729, Training F1 Score: 0.9689122568936411\nValidation Accuracy: 0.9513954028725592, Validation F1 Score: 0.9513032785478235\n\nEpoch 4, Training Loss: 0.08313656049765522, Validation Loss: 0.12556711872734005\nTraining Accuracy: 0.9697639245367452, Training F1 Score: 0.9697307508112591\nValidation Accuracy: 0.9521699095548307, Validation F1 Score: 0.9520812548867525\n\nEpoch 5, Training Loss: 0.08260462779331045, Validation Loss: 0.1244110328371666\nTraining Accuracy: 0.9703663225109669, Training F1 Score: 0.9703362695850719\nValidation Accuracy: 0.9527120642324208, Validation F1 Score: 0.9526296019048914\n\nEpoch 6, Training Loss: 0.08191505054432306, Validation Loss: 0.12361683995774218\nTraining Accuracy: 0.9709106321091029, Training F1 Score: 0.9708841859508409\nValidation Accuracy: 0.9531251344629657, Validation F1 Score: 0.9530502180589874\n\nEpoch 7, Training Loss: 0.08122739710419921, Validation Loss: 0.1230634311951835\nTraining Accuracy: 0.9712161339388867, Training F1 Score: 0.9711939220644021\nValidation Accuracy: 0.9534435427656773, Validation F1 Score: 0.953378042485009\n\nEpoch 8, Training Loss: 0.0806432022273442, Validation Loss: 0.12270629514332967\nTraining Accuracy: 0.9714764559206039, Training F1 Score: 0.971457000310194\nValidation Accuracy: 0.9535898384723286, Validation F1 Score: 0.953530203883076\n\nEpoch 9, Training Loss: 0.08011550122027293, Validation Loss: 0.12243304907538002\nTraining Accuracy: 0.9716313582568323, Training F1 Score: 0.9716155304778847\nValidation Accuracy: 0.9536500778809497, Validation F1 Score: 0.9535969394934191\n\nEpoch 10, Training Loss: 0.07965584695746028, Validation Loss: 0.12221468539583408\nTraining Accuracy: 0.9718056233850894, Training F1 Score: 0.9717908919911051\nValidation Accuracy: 0.9537619510683889, Validation F1 Score: 0.953714732590353\n\nEpoch 11, Training Loss: 0.07925545681993136, Validation Loss: 0.12203731921915661\nTraining Accuracy: 0.9719970998840384, Training F1 Score: 0.9719834316305348\nValidation Accuracy: 0.9539340636644493, Validation F1 Score: 0.9538877458290307\n\nEpoch 12, Training Loss: 0.078880332706173, Validation Loss: 0.1219336245505744\nTraining Accuracy: 0.9721369422709113, Training F1 Score: 0.9721226196955854\nValidation Accuracy: 0.9538910355154342, Validation F1 Score: 0.9538421113293266\n\nEpoch 13, Training Loss: 0.07854012161055508, Validation Loss: 0.12181511980269227\nTraining Accuracy: 0.972203636332343, Training F1 Score: 0.9721894532742538\nValidation Accuracy: 0.9540889650009036, Validation F1 Score: 0.9540400831229424\n\nEpoch 14, Training Loss: 0.07822926808814441, Validation Loss: 0.1217520253589387\nTraining Accuracy: 0.972233756231054, Training F1 Score: 0.9722190465485931\nValidation Accuracy: 0.9540975706307067, Validation F1 Score: 0.9540482776622469\n\nEpoch 15, Training Loss: 0.07793199917509425, Validation Loss: 0.12168064662126832\nTraining Accuracy: 0.9722703303937746, Training F1 Score: 0.9722555019512297\nValidation Accuracy: 0.9541061762605096, Validation F1 Score: 0.9540574666630423\n\nEpoch 16, Training Loss: 0.07766804529003418, Validation Loss: 0.12162756557324435\nTraining Accuracy: 0.9722982988711493, Training F1 Score: 0.9722831136743415\nValidation Accuracy: 0.9541405987797217, Validation F1 Score: 0.9540896658054177\n\nEpoch 17, Training Loss: 0.0773979578422796, Validation Loss: 0.12158412635566183\nTraining Accuracy: 0.9724101727806476, Training F1 Score: 0.9723949643872014\nValidation Accuracy: 0.9541750212989337, Validation F1 Score: 0.9541245404832287\n\nEpoch 18, Training Loss: 0.07715796088500299, Validation Loss: 0.12149042690290013\nTraining Accuracy: 0.9724704125780697, Training F1 Score: 0.9724556965670129\nValidation Accuracy: 0.9543471338949941, Validation F1 Score: 0.9542959772531546\n\nEpoch 19, Training Loss: 0.07691300141866775, Validation Loss: 0.12148629357638072\nTraining Accuracy: 0.9724790182634158, Training F1 Score: 0.972463866769034\nValidation Accuracy: 0.9543471338949941, Validation F1 Score: 0.9542973222437393\n\nEpoch 20, Training Loss: 0.0766591012800369, Validation Loss: 0.121525525923969\nTraining Accuracy: 0.9724747154207427, Training F1 Score: 0.9724590801444827\nValidation Accuracy: 0.9544073733036152, Validation F1 Score: 0.9543556602731672\n\nExecution time: 225.175343 seconds\n","output_type":"stream"}]},{"cell_type":"code","source":"for param_group in optimizer.param_groups:\n    param_group['lr'] = 0.00005\n\ntorch.cuda.synchronize()\nstart_time = time.time()\n\nevaluate_model(model, custom_train_loader, criterion, optimizer, 20, 1024 * 16)\n\nelapsed_time = time.time() - start_time\nprint(f\"Execution time: {elapsed_time:.6f} seconds\")","metadata":{"execution":{"iopub.status.busy":"2024-10-25T23:26:02.676805Z","iopub.execute_input":"2024-10-25T23:26:02.677218Z","iopub.status.idle":"2024-10-25T23:29:48.038697Z","shell.execute_reply.started":"2024-10-25T23:26:02.677176Z","shell.execute_reply":"2024-10-25T23:29:48.037728Z"},"jupyter":{"source_hidden":true,"outputs_hidden":true},"collapsed":true,"trusted":true},"execution_count":49,"outputs":[{"name":"stdout","text":"Epoch 1, Training Loss: 0.07552349267276798, Validation Loss: 0.1197168956791098\nTraining Accuracy: 0.9732406214165389, Training F1 Score: 0.9732420503617814\nValidation Accuracy: 0.955302358803129, Validation F1 Score: 0.9552878728476861\n\nEpoch 2, Training Loss: 0.07503097019361654, Validation Loss: 0.11997202698226901\nTraining Accuracy: 0.9732191072031737, Training F1 Score: 0.9732238919881614\nValidation Accuracy: 0.9551130349474626, Validation F1 Score: 0.9551015302573446\n\nEpoch 3, Training Loss: 0.07475445128560552, Validation Loss: 0.11981555840541769\nTraining Accuracy: 0.9733417382193547, Training F1 Score: 0.9733463907715331\nValidation Accuracy: 0.9551474574666747, Validation F1 Score: 0.9551355732769966\n\nEpoch 4, Training Loss: 0.07456040494285261, Validation Loss: 0.11974082116782037\nTraining Accuracy: 0.9734019780167767, Training F1 Score: 0.9734063382744743\nValidation Accuracy: 0.9552249081349019, Validation F1 Score: 0.9552137399415805\n\nEpoch 5, Training Loss: 0.07439698579560645, Validation Loss: 0.11966318305872482\nTraining Accuracy: 0.9734579149715259, Training F1 Score: 0.9734620786097586\nValidation Accuracy: 0.9552421193945079, Validation F1 Score: 0.9552314729487885\n\nEpoch 6, Training Loss: 0.07424660362666241, Validation Loss: 0.11957997338067129\nTraining Accuracy: 0.97349233771291, Training F1 Score: 0.9734958395762595\nValidation Accuracy: 0.955328175692538, Validation F1 Score: 0.9553168073745063\n\nEpoch 7, Training Loss: 0.07410681167491558, Validation Loss: 0.11950334243089573\nTraining Accuracy: 0.9735762431450338, Training F1 Score: 0.973579207907615\nValidation Accuracy: 0.9553712038415532, Validation F1 Score: 0.955358596399684\n\nEpoch 8, Training Loss: 0.07396739454713495, Validation Loss: 0.11943263494820074\nTraining Accuracy: 0.9736321800997829, Training F1 Score: 0.9736345595247704\nValidation Accuracy: 0.9554056263607652, Validation F1 Score: 0.955391974785208\n\nEpoch 9, Training Loss: 0.07382953955611514, Validation Loss: 0.11935770471656698\nTraining Accuracy: 0.9737160855319067, Training F1 Score: 0.973717366718652\nValidation Accuracy: 0.9553367813223411, Validation F1 Score: 0.9553207754188853\n\nEpoch 10, Training Loss: 0.07369668154708084, Validation Loss: 0.119282788066119\nTraining Accuracy: 0.9738021423853669, Training F1 Score: 0.9738027224121781\nValidation Accuracy: 0.9554486545097803, Validation F1 Score: 0.9554315885777809\n\nEpoch 11, Training Loss: 0.0735686412004923, Validation Loss: 0.11922922819433761\nTraining Accuracy: 0.9738322622840779, Training F1 Score: 0.9738317996008304\nValidation Accuracy: 0.9554400488799772, Validation F1 Score: 0.9554208358639331\n\nEpoch 12, Training Loss: 0.07343652581305829, Validation Loss: 0.11918052252113615\nTraining Accuracy: 0.9738838963961541, Training F1 Score: 0.9738827322905406\nValidation Accuracy: 0.9554056263607652, Validation F1 Score: 0.9553847729887621\n\nEpoch 13, Training Loss: 0.07330194690834699, Validation Loss: 0.11913645752422213\nTraining Accuracy: 0.9738903506601636, Training F1 Score: 0.9738881605897002\nValidation Accuracy: 0.9554572601395833, Validation F1 Score: 0.9554330407590936\n\nEpoch 14, Training Loss: 0.07317844102641093, Validation Loss: 0.1191066262305958\nTraining Accuracy: 0.9739032591881827, Training F1 Score: 0.9739001671533002\nValidation Accuracy: 0.9555088939184014, Validation F1 Score: 0.9554831636144101\n\nEpoch 15, Training Loss: 0.07304918917134305, Validation Loss: 0.11906678684582654\nTraining Accuracy: 0.9739355305082302, Training F1 Score: 0.9739317263918005\nValidation Accuracy: 0.9555433164376135, Validation F1 Score: 0.9555172992085761\n\nEpoch 16, Training Loss: 0.07290703269458179, Validation Loss: 0.11904567664359361\nTraining Accuracy: 0.9739591961429318, Training F1 Score: 0.9739546284479853\nValidation Accuracy: 0.9556637952548557, Validation F1 Score: 0.9556357269939179\n\nEpoch 17, Training Loss: 0.0727603956306746, Validation Loss: 0.11901761738012212\nTraining Accuracy: 0.9740344958897096, Training F1 Score: 0.9740286028125826\nValidation Accuracy: 0.9555949502164316, Validation F1 Score: 0.9555635724722259\n\nEpoch 18, Training Loss: 0.07262782528487055, Validation Loss: 0.11898735951554262\nTraining Accuracy: 0.9740753728951032, Training F1 Score: 0.9740683355883462\nValidation Accuracy: 0.9556551896250527, Validation F1 Score: 0.9556217049805579\n\nEpoch 19, Training Loss: 0.0725159666722497, Validation Loss: 0.11894409505079168\nTraining Accuracy: 0.9741140984791603, Training F1 Score: 0.9741065160858218\nValidation Accuracy: 0.9556551896250527, Validation F1 Score: 0.9556215670019289\n\nEpoch 20, Training Loss: 0.07240127469216072, Validation Loss: 0.11890096822920654\nTraining Accuracy: 0.9741313098498523, Training F1 Score: 0.9741241611879521\nValidation Accuracy: 0.9556896121442647, Validation F1 Score: 0.9556569010961985\n\nExecution time: 225.355403 seconds\n","output_type":"stream"}]},{"cell_type":"code","source":"for param_group in optimizer.param_groups:\n    param_group['lr'] = 0.00002\n\ntorch.cuda.synchronize()\nstart_time = time.time()\n\nevaluate_model(model, custom_train_loader, criterion, optimizer, 10, 1024 * 16)\n\nelapsed_time = time.time() - start_time\nprint(f\"Execution time: {elapsed_time:.6f} seconds\")","metadata":{"execution":{"iopub.status.busy":"2024-10-25T23:29:58.880497Z","iopub.execute_input":"2024-10-25T23:29:58.880881Z","iopub.status.idle":"2024-10-25T23:31:51.399062Z","shell.execute_reply.started":"2024-10-25T23:29:58.880842Z","shell.execute_reply":"2024-10-25T23:31:51.398148Z"},"jupyter":{"source_hidden":true,"outputs_hidden":true},"collapsed":true,"trusted":true},"execution_count":50,"outputs":[{"name":"stdout","text":"Epoch 1, Training Loss: 0.07162546105255108, Validation Loss: 0.1174174862091672\nTraining Accuracy: 0.9750499667605403, Training F1 Score: 0.9750510114475586\nValidation Accuracy: 0.9563780625285061, Validation F1 Score: 0.9563609844685513\n\nEpoch 2, Training Loss: 0.07130857129046957, Validation Loss: 0.11744424025951104\nTraining Accuracy: 0.9751446292993466, Training F1 Score: 0.975146459017872\nValidation Accuracy: 0.956283400600673, Validation F1 Score: 0.9562686970037793\n\nEpoch 3, Training Loss: 0.07118461743945978, Validation Loss: 0.1173951029039541\nTraining Accuracy: 0.9752134747821148, Training F1 Score: 0.9752154019454865\nValidation Accuracy: 0.9563350343794911, Validation F1 Score: 0.9563211028571166\n\nEpoch 4, Training Loss: 0.07111795450899827, Validation Loss: 0.11736570658515216\nTraining Accuracy: 0.9751919605687498, Training F1 Score: 0.9751938503366309\nValidation Accuracy: 0.9563952737881122, Validation F1 Score: 0.9563811408774756\n\nEpoch 5, Training Loss: 0.0710516974084443, Validation Loss: 0.11733769276225257\nTraining Accuracy: 0.9752263833101339, Training F1 Score: 0.9752280567145191\nValidation Accuracy: 0.9563952737881122, Validation F1 Score: 0.9563807360751408\n\nEpoch 6, Training Loss: 0.07098995431645459, Validation Loss: 0.1173129960625037\nTraining Accuracy: 0.9752435946808259, Training F1 Score: 0.9752449319739678\nValidation Accuracy: 0.9563780625285061, Validation F1 Score: 0.9563631508545\n\nEpoch 7, Training Loss: 0.0709316283610459, Validation Loss: 0.11728883880933402\nTraining Accuracy: 0.9752801688435465, Training F1 Score: 0.975281465537693\nValidation Accuracy: 0.9564296963073242, Validation F1 Score: 0.9564145395002114\n\nEpoch 8, Training Loss: 0.07087479912511455, Validation Loss: 0.11726486259886756\nTraining Accuracy: 0.9752844716862195, Training F1 Score: 0.97528540048845\nValidation Accuracy: 0.9564038794179152, Validation F1 Score: 0.9563882231356251\n\nEpoch 9, Training Loss: 0.07081637460523395, Validation Loss: 0.11724173518048608\nTraining Accuracy: 0.9752973802142385, Training F1 Score: 0.9752980015269016\nValidation Accuracy: 0.9563866681583092, Validation F1 Score: 0.9563699366447361\n\nEpoch 10, Training Loss: 0.07075987761029832, Validation Loss: 0.11721920532498208\nTraining Accuracy: 0.9753038344782481, Training F1 Score: 0.9753043094919502\nValidation Accuracy: 0.9563866681583092, Validation F1 Score: 0.9563696773494639\n\nExecution time: 112.512041 seconds\n","output_type":"stream"}]},{"cell_type":"code","source":"for param_group in optimizer.param_groups:\n    param_group['lr'] = 0.00001\n\ntorch.cuda.synchronize()\nstart_time = time.time()\n\nevaluate_model(model, custom_train_loader, criterion, optimizer, 10, 1024 * 16)\n\nelapsed_time = time.time() - start_time\nprint(f\"Execution time: {elapsed_time:.6f} seconds\")","metadata":{"execution":{"iopub.status.busy":"2024-10-25T23:32:38.988415Z","iopub.execute_input":"2024-10-25T23:32:38.988827Z","iopub.status.idle":"2024-10-25T23:34:31.713971Z","shell.execute_reply.started":"2024-10-25T23:32:38.988784Z","shell.execute_reply":"2024-10-25T23:34:31.713045Z"},"jupyter":{"source_hidden":true,"outputs_hidden":true},"collapsed":true,"trusted":true},"execution_count":51,"outputs":[{"name":"stdout","text":"Epoch 1, Training Loss: 0.07035026523862674, Validation Loss: 0.11706285030852569\nTraining Accuracy: 0.9754027998597273, Training F1 Score: 0.9754029572731007\nValidation Accuracy: 0.9566104145331876, Validation F1 Score: 0.9565938670133044\n\nEpoch 2, Training Loss: 0.07030765793167681, Validation Loss: 0.11703670855852995\nTraining Accuracy: 0.975469493921159, Training F1 Score: 0.9754709421150373\nValidation Accuracy: 0.9565845976437786, Validation F1 Score: 0.9565699827356388\n\nEpoch 3, Training Loss: 0.07029032349903579, Validation Loss: 0.1170174498138099\nTraining Accuracy: 0.9755189766118987, Training F1 Score: 0.9755207720875843\nValidation Accuracy: 0.9565501751245665, Validation F1 Score: 0.9565360604426596\n\nEpoch 4, Training Loss: 0.0702534009682425, Validation Loss: 0.11700455772719576\nTraining Accuracy: 0.9755490965106097, Training F1 Score: 0.975551051371212\nValidation Accuracy: 0.9565415694947634, Validation F1 Score: 0.9565283059965443\n\nEpoch 5, Training Loss: 0.07021668500728039, Validation Loss: 0.11699671094194065\nTraining Accuracy: 0.9755512479319463, Training F1 Score: 0.9755533673189493\nValidation Accuracy: 0.9565071469755514, Validation F1 Score: 0.9564940974373202\n\nEpoch 6, Training Loss: 0.07018231115818253, Validation Loss: 0.11699045235941029\nTraining Accuracy: 0.9755641564599653, Training F1 Score: 0.9755664102737409\nValidation Accuracy: 0.9565071469755514, Validation F1 Score: 0.9564940714640903\n\nEpoch 7, Training Loss: 0.07014942499282413, Validation Loss: 0.11698575812060187\nTraining Accuracy: 0.9755792164093208, Training F1 Score: 0.9755816366138964\nValidation Accuracy: 0.9564985413457484, Validation F1 Score: 0.9564860154565643\n\nEpoch 8, Training Loss: 0.07011520867659844, Validation Loss: 0.11698252180563158\nTraining Accuracy: 0.9755706107239748, Training F1 Score: 0.9755733294321743\nValidation Accuracy: 0.9564641188265364, Validation F1 Score: 0.9564517731753611\n\nEpoch 9, Training Loss: 0.07008327221152288, Validation Loss: 0.1169775913918881\nTraining Accuracy: 0.9756093363080319, Training F1 Score: 0.9756122726629574\nValidation Accuracy: 0.9564383019371273, Validation F1 Score: 0.9564261340108161\n\nEpoch 10, Training Loss: 0.07005075768328992, Validation Loss: 0.11697584175963925\nTraining Accuracy: 0.9755964277800129, Training F1 Score: 0.9755995030553659\nValidation Accuracy: 0.9564469075669303, Validation F1 Score: 0.9564354356591304\n\nExecution time: 112.719317 seconds\n","output_type":"stream"}]},{"cell_type":"code","source":"for param_group in optimizer.param_groups:\n    param_group['lr'] = 0.000005\n\ntorch.cuda.synchronize()\nstart_time = time.time()\n\nevaluate_model(model, custom_train_loader, criterion, optimizer, 5, 1024 * 16)\n\nelapsed_time = time.time() - start_time\nprint(f\"Execution time: {elapsed_time:.6f} seconds\")","metadata":{"execution":{"iopub.status.busy":"2024-10-25T23:34:40.280885Z","iopub.execute_input":"2024-10-25T23:34:40.281699Z","iopub.status.idle":"2024-10-25T23:35:36.594849Z","shell.execute_reply.started":"2024-10-25T23:34:40.281658Z","shell.execute_reply":"2024-10-25T23:35:36.593858Z"},"collapsed":true,"jupyter":{"outputs_hidden":true,"source_hidden":true},"trusted":true},"execution_count":52,"outputs":[{"name":"stdout","text":"Epoch 1, Training Loss: 0.06980081457204367, Validation Loss: 0.11680278580548555\nTraining Accuracy: 0.9755060680838796, Training F1 Score: 0.9754966495769104\nValidation Accuracy: 0.9566706539418087, Validation F1 Score: 0.9566379236439734\n\nEpoch 2, Training Loss: 0.06965290638954637, Validation Loss: 0.11674298313259643\nTraining Accuracy: 0.9755727621453113, Training F1 Score: 0.9755652985909335\nValidation Accuracy: 0.9567136820908239, Validation F1 Score: 0.9566845497275421\n\nEpoch 3, Training Loss: 0.06963831450617539, Validation Loss: 0.11674056503792135\nTraining Accuracy: 0.9755706107239748, Training F1 Score: 0.9755627924967688\nValidation Accuracy: 0.9567222877206268, Validation F1 Score: 0.9566925626043296\n\nEpoch 4, Training Loss: 0.06961822089154993, Validation Loss: 0.11673323757319491\nTraining Accuracy: 0.9755878220946669, Training F1 Score: 0.9755800967148666\nValidation Accuracy: 0.9566964708312178, Validation F1 Score: 0.9566668639237061\n\nEpoch 5, Training Loss: 0.06960027212148769, Validation Loss: 0.11672576597878435\nTraining Accuracy: 0.9755856706733304, Training F1 Score: 0.9755779567745576\nValidation Accuracy: 0.9566964708312178, Validation F1 Score: 0.956666934593115\n\nExecution time: 56.307585 seconds\n","output_type":"stream"}]},{"cell_type":"code","source":"for param_group in optimizer.param_groups:\n    param_group['lr'] = 0.000001\n\ntorch.cuda.synchronize()\nstart_time = time.time()\n\nevaluate_model(model, custom_train_loader, criterion, optimizer, 5, 1024 * 16)\n\nelapsed_time = time.time() - start_time\nprint(f\"Execution time: {elapsed_time:.6f} seconds\")","metadata":{"execution":{"iopub.status.busy":"2024-10-25T23:35:54.613059Z","iopub.execute_input":"2024-10-25T23:35:54.614038Z","iopub.status.idle":"2024-10-25T23:36:50.892697Z","shell.execute_reply.started":"2024-10-25T23:35:54.613986Z","shell.execute_reply":"2024-10-25T23:36:50.891651Z"},"collapsed":true,"jupyter":{"outputs_hidden":true,"source_hidden":true},"trusted":true},"execution_count":53,"outputs":[{"name":"stdout","text":"Epoch 1, Training Loss: 0.06940010032095444, Validation Loss: 0.11669928767384942\nTraining Accuracy: 0.9756631218414445, Training F1 Score: 0.9756562817781657\nValidation Accuracy: 0.9566878652014148, Validation F1 Score: 0.9566596748726762\n\nEpoch 2, Training Loss: 0.06936662093831844, Validation Loss: 0.11668678589864918\nTraining Accuracy: 0.9756588189987715, Training F1 Score: 0.9756519781471047\nValidation Accuracy: 0.9566018089033846, Validation F1 Score: 0.9565736300383773\n\nEpoch 3, Training Loss: 0.06936168336683038, Validation Loss: 0.1166864332975485\nTraining Accuracy: 0.975669576105454, Training F1 Score: 0.975662657453177\nValidation Accuracy: 0.9565673863841725, Validation F1 Score: 0.9565389470044268\n\nEpoch 4, Training Loss: 0.06935726788623922, Validation Loss: 0.11668566071036836\nTraining Accuracy: 0.975660970420108, Training F1 Score: 0.9756540682792527\nValidation Accuracy: 0.9565415694947634, Validation F1 Score: 0.9565131642815181\n\nEpoch 5, Training Loss: 0.06935356863348457, Validation Loss: 0.11668471155443673\nTraining Accuracy: 0.9756674246841176, Training F1 Score: 0.9756605089498638\nValidation Accuracy: 0.9565501751245665, Validation F1 Score: 0.9565217351155144\n\nExecution time: 56.272156 seconds\n","output_type":"stream"}]},{"cell_type":"code","source":"for param_group in optimizer.param_groups:\n    param_group['lr'] = 0.000001 * 10\n\ntorch.cuda.synchronize()\nstart_time = time.time()\n\nevaluate_model(model, custom_train_loader, criterion, optimizer, 5, 1024 * 16)\n\nelapsed_time = time.time() - start_time\nprint(f\"Execution time: {elapsed_time:.6f} seconds\")","metadata":{"execution":{"iopub.status.busy":"2024-10-25T23:38:07.602837Z","iopub.execute_input":"2024-10-25T23:38:07.603695Z","iopub.status.idle":"2024-10-25T23:39:03.704554Z","shell.execute_reply.started":"2024-10-25T23:38:07.603640Z","shell.execute_reply":"2024-10-25T23:39:03.703621Z"},"collapsed":true,"jupyter":{"outputs_hidden":true,"source_hidden":true},"trusted":true},"execution_count":54,"outputs":[{"name":"stdout","text":"Epoch 1, Training Loss: 0.06972777877932808, Validation Loss: 0.11686190500457024\nTraining Accuracy: 0.9756889388974826, Training F1 Score: 0.9756917234113511\nValidation Accuracy: 0.9565932032735815, Validation F1 Score: 0.956582053160514\n\nEpoch 2, Training Loss: 0.06985445251794688, Validation Loss: 0.1169492336232257\nTraining Accuracy: 0.9756351533640699, Training F1 Score: 0.9756391611620888\nValidation Accuracy: 0.9565329638649605, Validation F1 Score: 0.9565232216454832\n\nEpoch 3, Training Loss: 0.06982581798916436, Validation Loss: 0.11694324883201232\nTraining Accuracy: 0.9756545161560986, Training F1 Score: 0.9756585661433242\nValidation Accuracy: 0.9564813300861423, Validation F1 Score: 0.9564711404340759\n\nEpoch 4, Training Loss: 0.06979018771888866, Validation Loss: 0.11692988638058845\nTraining Accuracy: 0.975669576105454, Training F1 Score: 0.975673639918884\nValidation Accuracy: 0.9564899357159453, Validation F1 Score: 0.9564796249462926\n\nEpoch 5, Training Loss: 0.06975695590691057, Validation Loss: 0.11691773522784016\nTraining Accuracy: 0.975660970420108, Training F1 Score: 0.9756651024954466\nValidation Accuracy: 0.9564555131967333, Validation F1 Score: 0.9564451018679863\n\nExecution time: 56.094302 seconds\n","output_type":"stream"}]},{"cell_type":"code","source":"for name, param in model.named_parameters():\n    print(name)\n    print(param)","metadata":{"execution":{"iopub.status.busy":"2024-10-25T23:47:50.792706Z","iopub.execute_input":"2024-10-25T23:47:50.793519Z","iopub.status.idle":"2024-10-25T23:47:50.934198Z","shell.execute_reply.started":"2024-10-25T23:47:50.793479Z","shell.execute_reply":"2024-10-25T23:47:50.933171Z"},"trusted":true},"execution_count":55,"outputs":[{"name":"stdout","text":"first_layer.linear.weight\nParameter containing:\ntensor([[ 1.0582, -0.0018,  0.0028,  ..., -0.0036, -0.0012, -0.0028],\n        [-1.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n        [-0.0172,  1.0564, -0.0185,  ..., -0.0118, -0.0121, -0.0128],\n        ...,\n        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000, -1.0000,  0.0000],\n        [ 0.0315, -0.0280, -0.0218,  ..., -0.0173, -0.0192,  1.0129],\n        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000, -1.0000]],\n       device='cuda:0', requires_grad=True)\nfirst_layer.linear.bias\nParameter containing:\ntensor([-0.0009,  0.0000, -0.0090,  0.0000, -0.0052,  0.0000, -0.0073,  0.0000,\n        -0.0069,  0.0000, -0.0085,  0.0000, -0.0075,  0.0000, -0.0063,  0.0000,\n        -0.0054,  0.0000, -0.0100,  0.0000, -0.0088,  0.0000, -0.0117,  0.0000,\n        -0.0076,  0.0000, -0.0022,  0.0000, -0.0108,  0.0000, -0.0066,  0.0000,\n        -0.0116,  0.0000, -0.0079,  0.0000, -0.0129,  0.0000, -0.0082,  0.0000,\n        -0.0193,  0.0000, -0.0143,  0.0000, -0.0121,  0.0000, -0.0036,  0.0000,\n        -0.0106,  0.0000, -0.0132,  0.0000, -0.0125,  0.0000, -0.0143,  0.0000,\n        -0.0193,  0.0000, -0.0141,  0.0000, -0.0102,  0.0000, -0.0161,  0.0000,\n        -0.0160,  0.0000, -0.0091,  0.0000, -0.0167,  0.0000, -0.0146,  0.0000,\n        -0.0120,  0.0000, -0.0127,  0.0000, -0.0123,  0.0000, -0.0117,  0.0000,\n        -0.0212,  0.0000, -0.0134,  0.0000, -0.0098,  0.0000, -0.0154,  0.0000,\n        -0.0112,  0.0000, -0.0111,  0.0000, -0.0128,  0.0000, -0.0225,  0.0000,\n        -0.0190,  0.0000, -0.0196,  0.0000, -0.0268,  0.0000, -0.0157,  0.0000,\n        -0.0151,  0.0000, -0.0168,  0.0000], device='cuda:0',\n       requires_grad=True)\nlayers.0.linear.weight\nParameter containing:\ntensor([[ 1.0580e+00, -1.0000e+00, -1.4432e-03,  ...,  0.0000e+00,\n         -4.7810e-06,  0.0000e+00],\n        [-1.0000e+00,  1.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n          0.0000e+00,  0.0000e+00],\n        [-2.7754e-02,  0.0000e+00,  1.0670e+00,  ...,  0.0000e+00,\n          1.2989e-02,  0.0000e+00],\n        ...,\n        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  1.0000e+00,\n          0.0000e+00,  0.0000e+00],\n        [ 3.9003e-02,  0.0000e+00, -1.6783e-02,  ...,  0.0000e+00,\n          1.0835e+00, -1.0000e+00],\n        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n         -1.0000e+00,  1.0000e+00]], device='cuda:0', requires_grad=True)\nlayers.0.linear.bias\nParameter containing:\ntensor([-0.0026,  0.0000,  0.0020,  0.0000, -0.0081,  0.0000, -0.0018,  0.0000,\n        -0.0130,  0.0000, -0.0023,  0.0000, -0.0076,  0.0000, -0.0049,  0.0000,\n        -0.0241,  0.0000, -0.0059,  0.0000, -0.0039,  0.0000, -0.0062,  0.0000,\n         0.0014,  0.0000,  0.0006,  0.0000,  0.0054,  0.0000, -0.0045,  0.0000,\n         0.0076,  0.0000, -0.0045,  0.0000,  0.0005,  0.0000,  0.0018,  0.0000,\n        -0.0014,  0.0000, -0.0024,  0.0000,  0.0063,  0.0000, -0.0008,  0.0000,\n         0.0006,  0.0000, -0.0063,  0.0000, -0.0028,  0.0000,  0.0157,  0.0000,\n         0.0183,  0.0000, -0.0006,  0.0000,  0.0105,  0.0000,  0.0097,  0.0000,\n         0.0038,  0.0000,  0.0004,  0.0000, -0.0062,  0.0000, -0.0094,  0.0000,\n         0.0002,  0.0000, -0.0059,  0.0000,  0.0094,  0.0000,  0.0048,  0.0000,\n        -0.0054,  0.0000,  0.0055,  0.0000, -0.0022,  0.0000,  0.0039,  0.0000,\n        -0.0020,  0.0000, -0.0045,  0.0000, -0.0015,  0.0000, -0.0014,  0.0000,\n         0.0010,  0.0000, -0.0042,  0.0000, -0.0100,  0.0000, -0.0091,  0.0000,\n        -0.0102,  0.0000, -0.0081,  0.0000], device='cuda:0',\n       requires_grad=True)\nlayers.1.linear.weight\nParameter containing:\ntensor([[ 1.0560e+00, -1.0000e+00, -7.0975e-04,  ...,  0.0000e+00,\n         -1.4014e-02,  0.0000e+00],\n        [-1.0000e+00,  1.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n          0.0000e+00,  0.0000e+00],\n        [ 1.0765e-02,  0.0000e+00,  1.0595e+00,  ...,  0.0000e+00,\n          2.0046e-02,  0.0000e+00],\n        ...,\n        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  1.0000e+00,\n          0.0000e+00,  0.0000e+00],\n        [ 8.5255e-02,  0.0000e+00, -1.0277e-02,  ...,  0.0000e+00,\n          1.1013e+00, -1.0000e+00],\n        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n         -1.0000e+00,  1.0000e+00]], device='cuda:0', requires_grad=True)\nlayers.1.linear.bias\nParameter containing:\ntensor([-0.0027,  0.0000, -0.0057,  0.0000,  0.0118,  0.0000,  0.0006,  0.0000,\n        -0.0052,  0.0000, -0.0035,  0.0000, -0.0083,  0.0000, -0.0031,  0.0000,\n        -0.0025,  0.0000, -0.0058,  0.0000,  0.0007,  0.0000, -0.0055,  0.0000,\n         0.0067,  0.0000,  0.0023,  0.0000,  0.0136,  0.0000, -0.0040,  0.0000,\n        -0.0092,  0.0000, -0.0024,  0.0000, -0.0054,  0.0000,  0.0069,  0.0000,\n        -0.0028,  0.0000,  0.0206,  0.0000, -0.0042,  0.0000,  0.0004,  0.0000,\n         0.0009,  0.0000, -0.0026,  0.0000, -0.0062,  0.0000, -0.0016,  0.0000,\n         0.0214,  0.0000,  0.0049,  0.0000,  0.0133,  0.0000,  0.0088,  0.0000,\n         0.0166,  0.0000, -0.0004,  0.0000, -0.0027,  0.0000, -0.0058,  0.0000,\n        -0.0003,  0.0000,  0.0101,  0.0000, -0.0051,  0.0000,  0.0099,  0.0000,\n        -0.0066,  0.0000, -0.0058,  0.0000,  0.0032,  0.0000,  0.0017,  0.0000,\n         0.0079,  0.0000, -0.0015,  0.0000,  0.0075,  0.0000, -0.0084,  0.0000,\n        -0.0059,  0.0000, -0.0036,  0.0000, -0.0182,  0.0000, -0.0035,  0.0000,\n        -0.0008,  0.0000, -0.0080,  0.0000, -0.0024,  0.0000, -0.0021,  0.0000,\n        -0.0016,  0.0000,  0.0042,  0.0000, -0.0020,  0.0000, -0.0029,  0.0000,\n        -0.0070,  0.0000, -0.0078,  0.0000, -0.0214,  0.0000, -0.0073,  0.0000,\n         0.0036,  0.0000,  0.0031,  0.0000,  0.0127,  0.0000,  0.0057,  0.0000,\n        -0.0003,  0.0000,  0.0021,  0.0000, -0.0057,  0.0000,  0.0036,  0.0000,\n         0.0082,  0.0000,  0.0196,  0.0000, -0.0038,  0.0000, -0.0062,  0.0000,\n        -0.0059,  0.0000,  0.0047,  0.0000,  0.0048,  0.0000,  0.0044,  0.0000,\n         0.0075,  0.0000,  0.0003,  0.0000,  0.0036,  0.0000,  0.0020,  0.0000,\n         0.0027,  0.0000, -0.0067,  0.0000, -0.0149,  0.0000,  0.0098,  0.0000,\n        -0.0056,  0.0000,  0.0021,  0.0000,  0.0048,  0.0000, -0.0135,  0.0000,\n         0.0073,  0.0000,  0.0097,  0.0000, -0.0100,  0.0000, -0.0092,  0.0000,\n         0.0066,  0.0000,  0.0073,  0.0000,  0.0015,  0.0000,  0.0055,  0.0000,\n         0.0104,  0.0000,  0.0021,  0.0000, -0.0051,  0.0000, -0.0192,  0.0000,\n        -0.0053,  0.0000,  0.0001,  0.0000,  0.0012,  0.0000,  0.0019,  0.0000],\n       device='cuda:0', requires_grad=True)\nlayers.2.linear.weight\nParameter containing:\ntensor([[ 1.0515e+00, -1.0000e+00, -8.9997e-04,  ...,  0.0000e+00,\n         -3.3032e-03,  0.0000e+00],\n        [-1.0000e+00,  1.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n          0.0000e+00,  0.0000e+00],\n        [-3.7765e-02,  0.0000e+00,  1.0542e+00,  ...,  0.0000e+00,\n         -5.0881e-02,  0.0000e+00],\n        ...,\n        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  1.0000e+00,\n          0.0000e+00,  0.0000e+00],\n        [ 8.0258e-02,  0.0000e+00, -8.3675e-03,  ...,  0.0000e+00,\n          1.1415e+00, -1.0000e+00],\n        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n         -1.0000e+00,  1.0000e+00]], device='cuda:0', requires_grad=True)\nlayers.2.linear.bias\nParameter containing:\ntensor([-2.3064e-03,  0.0000e+00,  9.7290e-04,  0.0000e+00,  7.5344e-03,\n         0.0000e+00,  1.3360e-03,  0.0000e+00, -4.9248e-04,  0.0000e+00,\n        -6.1974e-03,  0.0000e+00,  9.1858e-04,  0.0000e+00,  9.8583e-04,\n         0.0000e+00, -6.4866e-03,  0.0000e+00, -1.7809e-03,  0.0000e+00,\n         4.5721e-03,  0.0000e+00,  7.4857e-03,  0.0000e+00,  1.1667e-02,\n         0.0000e+00,  7.9194e-03,  0.0000e+00,  2.0252e-02,  0.0000e+00,\n         6.2518e-04,  0.0000e+00,  8.6023e-03,  0.0000e+00,  4.0977e-03,\n         0.0000e+00,  2.1467e-02,  0.0000e+00,  1.5441e-02,  0.0000e+00,\n        -5.5631e-03,  0.0000e+00, -9.0918e-03,  0.0000e+00,  6.7551e-03,\n         0.0000e+00,  5.2106e-03,  0.0000e+00,  2.8027e-03,  0.0000e+00,\n         2.1602e-04,  0.0000e+00,  2.5556e-04,  0.0000e+00, -1.2627e-02,\n         0.0000e+00, -4.4537e-03,  0.0000e+00, -6.3378e-03,  0.0000e+00,\n        -4.2064e-03,  0.0000e+00,  2.0967e-04,  0.0000e+00, -1.7542e-03,\n         0.0000e+00,  1.3662e-02,  0.0000e+00, -3.6882e-03,  0.0000e+00,\n         3.7928e-03,  0.0000e+00,  7.8948e-03,  0.0000e+00,  2.1890e-02,\n         0.0000e+00,  8.7088e-03,  0.0000e+00,  8.7300e-03,  0.0000e+00,\n        -1.4557e-02,  0.0000e+00,  1.3903e-02,  0.0000e+00,  7.4847e-03,\n         0.0000e+00,  7.7491e-03,  0.0000e+00,  1.4613e-02,  0.0000e+00,\n         5.4892e-03,  0.0000e+00,  1.1902e-02,  0.0000e+00, -1.0589e-02,\n         0.0000e+00,  8.2999e-04,  0.0000e+00, -6.3196e-03,  0.0000e+00,\n        -1.8672e-02,  0.0000e+00,  9.9537e-04,  0.0000e+00, -1.3654e-03,\n         0.0000e+00,  8.3871e-04,  0.0000e+00, -2.2761e-03,  0.0000e+00,\n        -1.3205e-02,  0.0000e+00,  4.1196e-03,  0.0000e+00,  3.1030e-03,\n         0.0000e+00, -1.8040e-02,  0.0000e+00, -6.5152e-03,  0.0000e+00,\n        -4.0961e-03,  0.0000e+00, -3.5416e-03,  0.0000e+00, -2.6653e-02,\n         0.0000e+00, -1.0001e-02,  0.0000e+00,  4.6695e-03,  0.0000e+00,\n         6.2519e-04,  0.0000e+00,  1.0811e-02,  0.0000e+00,  9.0230e-03,\n         0.0000e+00,  9.8869e-04,  0.0000e+00,  4.0516e-03,  0.0000e+00,\n         4.1320e-03,  0.0000e+00,  8.5513e-03,  0.0000e+00,  9.0655e-03,\n         0.0000e+00, -2.4710e-04,  0.0000e+00, -4.7659e-03,  0.0000e+00,\n        -5.3619e-03,  0.0000e+00, -3.8654e-03,  0.0000e+00,  7.9765e-03,\n         0.0000e+00,  1.9329e-02,  0.0000e+00,  3.4023e-03,  0.0000e+00,\n         8.6806e-03,  0.0000e+00, -8.1995e-03,  0.0000e+00, -1.4925e-02,\n         0.0000e+00,  1.7461e-02,  0.0000e+00,  5.8596e-03,  0.0000e+00,\n         3.4349e-03,  0.0000e+00, -1.8760e-02,  0.0000e+00,  1.1767e-02,\n         0.0000e+00, -4.7693e-03,  0.0000e+00,  7.2279e-03,  0.0000e+00,\n         1.1617e-02,  0.0000e+00,  3.9509e-03,  0.0000e+00, -1.8255e-02,\n         0.0000e+00, -6.0143e-03,  0.0000e+00, -9.8603e-03,  0.0000e+00,\n         1.7048e-02,  0.0000e+00,  8.6915e-03,  0.0000e+00,  2.3049e-03,\n         0.0000e+00,  1.5364e-03,  0.0000e+00,  1.3692e-02,  0.0000e+00,\n         8.3940e-03,  0.0000e+00, -6.0433e-03,  0.0000e+00, -7.4749e-03,\n         0.0000e+00, -6.4590e-03,  0.0000e+00, -5.8362e-03,  0.0000e+00,\n        -1.0372e-02,  0.0000e+00, -9.0307e-03,  0.0000e+00, -6.1627e-03,\n         0.0000e+00, -1.9542e-03,  0.0000e+00, -7.8747e-03,  0.0000e+00,\n         8.7035e-03,  0.0000e+00, -1.8746e-03,  0.0000e+00, -1.1332e-02,\n         0.0000e+00, -7.6042e-03,  0.0000e+00, -7.2280e-03,  0.0000e+00,\n        -3.9169e-03,  0.0000e+00, -1.4916e-02,  0.0000e+00, -1.7514e-02,\n         0.0000e+00,  5.0703e-03,  0.0000e+00, -3.2169e-03,  0.0000e+00,\n         7.6535e-03,  0.0000e+00,  1.2361e-02,  0.0000e+00, -4.3121e-03,\n         0.0000e+00,  8.3459e-03,  0.0000e+00, -4.8980e-03,  0.0000e+00,\n         1.3433e-02,  0.0000e+00, -8.6115e-04,  0.0000e+00, -2.1512e-03,\n         0.0000e+00, -4.4849e-03,  0.0000e+00, -2.7590e-02,  0.0000e+00,\n        -4.0599e-03,  0.0000e+00,  9.4524e-03,  0.0000e+00, -7.1164e-03,\n         0.0000e+00,  4.1758e-03,  0.0000e+00,  7.2943e-03,  0.0000e+00,\n        -3.8423e-03,  0.0000e+00, -5.5999e-04,  0.0000e+00, -6.8715e-04,\n         0.0000e+00, -1.2236e-02,  0.0000e+00, -5.8266e-03,  0.0000e+00,\n         6.1529e-03,  0.0000e+00,  1.0693e-02,  0.0000e+00, -1.6101e-03,\n         0.0000e+00,  3.3719e-03,  0.0000e+00,  3.5838e-03,  0.0000e+00,\n         1.3298e-02,  0.0000e+00, -3.3981e-03,  0.0000e+00, -8.2592e-04,\n         0.0000e+00,  2.1603e-02,  0.0000e+00,  3.5283e-03,  0.0000e+00,\n         7.3548e-03,  0.0000e+00, -1.0942e-02,  0.0000e+00,  2.2252e-03,\n         0.0000e+00,  1.0406e-02,  0.0000e+00,  5.8080e-03,  0.0000e+00,\n        -3.9943e-03,  0.0000e+00, -1.6086e-02,  0.0000e+00, -5.6969e-03,\n         0.0000e+00, -1.6970e-02,  0.0000e+00, -1.1100e-03,  0.0000e+00,\n        -3.9166e-04,  0.0000e+00,  1.3918e-03,  0.0000e+00, -1.9495e-03,\n         0.0000e+00,  4.7042e-03,  0.0000e+00,  2.9621e-05,  0.0000e+00,\n        -5.4530e-03,  0.0000e+00, -8.2888e-03,  0.0000e+00, -1.0266e-02,\n         0.0000e+00, -6.2465e-03,  0.0000e+00, -8.3346e-03,  0.0000e+00,\n        -6.1560e-03,  0.0000e+00, -2.2404e-02,  0.0000e+00,  7.3079e-03,\n         0.0000e+00,  1.8717e-03,  0.0000e+00,  1.2070e-02,  0.0000e+00,\n         1.5478e-02,  0.0000e+00, -4.9370e-03,  0.0000e+00,  5.4135e-03,\n         0.0000e+00, -1.3386e-03,  0.0000e+00,  1.0103e-02,  0.0000e+00,\n         2.7275e-02,  0.0000e+00, -1.2419e-02,  0.0000e+00, -3.6265e-03,\n         0.0000e+00, -1.7646e-03,  0.0000e+00, -5.4284e-03,  0.0000e+00,\n         1.5902e-02,  0.0000e+00, -6.0788e-03,  0.0000e+00,  4.7590e-03,\n         0.0000e+00,  4.5706e-03,  0.0000e+00, -1.8449e-03,  0.0000e+00,\n        -5.9970e-03,  0.0000e+00, -1.7728e-03,  0.0000e+00,  1.2994e-02,\n         0.0000e+00,  4.9249e-03,  0.0000e+00, -1.9653e-02,  0.0000e+00,\n         1.5392e-02,  0.0000e+00, -1.9373e-02,  0.0000e+00,  9.4782e-04,\n         0.0000e+00,  5.2038e-03,  0.0000e+00, -1.1134e-02,  0.0000e+00,\n         1.2571e-02,  0.0000e+00,  9.5075e-03,  0.0000e+00, -1.0635e-02,\n         0.0000e+00,  2.7443e-03,  0.0000e+00,  7.0436e-04,  0.0000e+00,\n         4.0581e-03,  0.0000e+00, -6.5786e-03,  0.0000e+00,  1.0059e-02,\n         0.0000e+00,  1.3766e-02,  0.0000e+00,  3.8785e-03,  0.0000e+00,\n        -1.2523e-02,  0.0000e+00, -2.0606e-02,  0.0000e+00, -4.4753e-03,\n         0.0000e+00, -1.3468e-03,  0.0000e+00, -7.6805e-03,  0.0000e+00,\n        -2.4027e-03,  0.0000e+00], device='cuda:0', requires_grad=True)\nlayers.3.linear.weight\nParameter containing:\ntensor([[ 1.0428, -1.0000, -0.0038,  ...,  0.0000, -0.0205,  0.0000],\n        [-1.0000,  1.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n        [ 0.0285,  0.0000,  1.0091,  ...,  0.0000, -0.0286,  0.0000],\n        ...,\n        [ 0.0000,  0.0000,  0.0000,  ...,  1.0000,  0.0000,  0.0000],\n        [ 0.0365,  0.0000, -0.0055,  ...,  0.0000,  1.0394, -1.0000],\n        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000, -1.0000,  1.0000]],\n       device='cuda:0', requires_grad=True)\nlayers.3.linear.bias\nParameter containing:\ntensor([-1.7239e-03,  0.0000e+00, -5.5029e-03,  0.0000e+00,  5.9402e-03,\n         0.0000e+00,  1.7233e-03,  0.0000e+00,  9.7863e-03,  0.0000e+00,\n        -1.6832e-02,  0.0000e+00,  1.6889e-04,  0.0000e+00,  9.1117e-05,\n         0.0000e+00,  3.4642e-03,  0.0000e+00, -5.9661e-04,  0.0000e+00,\n         8.8420e-03,  0.0000e+00,  7.3009e-03,  0.0000e+00,  4.2667e-03,\n         0.0000e+00,  1.5185e-02,  0.0000e+00, -1.3443e-02,  0.0000e+00,\n         5.2630e-03,  0.0000e+00,  2.3087e-02,  0.0000e+00,  6.5898e-03,\n         0.0000e+00, -1.4711e-02,  0.0000e+00,  2.5327e-03,  0.0000e+00,\n        -5.9040e-03,  0.0000e+00,  1.1919e-02,  0.0000e+00, -3.8698e-03,\n         0.0000e+00,  1.2720e-02,  0.0000e+00,  1.5169e-02,  0.0000e+00,\n         1.3515e-03,  0.0000e+00,  1.3313e-02,  0.0000e+00, -9.7007e-04,\n         0.0000e+00, -5.2923e-03,  0.0000e+00, -1.2136e-02,  0.0000e+00,\n         3.1042e-03,  0.0000e+00, -7.3736e-03,  0.0000e+00, -4.5790e-05,\n         0.0000e+00,  1.2944e-02,  0.0000e+00, -6.8684e-03,  0.0000e+00,\n         8.5631e-03,  0.0000e+00,  9.4306e-03,  0.0000e+00,  7.2474e-03,\n         0.0000e+00, -8.9102e-03,  0.0000e+00, -6.3945e-03,  0.0000e+00,\n         2.6735e-03,  0.0000e+00, -6.4444e-03,  0.0000e+00, -1.4873e-03,\n         0.0000e+00,  5.4624e-03,  0.0000e+00,  3.9453e-03,  0.0000e+00,\n         1.1160e-02,  0.0000e+00,  2.4982e-03,  0.0000e+00, -9.4608e-03,\n         0.0000e+00, -7.9841e-03,  0.0000e+00, -1.4361e-02,  0.0000e+00,\n        -6.3680e-03,  0.0000e+00, -1.3580e-02,  0.0000e+00, -6.8920e-03,\n         0.0000e+00, -7.4771e-03,  0.0000e+00, -2.2409e-03,  0.0000e+00,\n        -2.3488e-03,  0.0000e+00,  4.5955e-03,  0.0000e+00, -4.4828e-03,\n         0.0000e+00, -2.2775e-02,  0.0000e+00, -1.9429e-02,  0.0000e+00,\n        -1.7653e-03,  0.0000e+00,  3.5734e-03,  0.0000e+00, -2.3574e-03,\n         0.0000e+00, -5.3923e-03,  0.0000e+00,  7.0328e-03,  0.0000e+00,\n         1.1323e-02,  0.0000e+00,  1.3768e-02,  0.0000e+00,  1.6538e-02,\n         0.0000e+00,  7.7429e-04,  0.0000e+00, -1.1931e-03,  0.0000e+00,\n         1.5307e-02,  0.0000e+00,  7.3076e-03,  0.0000e+00, -1.5013e-02,\n         0.0000e+00,  9.3867e-03,  0.0000e+00, -2.0653e-02,  0.0000e+00,\n        -5.7372e-03,  0.0000e+00, -9.8693e-03,  0.0000e+00,  1.5610e-02,\n         0.0000e+00, -1.1136e-02,  0.0000e+00,  1.5905e-03,  0.0000e+00,\n         1.2715e-02,  0.0000e+00,  2.0032e-03,  0.0000e+00, -6.1014e-03,\n         0.0000e+00, -4.0568e-03,  0.0000e+00, -2.8922e-04,  0.0000e+00,\n        -3.2093e-03,  0.0000e+00, -1.1783e-02,  0.0000e+00,  1.6458e-02,\n         0.0000e+00, -9.7311e-03,  0.0000e+00,  9.5985e-03,  0.0000e+00,\n         9.9308e-03,  0.0000e+00,  1.2073e-02,  0.0000e+00, -8.4789e-03,\n         0.0000e+00, -2.2418e-03,  0.0000e+00, -7.0011e-03,  0.0000e+00,\n        -3.4000e-03,  0.0000e+00,  2.5202e-03,  0.0000e+00,  2.5214e-03,\n         0.0000e+00,  6.7178e-03,  0.0000e+00,  3.9978e-03,  0.0000e+00,\n         4.5049e-03,  0.0000e+00, -9.1263e-03,  0.0000e+00, -5.0318e-03,\n         0.0000e+00, -1.1846e-02,  0.0000e+00, -6.5258e-03,  0.0000e+00,\n        -1.1323e-02,  0.0000e+00, -2.4886e-03,  0.0000e+00, -3.3655e-03,\n         0.0000e+00, -2.3154e-03,  0.0000e+00, -1.4069e-02,  0.0000e+00,\n        -9.0176e-03,  0.0000e+00, -2.0197e-04,  0.0000e+00, -1.7716e-02,\n         0.0000e+00, -1.8181e-02,  0.0000e+00, -8.4978e-03,  0.0000e+00,\n         2.1673e-03,  0.0000e+00, -4.9388e-03,  0.0000e+00, -7.0536e-03,\n         0.0000e+00,  1.1825e-02,  0.0000e+00,  1.1436e-02,  0.0000e+00,\n         1.4372e-02,  0.0000e+00,  1.7988e-02,  0.0000e+00, -1.5136e-03,\n         0.0000e+00,  2.6628e-03,  0.0000e+00,  7.7834e-03,  0.0000e+00,\n         6.4045e-03,  0.0000e+00,  1.5338e-02,  0.0000e+00,  4.0184e-04,\n         0.0000e+00, -4.7154e-03,  0.0000e+00, -2.5518e-02,  0.0000e+00,\n         8.4501e-03,  0.0000e+00,  1.5250e-02,  0.0000e+00,  1.1075e-02,\n         0.0000e+00,  1.2290e-03,  0.0000e+00,  1.8671e-02,  0.0000e+00,\n         3.5828e-03,  0.0000e+00, -8.5542e-03,  0.0000e+00,  2.8051e-03,\n         0.0000e+00,  8.2358e-03,  0.0000e+00, -2.9075e-03,  0.0000e+00,\n        -7.4208e-03,  0.0000e+00,  1.4884e-02,  0.0000e+00, -9.3785e-03,\n         0.0000e+00,  8.9148e-03,  0.0000e+00,  8.9159e-03,  0.0000e+00,\n         1.1056e-02,  0.0000e+00, -8.2879e-03,  0.0000e+00, -7.0870e-03,\n         0.0000e+00, -4.3137e-03,  0.0000e+00, -5.5251e-03,  0.0000e+00,\n         3.8214e-03,  0.0000e+00,  3.7553e-03,  0.0000e+00,  7.7115e-03,\n         0.0000e+00,  3.2954e-03,  0.0000e+00,  3.6377e-03,  0.0000e+00,\n        -5.4515e-03,  0.0000e+00, -2.2093e-02,  0.0000e+00, -1.2935e-02,\n         0.0000e+00, -7.6265e-03,  0.0000e+00, -9.4587e-03,  0.0000e+00,\n        -1.1678e-02,  0.0000e+00, -8.6569e-03,  0.0000e+00, -2.4270e-03,\n         0.0000e+00, -2.2076e-03,  0.0000e+00, -1.2796e-02,  0.0000e+00,\n        -1.0240e-02,  0.0000e+00, -2.2711e-02,  0.0000e+00, -1.0094e-02,\n         0.0000e+00,  1.2541e-03,  0.0000e+00, -9.9777e-05,  0.0000e+00,\n        -1.2416e-02,  0.0000e+00, -2.0518e-02,  0.0000e+00,  1.1232e-02,\n         0.0000e+00,  4.6697e-03,  0.0000e+00,  1.2933e-02,  0.0000e+00,\n         1.8137e-02,  0.0000e+00, -1.6043e-02,  0.0000e+00,  7.8443e-03,\n         0.0000e+00, -7.9775e-03,  0.0000e+00,  7.4163e-03,  0.0000e+00,\n         2.4700e-04,  0.0000e+00, -5.9648e-03,  0.0000e+00, -6.5768e-03,\n         0.0000e+00, -1.8297e-02,  0.0000e+00,  2.6374e-03,  0.0000e+00,\n         1.6018e-02,  0.0000e+00, -1.9301e-05,  0.0000e+00,  1.2917e-03,\n         0.0000e+00,  2.4319e-03,  0.0000e+00, -4.3143e-03,  0.0000e+00,\n        -2.3160e-02,  0.0000e+00, -2.9923e-03,  0.0000e+00, -8.7520e-03,\n         0.0000e+00,  1.2221e-03,  0.0000e+00, -1.5404e-02,  0.0000e+00,\n         1.6648e-02,  0.0000e+00, -2.5603e-02,  0.0000e+00,  5.2835e-03,\n         0.0000e+00,  9.8650e-03,  0.0000e+00, -4.0376e-03,  0.0000e+00,\n        -7.8221e-03,  0.0000e+00, -8.4361e-03,  0.0000e+00,  3.8138e-04,\n         0.0000e+00, -8.0345e-03,  0.0000e+00, -1.2843e-03,  0.0000e+00,\n         3.9389e-03,  0.0000e+00,  4.1956e-04,  0.0000e+00,  9.4966e-03,\n         0.0000e+00,  7.1735e-03,  0.0000e+00, -2.5800e-02,  0.0000e+00,\n        -5.9364e-03,  0.0000e+00, -4.4049e-03,  0.0000e+00, -7.0025e-03,\n         0.0000e+00, -1.1779e-02,  0.0000e+00, -9.3225e-03,  0.0000e+00,\n        -1.0205e-02,  0.0000e+00, -2.3135e-03,  0.0000e+00, -3.4499e-03,\n         0.0000e+00, -1.2718e-02,  0.0000e+00, -1.0432e-02,  0.0000e+00,\n        -1.1419e-02,  0.0000e+00, -1.6600e-02,  0.0000e+00, -1.3347e-02,\n         0.0000e+00,  4.1492e-03,  0.0000e+00, -1.3156e-02,  0.0000e+00,\n        -7.8895e-03,  0.0000e+00,  7.2386e-03,  0.0000e+00, -4.6614e-03,\n         0.0000e+00,  9.0655e-03,  0.0000e+00,  2.1770e-02,  0.0000e+00,\n        -1.1527e-02,  0.0000e+00, -1.0266e-02,  0.0000e+00,  6.2560e-03,\n         0.0000e+00,  8.0564e-03,  0.0000e+00, -3.8562e-03,  0.0000e+00,\n         4.0880e-03,  0.0000e+00, -7.0980e-03,  0.0000e+00,  7.1200e-05,\n         0.0000e+00,  2.4735e-03,  0.0000e+00,  1.4929e-02,  0.0000e+00,\n         4.1046e-03,  0.0000e+00,  1.5498e-03,  0.0000e+00,  7.7357e-03,\n         0.0000e+00,  1.0864e-02,  0.0000e+00, -4.2522e-03,  0.0000e+00,\n        -5.5160e-03,  0.0000e+00, -8.2499e-03,  0.0000e+00, -6.4695e-03,\n         0.0000e+00, -9.8465e-03,  0.0000e+00,  1.8210e-02,  0.0000e+00,\n        -3.0765e-03,  0.0000e+00,  6.6634e-03,  0.0000e+00,  7.0849e-03,\n         0.0000e+00,  1.2222e-02,  0.0000e+00, -1.2308e-03,  0.0000e+00,\n        -1.0662e-02,  0.0000e+00, -7.3941e-03,  0.0000e+00,  5.2296e-03,\n         0.0000e+00,  3.2702e-04,  0.0000e+00,  3.8527e-03,  0.0000e+00,\n         1.2576e-02,  0.0000e+00,  1.1174e-02,  0.0000e+00,  3.5744e-03,\n         0.0000e+00, -9.1975e-03,  0.0000e+00, -3.4856e-03,  0.0000e+00,\n        -8.7011e-03,  0.0000e+00, -5.4174e-03,  0.0000e+00, -1.0826e-02,\n         0.0000e+00, -5.0494e-03,  0.0000e+00, -1.3983e-02,  0.0000e+00,\n        -2.7692e-03,  0.0000e+00, -3.8985e-03,  0.0000e+00, -5.4023e-03,\n         0.0000e+00, -7.3980e-03,  0.0000e+00, -1.8628e-02,  0.0000e+00,\n        -1.4715e-02,  0.0000e+00,  2.6761e-03,  0.0000e+00, -3.0202e-03,\n         0.0000e+00, -2.8054e-02,  0.0000e+00, -1.2483e-02,  0.0000e+00,\n         6.6187e-03,  0.0000e+00, -1.1302e-02,  0.0000e+00,  9.7588e-03,\n         0.0000e+00,  1.6902e-02,  0.0000e+00, -1.4776e-02,  0.0000e+00,\n         2.7017e-03,  0.0000e+00, -2.5875e-03,  0.0000e+00,  2.2027e-03,\n         0.0000e+00,  6.9462e-03,  0.0000e+00,  1.1982e-02,  0.0000e+00,\n        -4.4136e-03,  0.0000e+00, -5.9292e-03,  0.0000e+00, -8.5835e-03,\n         0.0000e+00,  1.7832e-02,  0.0000e+00,  7.1288e-03,  0.0000e+00,\n         2.1388e-03,  0.0000e+00, -7.1761e-03,  0.0000e+00,  1.0073e-02,\n         0.0000e+00, -5.7693e-03,  0.0000e+00,  2.3847e-02,  0.0000e+00,\n        -1.9356e-03,  0.0000e+00, -6.5508e-03,  0.0000e+00, -2.0316e-02,\n         0.0000e+00,  1.2342e-02,  0.0000e+00, -5.9330e-03,  0.0000e+00,\n         4.1901e-03,  0.0000e+00,  9.7631e-03,  0.0000e+00, -2.9857e-04,\n         0.0000e+00, -1.7869e-02,  0.0000e+00, -1.3747e-02,  0.0000e+00,\n         2.5334e-03,  0.0000e+00,  7.0323e-03,  0.0000e+00,  5.5238e-03,\n         0.0000e+00, -2.9668e-02,  0.0000e+00,  3.6879e-03,  0.0000e+00,\n         1.2687e-02,  0.0000e+00,  3.8589e-04,  0.0000e+00, -1.2110e-02,\n         0.0000e+00, -7.5561e-03,  0.0000e+00, -1.8170e-02,  0.0000e+00,\n        -8.6282e-03,  0.0000e+00, -1.0493e-02,  0.0000e+00, -8.6538e-03,\n         0.0000e+00, -1.2333e-02,  0.0000e+00, -2.8378e-03,  0.0000e+00,\n        -8.4009e-03,  0.0000e+00, -5.3719e-04,  0.0000e+00, -1.9449e-02,\n         0.0000e+00, -2.0773e-02,  0.0000e+00, -1.7183e-02,  0.0000e+00,\n        -3.1050e-03,  0.0000e+00, -7.6242e-03,  0.0000e+00, -2.5777e-02,\n         0.0000e+00, -1.0669e-02,  0.0000e+00,  6.4315e-03,  0.0000e+00,\n        -1.1080e-02,  0.0000e+00, -1.4704e-02,  0.0000e+00,  2.0543e-02,\n         0.0000e+00,  4.4872e-03,  0.0000e+00, -6.2034e-03,  0.0000e+00,\n        -2.4208e-02,  0.0000e+00, -4.9017e-03,  0.0000e+00,  6.4845e-03,\n         0.0000e+00, -4.1821e-03,  0.0000e+00, -1.5267e-02,  0.0000e+00,\n        -7.4616e-03,  0.0000e+00, -1.4214e-02,  0.0000e+00,  1.6980e-02,\n         0.0000e+00,  2.1343e-02,  0.0000e+00,  1.0230e-04,  0.0000e+00,\n         5.5980e-03,  0.0000e+00, -6.3218e-03,  0.0000e+00, -1.8920e-02,\n         0.0000e+00,  2.1940e-02,  0.0000e+00,  6.9685e-04,  0.0000e+00,\n         3.9523e-04,  0.0000e+00,  6.8391e-03,  0.0000e+00,  1.9614e-02,\n         0.0000e+00, -1.9999e-02,  0.0000e+00,  1.0981e-03,  0.0000e+00,\n         8.4183e-03,  0.0000e+00, -8.6948e-03,  0.0000e+00, -2.2357e-02,\n         0.0000e+00, -2.4740e-02,  0.0000e+00,  3.7865e-03,  0.0000e+00,\n        -1.1683e-02,  0.0000e+00,  6.4391e-03,  0.0000e+00, -3.0095e-02,\n         0.0000e+00,  8.1862e-03,  0.0000e+00,  1.3659e-02,  0.0000e+00,\n         4.9838e-03,  0.0000e+00, -3.8174e-03,  0.0000e+00, -4.7798e-03,\n         0.0000e+00,  4.2106e-03,  0.0000e+00, -7.7527e-03,  0.0000e+00,\n        -3.3983e-03,  0.0000e+00, -1.4124e-03,  0.0000e+00, -1.2317e-02,\n         0.0000e+00, -3.0658e-03,  0.0000e+00,  1.2121e-02,  0.0000e+00,\n        -3.8924e-03,  0.0000e+00, -2.1392e-02,  0.0000e+00, -1.5484e-02,\n         0.0000e+00, -2.0233e-02,  0.0000e+00,  5.2136e-03,  0.0000e+00,\n         5.6975e-04,  0.0000e+00, -3.2067e-02,  0.0000e+00, -1.2172e-02,\n         0.0000e+00,  9.1833e-03,  0.0000e+00,  2.6756e-03,  0.0000e+00,\n         2.5328e-03,  0.0000e+00,  2.1704e-02,  0.0000e+00,  4.5936e-03,\n         0.0000e+00,  1.0021e-02,  0.0000e+00,  8.5074e-03,  0.0000e+00,\n        -2.0219e-02,  0.0000e+00, -1.6515e-02,  0.0000e+00,  8.3507e-03,\n         0.0000e+00, -2.0263e-02,  0.0000e+00, -1.6087e-02,  0.0000e+00,\n        -3.6017e-03,  0.0000e+00,  3.2143e-02,  0.0000e+00,  1.1002e-02,\n         0.0000e+00,  3.3284e-05,  0.0000e+00, -1.1376e-02,  0.0000e+00,\n        -5.8635e-03,  0.0000e+00, -1.1298e-02,  0.0000e+00, -2.2364e-03,\n         0.0000e+00,  9.0985e-03,  0.0000e+00, -6.0872e-03,  0.0000e+00,\n        -1.3930e-02,  0.0000e+00,  1.0236e-02,  0.0000e+00, -1.5551e-02,\n         0.0000e+00,  4.1434e-03,  0.0000e+00,  5.4567e-03,  0.0000e+00,\n        -3.5038e-03,  0.0000e+00,  1.5560e-03,  0.0000e+00,  3.6668e-05,\n         0.0000e+00, -6.2338e-03,  0.0000e+00, -1.4287e-03,  0.0000e+00,\n        -6.8310e-04,  0.0000e+00, -3.2316e-03,  0.0000e+00, -1.4259e-02,\n         0.0000e+00,  1.5619e-02,  0.0000e+00,  6.1284e-03,  0.0000e+00,\n         8.0631e-03,  0.0000e+00, -1.2187e-02,  0.0000e+00, -6.2181e-03,\n         0.0000e+00, -7.1535e-03,  0.0000e+00, -3.2268e-03,  0.0000e+00,\n        -1.5239e-03,  0.0000e+00, -8.8954e-03,  0.0000e+00], device='cuda:0',\n       requires_grad=True)\nlayers.4.linear.weight\nParameter containing:\ntensor([[ 1.0385, -1.0000, -0.0029,  ...,  0.0000,  0.0199,  0.0000],\n        [-1.0000,  1.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n        [ 0.0226,  0.0000,  1.0133,  ...,  0.0000,  0.0101,  0.0000],\n        ...,\n        [ 0.0000,  0.0000,  0.0000,  ...,  1.0000,  0.0000,  0.0000],\n        [-0.0028,  0.0000, -0.0198,  ...,  0.0000,  1.0362, -1.0000],\n        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000, -1.0000,  1.0000]],\n       device='cuda:0', requires_grad=True)\nlayers.4.linear.bias\nParameter containing:\ntensor([ 0.0009,  0.0000, -0.0023,  ...,  0.0000, -0.0039,  0.0000],\n       device='cuda:0', requires_grad=True)\nlayers.5.linear.weight\nParameter containing:\ntensor([[ 1.0380, -1.0000, -0.0014,  ...,  0.0000, -0.0552,  0.0000],\n        [-1.0000,  1.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n        [ 0.0095,  0.0000,  1.0229,  ...,  0.0000, -0.2145,  0.0000],\n        ...,\n        [ 0.0000,  0.0000,  0.0000,  ...,  1.0000,  0.0000,  0.0000],\n        [-0.0206,  0.0000, -0.0039,  ...,  0.0000,  0.9899, -1.0000],\n        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000, -1.0000,  1.0000]],\n       device='cuda:0', requires_grad=True)\nlayers.5.linear.bias\nParameter containing:\ntensor([ 0.0016,  0.0000,  0.0059,  ...,  0.0000, -0.0149,  0.0000],\n       device='cuda:0', requires_grad=True)\nlast_layer.linear.weight\nParameter containing:\ntensor([[ 0.0216,  0.0000, -0.0041,  ...,  0.0000, -0.0010,  0.0000],\n        [-0.0153,  0.0000, -0.0024,  ...,  0.0000,  0.0025,  0.0000],\n        [ 0.0225,  0.0000,  0.0217,  ...,  0.0000, -0.0028,  0.0000],\n        ...,\n        [ 0.0244,  0.0000,  0.0222,  ...,  0.0000, -0.0061,  0.0000],\n        [-0.0048,  0.0000,  0.0250,  ...,  0.0000, -0.0054,  0.0000],\n        [ 0.0419,  0.0000,  0.0089,  ...,  0.0000,  0.0022,  0.0000]],\n       device='cuda:0', requires_grad=True)\nlast_layer.linear.bias\nParameter containing:\ntensor([-0.0032, -0.0045,  0.0229, -0.0017,  0.0228,  0.0173,  0.0069],\n       device='cuda:0', requires_grad=True)\n","output_type":"stream"}]},{"cell_type":"code","source":"optimizer = optim.Adam(model.parameters(), lr=0.001)\n\ntorch.cuda.synchronize()\nstart_time = time.time()\n\nevaluate_model(model, custom_train_loader, criterion, optimizer, 10, 1024 * 16)\n\nelapsed_time = time.time() - start_time\nprint(f\"Execution time: {elapsed_time:.6f} seconds\")","metadata":{"execution":{"iopub.status.busy":"2024-10-25T22:46:37.808584Z","iopub.execute_input":"2024-10-25T22:46:37.808946Z","iopub.status.idle":"2024-10-25T22:48:32.150966Z","shell.execute_reply.started":"2024-10-25T22:46:37.808911Z","shell.execute_reply":"2024-10-25T22:48:32.149986Z"},"trusted":true},"execution_count":34,"outputs":[{"name":"stdout","text":"Epoch 1, Training Loss: 1.307600110554671, Validation Loss: 1.1856549278817368\nTraining Accuracy: 0.48759813170571137, Training F1 Score: 0.31964538402624754\nValidation Accuracy: 0.48760359026875383, Validation F1 Score: 0.3196513678755306\n\nEpoch 2, Training Loss: 0.9911411693507082, Validation Loss: 0.7952721405237344\nTraining Accuracy: 0.6690683700186528, Training F1 Score: 0.6359367341083367\nValidation Accuracy: 0.6669277041040248, Validation F1 Score: 0.6338367587537906\n\nEpoch 3, Training Loss: 0.7557137665118112, Validation Loss: 0.7275613861462613\nTraining Accuracy: 0.694726220877823, Training F1 Score: 0.6756646906635713\nValidation Accuracy: 0.692649931585243, Validation F1 Score: 0.6735216103720431\n\nEpoch 4, Training Loss: 0.7035255324110403, Validation Loss: 0.7264274120680633\nTraining Accuracy: 0.694420719048039, Training F1 Score: 0.665369836713696\nValidation Accuracy: 0.6925380583978038, Validation F1 Score: 0.6634341709434263\n\nEpoch 5, Training Loss: 0.6919250369417937, Validation Loss: 0.6794205591136816\nTraining Accuracy: 0.711879503193785, Training F1 Score: 0.6944658161266837\nValidation Accuracy: 0.7095341772587627, Validation F1 Score: 0.6920167789258129\n\nEpoch 6, Training Loss: 0.6713349439709929, Validation Loss: 0.6732845560130292\nTraining Accuracy: 0.7159456895197812, Training F1 Score: 0.6949784704966351\nValidation Accuracy: 0.7143619355782553, Validation F1 Score: 0.6933335993483508\n\nEpoch 7, Training Loss: 0.6676162276622313, Validation Loss: 0.6621454276528758\nTraining Accuracy: 0.7183380700459758, Training F1 Score: 0.7009741043159913\nValidation Accuracy: 0.7165305542886156, Validation F1 Score: 0.6991408472876103\n\nEpoch 8, Training Loss: 0.6570881444943417, Validation Loss: 0.6656963980706841\nTraining Accuracy: 0.7128196743178381, Training F1 Score: 0.6917259795037609\nValidation Accuracy: 0.7113843876664113, Validation F1 Score: 0.6904230100320844\n\nEpoch 9, Training Loss: 0.6565727318632418, Validation Loss: 0.6532329156860559\nTraining Accuracy: 0.716399639421784, Training F1 Score: 0.7046040417645962\nValidation Accuracy: 0.715213892928754, Validation F1 Score: 0.7034661880325563\n\nEpoch 10, Training Loss: 0.6489371429447157, Validation Loss: 0.6550490196007268\nTraining Accuracy: 0.7171892110522817, Training F1 Score: 0.6981683864485292\nValidation Accuracy: 0.7157044138275259, Validation F1 Score: 0.6967164392953092\n\nExecution time: 114.336429 seconds\n","output_type":"stream"}]},{"cell_type":"code","source":"optimizer = optim.Adam(model.parameters(), lr=0.001)\n\ntorch.cuda.synchronize()\nstart_time = time.time()\n\nevaluate_model(model, custom_train_loader, criterion, optimizer, 10, 1024 * 64)\n\nelapsed_time = time.time() - start_time\nprint(f\"Execution time: {elapsed_time:.6f} seconds\")","metadata":{"execution":{"iopub.status.busy":"2024-10-25T22:44:57.723430Z","iopub.execute_input":"2024-10-25T22:44:57.723817Z","iopub.status.idle":"2024-10-25T22:45:19.859125Z","shell.execute_reply.started":"2024-10-25T22:44:57.723781Z","shell.execute_reply":"2024-10-25T22:45:19.858040Z"},"trusted":true},"execution_count":29,"outputs":[{"name":"stdout","text":"Epoch 1, Training Loss: 1.8057236788097735, Validation Loss: 1.205128083886402\nTraining Accuracy: 0.5748275097943456, Training F1 Score: 0.532677902978648\nValidation Accuracy: 0.5737803671161674, Validation F1 Score: 0.5316521221622602\n\nEpoch 2, Training Loss: 1.0996317938779163, Validation Loss: 0.8743102936598277\nTraining Accuracy: 0.5977143299720961, Training F1 Score: 0.5214867473120899\nValidation Accuracy: 0.5950620896190287, Validation F1 Score: 0.5181048741384898\n\nEpoch 3, Training Loss: 0.7885691515017922, Validation Loss: 0.6949502639413176\nTraining Accuracy: 0.6891949166216661, Training F1 Score: 0.6746762224566707\nValidation Accuracy: 0.6881663984578711, Validation F1 Score: 0.673683985072721\n\nEpoch 4, Training Loss: 0.7210023260143712, Validation Loss: 0.7061048918152707\nTraining Accuracy: 0.7011890905726869, Training F1 Score: 0.6712697614928861\nValidation Accuracy: 0.6990611257884908, Validation F1 Score: 0.669062541191424\n\nEpoch 5, Training Loss: 0.6800031598499585, Validation Loss: 0.6806172465715489\nTraining Accuracy: 0.7074454238192461, Training F1 Score: 0.6844765493481739\nValidation Accuracy: 0.7057821226646472, Validation F1 Score: 0.6829197551902908\n\nEpoch 6, Training Loss: 0.6687077587608797, Validation Loss: 0.6644767260809562\nTraining Accuracy: 0.7135317947802216, Training F1 Score: 0.6978902085047243\nValidation Accuracy: 0.7111262187723209, Validation F1 Score: 0.6954051893974471\n\nEpoch 7, Training Loss: 0.65998355625635, Validation Loss: 0.6560055215248987\nTraining Accuracy: 0.7197601595494063, Training F1 Score: 0.6998319087757214\nValidation Accuracy: 0.7189315250036574, Validation F1 Score: 0.6988980191653685\n\nEpoch 8, Training Loss: 0.6528117292259604, Validation Loss: 0.6535614889476427\nTraining Accuracy: 0.7217781927630489, Training F1 Score: 0.7070074463504991\nValidation Accuracy: 0.7203944820701703, Validation F1 Score: 0.7053050091763746\n\nEpoch 9, Training Loss: 0.6493537404454007, Validation Loss: 0.6506796735528773\nTraining Accuracy: 0.7212252774795669, Training F1 Score: 0.7042400598634029\nValidation Accuracy: 0.719706031685929, Validation F1 Score: 0.7026555179623073\n\nEpoch 10, Training Loss: 0.6466265466089614, Validation Loss: 0.6484154391341898\nTraining Accuracy: 0.7226581240896799, Training F1 Score: 0.7094534519968434\nValidation Accuracy: 0.7214099463869263, Validation F1 Score: 0.7079455561295365\n\nExecution time: 22.128110 seconds\n","output_type":"stream"}]},{"cell_type":"code","source":"optimizer = optim.Adam(model.parameters(), lr=0.0001)\n\ntorch.cuda.synchronize()\nstart_time = time.time()\n\nevaluate_model(model, custom_train_loader, criterion, optimizer, 10, 1024 * 64)\n\nelapsed_time = time.time() - start_time\nprint(f\"Execution time: {elapsed_time:.6f} seconds\")","metadata":{"execution":{"iopub.status.busy":"2024-10-25T22:45:35.475009Z","iopub.execute_input":"2024-10-25T22:45:35.475903Z","iopub.status.idle":"2024-10-25T22:45:57.517692Z","shell.execute_reply.started":"2024-10-25T22:45:35.475862Z","shell.execute_reply":"2024-10-25T22:45:57.516727Z"},"trusted":true},"execution_count":30,"outputs":[{"name":"stdout","text":"Epoch 1, Training Loss: 0.6482883245601477, Validation Loss: 0.65033711338891\nTraining Accuracy: 0.7215953219494459, Training F1 Score: 0.7052375619531768\nValidation Accuracy: 0.7193704121236113, Validation F1 Score: 0.702937030999159\n\nEpoch 2, Training Loss: 0.6444209904029583, Validation Loss: 0.644916022551123\nTraining Accuracy: 0.7231981308451428, Training F1 Score: 0.7093322124349345\nValidation Accuracy: 0.7218402278770771, Validation F1 Score: 0.707716455939064\n\nEpoch 3, Training Loss: 0.6415352971006409, Validation Loss: 0.6437382442899314\nTraining Accuracy: 0.7228151778472448, Training F1 Score: 0.7109572041503743\nValidation Accuracy: 0.7216767209108199, Validation F1 Score: 0.709661205516383\n\nEpoch 4, Training Loss: 0.6395893395138763, Validation Loss: 0.6416136971650775\nTraining Accuracy: 0.7237101691232313, Training F1 Score: 0.7118557533517296\nValidation Accuracy: 0.722279114997031, Validation F1 Score: 0.7102082487662996\n\nEpoch 5, Training Loss: 0.6377461002811073, Validation Loss: 0.6399533749155033\nTraining Accuracy: 0.7240759107504373, Training F1 Score: 0.7118815988233035\nValidation Accuracy: 0.7225458895209246, Validation F1 Score: 0.7102408827010267\n\nEpoch 6, Training Loss: 0.6361884023895218, Validation Loss: 0.6385954779474694\nTraining Accuracy: 0.724346989838837, Training F1 Score: 0.7122267316575527\nValidation Accuracy: 0.722761030266, Validation F1 Score: 0.7104915377498504\n\nEpoch 7, Training Loss: 0.6348622786259834, Validation Loss: 0.637367222561128\nTraining Accuracy: 0.7244545609056623, Training F1 Score: 0.7127233697344064\nValidation Accuracy: 0.7233462130926052, Validation F1 Score: 0.7115349261789706\n\nEpoch 8, Training Loss: 0.6337136522309363, Validation Loss: 0.6363311844647711\nTraining Accuracy: 0.7246223717699098, Training F1 Score: 0.7132729004872475\nValidation Accuracy: 0.7235613538376806, Validation F1 Score: 0.7121081572841205\n\nEpoch 9, Training Loss: 0.6327641943486839, Validation Loss: 0.6354510091178429\nTraining Accuracy: 0.7249171164930112, Training F1 Score: 0.7138496822752333\nValidation Accuracy: 0.7238281283615742, Validation F1 Score: 0.7126812919990484\n\nEpoch 10, Training Loss: 0.6319759255752183, Validation Loss: 0.634703998236759\nTraining Accuracy: 0.7252183154801219, Training F1 Score: 0.7143845016364684\nValidation Accuracy: 0.7240346634768465, Validation F1 Score: 0.7131733611922708\n\nExecution time: 22.035221 seconds\n","output_type":"stream"}]},{"cell_type":"code","source":"optimizer = optim.Adam(model.parameters(), lr=0.001)\n\ntorch.cuda.synchronize()\nstart_time = time.time()\n\nevaluate_model(model, custom_train_loader, criterion, optimizer, 10, 1024 * 8)\n\nelapsed_time = time.time() - start_time\nprint(f\"Execution time: {elapsed_time:.6f} seconds\")","metadata":{"execution":{"iopub.status.busy":"2024-10-25T22:22:18.276543Z","iopub.execute_input":"2024-10-25T22:22:18.276950Z","iopub.status.idle":"2024-10-25T22:25:22.665425Z","shell.execute_reply.started":"2024-10-25T22:22:18.276916Z","shell.execute_reply":"2024-10-25T22:25:22.664084Z"},"collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":29,"outputs":[{"name":"stdout","text":"1.9459153413772583\n1.7211235761642456\n1.8293129205703735\n1.6529580354690552\n1.875167727470398\n1.629576325416565\n1.639758586883545\n1.4842135906219482\n1.396589756011963\n1.3132436275482178\n1.307774305343628\n1.5233104228973389\n1.513757348060608\n1.5241475105285645\n1.449304461479187\n1.3725199699401855\n1.195020079612732\n1.147672176361084\n1.5728925466537476\n1.0980587005615234\n1.1427743434906006\n1.1603469848632812\n1.219657301902771\n1.184799313545227\n1.119278907775879\n1.1146836280822754\n1.167554497718811\n1.1407761573791504\n1.0683541297912598\n1.019402265548706\n1.0105711221694946\n1.0071899890899658\n0.9998883008956909\n1.0079543590545654\n0.9901393055915833\n0.9908040165901184\n0.995794951915741\n0.979019284248352\n0.9791274666786194\n0.9843977093696594\n0.9760539531707764\n0.9427629709243774\n0.9618619680404663\n0.9578478336334229\n0.9528827667236328\n0.9406067132949829\n0.9433158040046692\n0.9316280484199524\n0.936485230922699\n0.9138314127922058\n0.9193710088729858\n0.9140663146972656\n0.9162853360176086\n0.9132581353187561\n0.9087826013565063\n0.9004576206207275\n0.9079145193099976\nEpoch 1, Training Loss: 1.1821705248261747, Validation Loss: 0.9017464874934649\nTraining Accuracy: 0.5321583704274229, Training F1 Score: 0.37746892513718233\nValidation Accuracy: 0.5313201896680808, Validation F1 Score: 0.3768451204418106\n\n0.9133857488632202\n0.88712477684021\n0.9079464673995972\n0.8786668181419373\n0.886823832988739\n0.9048061966896057\n0.8754847645759583\n0.8812323808670044\n0.8591244220733643\n0.8782084584236145\n0.8676658272743225\n0.8563917279243469\n0.8571519255638123\n0.8687681555747986\n0.8271817564964294\n0.8573375344276428\n0.8748981952667236\n0.847097635269165\n0.8283848762512207\n0.8403872847557068\n0.8343696594238281\n0.8370721340179443\n0.8534173369407654\n0.831762433052063\n0.8146287798881531\n0.8238430023193359\n0.8249449729919434\n0.799240231513977\n0.8018748760223389\n0.8037385940551758\n0.8154358267784119\n0.8125821948051453\n0.8101304173469543\n0.8422639966011047\n0.8057314157485962\n0.8087025284767151\n0.7890768051147461\n0.7864916920661926\n0.8349080681800842\n0.9670194983482361\n0.9408928155899048\n0.7791545391082764\n0.8205921649932861\n0.8660048246383667\n0.8019200563430786\n0.889962375164032\n0.7959867715835571\n0.8664832711219788\n0.801379919052124\n0.8585792183876038\n0.7966868281364441\n0.8553293943405151\n0.7960421442985535\n0.8399681448936462\n0.7819774150848389\n0.8402913212776184\n0.7833320498466492\nEpoch 2, Training Loss: 0.8425494693690363, Validation Loss: 0.8121798680365031\nTraining Accuracy: 0.6522076810044556, Training F1 Score: 0.620371235323394\nValidation Accuracy: 0.6523841897369259, Validation F1 Score: 0.6205135720638418\n\n0.8143486380577087\n0.7793270945549011\n0.7878173589706421\n0.7857553362846375\n0.7551807761192322\n0.7809087634086609\n0.7822860479354858\n0.7480477690696716\n0.7496415972709656\n0.7615989446640015\n0.7371546626091003\n0.7377171516418457\n0.7710978388786316\n0.8053349256515503\n0.7873377799987793\n0.8382079005241394\n0.7672658562660217\n0.7375617623329163\n0.7502925395965576\n0.7647984027862549\n0.7980818748474121\n0.7526378631591797\n0.7508722543716431\n0.8195764422416687\n0.8051065802574158\n0.7429699301719666\n0.7424045205116272\n0.7404130697250366\n0.7362485527992249\n0.7198135852813721\n0.7162980437278748\n0.7419934272766113\n0.73193359375\n0.7333527207374573\n0.7029129266738892\n0.7225480675697327\n0.7283950448036194\n0.7270557284355164\n0.7327434420585632\n0.72800213098526\n0.7290679216384888\n0.6871160268783569\n0.7024412155151367\n0.6983168721199036\n0.6992475390434265\n0.7082683444023132\n0.7075133323669434\n0.6959559917449951\n0.7126079201698303\n0.6991838216781616\n0.746497392654419\n0.8551938533782959\n1.0407792329788208\n0.7362483143806458\n0.7445343136787415\n0.9181450009346008\n0.7327769994735718\nEpoch 3, Training Loss: 0.7567576293616682, Validation Loss: 0.8711072370236466\nTraining Accuracy: 0.6184776972907151, Training F1 Score: 0.5951795183796955\nValidation Accuracy: 0.6164040515305113, Validation F1 Score: 0.5928049813474686\n\n0.8717121481895447\n0.7625643610954285\n0.8347501158714294\n0.7408841252326965\n0.843085527420044\n0.747543215751648\n0.8662691712379456\n0.7512957453727722\n0.7903832197189331\n0.7666417956352234\n0.7835971713066101\n0.7480990886688232\n0.7748139500617981\n0.7661831974983215\n0.7178018093109131\n0.7476569414138794\n0.7419782280921936\n0.7435086369514465\n0.7390154600143433\n0.7204705476760864\n0.7358365058898926\n0.7244966626167297\n0.7314500212669373\n0.7206047177314758\n0.7197480201721191\n0.7193335294723511\n0.7271759510040283\n0.7027417421340942\n0.7052708864212036\n0.6983981728553772\n0.704735517501831\n0.7049463391304016\n0.698845386505127\n0.7194703221321106\n0.6951478719711304\n0.7134574055671692\n0.7008743286132812\n0.6947189569473267\n0.6938210725784302\n0.7079896330833435\n0.7078868746757507\n0.6788877844810486\n0.7088194489479065\n0.7053380012512207\n0.7007603645324707\n0.7126426696777344\n0.7106685042381287\n0.7007856965065002\n0.7107728719711304\n0.6925312280654907\n0.7095058560371399\n0.6947970390319824\n0.7067562341690063\n0.6894262433052063\n0.6969889402389526\n0.7022041082382202\n0.7179440855979919\nEpoch 4, Training Loss: 0.7303027080123197, Validation Loss: 0.7033557478588953\nTraining Accuracy: 0.7143902118934874, Training F1 Score: 0.694933746315569\nValidation Accuracy: 0.7133464712614993, Validation F1 Score: 0.6938855253446047\n\n0.7018617391586304\n0.7009484171867371\n0.7309516072273254\n0.6968466639518738\n0.6969197988510132\n0.7237172722816467\n0.7288473844528198\n0.7019983530044556\n0.7002246975898743\n0.7108591794967651\n0.6996890306472778\n0.6957645416259766\n0.7142707109451294\n0.7187317609786987\n0.6956004500389099\n0.7117809057235718\n0.70310378074646\n0.7149626016616821\n0.7169985175132751\n0.7096678018569946\n0.7131718397140503\n0.7097287178039551\n0.7246291041374207\n0.7292733192443848\n0.7031104564666748\n0.7245643734931946\n0.7410220503807068\n0.6947567462921143\n0.7097572684288025\n0.7481021881103516\n0.7337783575057983\n0.7107706665992737\n0.6941086649894714\n0.732059895992279\n0.6924222111701965\n0.707506537437439\n0.7231422662734985\n0.6931687593460083\n0.6936437487602234\n0.7048438787460327\n0.7076263427734375\n0.6679494976997375\n0.6896337270736694\n0.6913332343101501\n0.6923850774765015\n0.7076491117477417\n0.6994699239730835\n0.6919549107551575\n0.701522707939148\n0.6789325475692749\n0.6945490837097168\n0.6855735778808594\n0.6945018768310547\n0.6758189797401428\n0.6965670585632324\n0.7143720984458923\n0.728364884853363\nEpoch 5, Training Loss: 0.7064878828129416, Validation Loss: 0.7019794450270277\nTraining Accuracy: 0.7102100002366564, Training F1 Score: 0.6917633968401667\nValidation Accuracy: 0.7079593470048106, Validation F1 Score: 0.6894747742426469\n\n0.6995300054550171\n0.6687338948249817\n0.7026953101158142\n0.6780915260314941\n0.6938965916633606\n0.6879370808601379\n0.6988824009895325\n0.685530424118042\n0.6961792707443237\n0.7471572160720825\n0.7579497694969177\n0.7275097966194153\n0.7072783708572388\n0.730379581451416\n0.7032691240310669\n0.7070744633674622\n0.7180943489074707\n0.7453377842903137\n0.7158488035202026\n0.7081576585769653\n0.7289052605628967\n0.707572340965271\n0.7211889028549194\n0.7360522150993347\n0.7072746157646179\n0.7064648866653442\n0.7201521396636963\n0.6862911581993103\n0.6932245492935181\n0.6876338720321655\n0.7018516063690186\n0.7107352614402771\n0.6876278519630432\n0.73543781042099\n0.725761353969574\n0.7246875166893005\n0.6937068700790405\n0.702770471572876\n0.7079275250434875\n0.6917824149131775\n0.7068943381309509\n0.6797854900360107\n0.6842365860939026\n0.6937711238861084\n0.6970776319503784\n0.6969907283782959\n0.6936631798744202\n0.6871494650840759\n0.698943018913269\n0.679225742816925\n0.6893207430839539\n0.6831713914871216\n0.6851694583892822\n0.6780673265457153\n0.6872529983520508\n0.6908349394798279\n0.7004780173301697\nEpoch 6, Training Loss: 0.7033220251057426, Validation Loss: 0.6929989289256899\nTraining Accuracy: 0.7146591395605507, Training F1 Score: 0.6936632467032021\nValidation Accuracy: 0.7130108516991817, Validation F1 Score: 0.6919493560036011\n\n0.6912354230880737\n0.6799501180648804\n0.7059183120727539\n0.6721982955932617\n0.6828256249427795\n0.6880696415901184\n0.6941065192222595\n0.6835876703262329\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","Cell \u001b[0;32mIn[29], line 6\u001b[0m\n\u001b[1;32m      3\u001b[0m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39msynchronize()\n\u001b[1;32m      4\u001b[0m start_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[0;32m----> 6\u001b[0m \u001b[43mevaluate_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcustom_train_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1024\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m8\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      8\u001b[0m elapsed_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;241m-\u001b[39m start_time\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExecution time: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00melapsed_time\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.6f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m seconds\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n","Cell \u001b[0;32mIn[4], line 29\u001b[0m, in \u001b[0;36mevaluate_model\u001b[0;34m(model, custom_train_loader, criterion, optimizer, num_epochs, batch_size)\u001b[0m\n\u001b[1;32m     27\u001b[0m         loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[1;32m     28\u001b[0m         optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[0;32m---> 29\u001b[0m         running_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mlen\u001b[39m(labels)\n\u001b[1;32m     30\u001b[0m         \u001b[38;5;28mprint\u001b[39m(loss\u001b[38;5;241m.\u001b[39mitem())\n\u001b[1;32m     32\u001b[0m avg_train_loss \u001b[38;5;241m=\u001b[39m running_loss \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mlen\u001b[39m(custom_train_loader\u001b[38;5;241m.\u001b[39mtrain_data_tensor)\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "],"ename":"KeyboardInterrupt","evalue":"","output_type":"error"}]},{"cell_type":"code","source":"optimizer = optim.Adam(model.parameters(), lr=0.001)\n\ntorch.cuda.synchronize()\nstart_time = time.time()\n\nevaluate_model(model, custom_train_loader, criterion, optimizer, 10, 1024 * 8)\n\nelapsed_time = time.time() - start_time\nprint(f\"Execution time: {elapsed_time:.6f} seconds\")","metadata":{"execution":{"iopub.status.busy":"2024-10-25T21:57:00.402710Z","iopub.execute_input":"2024-10-25T21:57:00.403103Z","iopub.status.idle":"2024-10-25T22:03:53.938277Z","shell.execute_reply.started":"2024-10-25T21:57:00.403066Z","shell.execute_reply":"2024-10-25T22:03:53.937207Z"},"collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":20,"outputs":[{"name":"stdout","text":"1.9459153413772583\n1.5398389101028442\n1.320652961730957\n1.2236943244934082\n1.2085390090942383\n1.2440117597579956\n1.2571783065795898\n1.2673126459121704\n1.2284069061279297\n1.2386085987091064\n1.2222321033477783\n1.2211248874664307\n1.1789835691452026\n1.1707390546798706\n1.1351127624511719\n1.1525695323944092\n1.1121410131454468\n1.1236003637313843\n1.1125309467315674\n1.0914899110794067\n1.0991089344024658\n1.0779876708984375\n1.078782558441162\n1.0558438301086426\n1.047053575515747\n1.055101990699768\n1.0477261543273926\n1.0337426662445068\n1.0097846984863281\n1.0111554861068726\n0.9982945919036865\n0.9946751594543457\n0.9888061881065369\n0.9877339005470276\n0.9601951241493225\n0.9653480052947998\n0.9570215940475464\n0.9451500773429871\n0.9471073746681213\n0.9473270773887634\n0.937891960144043\n0.9114409685134888\n0.9271887540817261\n0.9155710339546204\n0.9237993359565735\n0.9161682724952698\n0.9090586304664612\n0.9052014946937561\n0.9101273417472839\n0.8804532885551453\n0.8974512815475464\n0.8741264343261719\n0.8792372941970825\n0.8847140669822693\n0.8733091354370117\n0.8659079670906067\n0.8712577819824219\nEpoch 1, Training Loss: 1.06202195772282, Validation Loss: 0.866178046441048\nTraining Accuracy: 0.6419669154426872, Training F1 Score: 0.6050740054382276\nValidation Accuracy: 0.6371522249855855, Validation F1 Score: 0.6000284661188431\n\n0.8668470978736877\n0.8453180193901062\n0.8558576107025146\n0.844309389591217\n0.8332231640815735\n0.8547512292861938\n0.8459964394569397\n0.84201580286026\n0.8247949481010437\n0.8369894623756409\n0.8289358615875244\n0.8262630105018616\n0.8220441341400146\n0.8269965052604675\n0.8045281171798706\n0.8192514181137085\n0.8064925670623779\n0.8106364011764526\n0.8102640509605408\n0.8023528456687927\n0.8103712201118469\n0.8050587773323059\n0.8077144622802734\n0.8005549907684326\n0.7947306036949158\n0.7975394129753113\n0.8026953339576721\n0.7828181385993958\n0.7780875563621521\n0.7825893759727478\n0.7865694165229797\n0.7782526612281799\n0.7766598463058472\n0.7894143462181091\n0.7631625533103943\n0.7759716510772705\n0.768779456615448\n0.7659357190132141\n0.7625505924224854\n0.768392026424408\n0.7644655108451843\n0.7366784811019897\n0.7553293704986572\n0.7493911981582642\n0.7590270042419434\n0.7641292214393616\n0.7575345635414124\n0.7548074722290039\n0.7593512535095215\n0.7370809316635132\n0.7537412643432617\n0.7374584674835205\n0.7417235970497131\n0.7452512979507446\n0.739581823348999\n0.7397009134292603\n0.7477926015853882\nEpoch 2, Training Loss: 0.7905245261080819, Validation Loss: 0.746561316289517\nTraining Accuracy: 0.6953157103240256, Training F1 Score: 0.6768999124176165\nValidation Accuracy: 0.6888720601017185, Validation F1 Score: 0.6699168711879377\n\n0.7346556186676025\n0.7215641140937805\n0.733574390411377\n0.7311803102493286\n0.7201354503631592\n0.7389668822288513\n0.737521231174469\n0.7314075827598572\n0.7250005602836609\n0.7364376187324524\n0.727417528629303\n0.7210531234741211\n0.7236055731773376\n0.7294965386390686\n0.706984281539917\n0.7236992716789246\n0.7180538177490234\n0.7192732691764832\n0.7244002819061279\n0.7148046493530273\n0.7231098413467407\n0.7181951999664307\n0.7224574089050293\n0.7168299555778503\n0.7146393060684204\n0.7136268615722656\n0.7246865034103394\n0.7016814947128296\n0.702243983745575\n0.7056789398193359\n0.714279055595398\n0.7045210599899292\n0.7021873593330383\n0.7218614816665649\n0.6950188279151917\n0.7074695229530334\n0.7039498090744019\n0.7027354836463928\n0.695296049118042\n0.7052662372589111\n0.702372670173645\n0.6717819571495056\n0.6946036219596863\n0.6915156841278076\n0.6983603239059448\n0.7095035910606384\n0.6997005343437195\n0.6979705691337585\n0.7062448859214783\n0.6823374629020691\n0.7002111673355103\n0.6863288879394531\n0.6905539631843567\n0.6895211338996887\n0.6853669881820679\n0.6892926692962646\n0.6969855427742004\nEpoch 3, Training Loss: 0.7107227777022838, Validation Loss: 0.7007715448476958\nTraining Accuracy: 0.7126626205602732, Training F1 Score: 0.6981848364698149\nValidation Accuracy: 0.70616077037598, Validation F1 Score: 0.690996494247015\n\n0.6858496069908142\n0.6751379370689392\n0.6865630149841309\n0.6839081048965454\n0.6734668612480164\n0.691011905670166\n0.6907157301902771\n0.6859275102615356\n0.685491144657135\n0.6936891674995422\n0.684539258480072\n0.677874743938446\n0.679215133190155\n0.6894864439964294\n0.6660986542701721\n0.6813870072364807\n0.6809595227241516\n0.6803848147392273\n0.69024658203125\n0.6783205270767212\n0.6859181523323059\n0.6785758137702942\n0.6822773814201355\n0.681572675704956\n0.6805553436279297\n0.6750512719154358\n0.6894853711128235\n0.6660851836204529\n0.666461706161499\n0.6689125895500183\n0.6793180704116821\n0.6675019860267639\n0.6660603880882263\n0.6872745156288147\n0.6638209819793701\n0.6735367178916931\n0.6735041737556458\n0.6715975999832153\n0.665383517742157\n0.671464204788208\n0.6717435717582703\n0.6405395269393921\n0.6633826494216919\n0.6629592180252075\n0.664985179901123\n0.6767293810844421\n0.6684349179267883\n0.6656246185302734\n0.675907552242279\n0.6522465348243713\n0.6689980030059814\n0.6567825675010681\n0.6614648103713989\n0.6619517207145691\n0.6570707559585571\n0.6632443070411682\n0.666492760181427\nEpoch 4, Training Loss: 0.6743021874727311, Validation Loss: 0.6771201556278144\nTraining Accuracy: 0.7228366920606099, Training F1 Score: 0.7104554410218206\nValidation Accuracy: 0.7137337246026351, Validation F1 Score: 0.7007927355485872\n\n0.6596800684928894\n0.6490929126739502\n0.6596130728721619\n0.6557292342185974\n0.64901202917099\n0.6631435751914978\n0.663905918598175\n0.6591446399688721\n0.6585617661476135\n0.6682826280593872\n0.6578792333602905\n0.6503452062606812\n0.6540710926055908\n0.6609642505645752\n0.6392454504966736\n0.6554914116859436\n0.6564228534698486\n0.6567556858062744\n0.6642069816589355\n0.6545461416244507\n0.6596962213516235\n0.6525748372077942\n0.6566552519798279\n0.6557690501213074\n0.6566159129142761\n0.6495410799980164\n0.6647895574569702\n0.6419063806533813\n0.6435387134552002\n0.6450842618942261\n0.6556390523910522\n0.6451766490936279\n0.6435739398002625\n0.6667699217796326\n0.6407902240753174\n0.6525382995605469\n0.6522090435028076\n0.6476224660873413\n0.6426104307174683\n0.6502902507781982\n0.6515020728111267\n0.6198720335960388\n0.6447884440422058\n0.6409143209457397\n0.6457523703575134\n0.6586271524429321\n0.6503150463104248\n0.6465134620666504\n0.6572901606559753\n0.6337774395942688\n0.6539032459259033\n0.6387056708335876\n0.6412291526794434\n0.642973005771637\n0.6435654163360596\n0.6420251131057739\n0.6461136937141418\nEpoch 5, Training Loss: 0.6512049188531237, Validation Loss: 0.6623261091576861\nTraining Accuracy: 0.7296696062253528, Training F1 Score: 0.7191744338474262\nValidation Accuracy: 0.7193273839745962, Validation F1 Score: 0.708215696538162\n\n0.6405901312828064\n0.6298956274986267\n0.6423910856246948\n0.6380305290222168\n0.6316580772399902\n0.6455411314964294\n0.644279420375824\n0.639230489730835\n0.6441045999526978\n0.6526078581809998\n0.6395485997200012\n0.6357948184013367\n0.636556088924408\n0.645696759223938\n0.6236129999160767\n0.6390442848205566\n0.6382356882095337\n0.6375789046287537\n0.6497132182121277\n0.6405453681945801\n0.64353346824646\n0.6357967257499695\n0.640902578830719\n0.6402885317802429\n0.638340175151825\n0.6345968842506409\n0.6481881141662598\n0.6247753500938416\n0.6270732283592224\n0.6313959956169128\n0.6386825442314148\n0.6310142278671265\n0.6287189722061157\n0.6519671678543091\n0.6267728805541992\n0.6353668570518494\n0.638765811920166\n0.6334556341171265\n0.6264945268630981\n0.6332201361656189\n0.6354688405990601\n0.6045979261398315\n0.6297155022621155\n0.6251965761184692\n0.6277863383293152\n0.6395341157913208\n0.6328167915344238\n0.6279045343399048\n0.6431933045387268\n0.6198179721832275\n0.6340240836143494\n0.6236547827720642\n0.626417338848114\n0.625032901763916\n0.6258634924888611\n0.627926230430603\n0.6320009827613831\nEpoch 6, Training Loss: 0.6348367766876248, Validation Loss: 0.652198376266199\nTraining Accuracy: 0.7345554840805578, Training F1 Score: 0.7250490773781089\nValidation Accuracy: 0.7226663683381668, Validation F1 Score: 0.7125281353587919\n\n0.6269611120223999\n0.6139633059501648\n0.6253725290298462\n0.6244226098060608\n0.6183181405067444\n0.6285725831985474\n0.6285027861595154\n0.6235685348510742\n0.6322533488273621\n0.6377273201942444\n0.6253876686096191\n0.6208460330963135\n0.6224832534790039\n0.6311696767807007\n0.60920250415802\n0.6263284087181091\n0.6282744407653809\n0.6253019571304321\n0.6362271308898926\n0.6269425749778748\n0.6304098963737488\n0.6229901909828186\n0.626512348651886\n0.6280269622802734\n0.6247251033782959\n0.6180201768875122\n0.6343591213226318\n0.610504150390625\n0.6142514944076538\n0.6139920353889465\n0.6233136057853699\n0.6159437894821167\n0.6165044903755188\n0.6396247148513794\n0.6123362183570862\n0.6250498294830322\n0.6263983249664307\n0.6220167875289917\n0.6141995787620544\n0.6199488043785095\n0.6226097345352173\n0.5923181772232056\n0.6218068599700928\n0.6158400774002075\n0.6155552268028259\n0.6291421055793762\n0.6214219927787781\n0.614497184753418\n0.6295706033706665\n0.6035202145576477\n0.6224811673164368\n0.6088613867759705\n0.6133161783218384\n0.6108325123786926\n0.6117022037506104\n0.6150955557823181\n0.6160227656364441\nEpoch 7, Training Loss: 0.6213497510470509, Validation Loss: 0.6446979103480837\nTraining Accuracy: 0.7393359422902741, Training F1 Score: 0.7312063314717838\nValidation Accuracy: 0.7251792122406479, Validation F1 Score: 0.7164334235398127\n\n0.6143683791160583\n0.6051418781280518\n0.6161269545555115\n0.6098167896270752\n0.6044038534164429\n0.6124112606048584\n0.6119218468666077\n0.6138805150985718\n0.6197216510772705\n0.6274960041046143\n0.6147089004516602\n0.6038435101509094\n0.6092067956924438\n0.619554877281189\n0.5964962244033813\n0.6146834492683411\n0.6171368360519409\n0.6144979000091553\n0.6234126091003418\n0.6157156229019165\n0.6188645362854004\n0.6119776964187622\n0.6134889721870422\n0.6172353625297546\n0.6152035593986511\n0.6088208556175232\n0.6259109377861023\n0.6000638604164124\n0.6044042706489563\n0.604937732219696\n0.6120198965072632\n0.6059028506278992\n0.6039860844612122\n0.6301640272140503\n0.5989300012588501\n0.6096371412277222\n0.6142294406890869\n0.6088880300521851\n0.6016514897346497\n0.6085997223854065\n0.6082898378372192\n0.5785397887229919\n0.6071210503578186\n0.6053873300552368\n0.604621171951294\n0.6169138550758362\n0.6067002415657043\n0.6004173159599304\n0.6189982295036316\n0.5928942561149597\n0.6083449125289917\n0.5960401296615601\n0.6001578569412231\n0.5999113321304321\n0.5981482267379761\n0.6016543507575989\n0.6053317189216614\nEpoch 8, Training Loss: 0.609297676116243, Validation Loss: 0.6385401588927082\nTraining Accuracy: 0.7439550438997524, Training F1 Score: 0.7365129113503949\nValidation Accuracy: 0.7277092674027349, Validation F1 Score: 0.7197204323245608\n\n0.6001356840133667\n0.5903358459472656\n0.6041063070297241\n0.5996884703636169\n0.5930781960487366\n0.6068804860115051\n0.6028040051460266\n0.6077450513839722\n0.6087952256202698\n0.6165071129798889\n0.6034893989562988\n0.5940890312194824\n0.5938807725906372\n0.6053494215011597\n0.5844174027442932\n0.6029620170593262\n0.6068207621574402\n0.6059132814407349\n0.6151735782623291\n0.6068960428237915\n0.6090170741081238\n0.6011024713516235\n0.6009097099304199\n0.6085108518600464\n0.6038855910301208\n0.6017345786094666\n0.61704421043396\n0.5910250544548035\n0.5949019193649292\n0.5940493941307068\n0.5993461608886719\n0.5929173827171326\n0.5907216668128967\n0.6198831796646118\n0.5910750031471252\n0.5994083285331726\n0.6015554666519165\n0.5983631610870361\n0.5870850682258606\n0.5979452133178711\n0.6022977232933044\n0.5684174299240112\n0.5996069312095642\n0.5953813195228577\n0.5943591594696045\n0.607114315032959\n0.5990297198295593\n0.5936988592147827\n0.6091747283935547\n0.5865015983581543\n0.6017884016036987\n0.590881884098053\n0.5893540382385254\n0.5865111351013184\n0.5887816548347473\n0.5949515700340271\n0.5937685370445251\nEpoch 9, Training Loss: 0.5991679963604081, Validation Loss: 0.6346697085698951\nTraining Accuracy: 0.7469928508268988, Training F1 Score: 0.74032991419393\nValidation Accuracy: 0.7288021823877181, Validation F1 Score: 0.7216713264597182\n\n0.5889977812767029\n0.5845908522605896\n0.5932460427284241\n0.5883929133415222\n0.5793813467025757\n0.5931618213653564\n0.594096839427948\n0.5970730185508728\n0.5999138355255127\n0.6081573367118835\n0.5953133702278137\n0.5830124020576477\n0.5882480144500732\n0.5997481346130371\n0.5797631740570068\n0.5955656170845032\n0.5954467058181763\n0.5943889617919922\n0.6022729873657227\n0.5980809330940247\n0.5986320376396179\n0.5905011892318726\n0.5939759016036987\n0.5990516543388367\n0.5933065414428711\n0.5875763297080994\n0.6037804484367371\n0.5792585611343384\n0.5830461978912354\n0.5814114212989807\n0.5863688588142395\n0.580915093421936\n0.5835632085800171\n0.6078508496284485\n0.5766593217849731\n0.5881845951080322\n0.5886996388435364\n0.5877125859260559\n0.5769546031951904\n0.5871884226799011\n0.5890706181526184\n0.5599724054336548\n0.5889821648597717\n0.5885130763053894\n0.5875673890113831\n0.5992719531059265\n0.5907317996025085\n0.5853842496871948\n0.6028473377227783\n0.5808654427528381\n0.5948601961135864\n0.5787957310676575\n0.5839054584503174\n0.5783793926239014\n0.5795807838439941\n0.5842844247817993\n0.584730327129364\nEpoch 10, Training Loss: 0.5893409459618442, Validation Loss: 0.6300699025446134\nTraining Accuracy: 0.7491786949047888, Training F1 Score: 0.742936382314096\nValidation Accuracy: 0.7305663364973366, Validation F1 Score: 0.7237069960444409\n\nExecution time: 413.529334 seconds\n","output_type":"stream"}]},{"cell_type":"code","source":"optimizer = optim.Adam(model.parameters(), lr=0.01)\n\ntorch.cuda.synchronize()\nstart_time = time.time()\n\nevaluate_model(model, custom_train_loader, criterion, optimizer, 100, 1024 * 16)\n\nelapsed_time = time.time() - start_time\nprint(f\"Execution time: {elapsed_time:.6f} seconds\")","metadata":{"execution":{"iopub.status.busy":"2024-10-25T21:39:08.840139Z","iopub.execute_input":"2024-10-25T21:39:08.841001Z","iopub.status.idle":"2024-10-25T21:40:38.857853Z","shell.execute_reply.started":"2024-10-25T21:39:08.840949Z","shell.execute_reply":"2024-10-25T21:40:38.856606Z"},"collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":37,"outputs":[{"name":"stdout","text":"1.945919156074524\n1.2594339847564697\n1.2584514617919922\n1.3378372192382812\n1.3107519149780273\n1.2927818298339844\n1.2241499423980713\n1.1768137216567993\n1.1734135150909424\n1.1348594427108765\n1.0834970474243164\n1.041297435760498\n1.0289627313613892\n1.0330392122268677\n1.0062477588653564\n0.9907570481300354\n0.9803515672683716\n0.9421758651733398\n0.910335123538971\n0.8918401002883911\n0.8682236075401306\n0.8836233615875244\n0.8830834627151489\n0.8678690195083618\n0.8456855416297913\n0.8398438096046448\n0.827994167804718\n0.8162038922309875\n0.8145592212677002\nEpoch 1, Training Loss: 1.0629857906186324, Validation Loss: 0.8064155761279874\nTraining Accuracy: 0.6648752498338028, Training F1 Score: 0.6461636927342742\nValidation Accuracy: 0.6639071280431659, Validation F1 Score: 0.6451132605293023\n\n0.7956252694129944\n0.7927773594856262\n0.7903684377670288\n0.7921122908592224\n0.7751148343086243\n0.7722638845443726\n0.7749717831611633\n0.756729245185852\n0.7641393542289734\n0.7576382756233215\n0.7537832856178284\n0.753152072429657\n0.7386384010314941\n0.7394179701805115\n0.7238658666610718\n0.7304952144622803\n0.7348795533180237\n0.7238478064537048\n0.7204421758651733\n0.7112009525299072\n0.6993522644042969\n0.7061981558799744\n0.711076021194458\n0.7110734581947327\n0.7008604407310486\n0.7020975947380066\n0.6943321228027344\n0.6965488195419312\n0.6981582641601562\nEpoch 2, Training Loss: 0.7395606029433984, Validation Loss: 0.6987798511758837\nTraining Accuracy: 0.7045560649643187, Training F1 Score: 0.6940828363102789\nValidation Accuracy: 0.7029336591998485, Validation F1 Score: 0.6922885203894923\n\n0.6843240261077881\n0.686623215675354\n0.6845201253890991\n0.6965410709381104\n0.6934341788291931\n0.6860092282295227\n0.6947659254074097\n0.6833706498146057\n0.6848920583724976\n0.6886917948722839\n0.6868825554847717\n0.6882127523422241\n0.6796914339065552\n0.6773462295532227\n0.6688565611839294\n0.6749687790870667\n0.682157576084137\n0.6755722761154175\n0.6740127205848694\n0.6676151156425476\n0.6574942469596863\n0.6655563116073608\n0.6712979674339294\n0.6699316501617432\n0.6641872525215149\n0.6628891825675964\n0.6621981859207153\n0.6595703363418579\n0.6615351438522339\nEpoch 3, Training Loss: 0.6773488302266372, Validation Loss: 0.6633577345134717\nTraining Accuracy: 0.721991183475363, Training F1 Score: 0.7128404701202207\nValidation Accuracy: 0.7194564684216415, Validation F1 Score: 0.7102543753165348\n\n0.6489841341972351\n0.6545065641403198\n0.6516802310943604\n0.6588027477264404\n0.6605714559555054\n0.6518654823303223\n0.6629897356033325\n0.6509107351303101\n0.655656635761261\n0.6597212553024292\n0.66037517786026\n0.6590597629547119\n0.6506092548370361\n0.6524829864501953\n0.6442427635192871\n0.6466118097305298\n0.6556757688522339\n0.6516703963279724\n0.6517086029052734\n0.6456155776977539\n0.6333063244819641\n0.6428903341293335\n0.6501927971839905\n0.651140570640564\n0.6471289396286011\n0.6468730568885803\n0.641340434551239\n0.6447465419769287\n0.6480215191841125\nEpoch 4, Training Loss: 0.6510796259481274, Validation Loss: 0.647949358506973\nTraining Accuracy: 0.7269437553919997, Training F1 Score: 0.7173861227597613\nValidation Accuracy: 0.7239227902894073, Validation F1 Score: 0.7141274273734537\n\n0.6327071189880371\n0.6395615935325623\n0.6341968774795532\n0.6406786441802979\n0.6460716128349304\n0.6365838646888733\n0.6469699740409851\n0.6376144289970398\n0.6422721147537231\n0.6477544903755188\n0.6463274359703064\n0.6465003490447998\n0.6347612142562866\n0.6435725688934326\n0.6294664740562439\n0.6349653005599976\n0.6436874270439148\n0.637472927570343\n0.6396259665489197\n0.6339115500450134\n0.6236643195152283\n0.6324047446250916\n0.6370176672935486\n0.6372263431549072\n0.6339843273162842\n0.6337368488311768\n0.6301236152648926\n0.6328243613243103\n0.6323471665382385\nEpoch 5, Training Loss: 0.6376332129640285, Validation Loss: 0.6369270523114589\nTraining Accuracy: 0.7306764714108376, Training F1 Score: 0.7217022145880279\nValidation Accuracy: 0.7292410695076719, Validation F1 Score: 0.7201915362667437\n\n0.6226105093955994\n0.6283614635467529\n0.6245259642601013\n0.6325125098228455\n0.6365595459938049\n0.6266108155250549\n0.6370055079460144\n0.6269088983535767\n0.6333414912223816\n0.6361057162284851\n0.6330385208129883\n0.634916365146637\n0.625228226184845\n0.6300432085990906\n0.6174384951591492\n0.6273131966590881\n0.6360898017883301\n0.6275559663772583\n0.6283318996429443\n0.6234714984893799\n0.6148571372032166\n0.6205337047576904\n0.6255322694778442\n0.6246219873428345\n0.6290784478187561\n0.6200164556503296\n0.6179576516151428\n0.6219100952148438\n0.6216742396354675\nEpoch 6, Training Loss: 0.6271589185573473, Validation Loss: 0.6258849729517307\nTraining Accuracy: 0.7349491941851384, Training F1 Score: 0.7277973465548269\nValidation Accuracy: 0.7322530399387279, Validation F1 Score: 0.7250612210416604\n\n0.6099963784217834\n0.6164383888244629\n0.6131207346916199\n0.6206695437431335\n0.627464234828949\n0.6153466105461121\n0.6236804723739624\n0.6176882982254028\n0.622158944606781\n0.6229302883148193\n0.6210846304893494\n0.6252782940864563\n0.6149914860725403\n0.6217057704925537\n0.6129623651504517\n0.6159201860427856\n0.6223230361938477\n0.6188725829124451\n0.61674964427948\n0.6128824949264526\n0.605702817440033\n0.6123389601707458\n0.6159301996231079\n0.6167401075363159\n0.6153117418289185\n0.6123458743095398\n0.6083085536956787\n0.6147006154060364\n0.6094707250595093\nEpoch 7, Training Loss: 0.6168188121904599, Validation Loss: 0.6163337704927155\nTraining Accuracy: 0.7402502963582891, Training F1 Score: 0.7332267506214083\nValidation Accuracy: 0.7361427846096916, Validation F1 Score: 0.7290603539585231\n\n0.60112464427948\n0.6092321872711182\n0.6005604863166809\n0.6096667051315308\n0.6166281700134277\n0.6068913340568542\n0.6128931045532227\n0.6077392101287842\n0.6117896437644958\n0.6175323724746704\n0.6132165193557739\n0.6141006946563721\n0.6102764010429382\n0.6156056523323059\n0.6045619249343872\n0.6059285998344421\n0.6161020398139954\n0.6096060872077942\n0.6142697334289551\n0.6068249344825745\n0.5924403667449951\n0.6039456129074097\n0.6094525456428528\n0.607915997505188\n0.6103335022926331\n0.6046386361122131\n0.5987126231193542\n0.6093751192092896\n0.603145182132721\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","Cell \u001b[0;32mIn[37], line 6\u001b[0m\n\u001b[1;32m      3\u001b[0m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39msynchronize()\n\u001b[1;32m      4\u001b[0m start_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[0;32m----> 6\u001b[0m \u001b[43mevaluate_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcustom_train_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1024\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m16\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      8\u001b[0m elapsed_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;241m-\u001b[39m start_time\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExecution time: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00melapsed_time\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.6f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m seconds\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n","Cell \u001b[0;32mIn[7], line 47\u001b[0m, in \u001b[0;36mevaluate_model\u001b[0;34m(model, custom_train_loader, criterion, optimizer, num_epochs, batch_size)\u001b[0m\n\u001b[1;32m     43\u001b[0m         val_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m unregularized_criterion(val_outputs, val_labels)\u001b[38;5;241m.\u001b[39mitem() \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mlen\u001b[39m(val_labels)\n\u001b[1;32m     45\u001b[0m avg_val_loss \u001b[38;5;241m=\u001b[39m val_loss \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mlen\u001b[39m(custom_train_loader\u001b[38;5;241m.\u001b[39mval_data_tensor)\n\u001b[0;32m---> 47\u001b[0m train_accuracy, train_f1 \u001b[38;5;241m=\u001b[39m \u001b[43mcalculate_metrics\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcustom_train_loader\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_data_tensor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcustom_train_loader\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_labels_tensor\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     48\u001b[0m val_accuracy, val_f1 \u001b[38;5;241m=\u001b[39m calculate_metrics(model, custom_train_loader\u001b[38;5;241m.\u001b[39mval_data_tensor, custom_train_loader\u001b[38;5;241m.\u001b[39mval_labels_tensor)\n\u001b[1;32m     50\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;250m \u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Training Loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mavg_train_loss\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Validation Loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mavg_val_loss\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n","Cell \u001b[0;32mIn[5], line 14\u001b[0m, in \u001b[0;36mcalculate_metrics\u001b[0;34m(model, data_tensor, labels_tensor, batch_size)\u001b[0m\n\u001b[1;32m     12\u001b[0m         outputs \u001b[38;5;241m=\u001b[39m model(inputs)\n\u001b[1;32m     13\u001b[0m         _, preds \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mmax(outputs, \u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m---> 14\u001b[0m         \u001b[43mall_preds\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mextend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpreds\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcpu\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnumpy\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     15\u001b[0m         all_labels\u001b[38;5;241m.\u001b[39mextend(labels\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mnumpy())\n\u001b[1;32m     17\u001b[0m accuracy \u001b[38;5;241m=\u001b[39m accuracy_score(all_labels, all_preds)\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "],"ename":"KeyboardInterrupt","evalue":"","output_type":"error"}]},{"cell_type":"code","source":"for name, param in model.named_parameters():\n    print(name)\n    print(param)","metadata":{"execution":{"iopub.status.busy":"2024-10-25T21:36:25.295054Z","iopub.execute_input":"2024-10-25T21:36:25.295422Z","iopub.status.idle":"2024-10-25T21:36:25.325956Z","shell.execute_reply.started":"2024-10-25T21:36:25.295389Z","shell.execute_reply":"2024-10-25T21:36:25.324879Z"},"trusted":true},"execution_count":31,"outputs":[{"name":"stdout","text":"copy_tensor\nParameter containing:\ntensor([[[[ 0.8277,  0.4410,  0.3069,  ...,  0.5385, -0.9969,  1.3867],\n          [-1.1852,  0.8048, -0.3854,  ...,  0.0073, -0.0906, -0.4299],\n          [-0.5277,  1.8312,  2.6152,  ..., -0.6186,  0.3207, -0.1636],\n          ...,\n          [-1.4442, -0.9133, -0.1195,  ..., -1.0740,  0.2558, -1.7473],\n          [-0.8845, -0.4335,  0.7672,  ...,  0.0748, -1.8827, -1.0929],\n          [-1.0942, -1.1413,  0.8970,  ...,  0.5688,  0.1352, -0.1426]],\n\n         [[-0.3369,  0.1095,  0.8045,  ...,  0.7055, -0.9463,  1.8010],\n          [ 0.4168,  0.8493, -0.1739,  ...,  0.9907, -0.4204, -0.9187],\n          [-0.3940,  0.1856,  0.5866,  ..., -0.0302,  1.5826,  0.6268],\n          ...,\n          [ 0.7070,  0.5277, -0.2452,  ...,  1.1998, -0.1681,  0.7360],\n          [ 0.6341,  1.0665, -0.0837,  ..., -1.4147, -0.1684,  2.0537],\n          [-1.1398,  0.4702, -0.2144,  ...,  1.6041,  1.3350, -1.1957]],\n\n         [[-0.0395, -0.2068, -0.6581,  ..., -1.0872,  1.6298,  0.4546],\n          [-0.8353,  0.9366, -0.0697,  ..., -1.0988, -1.1947,  1.1399],\n          [-0.3183, -1.8403,  0.5869,  ..., -0.2028,  2.7595, -1.6938],\n          ...,\n          [-1.6379,  0.2097,  0.0590,  ...,  1.4509,  0.1137,  0.7244],\n          [-1.5736, -0.6769,  1.1836,  ...,  1.4082, -1.1051, -0.5578],\n          [ 1.4432,  0.4759, -0.0501,  ..., -0.7416, -0.3644,  1.6736]],\n\n         ...,\n\n         [[ 1.2195,  1.0678,  0.1448,  ...,  0.8914, -0.5132, -0.8008],\n          [-0.5625,  0.5263, -0.6625,  ...,  1.6862, -0.3205,  0.4128],\n          [-0.7953, -0.1380,  0.1330,  ..., -0.2872,  0.6248, -0.7247],\n          ...,\n          [ 1.3122, -0.5718, -0.2433,  ..., -0.6326,  1.0065, -0.0074],\n          [-0.0175, -1.2917, -1.3723,  ..., -0.0900, -0.6113, -1.5914],\n          [ 0.2162, -1.2571,  1.1768,  ..., -1.0795,  0.7880, -1.1169]],\n\n         [[ 1.5555, -0.6738, -0.0406,  ..., -0.6737,  0.7246,  0.9503],\n          [ 1.4287,  0.3253, -1.2883,  ..., -0.5223, -0.8058,  0.5319],\n          [ 0.5660,  0.5659,  1.2707,  ...,  0.4913, -0.6219, -0.5646],\n          ...,\n          [ 1.5617,  0.0585,  0.9622,  ...,  0.2714, -0.4025,  1.1836],\n          [ 0.7012, -0.0542, -1.4298,  ..., -0.6310, -0.2037,  0.8105],\n          [ 1.3559, -0.9023,  1.3565,  ...,  0.1810,  0.1379, -0.1324]],\n\n         [[-1.7030,  0.8345, -0.0087,  ...,  0.6046, -0.0779,  0.7157],\n          [ 1.5428, -0.6885, -0.5660,  ...,  0.4946,  1.4254,  1.2900],\n          [-0.5911, -0.2174,  0.6843,  ...,  1.0210, -0.6519,  0.6133],\n          ...,\n          [-1.6853,  0.8649, -0.2230,  ..., -0.3249,  0.3694,  0.9081],\n          [-1.0055,  1.1137, -1.4368,  ..., -0.5136, -0.7657,  0.6990],\n          [ 0.0417,  2.1845,  0.1461,  ...,  0.4816, -0.8489, -0.3807]]]],\n       device='cuda:0', requires_grad=True)\nfinal_tensor\nParameter containing:\ntensor([[[ 0.0141,  0.0345,  0.0433,  ...,  0.0472,  0.0598,  0.0000],\n         [ 0.0449,  0.0192, -0.0060,  ..., -0.0746, -0.0692,  0.0000],\n         [-0.0429,  0.0072,  0.0350,  ...,  0.0582,  0.0356,  0.0000],\n         ...,\n         [-0.0303, -0.0354, -0.0389,  ..., -0.0742, -0.0686,  0.0000],\n         [-0.0406, -0.0242,  0.0012,  ...,  0.0689,  0.0414,  0.0000],\n         [-0.0260, -0.0105, -0.0079,  ..., -0.0623, -0.0611,  0.0000]],\n\n        [[-0.0449, -0.0633, -0.0574,  ...,  0.0381,  0.0409,  0.0000],\n         [ 0.0287,  0.0321,  0.0309,  ...,  0.0333,  0.0278,  0.0000],\n         [ 0.0448,  0.0659,  0.0659,  ..., -0.0438, -0.0101,  0.0000],\n         ...,\n         [-0.0287, -0.0354, -0.0324,  ..., -0.0341, -0.0346,  0.0000],\n         [ 0.0196,  0.0331,  0.0292,  ..., -0.0339, -0.0397,  0.0000],\n         [-0.0406, -0.0429, -0.0363,  ..., -0.0177, -0.0179,  0.0000]],\n\n        [[ 0.0398, -0.0159, -0.0489,  ..., -0.1021, -0.0821,  0.0000],\n         [ 0.0326,  0.0474,  0.0399,  ..., -0.1051, -0.0934,  0.0000],\n         [-0.0465,  0.0226,  0.0718,  ...,  0.0924,  0.0802,  0.0000],\n         ...,\n         [-0.0331, -0.0315, -0.0312,  ..., -0.0764, -0.0750,  0.0000],\n         [-0.0371, -0.0067,  0.0367,  ...,  0.0750,  0.0603,  0.0000],\n         [-0.0150, -0.0355, -0.0470,  ..., -0.1069, -0.0959,  0.0000]],\n\n        ...,\n\n        [[ 0.0118,  0.0056,  0.0031,  ...,  0.0419,  0.0423,  0.0000],\n         [ 0.0285,  0.0291,  0.0292,  ...,  0.0300,  0.0246,  0.0000],\n         [ 0.0190,  0.0257,  0.0271,  ..., -0.0421, -0.0200,  0.0000],\n         ...,\n         [-0.0373, -0.0362, -0.0335,  ..., -0.0326, -0.0291,  0.0000],\n         [ 0.0004,  0.0009, -0.0015,  ..., -0.0379, -0.0422,  0.0000],\n         [-0.0404, -0.0396, -0.0380,  ..., -0.0051,  0.0060,  0.0000]],\n\n        [[ 0.0198,  0.0402,  0.0477,  ...,  0.0363,  0.0214,  0.0000],\n         [ 0.0350,  0.0183,  0.0127,  ..., -0.0040,  0.0007,  0.0000],\n         [-0.0209,  0.0078,  0.0154,  ..., -0.0220, -0.0398,  0.0000],\n         ...,\n         [-0.0327, -0.0354, -0.0380,  ..., -0.0372, -0.0283,  0.0000],\n         [-0.0277, -0.0186, -0.0195,  ..., -0.0306, -0.0260,  0.0000],\n         [-0.0165, -0.0324, -0.0391,  ..., -0.0453, -0.0351,  0.0000]],\n\n        [[ 0.0350,  0.0446,  0.0384,  ..., -0.0088, -0.0070,  0.0000],\n         [ 0.0322,  0.0256,  0.0279,  ...,  0.0316,  0.0280,  0.0000],\n         [ 0.0279,  0.0021, -0.0129,  ..., -0.0185, -0.0229,  0.0000],\n         ...,\n         [-0.0489, -0.0407, -0.0357,  ..., -0.0255, -0.0270,  0.0000],\n         [-0.0145, -0.0249, -0.0263,  ..., -0.0254, -0.0228,  0.0000],\n         [-0.0471, -0.0357, -0.0281,  ...,  0.0197,  0.0344,  0.0000]]],\n       device='cuda:0', requires_grad=True)\n","output_type":"stream"}]},{"cell_type":"code","source":"criterion = CustomLoss(nn.CrossEntropyLoss(), first_order_weight=0.0001, second_order_weight=0.0)\noptimizer = optim.Adam(model.parameters(), lr=0.001)\n\ntorch.cuda.synchronize()\nstart_time = time.time()\n\nevaluate_model(model, custom_train_loader, criterion, optimizer, 100, 1024 * 16)\n\nelapsed_time = time.time() - start_time\nprint(f\"Execution time: {elapsed_time:.6f} seconds\")","metadata":{"execution":{"iopub.status.busy":"2024-10-25T18:39:42.735654Z","iopub.execute_input":"2024-10-25T18:39:42.736035Z","iopub.status.idle":"2024-10-25T18:42:41.648922Z","shell.execute_reply.started":"2024-10-25T18:39:42.735983Z","shell.execute_reply":"2024-10-25T18:42:41.647534Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"criterion = CustomLoss(nn.CrossEntropyLoss(), first_order_weight=0.001, second_order_weight=0.0)\noptimizer = optim.Adam(model.parameters(), lr=0.001)\n\ntorch.cuda.synchronize()\nstart_time = time.time()\n\nevaluate_model(model, custom_train_loader, criterion, optimizer, 100, 1024 * 16)\n\nelapsed_time = time.time() - start_time\nprint(f\"Execution time: {elapsed_time:.6f} seconds\")","metadata":{"execution":{"iopub.status.busy":"2024-10-25T18:42:55.432652Z","iopub.execute_input":"2024-10-25T18:42:55.433127Z","iopub.status.idle":"2024-10-25T18:44:35.489376Z","shell.execute_reply.started":"2024-10-25T18:42:55.433088Z","shell.execute_reply":"2024-10-25T18:44:35.488042Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"criterion = CustomLoss(nn.CrossEntropyLoss(), first_order_weight=0.001, second_order_weight=0.0)\noptimizer = optim.Adam(model.parameters(), lr=0.001)\n\ntorch.cuda.synchronize()\nstart_time = time.time()\n\nevaluate_model(model, custom_train_loader, criterion, optimizer, 100, 1024 * 16)\n\nelapsed_time = time.time() - start_time\nprint(f\"Execution time: {elapsed_time:.6f} seconds\")","metadata":{"execution":{"iopub.status.busy":"2024-10-25T18:45:06.758945Z","iopub.execute_input":"2024-10-25T18:45:06.759359Z","iopub.status.idle":"2024-10-25T18:48:48.350121Z","shell.execute_reply.started":"2024-10-25T18:45:06.75932Z","shell.execute_reply":"2024-10-25T18:48:48.348783Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"criterion = CustomLoss(nn.CrossEntropyLoss(), first_order_weight=0.0001, second_order_weight=0.0)\noptimizer = optim.Adam(model.parameters(), lr=0.001)\n\ntorch.cuda.synchronize()\nstart_time = time.time()\n\nevaluate_model(model, custom_train_loader, criterion, optimizer, 100, 1024 * 16)\n\nelapsed_time = time.time() - start_time\nprint(f\"Execution time: {elapsed_time:.6f} seconds\")","metadata":{"execution":{"iopub.status.busy":"2024-10-25T18:48:58.85392Z","iopub.execute_input":"2024-10-25T18:48:58.854389Z","iopub.status.idle":"2024-10-25T18:54:14.465695Z","shell.execute_reply.started":"2024-10-25T18:48:58.854345Z","shell.execute_reply":"2024-10-25T18:54:14.464216Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"criterion = CustomLoss(nn.CrossEntropyLoss(), first_order_weight=0.00001, second_order_weight=0.0)\noptimizer = optim.Adam(model.parameters(), lr=0.001)\n\ntorch.cuda.synchronize()\nstart_time = time.time()\n\nevaluate_model(model, custom_train_loader, criterion, optimizer, 100, 1024 * 16)\n\nelapsed_time = time.time() - start_time\nprint(f\"Execution time: {elapsed_time:.6f} seconds\")","metadata":{"execution":{"iopub.status.busy":"2024-10-25T18:54:27.509392Z","iopub.execute_input":"2024-10-25T18:54:27.509796Z","iopub.status.idle":"2024-10-25T19:14:16.237453Z","shell.execute_reply.started":"2024-10-25T18:54:27.509759Z","shell.execute_reply":"2024-10-25T19:14:16.235807Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for param_group in optimizer.param_groups:\n    param_group['lr'] = 0.0001\n\ntorch.cuda.synchronize()\nstart_time = time.time()\n\nevaluate_model(model, custom_train_loader, criterion, optimizer, 20, 1024 * 16)\n\nelapsed_time = time.time() - start_time\nprint(f\"Execution time: {elapsed_time:.6f} seconds\")","metadata":{"execution":{"iopub.status.busy":"2024-10-24T20:31:06.434635Z","iopub.execute_input":"2024-10-24T20:31:06.435046Z","iopub.status.idle":"2024-10-24T20:35:30.657561Z","shell.execute_reply.started":"2024-10-24T20:31:06.435006Z","shell.execute_reply":"2024-10-24T20:35:30.656405Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for param_group in optimizer.param_groups:\n    param_group['lr'] = 0.001\n\ntorch.cuda.synchronize()\nstart_time = time.time()\n\nevaluate_model(model, custom_train_loader, criterion, optimizer, 20, 1024 * 16)\n\nelapsed_time = time.time() - start_time\nprint(f\"Execution time: {elapsed_time:.6f} seconds\")","metadata":{"execution":{"iopub.status.busy":"2024-10-24T20:35:42.410317Z","iopub.execute_input":"2024-10-24T20:35:42.411175Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"optimizer = optim.Adam(model.parameters(), lr=0.0001)\n\ntorch.cuda.synchronize()\nstart_time = time.time()\n\nevaluate_model(model, custom_train_loader, criterion, optimizer, 100, 1024 * 16)\n\nelapsed_time = time.time() - start_time\nprint(f\"Execution time: {elapsed_time:.6f} seconds\")","metadata":{"execution":{"iopub.status.busy":"2024-10-24T20:07:59.603137Z","iopub.execute_input":"2024-10-24T20:07:59.604116Z","iopub.status.idle":"2024-10-24T20:14:43.247548Z","shell.execute_reply.started":"2024-10-24T20:07:59.60407Z","shell.execute_reply":"2024-10-24T20:14:43.246252Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"optimizer = optim.Adam(model.parameters(), lr=0.0001)\n\ntorch.cuda.synchronize()\nstart_time = time.time()\n\nevaluate_model(model, custom_train_loader, criterion, optimizer, 100, 1024 * 16)\n\nelapsed_time = time.time() - start_time\nprint(f\"Execution time: {elapsed_time:.6f} seconds\")","metadata":{"execution":{"iopub.status.busy":"2024-10-24T19:59:00.051924Z","iopub.execute_input":"2024-10-24T19:59:00.052696Z","iopub.status.idle":"2024-10-24T20:00:44.752631Z","shell.execute_reply.started":"2024-10-24T19:59:00.052655Z","shell.execute_reply":"2024-10-24T20:00:44.751497Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"optimizer = optim.Adam(model.parameters(), lr=0.0001)\n\ntorch.cuda.synchronize()\nstart_time = time.time()\n\nevaluate_model(model, custom_train_loader, criterion, optimizer, 100, 1024 * 16)\n\nelapsed_time = time.time() - start_time\nprint(f\"Execution time: {elapsed_time:.6f} seconds\")","metadata":{"execution":{"iopub.status.busy":"2024-10-24T19:55:09.217048Z","iopub.execute_input":"2024-10-24T19:55:09.217451Z","iopub.status.idle":"2024-10-24T19:56:03.162953Z","shell.execute_reply.started":"2024-10-24T19:55:09.217415Z","shell.execute_reply":"2024-10-24T19:56:03.161589Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"optimizer = optim.Adam(model.parameters(), lr=0.0001)\n\ntorch.cuda.synchronize()\nstart_time = time.time()\n\nevaluate_model(model, custom_train_loader, criterion, optimizer, 100, 1024 * 16)\n\nelapsed_time = time.time() - start_time\nprint(f\"Execution time: {elapsed_time:.6f} seconds\")","metadata":{"execution":{"iopub.status.busy":"2024-10-24T18:56:29.330895Z","iopub.execute_input":"2024-10-24T18:56:29.331302Z","iopub.status.idle":"2024-10-24T19:54:44.963521Z","shell.execute_reply.started":"2024-10-24T18:56:29.331265Z","shell.execute_reply":"2024-10-24T19:54:44.962183Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"evaluate_model(model, custom_train_loader, criterion, optimizer, 100, 1024)","metadata":{"execution":{"iopub.status.busy":"2024-10-24T17:39:07.079542Z","iopub.execute_input":"2024-10-24T17:39:07.080511Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"optimizer = optim.Adam(model.parameters(), lr=0.0001)\n\ntorch.cuda.synchronize()\nstart_time = time.time()\n\nevaluate_model(model, custom_train_loader, criterion, optimizer, 1, 1024 * 16)\n\nelapsed_time = time.time() - start_time\nprint(f\"Execution time: {elapsed_time:.6f} seconds\")","metadata":{"execution":{"iopub.status.busy":"2024-10-24T17:15:04.57983Z","iopub.execute_input":"2024-10-24T17:15:04.580842Z","iopub.status.idle":"2024-10-24T17:16:20.59689Z","shell.execute_reply.started":"2024-10-24T17:15:04.580785Z","shell.execute_reply":"2024-10-24T17:16:20.595871Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"optimizer = optim.Adam(model.parameters(), lr=0.001, fused=True)\n\ntorch.cuda.synchronize()\nstart_time = time.time()\n\nevaluate_model(model, custom_train_loader, criterion, optimizer, 1, 1024 * 16)\n\nelapsed_time = time.time() - start_time\nprint(f\"Execution time: {elapsed_time:.6f} seconds\")","metadata":{"execution":{"iopub.status.busy":"2024-10-24T17:16:58.203043Z","iopub.execute_input":"2024-10-24T17:16:58.204186Z","iopub.status.idle":"2024-10-24T17:18:14.049673Z","shell.execute_reply.started":"2024-10-24T17:16:58.204141Z","shell.execute_reply":"2024-10-24T17:18:14.048704Z"},"collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"optimizer = optim.Adam(model.parameters(), lr=0.001)\n\nstart_time = time.time()\n\nevaluate_model(model, custom_train_loader, criterion, optimizer, 100, 1024 * 32)\n\nelapsed_time = time.time() - start_time\nprint(f\"Execution time: {elapsed_time:.6f} seconds\")","metadata":{"execution":{"iopub.status.busy":"2024-10-24T07:50:45.621595Z","iopub.execute_input":"2024-10-24T07:50:45.622302Z","iopub.status.idle":"2024-10-24T07:57:13.359955Z","shell.execute_reply.started":"2024-10-24T07:50:45.622262Z","shell.execute_reply":"2024-10-24T07:57:13.359034Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"optimizer = optim.Adam(model.parameters(), lr=0.001)\nevaluate_model(model, custom_train_loader, criterion, optimizer, 100, 1024 * 32)","metadata":{"execution":{"iopub.status.busy":"2024-10-24T07:59:10.734844Z","iopub.execute_input":"2024-10-24T07:59:10.735789Z","iopub.status.idle":"2024-10-24T08:02:42.190508Z","shell.execute_reply.started":"2024-10-24T07:59:10.735744Z","shell.execute_reply":"2024-10-24T08:02:42.189246Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"optimizer = optim.Adam(model.parameters(), lr=0.0001)\nevaluate_model(model, custom_train_loader, criterion, optimizer, 10000, 1024 * 32)","metadata":{"execution":{"iopub.status.busy":"2024-10-24T08:03:00.951549Z","iopub.execute_input":"2024-10-24T08:03:00.952413Z","iopub.status.idle":"2024-10-24T08:03:24.534493Z","shell.execute_reply.started":"2024-10-24T08:03:00.95237Z","shell.execute_reply":"2024-10-24T08:03:24.533156Z"},"collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"optimizer = optim.Adam(model.parameters(), lr=0.001)\nevaluate_model(model, custom_train_loader, criterion, optimizer, 10000, 1024 * 32)","metadata":{"execution":{"iopub.status.busy":"2024-10-24T08:17:15.933392Z","iopub.execute_input":"2024-10-24T08:17:15.934187Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"optimizer = optim.SGD(model.parameters(), lr=0.05)\n\nstart_time = time.time()\n\nevaluate_model(model, custom_train_loader, criterion, optimizer, 1, 1024 * 16)\n\nelapsed_time = time.time() - start_time\nprint(f\"Execution time: {elapsed_time:.6f} seconds\")","metadata":{"execution":{"iopub.status.busy":"2024-10-24T07:33:36.01737Z","iopub.execute_input":"2024-10-24T07:33:36.018146Z","iopub.status.idle":"2024-10-24T07:34:49.244299Z","shell.execute_reply.started":"2024-10-24T07:33:36.018104Z","shell.execute_reply":"2024-10-24T07:34:49.24338Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"optimizer = optim.Adam(model.parameters(), lr=0.01)\n\nstart_time = time.time()\n\nevaluate_model(model, custom_train_loader, criterion, optimizer, 1, 1024 * 16)\n\nelapsed_time = time.time() - start_time\nprint(f\"Execution time: {elapsed_time:.6f} seconds\")","metadata":{"execution":{"iopub.status.busy":"2024-10-24T07:35:05.150354Z","iopub.execute_input":"2024-10-24T07:35:05.150802Z","iopub.status.idle":"2024-10-24T07:36:05.225648Z","shell.execute_reply.started":"2024-10-24T07:35:05.150759Z","shell.execute_reply":"2024-10-24T07:36:05.224274Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"optimizer = optim.Adam(model.parameters(), lr=0.1)\n\nstart_time = time.time()\n\nevaluate_model(model, custom_train_loader, criterion, optimizer, 100, 1024 * 16)\n\nelapsed_time = time.time() - start_time\nprint(f\"Execution time: {elapsed_time:.6f} seconds\")","metadata":{"execution":{"iopub.status.busy":"2024-10-24T07:14:27.490605Z","iopub.execute_input":"2024-10-24T07:14:27.491346Z","iopub.status.idle":"2024-10-24T07:25:51.672203Z","shell.execute_reply.started":"2024-10-24T07:14:27.491308Z","shell.execute_reply":"2024-10-24T07:25:51.670786Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"optimizer = optim.Adam(model.parameters(), lr=0.1)\noptimizer = torch.optim.LBFGS(model.parameters(), lr=0.01)\n\nstart_time = time.time()\n\nevaluate_model(model, custom_train_loader, criterion, optimizer, 100, 1024 * 1000)\n\nelapsed_time = time.time() - start_time\nprint(f\"Execution time: {elapsed_time:.6f} seconds\")","metadata":{"execution":{"iopub.status.busy":"2024-10-24T01:31:59.450861Z","iopub.execute_input":"2024-10-24T01:31:59.451256Z","iopub.status.idle":"2024-10-24T01:50:43.506223Z","shell.execute_reply.started":"2024-10-24T01:31:59.45122Z","shell.execute_reply":"2024-10-24T01:50:43.504952Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"optimizer = optim.Adam(model.parameters(), lr=0.1)\noptimizer = torch.optim.LBFGS(model.parameters(), lr=0.01)\n\nstart_time = time.time()\n\nevaluate_model(model, custom_train_loader, criterion, optimizer, 100, 1024 * 1000)\n\nelapsed_time = time.time() - start_time\nprint(f\"Execution time: {elapsed_time:.6f} seconds\")","metadata":{"execution":{"iopub.status.busy":"2024-10-24T01:09:38.995988Z","iopub.execute_input":"2024-10-24T01:09:38.996854Z","iopub.status.idle":"2024-10-24T01:10:15.827012Z","shell.execute_reply.started":"2024-10-24T01:09:38.996813Z","shell.execute_reply":"2024-10-24T01:10:15.825658Z"},"collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"torch.cuda.synchronize()\n\nwith profiler.profile(with_stack=True, use_device='cuda') as prof:\n    optimizer = optim.Adam(model.parameters(), lr=0.01)\n    \n    start_time = time.time()\n    \n    evaluate_model(model, custom_train_loader, criterion, optimizer, 1, 1024 * 1000)\n    \n    elapsed_time = time.time() - start_time\n    print(f\"Execution time: {elapsed_time:.6f} seconds\")","metadata":{"execution":{"iopub.status.busy":"2024-10-24T01:10:24.575911Z","iopub.execute_input":"2024-10-24T01:10:24.576805Z","iopub.status.idle":"2024-10-24T01:10:43.454223Z","shell.execute_reply.started":"2024-10-24T01:10:24.576764Z","shell.execute_reply":"2024-10-24T01:10:43.453254Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"prof_averages = prof.key_averages().table(sort_by=\"self_cuda_time_total\", row_limit=20)\nprint(prof_averages)","metadata":{"execution":{"iopub.status.busy":"2024-10-24T01:18:27.96122Z","iopub.execute_input":"2024-10-24T01:18:27.961858Z","iopub.status.idle":"2024-10-24T01:18:34.275315Z","shell.execute_reply.started":"2024-10-24T01:18:27.961818Z","shell.execute_reply":"2024-10-24T01:18:34.274269Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"prof_averages = prof.key_averages().table(sort_by=\"self_cpu_time_total\", row_limit=20)\nprint(prof_averages)","metadata":{"execution":{"iopub.status.busy":"2024-10-24T01:18:46.202619Z","iopub.execute_input":"2024-10-24T01:18:46.203288Z","iopub.status.idle":"2024-10-24T01:18:52.299679Z","shell.execute_reply.started":"2024-10-24T01:18:46.203248Z","shell.execute_reply":"2024-10-24T01:18:52.298653Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(model.copy_tensor[0:, 0])\nprint(model.copy_tensor.shape)\nprint(model.copy_tensor[0:, 0].shape)\ntensor_data = model.copy_tensor[0, 0, :].cpu().detach().numpy()\n\n# Create a grid for x and y from 0 to 1, matching the dimensions of the tensor\nx = np.linspace(0, 1, tensor_data.shape[1])\ny = np.linspace(0, 1, tensor_data.shape[0])\nX, Y = np.meshgrid(x, y)\n\n# Z values are the tensor data (already 2D, so no need to flatten)\nZ = tensor_data\n\nfig = plt.figure(figsize=(8, 6))\nax = fig.add_subplot(111, projection='3d')\n\n# Create surface plot, connecting the points\nsurf = ax.plot_surface(X, Y, Z, cmap='viridis', edgecolor='none')\n\n# Labels and title\nax.set_xlabel('X')\nax.set_ylabel('Y')\nax.set_zlabel('Z')\n\nplt.title('3D Surface Plot of Tensor Data')\nplt.colorbar(surf, label='Z values')\n\nplt.show()\n\n# print(model.copy_tensor[1, 0, :].reshape(-1).shape)\n# tensor = model.copy_tensor[0, 0, :].reshape(-1).detach().cpu()\n\n# x_values = torch.arange(len(tensor))\n\n# plt.scatter(x_values, tensor, marker='o')\n# plt.xlabel('Index')\n# plt.ylabel('Value')\n# plt.title('1D Tensor Values')\n# plt.show()\n","metadata":{"execution":{"iopub.status.busy":"2024-10-24T18:20:19.076796Z","iopub.execute_input":"2024-10-24T18:20:19.077729Z","iopub.status.idle":"2024-10-24T18:20:19.554754Z","shell.execute_reply.started":"2024-10-24T18:20:19.077688Z","shell.execute_reply":"2024-10-24T18:20:19.553749Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import numpy as np\nimport matplotlib.pyplot as plt\n\n# Assuming tensor_data is defined here or generated dynamically\n# Loop 45 times for creating the plots\nfor i in range(45):\n    # Replace this with your method of generating or updating tensor_data for each plot\n    # Example: tensor_data = np.random.rand(50, 50) # Random data for demo\n    tensor_data = model.copy_tensor[i, 0, :].cpu().detach().numpy()\n\n    # Create a grid for x and y from 0 to 1, matching the dimensions of the tensor\n    x = np.linspace(0, 1, tensor_data.shape[1])\n    y = np.linspace(0, 1, tensor_data.shape[0])\n    X, Y = np.meshgrid(x, y)\n\n    # Flatten the X, Y, and tensor data (Z values) for scatter plot\n    X_flat = X.flatten()\n    Y_flat = Y.flatten()\n    Z_flat = tensor_data.flatten()\n\n    fig = plt.figure(figsize=(8, 6))\n    ax = fig.add_subplot(111, projection='3d')\n\n    # Scatter plot in 3D\n    sc = ax.scatter(X_flat, Y_flat, Z_flat, c=Z_flat, cmap='viridis')\n\n    # Labels and title\n    ax.set_xlabel('X')\n    ax.set_ylabel('Y')\n    ax.set_zlabel('Z')\n    plt.title(f'3D Scatter Plot of Tensor Data - Plot {i+1}')\n\n    # Add color bar\n    plt.colorbar(sc, label='Z values')\n\n    # Show the plot for each iteration\n    plt.show()\n","metadata":{"execution":{"iopub.status.busy":"2024-10-24T08:15:06.108447Z","iopub.execute_input":"2024-10-24T08:15:06.109114Z","iopub.status.idle":"2024-10-24T08:15:32.071875Z","shell.execute_reply.started":"2024-10-24T08:15:06.109057Z","shell.execute_reply":"2024-10-24T08:15:32.070957Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_train = custom_train_loader.train_data_tensor.cpu().numpy()\ny_train = custom_train_loader.train_labels_tensor.cpu().numpy()\nX_val = custom_train_loader.val_data_tensor.cpu().numpy()\ny_val = custom_train_loader.val_labels_tensor.cpu().numpy()\n\n\"\"\"\nlog_reg = LogisticRegression(penalty='l2', C=55.0)\n\n\"\"\"\nlog_reg = LogisticRegression(solver='lbfgs', max_iter=1000, penalty=None)\nlog_reg = LogisticRegression()\n\nlog_reg.fit(X_train, y_train)\n\ny_train_pred = log_reg.predict(X_train)\ny_val_pred = log_reg.predict(X_val)\n\ntrain_accuracy = accuracy_score(y_train, y_train_pred)\nval_accuracy = accuracy_score(y_val, y_val_pred)\n\ntrain_log_loss = log_loss(y_train, log_reg.predict_proba(X_train))\nval_log_loss = log_loss(y_val, log_reg.predict_proba(X_val))\n\nprint(f'Training Accuracy: {train_accuracy}')\nprint(f'Training Log Loss: {train_log_loss}')\nprint()\nprint(f'Validation Accuracy: {val_accuracy}')\nprint(f'Validation Log Loss: {val_log_loss}')","metadata":{"execution":{"iopub.status.busy":"2024-10-24T07:49:13.56122Z","iopub.execute_input":"2024-10-24T07:49:13.561601Z","iopub.status.idle":"2024-10-24T07:49:35.280478Z","shell.execute_reply.started":"2024-10-24T07:49:13.561566Z","shell.execute_reply":"2024-10-24T07:49:35.279174Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}