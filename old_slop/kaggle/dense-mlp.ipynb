{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np\nimport keras\nimport tensorflow as tf\nfrom keras import layers\nfrom keras import models\nfrom keras.datasets import california_housing\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport matplotlib.pyplot as plt\n\nfrom sklearn.model_selection import train_test_split\nfrom torch.utils.data import TensorDataset, DataLoader\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\nfrom torchinfo import summary\nfrom torch.optim.lr_scheduler import StepLR\n\nfrom sklearn.metrics import f1_score\nfrom sklearn.preprocessing import StandardScaler\n\nimport sys\nimport math\n\nimport numpy as np\nimport math\n\nimport numpy as np\nimport keras\nimport tensorflow as tf\nfrom keras import layers\nfrom keras import models\nfrom keras.datasets import mnist\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport matplotlib.pyplot as plt\n\nfrom sklearn.model_selection import train_test_split\nfrom torch.utils.data import TensorDataset, DataLoader\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\nfrom torchinfo import summary\nfrom torch.optim.lr_scheduler import StepLR\n\nimport pandas as pd\nfrom sklearn.preprocessing import StandardScaler, LabelEncoder\n\nimport numpy as np\nimport pandas as pd\nfrom scipy.special import logit\nfrom sklearn.preprocessing import StandardScaler, PowerTransformer\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import log_loss, accuracy_score\nfrom sklearn.model_selection import LeaveOneOut, cross_val_predict, StratifiedKFold\nfrom scipy.stats import norm","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-06-13T03:18:19.501282Z","iopub.execute_input":"2024-06-13T03:18:19.501674Z","iopub.status.idle":"2024-06-13T03:18:19.514302Z","shell.execute_reply.started":"2024-06-13T03:18:19.501645Z","shell.execute_reply":"2024-06-13T03:18:19.513388Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"def calculate_metrics(model, loader):\n    model.eval()\n    all_preds = []\n    all_labels = []\n    with torch.no_grad():\n        for inputs, labels in loader:\n            inputs, labels = inputs.to(device), labels.to(device)\n            outputs = model(inputs)\n            _, preds = torch.max(outputs, 1)\n            all_preds.extend(preds.cpu().numpy())\n            all_labels.extend(labels.cpu().numpy())\n    accuracy = accuracy_score(all_labels, all_preds)\n    f1 = f1_score(all_labels, all_preds, average='weighted')\n    return accuracy, f1","metadata":{"execution":{"iopub.status.busy":"2024-06-13T03:18:19.744237Z","iopub.execute_input":"2024-06-13T03:18:19.744579Z","iopub.status.idle":"2024-06-13T03:18:19.751408Z","shell.execute_reply.started":"2024-06-13T03:18:19.744550Z","shell.execute_reply":"2024-06-13T03:18:19.750474Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"class CustomDataLoader:\n    def __init__(self, features, labels, batch_size=1, validation_size=0.0, shuffle=False):\n\n        if validation_size > 0:\n            train_data, val_data, train_labels, val_labels = train_test_split(features, labels, test_size=validation_size, stratify=labels, random_state=42)\n            train_data_tensor = torch.tensor(train_data).float().to(device)\n            train_labels_tensor = torch.tensor(train_labels).long().to(device)\n            val_data_tensor = torch.tensor(val_data).float().to(device)\n            val_labels_tensor = torch.tensor(val_labels).long().to(device)\n    \n            train_dataset = TensorDataset(train_data_tensor, train_labels_tensor)\n            val_dataset = TensorDataset(val_data_tensor, val_labels_tensor)\n\n            self.train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=False)\n            self.val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n        else:\n            features_tensor = torch.tensor(features).float().to(device)\n            labels_tensor = torch.tensor(labels).long().to(device)\n\n            dataset = TensorDataset(features_tensor, labels_tensor)\n\n            self.train_loader = DataLoader(dataset, batch_size=batch_size, shuffle=False)\n            self.val_loader = None\n\n    def get_train_loader(self):\n        return self.train_loader\n    \n    def get_val_loader(self):\n        return self.val_loader","metadata":{"execution":{"iopub.status.busy":"2024-06-13T03:18:20.144690Z","iopub.execute_input":"2024-06-13T03:18:20.145022Z","iopub.status.idle":"2024-06-13T03:18:20.154955Z","shell.execute_reply.started":"2024-06-13T03:18:20.144994Z","shell.execute_reply":"2024-06-13T03:18:20.154009Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"def evaluate_model(model, custom_train_loader, criterion, optimizer):\n    num_epochs = 1000\n    for epoch in range(num_epochs):\n        running_loss = 0.0\n        for inputs, labels in custom_train_loader.get_train_loader():        \n            optimizer.zero_grad()\n            outputs = model(inputs.view(-1, 12 * 1))\n            loss = criterion(outputs, labels)\n            loss.backward()\n            optimizer.step()\n            \n            running_loss += loss.item()\n            \n        if (epoch + 1) % 10 == 0:\n            print(f'Epoch {epoch+1}, Loss: {running_loss / len(custom_train_loader.get_train_loader())}')\n\n            model.eval()\n            running_val_loss = 0.0\n            with torch.no_grad():\n                for inputs, labels in custom_train_loader.get_val_loader():\n                    inputs, labels = inputs.to(device), labels.to(device)\n                    outputs = model(inputs.view(-1, 12 * 1))\n                    val_loss = criterion(outputs, labels)\n                    running_val_loss += val_loss.item()\n\n                avg_val_loss = running_val_loss / len(custom_train_loader.get_val_loader())\n                print(f'Validation Loss: {avg_val_loss}')\n            \n            val_accuracy, val_f1 = calculate_metrics(model, custom_train_loader.get_val_loader())\n            print(f'Val Accuracy: {val_accuracy:.4f}, Val F1-score: {val_f1:.4f}')\n            print()","metadata":{"execution":{"iopub.status.busy":"2024-06-13T05:41:59.565381Z","iopub.execute_input":"2024-06-13T05:41:59.566207Z","iopub.status.idle":"2024-06-13T05:41:59.575995Z","shell.execute_reply.started":"2024-06-13T05:41:59.566176Z","shell.execute_reply":"2024-06-13T05:41:59.575032Z"},"trusted":true},"execution_count":176,"outputs":[]},{"cell_type":"code","source":"device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")","metadata":{"execution":{"iopub.status.busy":"2024-06-13T05:41:59.989952Z","iopub.execute_input":"2024-06-13T05:41:59.990625Z","iopub.status.idle":"2024-06-13T05:41:59.995093Z","shell.execute_reply.started":"2024-06-13T05:41:59.990591Z","shell.execute_reply":"2024-06-13T05:41:59.994188Z"},"trusted":true},"execution_count":177,"outputs":[]},{"cell_type":"code","source":"data = pd.read_csv('/kaggle/input/brain-tumor/Brain Tumor.csv')\ndata = data.dropna()","metadata":{"execution":{"iopub.status.busy":"2024-06-13T05:42:00.397117Z","iopub.execute_input":"2024-06-13T05:42:00.397499Z","iopub.status.idle":"2024-06-13T05:42:00.425503Z","shell.execute_reply.started":"2024-06-13T05:42:00.397469Z","shell.execute_reply":"2024-06-13T05:42:00.424710Z"},"trusted":true},"execution_count":178,"outputs":[]},{"cell_type":"code","source":"X = data.drop(\n    [\n        \"Image\",\n        \"Class\",\n    ],\n    axis=1,\n)\ny = data[\"Class\"]","metadata":{"execution":{"iopub.status.busy":"2024-06-13T05:42:31.706885Z","iopub.execute_input":"2024-06-13T05:42:31.707542Z","iopub.status.idle":"2024-06-13T05:42:31.713041Z","shell.execute_reply.started":"2024-06-13T05:42:31.707508Z","shell.execute_reply":"2024-06-13T05:42:31.711994Z"},"trusted":true},"execution_count":189,"outputs":[]},{"cell_type":"code","source":"X.columns\ncolumn_count = X.shape[1]\ncolumn_count","metadata":{"execution":{"iopub.status.busy":"2024-06-13T05:42:36.734842Z","iopub.execute_input":"2024-06-13T05:42:36.735452Z","iopub.status.idle":"2024-06-13T05:42:36.741028Z","shell.execute_reply.started":"2024-06-13T05:42:36.735418Z","shell.execute_reply":"2024-06-13T05:42:36.740244Z"},"trusted":true},"execution_count":192,"outputs":[{"execution_count":192,"output_type":"execute_result","data":{"text/plain":"12"},"metadata":{}}]},{"cell_type":"code","source":"X['log_mean'] = np.log(X['Mean'])\nX['log_variance'] = np.log(X['Variance'])\nX['log_std_dev'] = np.log(X['Standard Deviation'])\nX['log_entropy'] = np.log(X['Entropy'])  # possibly not need to take log\nX['log_skewness'] = np.log(X['Skewness']) # possibly not need to take log\nX['log_kurtosis'] = np.log(X['Kurtosis'])\nX['log_contrast'] = np.log(X['Contrast'])\nX['log_energy'] = np.log(X['Energy'] / (1 - X['Energy'])) # may take this out\nX['log_asm'] = np.log(X['ASM'] / (1 - X['ASM'])) # may take this out\nX['log_homogeneity'] = np.log(X['Homogeneity'] / (1 - X['Homogeneity'])) # may take this out\nX['log_dissimilarity'] = np.log(X['Dissimilarity'])\nX['log_correlation'] = np.log(X['Correlation'] / (1 - X['Correlation']))\n\nnew_columns = [\n    'log_mean', 'log_variance', 'log_std_dev', 'log_entropy', 'log_skewness', \n    'log_kurtosis', 'log_contrast', 'log_energy', 'log_asm', 'log_homogeneity', \n    'log_dissimilarity', 'log_correlation'\n]\n\nX = X.loc[:, new_columns]\n\n# negative_columns = X * -1\n# negative_columns.columns = [f\"negative_{col}\" for col in new_columns]\n\n# X = pd.concat([X, negative_columns], axis=1)","metadata":{"execution":{"iopub.status.busy":"2024-06-13T05:42:32.598848Z","iopub.execute_input":"2024-06-13T05:42:32.599545Z","iopub.status.idle":"2024-06-13T05:42:32.616673Z","shell.execute_reply.started":"2024-06-13T05:42:32.599514Z","shell.execute_reply":"2024-06-13T05:42:32.615731Z"},"trusted":true},"execution_count":191,"outputs":[]},{"cell_type":"code","source":"x_scaler = StandardScaler()\nx_scaled = x_scaler.fit_transform(X)\n\nlabel_encoder = LabelEncoder()\ny_encoded = label_encoder.fit_transform(y)","metadata":{"execution":{"iopub.status.busy":"2024-06-13T05:42:38.482692Z","iopub.execute_input":"2024-06-13T05:42:38.483044Z","iopub.status.idle":"2024-06-13T05:42:38.494173Z","shell.execute_reply.started":"2024-06-13T05:42:38.483016Z","shell.execute_reply":"2024-06-13T05:42:38.493152Z"},"trusted":true},"execution_count":193,"outputs":[]},{"cell_type":"code","source":"custom_train_loader = CustomDataLoader(x_scaled, y_encoded, batch_size=16, validation_size=0.2, shuffle=True)","metadata":{"execution":{"iopub.status.busy":"2024-06-13T05:42:40.271096Z","iopub.execute_input":"2024-06-13T05:42:40.271490Z","iopub.status.idle":"2024-06-13T05:42:40.281248Z","shell.execute_reply.started":"2024-06-13T05:42:40.271454Z","shell.execute_reply":"2024-06-13T05:42:40.280285Z"},"trusted":true},"execution_count":194,"outputs":[]},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nfrom numpy import pi\n\nclass CustomLayer1(nn.Module):\n    def __init__(self, input_dim, output_dim):\n        super(CustomLayer1, self).__init__()\n        self.sigmoid = nn.Sigmoid()\n        self.hard_sigmoid = nn.Hardsigmoid()\n        self.linear_sigmoid = nn.Linear(input_dim, output_dim)\n        self.relu = nn.ReLU()\n        self.softplus = nn.Softplus()\n        self.tanh = nn.Tanh()\n\n        nn.init.zeros_(self.linear_sigmoid.weight)\n        nn.init.zeros_(self.linear_sigmoid.bias)\n\n    def forward(self, x, prev_x):\n#         sigmoid_gate = self.sigmoid(2 * self.linear_sigmoid(prev_x))\n#         return x * sigmoid_gate * 2\n#         cdf_gate = 1 + torch.erf(self.linear_sigmoid(prev_x) / torch.sqrt(torch.tensor(2.0)))\n        err_gate = 1 + torch.erf(self.linear_sigmoid(prev_x) * np.sqrt(pi) / 2)\n        return x * err_gate","metadata":{"execution":{"iopub.status.busy":"2024-06-13T05:43:53.399590Z","iopub.execute_input":"2024-06-13T05:43:53.400177Z","iopub.status.idle":"2024-06-13T05:43:53.409590Z","shell.execute_reply.started":"2024-06-13T05:43:53.400134Z","shell.execute_reply":"2024-06-13T05:43:53.408615Z"},"trusted":true},"execution_count":201,"outputs":[]},{"cell_type":"code","source":"class DenseMLP(nn.Module):\n    def __init__(self, input_size, num_layers, output_size):\n        super(DenseMLP, self).__init__()        \n        self.num_layers = num_layers\n        self.layers = nn.ModuleList()\n        self.custom_layers = nn.ModuleList()\n   \n        layer_size = input_size\n        for i in range(1, num_layers):\n            self.layers.append(nn.Linear(layer_size, layer_size))\n            self.custom_layers.append(CustomLayer1(layer_size, layer_size))\n            layer_size *= 2\n    \n        self.layers.append(nn.Linear(layer_size, output_size))       \n        self.custom_layers.append(CustomLayer1(layer_size, output_size))\n        self._initialize_weights()\n\n    def forward(self, x):\n        outputs = [x]\n        \n        for layer, custom_layer in zip(self.layers, self.custom_layers):\n            concatenated_outputs = torch.cat(outputs, dim=1)\n            out = layer(concatenated_outputs)\n            out = custom_layer(out, concatenated_outputs)\n            outputs.append(out)\n\n        return outputs[-1]\n    \n    def _initialize_weights(self):\n        for i, layer in enumerate(self.layers):\n            if isinstance(layer, nn.Linear):\n                if i == len(self.layers) - 1:\n                    layer.weight.data.fill_(0)\n                else:\n                    in_features = layer.weight.size(1)\n                    eye_matrix = torch.eye(in_features)\n                    layer.weight.data = eye_matrix\n                if layer.bias is not None:\n                    nn.init.normal_(layer.bias, mean=0.0, std=1.0)\n                    layer.bias.data.zero_()\n\nmodel = DenseMLP(12, 2, 2).to(device)\ncriterion = nn.CrossEntropyLoss()\nsummary(model, input_size=(1,12))","metadata":{"execution":{"iopub.status.busy":"2024-06-13T05:45:51.533677Z","iopub.execute_input":"2024-06-13T05:45:51.534387Z","iopub.status.idle":"2024-06-13T05:45:51.555827Z","shell.execute_reply.started":"2024-06-13T05:45:51.534355Z","shell.execute_reply":"2024-06-13T05:45:51.554859Z"},"trusted":true},"execution_count":210,"outputs":[{"execution_count":210,"output_type":"execute_result","data":{"text/plain":"==========================================================================================\nLayer (type:depth-idx)                   Output Shape              Param #\n==========================================================================================\nDenseMLP                                 [1, 2]                    --\n├─ModuleList: 1-3                        --                        (recursive)\n│    └─Linear: 2-1                       [1, 12]                   156\n├─ModuleList: 1-4                        --                        (recursive)\n│    └─CustomLayer1: 2-2                 [1, 12]                   --\n│    │    └─Linear: 3-1                  [1, 12]                   156\n├─ModuleList: 1-3                        --                        (recursive)\n│    └─Linear: 2-3                       [1, 2]                    50\n├─ModuleList: 1-4                        --                        (recursive)\n│    └─CustomLayer1: 2-4                 [1, 2]                    --\n│    │    └─Linear: 3-2                  [1, 2]                    50\n==========================================================================================\nTotal params: 412\nTrainable params: 412\nNon-trainable params: 0\nTotal mult-adds (M): 0.00\n==========================================================================================\nInput size (MB): 0.00\nForward/backward pass size (MB): 0.00\nParams size (MB): 0.00\nEstimated Total Size (MB): 0.00\n=========================================================================================="},"metadata":{}}]},{"cell_type":"code","source":"optimizer = optim.Adam(model.parameters(), lr=0.01)\nevaluate_model(model, custom_train_loader, criterion, optimizer)","metadata":{"execution":{"iopub.status.busy":"2024-06-13T05:45:52.137249Z","iopub.execute_input":"2024-06-13T05:45:52.137609Z"},"trusted":true},"execution_count":null,"outputs":[{"name":"stdout","text":"Epoch 10, Loss: 0.03374789767039052\nValidation Loss: 0.018516283520644567\nVal Accuracy: 0.9973, Val F1-score: 0.9973\n\nEpoch 20, Loss: 0.025564137760444347\nValidation Loss: 0.017076328937856516\nVal Accuracy: 0.9960, Val F1-score: 0.9960\n\nEpoch 30, Loss: 0.017167417661641225\nValidation Loss: 0.01940715803722422\nVal Accuracy: 0.9960, Val F1-score: 0.9960\n\n","output_type":"stream"}]},{"cell_type":"code","source":"for name, param in model.named_parameters():\n    print(f\"{name}: {param.data}\")","metadata":{"execution":{"iopub.status.busy":"2024-06-13T05:37:19.603693Z","iopub.execute_input":"2024-06-13T05:37:19.604501Z","iopub.status.idle":"2024-06-13T05:37:19.616604Z","shell.execute_reply.started":"2024-06-13T05:37:19.604459Z","shell.execute_reply":"2024-06-13T05:37:19.615562Z"},"trusted":true},"execution_count":175,"outputs":[{"name":"stdout","text":"layers.0.weight: tensor([[-6.8681, -0.2523, -0.2523, -2.5118, -0.1016, -5.4633,  5.3171,  0.5473,\n         -1.4294,  2.5647, -0.6739,  2.9581,  6.8681,  0.2523,  0.2523,  2.5118,\n          0.1016,  5.4633, -5.3171, -0.5473,  1.4294, -2.5647,  0.6739, -2.9581],\n        [ 2.4487,  1.1285,  1.1285, -1.4642,  1.8403,  5.7803,  1.2572, -1.7371,\n         -1.5784,  0.8376, -2.1668, -0.0709, -2.4487, -1.1285, -1.1285,  1.4642,\n         -1.8403, -5.7803, -1.2572,  1.7371,  1.5784, -0.8376,  2.1668,  0.0709]],\n       device='cuda:0')\nlayers.0.bias: tensor([-5.2871, -0.1532], device='cuda:0')\ncustom_layers.0.linear_sigmoid.weight: tensor([[-2.8622,  3.5002,  3.5002, -1.2077, -0.3514, -2.7014,  0.1911, -4.0444,\n         -2.4567,  4.5526, -2.3954,  0.5576,  2.8622, -3.5002, -3.5002,  1.2077,\n          0.3514,  2.7014, -0.1911,  4.0444,  2.4567, -4.5526,  2.3954, -0.5576],\n        [-1.2468,  0.1080,  0.1080, -0.8848,  1.2655,  0.4896, -1.3350, -0.4135,\n         -0.7122,  6.3164,  2.3583, -1.3577,  1.2468, -0.1080, -0.1080,  0.8848,\n         -1.2655, -0.4896,  1.3350,  0.4135,  0.7122, -6.3164, -2.3583,  1.3577]],\n       device='cuda:0')\ncustom_layers.0.linear_sigmoid.bias: tensor([13.5981, 12.5767], device='cuda:0')\n","output_type":"stream"}]},{"cell_type":"code","source":"optimizer = optim.Adam(model.parameters(), lr=0.01)\nevaluate_model(model, custom_train_loader, criterion, optimizer)","metadata":{"execution":{"iopub.status.busy":"2024-06-13T05:23:13.264109Z","iopub.execute_input":"2024-06-13T05:23:13.265083Z","iopub.status.idle":"2024-06-13T05:28:01.557539Z","shell.execute_reply.started":"2024-06-13T05:23:13.265041Z","shell.execute_reply":"2024-06-13T05:28:01.556527Z"},"collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":170,"outputs":[{"name":"stdout","text":"Epoch 10, Loss: 0.034474270050600234\nValidation Loss: 0.02292079052222107\nVal Accuracy: 0.9920, Val F1-score: 0.9920\n\nEpoch 20, Loss: 0.029318530854267666\nValidation Loss: 0.020633031149448772\nVal Accuracy: 0.9920, Val F1-score: 0.9920\n\nEpoch 30, Loss: 0.027947709126820167\nValidation Loss: 0.019263102932484344\nVal Accuracy: 0.9934, Val F1-score: 0.9934\n\nEpoch 40, Loss: 0.026803459035710103\nValidation Loss: 0.0184571918958909\nVal Accuracy: 0.9947, Val F1-score: 0.9947\n\nEpoch 50, Loss: 0.0262053241051189\nValidation Loss: 0.017474125321655265\nVal Accuracy: 0.9947, Val F1-score: 0.9947\n\nEpoch 60, Loss: 0.025492683169273184\nValidation Loss: 0.01674341122856049\nVal Accuracy: 0.9947, Val F1-score: 0.9947\n\nEpoch 70, Loss: 0.02474595881091662\nValidation Loss: 0.015327555445253438\nVal Accuracy: 0.9947, Val F1-score: 0.9947\n\nEpoch 80, Loss: 0.02411196921149928\nValidation Loss: 0.01419157876420248\nVal Accuracy: 0.9960, Val F1-score: 0.9960\n\nEpoch 90, Loss: 0.023413314129443495\nValidation Loss: 0.013083279811250273\nVal Accuracy: 0.9960, Val F1-score: 0.9960\n\nEpoch 100, Loss: 0.02260296596606533\nValidation Loss: 0.013925569803442764\nVal Accuracy: 0.9947, Val F1-score: 0.9947\n\nEpoch 110, Loss: 0.022259081442806597\nValidation Loss: 0.013500087164288743\nVal Accuracy: 0.9947, Val F1-score: 0.9947\n\nEpoch 120, Loss: 0.02201081736012942\nValidation Loss: 0.013222828059686739\nVal Accuracy: 0.9947, Val F1-score: 0.9947\n\nEpoch 130, Loss: 0.02171354109937152\nValidation Loss: 0.012930002203688673\nVal Accuracy: 0.9960, Val F1-score: 0.9960\n\nEpoch 140, Loss: 0.021466252794505\nValidation Loss: 0.012790779605817685\nVal Accuracy: 0.9960, Val F1-score: 0.9960\n\nEpoch 150, Loss: 0.02128290107867499\nValidation Loss: 0.012611874361520373\nVal Accuracy: 0.9960, Val F1-score: 0.9960\n\nEpoch 160, Loss: 0.021116252426866464\nValidation Loss: 0.012380167634854425\nVal Accuracy: 0.9960, Val F1-score: 0.9960\n\nEpoch 170, Loss: 0.0208504966280246\nValidation Loss: 0.012346208144019025\nVal Accuracy: 0.9947, Val F1-score: 0.9947\n\nEpoch 180, Loss: 0.020603780975908235\nValidation Loss: 0.010493711254990027\nVal Accuracy: 0.9960, Val F1-score: 0.9960\n\nEpoch 190, Loss: 0.02048015972104231\nValidation Loss: 0.009902705581855761\nVal Accuracy: 0.9973, Val F1-score: 0.9973\n\nEpoch 200, Loss: 0.019864377968655457\nValidation Loss: 0.009280324752632888\nVal Accuracy: 0.9973, Val F1-score: 0.9973\n\nEpoch 210, Loss: 0.01933887185873212\nValidation Loss: 0.008562319367873291\nVal Accuracy: 0.9973, Val F1-score: 0.9973\n\nEpoch 220, Loss: 0.02009473597707162\nValidation Loss: 0.008683872595061834\nVal Accuracy: 0.9973, Val F1-score: 0.9973\n\nEpoch 230, Loss: 0.019035488751978604\nValidation Loss: 0.00938831459343703\nVal Accuracy: 0.9973, Val F1-score: 0.9973\n\nEpoch 240, Loss: 0.0191596334124846\nValidation Loss: 0.00842737635466051\nVal Accuracy: 0.9973, Val F1-score: 0.9973\n\nEpoch 250, Loss: 0.018905530602622816\nValidation Loss: 0.008411765627490317\nVal Accuracy: 0.9973, Val F1-score: 0.9973\n\nEpoch 260, Loss: 0.018875313797336816\nValidation Loss: 0.007650538736105735\nVal Accuracy: 0.9973, Val F1-score: 0.9973\n\nEpoch 270, Loss: 0.018574353120627128\nValidation Loss: 0.007477667375435241\nVal Accuracy: 0.9973, Val F1-score: 0.9973\n\nEpoch 280, Loss: 0.018583122714206247\nValidation Loss: 0.008097987093899897\nVal Accuracy: 0.9973, Val F1-score: 0.9973\n\nEpoch 290, Loss: 0.018355547775295765\nValidation Loss: 0.0073294985954414455\nVal Accuracy: 0.9973, Val F1-score: 0.9973\n\nEpoch 300, Loss: 0.018518799716217182\nValidation Loss: 0.007295229031247648\nVal Accuracy: 0.9973, Val F1-score: 0.9973\n\nEpoch 310, Loss: 0.01834328223409928\nValidation Loss: 0.0073559593614784076\nVal Accuracy: 0.9973, Val F1-score: 0.9973\n\nEpoch 320, Loss: 0.018269019173075135\nValidation Loss: 0.007126635215859285\nVal Accuracy: 0.9973, Val F1-score: 0.9973\n\nEpoch 330, Loss: 0.017859809795125754\nValidation Loss: 0.0071674366790404065\nVal Accuracy: 0.9973, Val F1-score: 0.9973\n\nEpoch 340, Loss: 0.018373917108030515\nValidation Loss: 0.008137329818391473\nVal Accuracy: 0.9960, Val F1-score: 0.9960\n\nEpoch 350, Loss: 0.018217857229916205\nValidation Loss: 0.007984724107600982\nVal Accuracy: 0.9960, Val F1-score: 0.9960\n\nEpoch 360, Loss: 0.018201215231867626\nValidation Loss: 0.008013458348296846\nVal Accuracy: 0.9960, Val F1-score: 0.9960\n\nEpoch 370, Loss: 0.018206032248401612\nValidation Loss: 0.008275424759325745\nVal Accuracy: 0.9973, Val F1-score: 0.9973\n\nEpoch 380, Loss: 0.018535287743569825\nValidation Loss: 0.008527394324815418\nVal Accuracy: 0.9960, Val F1-score: 0.9960\n\nEpoch 390, Loss: 0.0181430697710744\nValidation Loss: 0.00768899725828831\nVal Accuracy: 0.9973, Val F1-score: 0.9973\n\nEpoch 400, Loss: 0.01803486695373113\nValidation Loss: 0.007351602680144491\nVal Accuracy: 0.9987, Val F1-score: 0.9987\n\nEpoch 410, Loss: 0.01790964033794984\nValidation Loss: 0.0066990539124797506\nVal Accuracy: 0.9973, Val F1-score: 0.9973\n\nEpoch 420, Loss: 0.018013307307357413\nValidation Loss: 0.007266970376385018\nVal Accuracy: 0.9987, Val F1-score: 0.9987\n\nEpoch 430, Loss: 0.017850668350476967\nValidation Loss: 0.006701918288626985\nVal Accuracy: 0.9987, Val F1-score: 0.9987\n\nEpoch 440, Loss: 0.017867924980019927\nValidation Loss: 0.0066626155297685825\nVal Accuracy: 0.9987, Val F1-score: 0.9987\n\nEpoch 450, Loss: 0.01851168172230121\nValidation Loss: 0.00770044437332847\nVal Accuracy: 0.9960, Val F1-score: 0.9960\n\nEpoch 460, Loss: 0.01793546246655447\nValidation Loss: 0.006974208996261666\nVal Accuracy: 0.9987, Val F1-score: 0.9987\n\nEpoch 470, Loss: 0.017107273825636246\nValidation Loss: 0.007094344136764501\nVal Accuracy: 0.9973, Val F1-score: 0.9973\n\nEpoch 480, Loss: 0.0182546106428565\nValidation Loss: 0.007659088087940802\nVal Accuracy: 0.9973, Val F1-score: 0.9973\n\nEpoch 490, Loss: 0.01791395533396855\nValidation Loss: 0.006922381612165296\nVal Accuracy: 0.9987, Val F1-score: 0.9987\n\nEpoch 500, Loss: 0.017852448729157446\nValidation Loss: 0.006683975707442613\nVal Accuracy: 0.9987, Val F1-score: 0.9987\n\nEpoch 510, Loss: 0.01794166079602561\nValidation Loss: 0.0064996993132637\nVal Accuracy: 0.9987, Val F1-score: 0.9987\n\nEpoch 520, Loss: 0.016713788415057974\nValidation Loss: 0.006400144295932127\nVal Accuracy: 0.9987, Val F1-score: 0.9987\n\nEpoch 530, Loss: 0.01656610889038283\nValidation Loss: 0.006344012570999998\nVal Accuracy: 0.9987, Val F1-score: 0.9987\n\nEpoch 540, Loss: 0.017951816845359073\nValidation Loss: 0.006517551150739227\nVal Accuracy: 1.0000, Val F1-score: 1.0000\n\nEpoch 550, Loss: 0.01648259204704922\nValidation Loss: 0.006323167523902384\nVal Accuracy: 0.9987, Val F1-score: 0.9987\n\nEpoch 560, Loss: 0.020100767015731453\nValidation Loss: 0.006571665367565889\nVal Accuracy: 0.9973, Val F1-score: 0.9973\n\nEpoch 570, Loss: 0.016450636663694312\nValidation Loss: 0.006205710930650217\nVal Accuracy: 1.0000, Val F1-score: 1.0000\n\nEpoch 580, Loss: 0.018909648269108344\nValidation Loss: 0.007844828853832494\nVal Accuracy: 0.9960, Val F1-score: 0.9960\n\nEpoch 590, Loss: 0.01777576341444876\nValidation Loss: 0.006396158440083492\nVal Accuracy: 1.0000, Val F1-score: 1.0000\n\nEpoch 600, Loss: 0.01645580278196045\nValidation Loss: 0.00625719670615658\nVal Accuracy: 1.0000, Val F1-score: 1.0000\n\nEpoch 610, Loss: 0.0163525978648837\nValidation Loss: 0.006257608708620725\nVal Accuracy: 1.0000, Val F1-score: 1.0000\n\nEpoch 620, Loss: 0.016404776357171188\nValidation Loss: 0.006140622617592702\nVal Accuracy: 1.0000, Val F1-score: 1.0000\n\nEpoch 630, Loss: 0.019448053500867306\nValidation Loss: 0.006014902192546264\nVal Accuracy: 0.9987, Val F1-score: 0.9987\n\nEpoch 640, Loss: 0.016345773729429833\nValidation Loss: 0.006170862524636019\nVal Accuracy: 1.0000, Val F1-score: 1.0000\n\nEpoch 650, Loss: 0.016238007583984703\nValidation Loss: 0.006151701968893697\nVal Accuracy: 1.0000, Val F1-score: 1.0000\n\nEpoch 660, Loss: 0.01792166313289467\nValidation Loss: 0.0074149764295675285\nVal Accuracy: 0.9973, Val F1-score: 0.9973\n\nEpoch 670, Loss: 0.017563068498031967\nValidation Loss: 0.006242158139148894\nVal Accuracy: 1.0000, Val F1-score: 1.0000\n\nEpoch 680, Loss: 0.016323125008865798\nValidation Loss: 0.006271380853221172\nVal Accuracy: 1.0000, Val F1-score: 1.0000\n\nEpoch 690, Loss: 0.019557676772196875\nValidation Loss: 0.005923775860193399\nVal Accuracy: 0.9973, Val F1-score: 0.9973\n\nEpoch 700, Loss: 0.01627532614485723\nValidation Loss: 0.00611887516788201\nVal Accuracy: 1.0000, Val F1-score: 1.0000\n\nEpoch 710, Loss: 0.019774659176497848\nValidation Loss: 0.006198306024160456\nVal Accuracy: 0.9987, Val F1-score: 0.9987\n\nEpoch 720, Loss: 0.01625904134755828\nValidation Loss: 0.005997899817313505\nVal Accuracy: 1.0000, Val F1-score: 1.0000\n\nEpoch 730, Loss: 0.019408280805840436\nValidation Loss: 0.0068723765825756305\nVal Accuracy: 0.9973, Val F1-score: 0.9973\n\nEpoch 740, Loss: 0.01616362039315807\nValidation Loss: 0.006183317923649234\nVal Accuracy: 1.0000, Val F1-score: 1.0000\n\nEpoch 750, Loss: 0.01622681893499421\nValidation Loss: 0.006046651374956191\nVal Accuracy: 1.0000, Val F1-score: 1.0000\n\nEpoch 760, Loss: 0.017901860198031222\nValidation Loss: 0.006824783260902893\nVal Accuracy: 0.9987, Val F1-score: 0.9987\n\nEpoch 770, Loss: 0.016198825115275393\nValidation Loss: 0.006219406615625071\nVal Accuracy: 1.0000, Val F1-score: 1.0000\n\nEpoch 780, Loss: 0.01863934481254738\nValidation Loss: 0.008075889246184076\nVal Accuracy: 0.9973, Val F1-score: 0.9973\n\nEpoch 790, Loss: 0.016080258186359015\nValidation Loss: 0.006387605870938155\nVal Accuracy: 0.9987, Val F1-score: 0.9987\n\nEpoch 800, Loss: 0.01619472948960184\nValidation Loss: 0.006138794827845591\nVal Accuracy: 1.0000, Val F1-score: 1.0000\n\nEpoch 810, Loss: 0.016100180843554152\nValidation Loss: 0.006080780826776137\nVal Accuracy: 1.0000, Val F1-score: 1.0000\n\nEpoch 820, Loss: 0.016174016644721116\nValidation Loss: 0.005969880917270724\nVal Accuracy: 1.0000, Val F1-score: 1.0000\n\nEpoch 830, Loss: 0.01836783817955596\nValidation Loss: 0.00631857122408445\nVal Accuracy: 0.9973, Val F1-score: 0.9973\n\nEpoch 840, Loss: 0.01613845278672204\nValidation Loss: 0.005899606896186664\nVal Accuracy: 1.0000, Val F1-score: 1.0000\n\nEpoch 850, Loss: 0.017862199728744994\nValidation Loss: 0.005685261741877075\nVal Accuracy: 1.0000, Val F1-score: 1.0000\n\nEpoch 860, Loss: 0.017528099118095174\nValidation Loss: 0.006079763294941938\nVal Accuracy: 1.0000, Val F1-score: 1.0000\n\nEpoch 870, Loss: 0.016137033008643813\nValidation Loss: 0.006122552970849071\nVal Accuracy: 1.0000, Val F1-score: 1.0000\n\nEpoch 880, Loss: 0.01755043895830275\nValidation Loss: 0.0061872089110561745\nVal Accuracy: 1.0000, Val F1-score: 1.0000\n\nEpoch 890, Loss: 0.019527959415133165\nValidation Loss: 0.005578746827721452\nVal Accuracy: 1.0000, Val F1-score: 1.0000\n\nEpoch 900, Loss: 0.01611295673131443\nValidation Loss: 0.00619558717441097\nVal Accuracy: 1.0000, Val F1-score: 1.0000\n\nEpoch 910, Loss: 0.016124512390760955\nValidation Loss: 0.006052051184286977\nVal Accuracy: 1.0000, Val F1-score: 1.0000\n\nEpoch 920, Loss: 0.01698744485972081\nValidation Loss: 0.006803037577753468\nVal Accuracy: 0.9973, Val F1-score: 0.9973\n\nEpoch 930, Loss: 0.017672312064090128\nValidation Loss: 0.006229777229367794\nVal Accuracy: 1.0000, Val F1-score: 1.0000\n\nEpoch 940, Loss: 0.017935829088886412\nValidation Loss: 0.006655421603585528\nVal Accuracy: 1.0000, Val F1-score: 1.0000\n\nEpoch 950, Loss: 0.016135658379521432\nValidation Loss: 0.006458619496108635\nVal Accuracy: 0.9987, Val F1-score: 0.9987\n\nEpoch 960, Loss: 0.01613514346120599\nValidation Loss: 0.006252715242395457\nVal Accuracy: 1.0000, Val F1-score: 1.0000\n\nEpoch 970, Loss: 0.016120741093663456\nValidation Loss: 0.006085624092614002\nVal Accuracy: 1.0000, Val F1-score: 1.0000\n\nEpoch 980, Loss: 0.018123263567049904\nValidation Loss: 0.0056792574010830306\nVal Accuracy: 1.0000, Val F1-score: 1.0000\n\nEpoch 990, Loss: 0.016046609813369302\nValidation Loss: 0.00607618038499765\nVal Accuracy: 1.0000, Val F1-score: 1.0000\n\nEpoch 1000, Loss: 0.01608741062843274\nValidation Loss: 0.005967529504875557\nVal Accuracy: 1.0000, Val F1-score: 1.0000\n\n","output_type":"stream"}]},{"cell_type":"code","source":"for name, param in model.named_parameters():\n    print(f\"{name}: {param.data}\")","metadata":{"execution":{"iopub.status.busy":"2024-06-13T05:29:39.175509Z","iopub.execute_input":"2024-06-13T05:29:39.176194Z","iopub.status.idle":"2024-06-13T05:29:39.188638Z","shell.execute_reply.started":"2024-06-13T05:29:39.176151Z","shell.execute_reply":"2024-06-13T05:29:39.187679Z"},"trusted":true},"execution_count":171,"outputs":[{"name":"stdout","text":"layers.0.weight: tensor([[-6.8731, -0.2201, -0.2201, -2.3613,  0.0142, -5.7033,  4.9509,  0.6240,\n         -1.2830,  2.7564, -0.5424,  2.8271,  6.8731,  0.2201,  0.2201,  2.3613,\n         -0.0142,  5.7033, -4.9509, -0.6240,  1.2830, -2.7564,  0.5424, -2.8271],\n        [ 2.7096,  1.0830,  1.0830, -1.4308,  1.6737,  5.9750,  0.9735, -1.4588,\n         -1.4514,  0.8855, -2.1203, -0.1945, -2.7096, -1.0830, -1.0830,  1.4308,\n         -1.6737, -5.9750, -0.9735,  1.4588,  1.4514, -0.8855,  2.1203,  0.1945]],\n       device='cuda:0')\nlayers.0.bias: tensor([-5.2766, -0.1409], device='cuda:0')\ncustom_layers.0.linear_sigmoid.weight: tensor([[-2.8046,  3.4831,  3.4831, -1.0879, -0.1621, -2.5287, -0.1675, -3.5384,\n         -2.1596,  4.3878, -2.2100, -0.1946,  2.8046, -3.4831, -3.4831,  1.0879,\n          0.1621,  2.5287,  0.1675,  3.5384,  2.1596, -4.3878,  2.2100,  0.1946],\n        [ 0.1176,  0.1723,  0.1723, -0.3653,  0.4894,  0.4409, -0.6923,  0.0401,\n         -0.1684,  2.3765,  0.9306, -0.6390, -0.1176, -0.1723, -0.1723,  0.3653,\n         -0.4894, -0.4409,  0.6923, -0.0401,  0.1684, -2.3765, -0.9306,  0.6390]],\n       device='cuda:0')\ncustom_layers.0.linear_sigmoid.bias: tensor([12.7407,  8.9562], device='cuda:0')\n","output_type":"stream"}]},{"cell_type":"code","source":"optimizer = optim.Adam(model.parameters(), lr=0.01)\nevaluate_model(model, custom_train_loader, criterion, optimizer)","metadata":{"execution":{"iopub.status.busy":"2024-06-13T05:16:27.023730Z","iopub.execute_input":"2024-06-13T05:16:27.024290Z","iopub.status.idle":"2024-06-13T05:17:51.522794Z","shell.execute_reply.started":"2024-06-13T05:16:27.024251Z","shell.execute_reply":"2024-06-13T05:17:51.521439Z"},"collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":162,"outputs":[{"name":"stdout","text":"Epoch 10, Loss: 0.03519804935462566\nValidation Loss: 0.02270101151104124\nVal Accuracy: 0.9907, Val F1-score: 0.9907\n\nEpoch 20, Loss: 0.029666416785951857\nValidation Loss: 0.02001497138595217\nVal Accuracy: 0.9920, Val F1-score: 0.9920\n\nEpoch 30, Loss: 0.027499210976707725\nValidation Loss: 0.01910037436393471\nVal Accuracy: 0.9934, Val F1-score: 0.9934\n\nEpoch 40, Loss: 0.026332393833173522\nValidation Loss: 0.018331008238135382\nVal Accuracy: 0.9947, Val F1-score: 0.9947\n\nEpoch 50, Loss: 0.025513594991833897\nValidation Loss: 0.017594824046928654\nVal Accuracy: 0.9947, Val F1-score: 0.9947\n\nEpoch 60, Loss: 0.024862841031362316\nValidation Loss: 0.01695466785933301\nVal Accuracy: 0.9947, Val F1-score: 0.9947\n\nEpoch 70, Loss: 0.02431546530255382\nValidation Loss: 0.016420449978037748\nVal Accuracy: 0.9947, Val F1-score: 0.9947\n\nEpoch 80, Loss: 0.02382908844492463\nValidation Loss: 0.01600841553547146\nVal Accuracy: 0.9947, Val F1-score: 0.9947\n\nEpoch 90, Loss: 0.023368695846767405\nValidation Loss: 0.01785329153908795\nVal Accuracy: 0.9947, Val F1-score: 0.9947\n\nEpoch 100, Loss: 0.02222351386003851\nValidation Loss: 0.020493435611437388\nVal Accuracy: 0.9947, Val F1-score: 0.9947\n\nEpoch 110, Loss: 0.02157364560942973\nValidation Loss: 0.021028049752594313\nVal Accuracy: 0.9947, Val F1-score: 0.9947\n\nEpoch 120, Loss: 0.02111632458551654\nValidation Loss: 0.02151010940267639\nVal Accuracy: 0.9947, Val F1-score: 0.9947\n\nEpoch 130, Loss: 0.020712174625364763\nValidation Loss: 0.021779030107619946\nVal Accuracy: 0.9947, Val F1-score: 0.9947\n\nEpoch 140, Loss: 0.02029939301041594\nValidation Loss: 0.021919802670002042\nVal Accuracy: 0.9947, Val F1-score: 0.9947\n\nEpoch 150, Loss: 0.019869182092260827\nValidation Loss: 0.021755752468314665\nVal Accuracy: 0.9947, Val F1-score: 0.9947\n\nEpoch 160, Loss: 0.019254639597501517\nValidation Loss: 0.02148800046376742\nVal Accuracy: 0.9947, Val F1-score: 0.9947\n\nEpoch 170, Loss: 0.018472717725644006\nValidation Loss: 0.02085660183808861\nVal Accuracy: 0.9947, Val F1-score: 0.9947\n\nEpoch 180, Loss: 0.017648030492114473\nValidation Loss: 0.020213865866005893\nVal Accuracy: 0.9947, Val F1-score: 0.9947\n\nEpoch 190, Loss: 0.017138949950211077\nValidation Loss: 0.019900449358932686\nVal Accuracy: 0.9947, Val F1-score: 0.9947\n\nEpoch 200, Loss: 0.016723646833791614\nValidation Loss: 0.01983342536943618\nVal Accuracy: 0.9947, Val F1-score: 0.9947\n\nEpoch 210, Loss: 0.016417571967399493\nValidation Loss: 0.019815058823207405\nVal Accuracy: 0.9947, Val F1-score: 0.9947\n\nEpoch 220, Loss: 0.016153781702012707\nValidation Loss: 0.019874171261591078\nVal Accuracy: 0.9947, Val F1-score: 0.9947\n\nEpoch 230, Loss: 0.015942272036195117\nValidation Loss: 0.019914848530748246\nVal Accuracy: 0.9947, Val F1-score: 0.9947\n\nEpoch 240, Loss: 0.01579238717602204\nValidation Loss: 0.019950358835937026\nVal Accuracy: 0.9947, Val F1-score: 0.9947\n\nEpoch 250, Loss: 0.01566006401150257\nValidation Loss: 0.01996432144783696\nVal Accuracy: 0.9947, Val F1-score: 0.9947\n\nEpoch 260, Loss: 0.015506279501668557\nValidation Loss: 0.020005394525748788\nVal Accuracy: 0.9947, Val F1-score: 0.9947\n\nEpoch 270, Loss: 0.015381033257298688\nValidation Loss: 0.020096921907606147\nVal Accuracy: 0.9947, Val F1-score: 0.9947\n\nEpoch 280, Loss: 0.01527261326151783\nValidation Loss: 0.020213805772132238\nVal Accuracy: 0.9947, Val F1-score: 0.9947\n\nEpoch 290, Loss: 0.015173867139602497\nValidation Loss: 0.02034259984282838\nVal Accuracy: 0.9947, Val F1-score: 0.9947\n\nEpoch 300, Loss: 0.015079641753605214\nValidation Loss: 0.020471112889722958\nVal Accuracy: 0.9947, Val F1-score: 0.9947\n\nEpoch 310, Loss: 0.014983564964821058\nValidation Loss: 0.02057465159684742\nVal Accuracy: 0.9947, Val F1-score: 0.9947\n\nEpoch 320, Loss: 0.014878760947437116\nValidation Loss: 0.0206205149261643\nVal Accuracy: 0.9947, Val F1-score: 0.9947\n\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","Cell \u001b[0;32mIn[162], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m optimizer \u001b[38;5;241m=\u001b[39m optim\u001b[38;5;241m.\u001b[39mAdam(model\u001b[38;5;241m.\u001b[39mparameters(), lr\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.01\u001b[39m)\n\u001b[0;32m----> 2\u001b[0m \u001b[43mevaluate_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcustom_train_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m)\u001b[49m\n","Cell \u001b[0;32mIn[112], line 9\u001b[0m, in \u001b[0;36mevaluate_model\u001b[0;34m(model, custom_train_loader, criterion, optimizer)\u001b[0m\n\u001b[1;32m      7\u001b[0m outputs \u001b[38;5;241m=\u001b[39m model(inputs\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m12\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m2\u001b[39m))\n\u001b[1;32m      8\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(outputs, labels)\n\u001b[0;32m----> 9\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     10\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m     12\u001b[0m running_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem()\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/_tensor.py:492\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    482\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    483\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    484\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    485\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    490\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    491\u001b[0m     )\n\u001b[0;32m--> 492\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    493\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    494\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/autograd/__init__.py:251\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    246\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    248\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    249\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    250\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 251\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    252\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    253\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    254\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    255\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    256\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    257\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    258\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    259\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "],"ename":"KeyboardInterrupt","evalue":"","output_type":"error"}]},{"cell_type":"code","source":"optimizer = optim.Adam(model.parameters(), lr=0.01)\nevaluate_model(model, custom_train_loader, criterion, optimizer)","metadata":{"execution":{"iopub.status.busy":"2024-06-13T05:08:22.276363Z","iopub.execute_input":"2024-06-13T05:08:22.277009Z","iopub.status.idle":"2024-06-13T05:12:51.226679Z","shell.execute_reply.started":"2024-06-13T05:08:22.276974Z","shell.execute_reply":"2024-06-13T05:12:51.225735Z"},"collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":156,"outputs":[{"name":"stdout","text":"Epoch 10, Loss: 0.03440670182459702\nValidation Loss: 0.022910369195869862\nVal Accuracy: 0.9920, Val F1-score: 0.9920\n\nEpoch 20, Loss: 0.030651397335147268\nValidation Loss: 0.020970775986825174\nVal Accuracy: 0.9907, Val F1-score: 0.9907\n\nEpoch 30, Loss: 0.02788389798510127\nValidation Loss: 0.019523509080765205\nVal Accuracy: 0.9920, Val F1-score: 0.9920\n\nEpoch 40, Loss: 0.026886521477237656\nValidation Loss: 0.018615830441528185\nVal Accuracy: 0.9934, Val F1-score: 0.9934\n\nEpoch 50, Loss: 0.02604515223548434\nValidation Loss: 0.01735817660843016\nVal Accuracy: 0.9947, Val F1-score: 0.9947\n\nEpoch 60, Loss: 0.025326186462544687\nValidation Loss: 0.015755007010568534\nVal Accuracy: 0.9960, Val F1-score: 0.9960\n\nEpoch 70, Loss: 0.02461849710407502\nValidation Loss: 0.014626990471801568\nVal Accuracy: 0.9960, Val F1-score: 0.9960\n\nEpoch 80, Loss: 0.024055235832701995\nValidation Loss: 0.0138137694695691\nVal Accuracy: 0.9960, Val F1-score: 0.9960\n\nEpoch 90, Loss: 0.02321863984868032\nValidation Loss: 0.012953134471861935\nVal Accuracy: 0.9960, Val F1-score: 0.9960\n\nEpoch 100, Loss: 0.022411487122624786\nValidation Loss: 0.013398734908037113\nVal Accuracy: 0.9947, Val F1-score: 0.9947\n\nEpoch 110, Loss: 0.022360289112364182\nValidation Loss: 0.01146830580603364\nVal Accuracy: 0.9960, Val F1-score: 0.9960\n\nEpoch 120, Loss: 0.021812005209615865\nValidation Loss: 0.011245386013987021\nVal Accuracy: 0.9960, Val F1-score: 0.9960\n\nEpoch 130, Loss: 0.02154969530929448\nValidation Loss: 0.010905265625960206\nVal Accuracy: 0.9960, Val F1-score: 0.9960\n\nEpoch 140, Loss: 0.021370312416590143\nValidation Loss: 0.01059900415161034\nVal Accuracy: 0.9960, Val F1-score: 0.9960\n\nEpoch 150, Loss: 0.021099852259101767\nValidation Loss: 0.01090863149279168\nVal Accuracy: 0.9947, Val F1-score: 0.9947\n\nEpoch 160, Loss: 0.02067270140624623\nValidation Loss: 0.010623317044798833\nVal Accuracy: 0.9960, Val F1-score: 0.9960\n\nEpoch 170, Loss: 0.02031812315124351\nValidation Loss: 0.0099853185859331\nVal Accuracy: 0.9973, Val F1-score: 0.9973\n\nEpoch 180, Loss: 0.019980037291965364\nValidation Loss: 0.009158102662861248\nVal Accuracy: 0.9973, Val F1-score: 0.9973\n\nEpoch 190, Loss: 0.019779446019540393\nValidation Loss: 0.008742640710693195\nVal Accuracy: 0.9973, Val F1-score: 0.9973\n\nEpoch 200, Loss: 0.019621360385421425\nValidation Loss: 0.00858202537995112\nVal Accuracy: 0.9973, Val F1-score: 0.9973\n\nEpoch 210, Loss: 0.019346934152421547\nValidation Loss: 0.009108486526713477\nVal Accuracy: 0.9973, Val F1-score: 0.9973\n\nEpoch 220, Loss: 0.019254037533743433\nValidation Loss: 0.008787289851047339\nVal Accuracy: 0.9973, Val F1-score: 0.9973\n\nEpoch 230, Loss: 0.019066188030644186\nValidation Loss: 0.008358961910545304\nVal Accuracy: 0.9973, Val F1-score: 0.9973\n\nEpoch 240, Loss: 0.018692127542563535\nValidation Loss: 0.007887913561077653\nVal Accuracy: 0.9973, Val F1-score: 0.9973\n\nEpoch 250, Loss: 0.018731014066720345\nValidation Loss: 0.008062657634596349\nVal Accuracy: 0.9973, Val F1-score: 0.9973\n\nEpoch 260, Loss: 0.01970365641096474\nValidation Loss: 0.008659705082112149\nVal Accuracy: 0.9960, Val F1-score: 0.9960\n\nEpoch 270, Loss: 0.018546293951525964\nValidation Loss: 0.007535135879839079\nVal Accuracy: 0.9973, Val F1-score: 0.9973\n\nEpoch 280, Loss: 0.018604522440359244\nValidation Loss: 0.008353291755023898\nVal Accuracy: 0.9960, Val F1-score: 0.9960\n\nEpoch 290, Loss: 0.018490874258570027\nValidation Loss: 0.00837634059045437\nVal Accuracy: 0.9960, Val F1-score: 0.9960\n\nEpoch 300, Loss: 0.019202766850665534\nValidation Loss: 0.008804036317374178\nVal Accuracy: 0.9960, Val F1-score: 0.9960\n\nEpoch 310, Loss: 0.01851858503406465\nValidation Loss: 0.008410716332392099\nVal Accuracy: 0.9973, Val F1-score: 0.9973\n\nEpoch 320, Loss: 0.018276051475960044\nValidation Loss: 0.007296995911647268\nVal Accuracy: 0.9973, Val F1-score: 0.9973\n\nEpoch 330, Loss: 0.018208006800235398\nValidation Loss: 0.008055690620977884\nVal Accuracy: 0.9960, Val F1-score: 0.9960\n\nEpoch 340, Loss: 0.01789582180245114\nValidation Loss: 0.007051094151447519\nVal Accuracy: 0.9973, Val F1-score: 0.9973\n\nEpoch 350, Loss: 0.018118910788820462\nValidation Loss: 0.00728014233276042\nVal Accuracy: 0.9973, Val F1-score: 0.9973\n\nEpoch 360, Loss: 0.01814459153404256\nValidation Loss: 0.007633189881135631\nVal Accuracy: 0.9987, Val F1-score: 0.9987\n\nEpoch 370, Loss: 0.01811822137049159\nValidation Loss: 0.008030470909340911\nVal Accuracy: 0.9973, Val F1-score: 0.9973\n\nEpoch 380, Loss: 0.020488878107178376\nValidation Loss: 0.008335086820678109\nVal Accuracy: 0.9960, Val F1-score: 0.9960\n\nEpoch 390, Loss: 0.020014858417194952\nValidation Loss: 0.006542965701238283\nVal Accuracy: 0.9973, Val F1-score: 0.9973\n\nEpoch 400, Loss: 0.018432867136520297\nValidation Loss: 0.007761664825197319\nVal Accuracy: 0.9960, Val F1-score: 0.9960\n\nEpoch 410, Loss: 0.017937544468713925\nValidation Loss: 0.0069527938010528105\nVal Accuracy: 0.9987, Val F1-score: 0.9987\n\nEpoch 420, Loss: 0.017979260761678377\nValidation Loss: 0.007138532934845898\nVal Accuracy: 0.9987, Val F1-score: 0.9987\n\nEpoch 430, Loss: 0.01738109909458744\nValidation Loss: 0.0069255857723729735\nVal Accuracy: 0.9973, Val F1-score: 0.9973\n\nEpoch 440, Loss: 0.017704378677409358\nValidation Loss: 0.006899648245409414\nVal Accuracy: 0.9973, Val F1-score: 0.9973\n\nEpoch 450, Loss: 0.017357799446610887\nValidation Loss: 0.007595862538992175\nVal Accuracy: 0.9973, Val F1-score: 0.9973\n\nEpoch 460, Loss: 0.017878484618615444\nValidation Loss: 0.007255009565988975\nVal Accuracy: 0.9987, Val F1-score: 0.9987\n\nEpoch 470, Loss: 0.017815518036549537\nValidation Loss: 0.006569515157305972\nVal Accuracy: 0.9987, Val F1-score: 0.9987\n\nEpoch 480, Loss: 0.017826683595129022\nValidation Loss: 0.006542576454021305\nVal Accuracy: 0.9987, Val F1-score: 0.9987\n\nEpoch 490, Loss: 0.017525756492716406\nValidation Loss: 0.006910842420351078\nVal Accuracy: 0.9973, Val F1-score: 0.9973\n\nEpoch 500, Loss: 0.016671040933410982\nValidation Loss: 0.006484539778650007\nVal Accuracy: 0.9987, Val F1-score: 0.9987\n\nEpoch 510, Loss: 0.01657742765663748\nValidation Loss: 0.006453711174104981\nVal Accuracy: 0.9987, Val F1-score: 0.9987\n\nEpoch 520, Loss: 0.01808660826008649\nValidation Loss: 0.006842392262103776\nVal Accuracy: 1.0000, Val F1-score: 1.0000\n\nEpoch 530, Loss: 0.017910194738716065\nValidation Loss: 0.0075932246938036725\nVal Accuracy: 0.9973, Val F1-score: 0.9973\n\nEpoch 540, Loss: 0.017673925130792786\nValidation Loss: 0.006464352393348231\nVal Accuracy: 0.9987, Val F1-score: 0.9987\n\nEpoch 550, Loss: 0.0165686426340357\nValidation Loss: 0.006379401034934655\nVal Accuracy: 0.9987, Val F1-score: 0.9987\n\nEpoch 560, Loss: 0.016556849787083956\nValidation Loss: 0.006310005824192426\nVal Accuracy: 0.9987, Val F1-score: 0.9987\n\nEpoch 570, Loss: 0.016451083835446675\nValidation Loss: 0.006280552128517414\nVal Accuracy: 1.0000, Val F1-score: 1.0000\n\nEpoch 580, Loss: 0.019035149641381817\nValidation Loss: 0.007428411878828219\nVal Accuracy: 0.9960, Val F1-score: 0.9960\n\nEpoch 590, Loss: 0.017744666415111344\nValidation Loss: 0.006790399247422603\nVal Accuracy: 1.0000, Val F1-score: 1.0000\n\nEpoch 600, Loss: 0.0177573784371015\nValidation Loss: 0.006375334502262575\nVal Accuracy: 1.0000, Val F1-score: 1.0000\n\nEpoch 610, Loss: 0.016445364658493257\nValidation Loss: 0.006313443427864662\nVal Accuracy: 1.0000, Val F1-score: 1.0000\n\nEpoch 620, Loss: 0.016379538961206506\nValidation Loss: 0.006223502849587466\nVal Accuracy: 1.0000, Val F1-score: 1.0000\n\nEpoch 630, Loss: 0.016411438428522086\nValidation Loss: 0.006122578347036445\nVal Accuracy: 1.0000, Val F1-score: 1.0000\n\nEpoch 640, Loss: 0.01774713708199892\nValidation Loss: 0.006640916196122504\nVal Accuracy: 1.0000, Val F1-score: 1.0000\n\nEpoch 650, Loss: 0.016438273828437395\nValidation Loss: 0.00624933415591548\nVal Accuracy: 1.0000, Val F1-score: 1.0000\n\nEpoch 660, Loss: 0.016301771995374845\nValidation Loss: 0.006314368160052375\nVal Accuracy: 1.0000, Val F1-score: 1.0000\n\nEpoch 670, Loss: 0.016352693686827506\nValidation Loss: 0.0061446234223918594\nVal Accuracy: 1.0000, Val F1-score: 1.0000\n\nEpoch 680, Loss: 0.01631333643635476\nValidation Loss: 0.006092843055338193\nVal Accuracy: 1.0000, Val F1-score: 1.0000\n\nEpoch 690, Loss: 0.020273327665999496\nValidation Loss: 0.007040501704872397\nVal Accuracy: 0.9973, Val F1-score: 0.9973\n\nEpoch 700, Loss: 0.01624727348039398\nValidation Loss: 0.006179825158043902\nVal Accuracy: 1.0000, Val F1-score: 1.0000\n\nEpoch 710, Loss: 0.01628531066712192\nValidation Loss: 0.006048853123025388\nVal Accuracy: 1.0000, Val F1-score: 1.0000\n\nEpoch 720, Loss: 0.016258636265381574\nValidation Loss: 0.006010957481764834\nVal Accuracy: 1.0000, Val F1-score: 1.0000\n\nEpoch 730, Loss: 0.01919681576181499\nValidation Loss: 0.0077225393747992754\nVal Accuracy: 0.9973, Val F1-score: 0.9973\n\nEpoch 740, Loss: 0.017679074359521194\nValidation Loss: 0.006144196327299826\nVal Accuracy: 1.0000, Val F1-score: 1.0000\n\nEpoch 750, Loss: 0.017788378078177562\nValidation Loss: 0.006077451893869619\nVal Accuracy: 1.0000, Val F1-score: 1.0000\n\nEpoch 760, Loss: 0.01625133264128564\nValidation Loss: 0.006324149665124956\nVal Accuracy: 1.0000, Val F1-score: 1.0000\n\nEpoch 770, Loss: 0.01624702530467799\nValidation Loss: 0.006145027208179184\nVal Accuracy: 1.0000, Val F1-score: 1.0000\n\nEpoch 780, Loss: 0.017945573445215954\nValidation Loss: 0.006582212452407532\nVal Accuracy: 1.0000, Val F1-score: 1.0000\n\nEpoch 790, Loss: 0.017509558798606824\nValidation Loss: 0.006334766484918338\nVal Accuracy: 1.0000, Val F1-score: 1.0000\n\nEpoch 800, Loss: 0.017826931035804187\nValidation Loss: 0.006942540703294142\nVal Accuracy: 1.0000, Val F1-score: 1.0000\n\nEpoch 810, Loss: 0.016266579024078583\nValidation Loss: 0.006517546119081767\nVal Accuracy: 0.9987, Val F1-score: 0.9987\n\nEpoch 820, Loss: 0.01625315088749667\nValidation Loss: 0.006265761632200236\nVal Accuracy: 1.0000, Val F1-score: 1.0000\n\nEpoch 830, Loss: 0.01624382950233677\nValidation Loss: 0.00609540361528893\nVal Accuracy: 1.0000, Val F1-score: 1.0000\n\nEpoch 840, Loss: 0.017672158877356056\nValidation Loss: 0.0075106755600300446\nVal Accuracy: 0.9987, Val F1-score: 0.9987\n\nEpoch 850, Loss: 0.017598795502306263\nValidation Loss: 0.006680476635303319\nVal Accuracy: 1.0000, Val F1-score: 1.0000\n\nEpoch 860, Loss: 0.01617430869543419\nValidation Loss: 0.006413516859119284\nVal Accuracy: 1.0000, Val F1-score: 1.0000\n\nEpoch 870, Loss: 0.0162042161899703\nValidation Loss: 0.006193695744303914\nVal Accuracy: 1.0000, Val F1-score: 1.0000\n\nEpoch 880, Loss: 0.01619451968859701\nValidation Loss: 0.0060359070738876435\nVal Accuracy: 1.0000, Val F1-score: 1.0000\n\nEpoch 890, Loss: 0.017732010072189976\nValidation Loss: 0.00744763520030108\nVal Accuracy: 0.9987, Val F1-score: 0.9987\n\nEpoch 900, Loss: 0.017638296196852074\nValidation Loss: 0.008006634659919243\nVal Accuracy: 0.9987, Val F1-score: 0.9987\n\nEpoch 910, Loss: 0.017450711392971885\nValidation Loss: 0.007937340098832143\nVal Accuracy: 0.9987, Val F1-score: 0.9987\n\nEpoch 920, Loss: 0.01739485989608368\nValidation Loss: 0.007906723782779087\nVal Accuracy: 0.9987, Val F1-score: 0.9987\n\nEpoch 930, Loss: 0.017371450354484248\nValidation Loss: 0.007766528115851952\nVal Accuracy: 0.9987, Val F1-score: 0.9987\n\nEpoch 940, Loss: 0.01765827835877771\nValidation Loss: 0.0074185288902395286\nVal Accuracy: 0.9987, Val F1-score: 0.9987\n\nEpoch 950, Loss: 0.01735987877228931\nValidation Loss: 0.007614150515660019\nVal Accuracy: 0.9987, Val F1-score: 0.9987\n\nEpoch 960, Loss: 0.01736439798962678\nValidation Loss: 0.0074586018653456\nVal Accuracy: 0.9987, Val F1-score: 0.9987\n\nEpoch 970, Loss: 0.017345942745726557\nValidation Loss: 0.007531391309762399\nVal Accuracy: 0.9987, Val F1-score: 0.9987\n\nEpoch 980, Loss: 0.01734844595491478\nValidation Loss: 0.007314789177862509\nVal Accuracy: 0.9987, Val F1-score: 0.9987\n\nEpoch 990, Loss: 0.017344682974757972\nValidation Loss: 0.007206313562731825\nVal Accuracy: 0.9987, Val F1-score: 0.9987\n\nEpoch 1000, Loss: 0.01785132404183914\nValidation Loss: 0.007725779969266562\nVal Accuracy: 0.9973, Val F1-score: 0.9973\n\n","output_type":"stream"}]},{"cell_type":"code","source":"for name, param in model.named_parameters():\n    print(f\"{name}: {param.data}\")","metadata":{"execution":{"iopub.status.busy":"2024-06-13T05:13:15.859952Z","iopub.execute_input":"2024-06-13T05:13:15.860332Z","iopub.status.idle":"2024-06-13T05:13:15.872531Z","shell.execute_reply.started":"2024-06-13T05:13:15.860300Z","shell.execute_reply":"2024-06-13T05:13:15.871603Z"},"trusted":true},"execution_count":157,"outputs":[{"name":"stdout","text":"layers.0.weight: tensor([[-6.7005, -0.2762, -0.2762, -2.3480, -0.1286, -5.7162,  4.4786,  0.8280,\n         -1.1978,  2.8410, -0.5636,  2.5976,  6.7005,  0.2762,  0.2762,  2.3480,\n          0.1286,  5.7162, -4.4786, -0.8280,  1.1978, -2.8410,  0.5636, -2.5976],\n        [ 2.9228,  1.0347,  1.0347, -1.4918,  1.6076,  5.9544,  0.4143, -1.2659,\n         -1.3995,  1.2163, -1.9302, -0.4551, -2.9228, -1.0347, -1.0347,  1.4918,\n         -1.6076, -5.9544, -0.4143,  1.2659,  1.3995, -1.2163,  1.9302,  0.4551]],\n       device='cuda:0')\nlayers.0.bias: tensor([-5.5882, -0.3558], device='cuda:0')\ncustom_layers.0.linear_sigmoid.weight: tensor([[-0.7368,  3.4871,  3.4871, -1.1093, -0.4040, -1.1764,  0.2788, -3.3364,\n         -2.0681,  3.1208, -5.5544,  0.5153,  0.7368, -3.4871, -3.4871,  1.1093,\n          0.4040,  1.1764, -0.2788,  3.3364,  2.0681, -3.1208,  5.5544, -0.5153],\n        [ 0.1622,  0.0336,  0.0336, -0.5137,  0.0892,  0.1958, -0.5015, -0.1368,\n         -0.3423,  2.0886,  0.9950, -0.2834, -0.1622, -0.0336, -0.0336,  0.5137,\n         -0.0892, -0.1958,  0.5015,  0.1368,  0.3423, -2.0886, -0.9950,  0.2834]],\n       device='cuda:0')\ncustom_layers.0.linear_sigmoid.bias: tensor([12.4561,  7.7045], device='cuda:0')\n","output_type":"stream"}]},{"cell_type":"code","source":"optimizer = optim.Adam(model.parameters(), lr=0.01)\nevaluate_model(model, custom_train_loader, criterion, optimizer)","metadata":{"execution":{"iopub.status.busy":"2024-06-13T04:38:00.578587Z","iopub.execute_input":"2024-06-13T04:38:00.578952Z","iopub.status.idle":"2024-06-13T04:41:32.239071Z","shell.execute_reply.started":"2024-06-13T04:38:00.578925Z","shell.execute_reply":"2024-06-13T04:41:32.238165Z"},"trusted":true},"execution_count":130,"outputs":[{"name":"stdout","text":"Epoch 10, Loss: 0.04650792091464003\nValidation Loss: 0.025719109962665243\nVal Accuracy: 0.9947, Val F1-score: 0.9947\n\nEpoch 20, Loss: 0.04112769169230926\nValidation Loss: 0.021372774998781097\nVal Accuracy: 0.9947, Val F1-score: 0.9947\n\nEpoch 30, Loss: 0.038246722713531064\nValidation Loss: 0.019369842237210833\nVal Accuracy: 0.9947, Val F1-score: 0.9947\n\nEpoch 40, Loss: 0.036295082384346866\nValidation Loss: 0.018089132215663994\nVal Accuracy: 0.9947, Val F1-score: 0.9947\n\nEpoch 50, Loss: 0.03483528974232733\nValidation Loss: 0.01716014178517374\nVal Accuracy: 0.9947, Val F1-score: 0.9947\n\nEpoch 60, Loss: 0.033692508762383284\nValidation Loss: 0.016447162977480428\nVal Accuracy: 0.9947, Val F1-score: 0.9947\n\nEpoch 70, Loss: 0.03277765942127511\nValidation Loss: 0.015881943373263614\nVal Accuracy: 0.9947, Val F1-score: 0.9947\n\nEpoch 80, Loss: 0.032035941505397666\nValidation Loss: 0.01542384399507076\nVal Accuracy: 0.9960, Val F1-score: 0.9960\n\nEpoch 90, Loss: 0.031429176907848584\nValidation Loss: 0.01504643959265195\nVal Accuracy: 0.9960, Val F1-score: 0.9960\n\nEpoch 100, Loss: 0.03092891872155532\nValidation Loss: 0.014731742619308838\nVal Accuracy: 0.9960, Val F1-score: 0.9960\n\nEpoch 110, Loss: 0.030513296016944982\nValidation Loss: 0.014466927694911647\nVal Accuracy: 0.9973, Val F1-score: 0.9973\n\nEpoch 120, Loss: 0.030165311577129327\nValidation Loss: 0.014242630584287022\nVal Accuracy: 0.9973, Val F1-score: 0.9973\n\nEpoch 130, Loss: 0.029871673889564587\nValidation Loss: 0.014051721074830917\nVal Accuracy: 0.9973, Val F1-score: 0.9973\n\nEpoch 140, Loss: 0.02962193340707537\nValidation Loss: 0.0138886194973414\nVal Accuracy: 0.9973, Val F1-score: 0.9973\n\nEpoch 150, Loss: 0.02940786676695025\nValidation Loss: 0.013748969528364796\nVal Accuracy: 0.9973, Val F1-score: 0.9973\n\nEpoch 160, Loss: 0.029222955079237986\nValidation Loss: 0.013629162833922237\nVal Accuracy: 0.9973, Val F1-score: 0.9973\n\nEpoch 170, Loss: 0.029062074300719557\nValidation Loss: 0.013526254181215336\nVal Accuracy: 0.9973, Val F1-score: 0.9973\n\nEpoch 180, Loss: 0.028921041297402188\nValidation Loss: 0.013437838559714995\nVal Accuracy: 0.9973, Val F1-score: 0.9973\n\nEpoch 190, Loss: 0.028796552827655447\nValidation Loss: 0.013361841033618779\nVal Accuracy: 0.9973, Val F1-score: 0.9973\n\nEpoch 200, Loss: 0.028685920064403535\nValidation Loss: 0.013296520607809725\nVal Accuracy: 0.9973, Val F1-score: 0.9973\n\nEpoch 210, Loss: 0.028586947242700493\nValidation Loss: 0.01324045150298995\nVal Accuracy: 0.9973, Val F1-score: 0.9973\n\nEpoch 220, Loss: 0.028497864612983818\nValidation Loss: 0.013192368882869232\nVal Accuracy: 0.9960, Val F1-score: 0.9960\n\nEpoch 230, Loss: 0.028417177497869015\nValidation Loss: 0.013151199718890894\nVal Accuracy: 0.9960, Val F1-score: 0.9960\n\nEpoch 240, Loss: 0.02834367312617942\nValidation Loss: 0.013116044386947578\nVal Accuracy: 0.9960, Val F1-score: 0.9960\n\nEpoch 250, Loss: 0.028276361811458147\nValidation Loss: 0.01308611691039611\nVal Accuracy: 0.9960, Val F1-score: 0.9960\n\nEpoch 260, Loss: 0.028214374029755677\nValidation Loss: 0.013060750526771395\nVal Accuracy: 0.9960, Val F1-score: 0.9960\n\nEpoch 270, Loss: 0.02815700813281909\nValidation Loss: 0.013039327313113821\nVal Accuracy: 0.9960, Val F1-score: 0.9960\n\nEpoch 280, Loss: 0.028103667373042776\nValidation Loss: 0.01302134960061115\nVal Accuracy: 0.9960, Val F1-score: 0.9960\n\nEpoch 290, Loss: 0.028053816156407552\nValidation Loss: 0.013006404947210134\nVal Accuracy: 0.9960, Val F1-score: 0.9960\n\nEpoch 300, Loss: 0.02800706368318407\nValidation Loss: 0.012994087551722563\nVal Accuracy: 0.9960, Val F1-score: 0.9960\n\nEpoch 310, Loss: 0.02796300928210296\nValidation Loss: 0.012984081166109718\nVal Accuracy: 0.9960, Val F1-score: 0.9960\n\nEpoch 320, Loss: 0.027921343603468683\nValidation Loss: 0.012976046783270098\nVal Accuracy: 0.9960, Val F1-score: 0.9960\n\nEpoch 330, Loss: 0.027881782952025157\nValidation Loss: 0.012969778710782975\nVal Accuracy: 0.9960, Val F1-score: 0.9960\n\nEpoch 340, Loss: 0.02784409446333787\nValidation Loss: 0.01296503196078902\nVal Accuracy: 0.9960, Val F1-score: 0.9960\n\nEpoch 350, Loss: 0.027808084113949295\nValidation Loss: 0.012961575242949644\nVal Accuracy: 0.9960, Val F1-score: 0.9960\n\nEpoch 360, Loss: 0.027773588437306533\nValidation Loss: 0.01295930309746988\nVal Accuracy: 0.9960, Val F1-score: 0.9960\n\nEpoch 370, Loss: 0.027740420509448444\nValidation Loss: 0.01295800252122111\nVal Accuracy: 0.9960, Val F1-score: 0.9960\n\nEpoch 380, Loss: 0.027708471871668947\nValidation Loss: 0.012957587229342002\nVal Accuracy: 0.9960, Val F1-score: 0.9960\n\nEpoch 390, Loss: 0.027677585832419235\nValidation Loss: 0.012957914043283836\nVal Accuracy: 0.9960, Val F1-score: 0.9960\n\nEpoch 400, Loss: 0.02764769722413259\nValidation Loss: 0.012958873399990503\nVal Accuracy: 0.9960, Val F1-score: 0.9960\n\nEpoch 410, Loss: 0.027618714306617408\nValidation Loss: 0.012960401392964135\nVal Accuracy: 0.9960, Val F1-score: 0.9960\n\nEpoch 420, Loss: 0.027590552426994132\nValidation Loss: 0.012962363244866992\nVal Accuracy: 0.9960, Val F1-score: 0.9960\n\nEpoch 430, Loss: 0.02756311600645493\nValidation Loss: 0.012964777272372885\nVal Accuracy: 0.9960, Val F1-score: 0.9960\n\nEpoch 440, Loss: 0.027536372740643047\nValidation Loss: 0.012967490617597832\nVal Accuracy: 0.9960, Val F1-score: 0.9960\n\nEpoch 450, Loss: 0.02751025140681666\nValidation Loss: 0.012970512778110788\nVal Accuracy: 0.9960, Val F1-score: 0.9960\n\nEpoch 460, Loss: 0.027484714679515312\nValidation Loss: 0.012973734470354733\nVal Accuracy: 0.9960, Val F1-score: 0.9960\n\nEpoch 470, Loss: 0.027459694928241397\nValidation Loss: 0.012977167091984635\nVal Accuracy: 0.9960, Val F1-score: 0.9960\n\nEpoch 480, Loss: 0.02743520550446375\nValidation Loss: 0.012980765445831821\nVal Accuracy: 0.9960, Val F1-score: 0.9960\n\nEpoch 490, Loss: 0.027411160732376316\nValidation Loss: 0.012984499852355688\nVal Accuracy: 0.9960, Val F1-score: 0.9960\n\nEpoch 500, Loss: 0.0273875569541454\nValidation Loss: 0.012988308819672056\nVal Accuracy: 0.9960, Val F1-score: 0.9960\n\nEpoch 510, Loss: 0.027364350431015875\nValidation Loss: 0.012992201018334981\nVal Accuracy: 0.9960, Val F1-score: 0.9960\n\nEpoch 520, Loss: 0.027341551516685055\nValidation Loss: 0.01299612397472553\nVal Accuracy: 0.9960, Val F1-score: 0.9960\n\nEpoch 530, Loss: 0.02731908462291353\nValidation Loss: 0.013000082141900293\nVal Accuracy: 0.9960, Val F1-score: 0.9960\n\nEpoch 540, Loss: 0.02729696470457562\nValidation Loss: 0.013004055559292738\nVal Accuracy: 0.9947, Val F1-score: 0.9947\n\nEpoch 550, Loss: 0.027275179498853202\nValidation Loss: 0.013008020028204706\nVal Accuracy: 0.9947, Val F1-score: 0.9947\n\nEpoch 560, Loss: 0.027253715435950586\nValidation Loss: 0.013011947568107493\nVal Accuracy: 0.9947, Val F1-score: 0.9947\n\nEpoch 570, Loss: 0.027232538340906595\nValidation Loss: 0.0130158312254783\nVal Accuracy: 0.9947, Val F1-score: 0.9947\n\nEpoch 580, Loss: 0.027211643496484996\nValidation Loss: 0.013019696783013993\nVal Accuracy: 0.9947, Val F1-score: 0.9947\n\nEpoch 590, Loss: 0.027191019833307586\nValidation Loss: 0.01302349516402046\nVal Accuracy: 0.9947, Val F1-score: 0.9947\n\nEpoch 600, Loss: 0.027170706271347447\nValidation Loss: 0.013027280741747896\nVal Accuracy: 0.9947, Val F1-score: 0.9947\n\nEpoch 610, Loss: 0.02715058624721317\nValidation Loss: 0.0130309630545374\nVal Accuracy: 0.9947, Val F1-score: 0.9947\n\nEpoch 620, Loss: 0.027130761244460167\nValidation Loss: 0.013034596824714603\nVal Accuracy: 0.9947, Val F1-score: 0.9947\n\nEpoch 630, Loss: 0.027111163731509198\nValidation Loss: 0.013038155977236935\nVal Accuracy: 0.9947, Val F1-score: 0.9947\n\nEpoch 640, Loss: 0.027091797437712013\nValidation Loss: 0.013041641319148312\nVal Accuracy: 0.9947, Val F1-score: 0.9947\n\nEpoch 650, Loss: 0.02707267584890206\nValidation Loss: 0.013045063386338521\nVal Accuracy: 0.9947, Val F1-score: 0.9947\n\nEpoch 660, Loss: 0.027053773907403135\nValidation Loss: 0.013048369997155381\nVal Accuracy: 0.9947, Val F1-score: 0.9947\n\nEpoch 670, Loss: 0.027035072067298292\nValidation Loss: 0.013051630299363145\nVal Accuracy: 0.9947, Val F1-score: 0.9947\n\nEpoch 680, Loss: 0.02701659677228553\nValidation Loss: 0.013054756243112328\nVal Accuracy: 0.9947, Val F1-score: 0.9947\n\nEpoch 690, Loss: 0.026998347700249743\nValidation Loss: 0.013057842478436518\nVal Accuracy: 0.9947, Val F1-score: 0.9947\n\nEpoch 700, Loss: 0.026980301475723823\nValidation Loss: 0.013060831522693661\nVal Accuracy: 0.9947, Val F1-score: 0.9947\n\nEpoch 710, Loss: 0.02696246470415125\nValidation Loss: 0.013063722806331649\nVal Accuracy: 0.9947, Val F1-score: 0.9947\n\nEpoch 720, Loss: 0.026944838218994883\nValidation Loss: 0.013066555969373894\nVal Accuracy: 0.9947, Val F1-score: 0.9947\n\nEpoch 730, Loss: 0.026927397829023817\nValidation Loss: 0.013069270881127673\nVal Accuracy: 0.9947, Val F1-score: 0.9947\n\nEpoch 740, Loss: 0.02691016042918486\nValidation Loss: 0.013071894257924063\nVal Accuracy: 0.9947, Val F1-score: 0.9947\n\nEpoch 750, Loss: 0.026893128506325262\nValidation Loss: 0.013074452974251471\nVal Accuracy: 0.9960, Val F1-score: 0.9960\n\nEpoch 760, Loss: 0.02687628343824746\nValidation Loss: 0.01307693292674609\nVal Accuracy: 0.9960, Val F1-score: 0.9960\n\nEpoch 770, Loss: 0.026859636935807344\nValidation Loss: 0.013079318919551497\nVal Accuracy: 0.9960, Val F1-score: 0.9960\n\nEpoch 780, Loss: 0.026843184855144944\nValidation Loss: 0.013081602812538526\nVal Accuracy: 0.9960, Val F1-score: 0.9960\n\nEpoch 790, Loss: 0.0268269043674909\nValidation Loss: 0.013083842836825474\nVal Accuracy: 0.9960, Val F1-score: 0.9960\n\nEpoch 800, Loss: 0.02681083776312657\nValidation Loss: 0.013085952127009174\nVal Accuracy: 0.9960, Val F1-score: 0.9960\n\nEpoch 810, Loss: 0.026794919358796923\nValidation Loss: 0.013088018447641995\nVal Accuracy: 0.9960, Val F1-score: 0.9960\n\nEpoch 820, Loss: 0.026779217627056935\nValidation Loss: 0.013089994339774572\nVal Accuracy: 0.9960, Val F1-score: 0.9960\n\nEpoch 830, Loss: 0.026763711520505325\nValidation Loss: 0.01309187842097496\nVal Accuracy: 0.9960, Val F1-score: 0.9960\n\nEpoch 840, Loss: 0.026748384361424966\nValidation Loss: 0.013093721186597426\nVal Accuracy: 0.9960, Val F1-score: 0.9960\n\nEpoch 850, Loss: 0.02673320927462276\nValidation Loss: 0.01309547686289155\nVal Accuracy: 0.9960, Val F1-score: 0.9960\n\nEpoch 860, Loss: 0.02671824072162313\nValidation Loss: 0.013097156420352954\nVal Accuracy: 0.9960, Val F1-score: 0.9960\n\nEpoch 870, Loss: 0.026703433302943252\nValidation Loss: 0.0130987598619375\nVal Accuracy: 0.9960, Val F1-score: 0.9960\n\nEpoch 880, Loss: 0.026688817280465005\nValidation Loss: 0.013100310780430391\nVal Accuracy: 0.9960, Val F1-score: 0.9960\n\nEpoch 890, Loss: 0.026674379050216874\nValidation Loss: 0.013101775313449101\nVal Accuracy: 0.9960, Val F1-score: 0.9960\n\nEpoch 900, Loss: 0.026660094514997183\nValidation Loss: 0.013103175062402292\nVal Accuracy: 0.9960, Val F1-score: 0.9960\n\nEpoch 910, Loss: 0.026646000915871045\nValidation Loss: 0.013104528637351601\nVal Accuracy: 0.9960, Val F1-score: 0.9960\n\nEpoch 920, Loss: 0.026632069506838833\nValidation Loss: 0.013105799420486619\nVal Accuracy: 0.9960, Val F1-score: 0.9960\n\nEpoch 930, Loss: 0.026618322788597354\nValidation Loss: 0.013107039862537325\nVal Accuracy: 0.9960, Val F1-score: 0.9960\n\nEpoch 940, Loss: 0.026604749485981995\nValidation Loss: 0.013108175172362735\nVal Accuracy: 0.9960, Val F1-score: 0.9960\n\nEpoch 950, Loss: 0.026591349351918172\nValidation Loss: 0.013109281580076034\nVal Accuracy: 0.9960, Val F1-score: 0.9960\n\nEpoch 960, Loss: 0.02657810471468322\nValidation Loss: 0.013110333624316203\nVal Accuracy: 0.9960, Val F1-score: 0.9960\n\nEpoch 970, Loss: 0.02656503650406006\nValidation Loss: 0.013111337570156442\nVal Accuracy: 0.9960, Val F1-score: 0.9960\n\nEpoch 980, Loss: 0.026552129763297638\nValidation Loss: 0.013112271578468912\nVal Accuracy: 0.9960, Val F1-score: 0.9960\n\nEpoch 990, Loss: 0.02653939231638909\nValidation Loss: 0.013113160097229107\nVal Accuracy: 0.9960, Val F1-score: 0.9960\n\nEpoch 1000, Loss: 0.026526809751125275\nValidation Loss: 0.013113994538571205\nVal Accuracy: 0.9960, Val F1-score: 0.9960\n\n","output_type":"stream"}]},{"cell_type":"code","source":"for name, param in model.named_parameters():\n    print(f\"{name}: {param.data}\")","metadata":{"execution":{"iopub.status.busy":"2024-06-13T04:41:44.816856Z","iopub.execute_input":"2024-06-13T04:41:44.817569Z","iopub.status.idle":"2024-06-13T04:41:44.825647Z","shell.execute_reply.started":"2024-06-13T04:41:44.817537Z","shell.execute_reply":"2024-06-13T04:41:44.824693Z"},"collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":131,"outputs":[{"name":"stdout","text":"layers.0.weight: tensor([[-6.5369,  0.5586,  0.5586,  7.3472,  0.0885, -6.7856, -0.0075, -5.7633,\n          1.5288,  0.7641,  1.4206,  0.2905,  6.5369, -0.5586, -0.5586, -7.3472,\n         -0.0885,  6.7856,  0.0075,  5.7633, -1.5288, -0.7641, -1.4206, -0.2905],\n        [ 6.5367, -0.5586, -0.5586, -7.3478, -0.0883,  6.7859,  0.0076,  5.7627,\n         -1.5294, -0.7648, -1.4203, -0.2906, -6.5367,  0.5586,  0.5586,  7.3478,\n          0.0883, -6.7859, -0.0076, -5.7627,  1.5294,  0.7648,  1.4203,  0.2906]],\n       device='cuda:0')\nlayers.0.bias: tensor([-3.5744,  3.5738], device='cuda:0')\n","output_type":"stream"}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}