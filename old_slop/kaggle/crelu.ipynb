{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":30732,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import torch\nimport numpy as np\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torchinfo import summary\nimport torch.nn.functional as F\nfrom sklearn.model_selection import train_test_split\nfrom torch.utils.data import DataLoader, TensorDataset\nfrom torch.profiler import profile, record_function, ProfilerActivity\nimport numpy as np\nfrom sklearn.preprocessing import StandardScaler\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport torch","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-06-22T17:26:43.806322Z","iopub.execute_input":"2024-06-22T17:26:43.806693Z","iopub.status.idle":"2024-06-22T17:26:43.812778Z","shell.execute_reply.started":"2024-06-22T17:26:43.806664Z","shell.execute_reply":"2024-06-22T17:26:43.811795Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import DataLoader, TensorDataset\nfrom sklearn.model_selection import train_test_split\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport imageio\nimport os\n\nclass CustomDataLoader:\n    def __init__(self, features, labels, batch_size=1, validation_size=0.0, shuffle=False):\n        if validation_size > 0:\n            train_data, val_data, train_labels, val_labels = train_test_split(features, labels, test_size=validation_size, random_state=42)\n            self.train_loader = DataLoader(TensorDataset(torch.tensor(train_data).float(), torch.tensor(train_labels).float()), batch_size=batch_size, shuffle=shuffle)\n            self.val_loader = DataLoader(TensorDataset(torch.tensor(val_data).float(), torch.tensor(val_labels).float()), batch_size=batch_size, shuffle=shuffle)\n        else:\n            self.train_loader = DataLoader(TensorDataset(torch.tensor(features).float(), torch.tensor(labels).float()), batch_size=batch_size, shuffle=shuffle)\n            self.val_loader = None\n\n    def get_train_loader(self):\n        return self.train_loader\n\n    def get_val_loader(self):\n        return self.val_loader\n\ndef evaluate_model(model, custom_train_loader, criterion, optimizer):\n    num_epochs = 1200\n    parameters = []\n    image_folder = 'training_images'\n    os.makedirs(image_folder, exist_ok=True)\n    \n    for epoch in range(num_epochs):\n        running_loss = 0.0\n        for inputs, labels in custom_train_loader.get_train_loader():\n            inputs, labels = inputs.to(device), labels.to(device)\n            optimizer.zero_grad()\n            outputs = model(inputs.view(-1, 3))\n            loss = criterion(outputs, labels.view(-1, 1))\n\n            if torch.isnan(loss):\n                print(\"Loss is NaN or Inf\")\n                print(parameters)\n\n                for name, param in model.named_parameters():\n                    print(f\"{name}: {param}\")\n                break\n\n            parameters = []\n            for name, param in model.named_parameters():\n                parameters.append(f\"{name}: {param}\")\n\n            loss.backward()\n            optimizer.step()\n\n            running_loss += loss.item()\n        avg_train_loss = running_loss / len(custom_train_loader.get_train_loader())\n            \n        model.eval()\n        running_val_loss = 0.0\n        with torch.no_grad():\n            for inputs, labels in custom_train_loader.get_val_loader():\n                inputs, labels = inputs.to(device), labels.to(device)\n                outputs = model(inputs.view(-1, 3))\n                val_loss = criterion(outputs, labels.view(-1, 1))\n                running_val_loss += val_loss.item()\n\n        avg_val_loss = running_val_loss / len(custom_train_loader.get_val_loader())\n        print(f\"Epoch {epoch+1:4d} | Train Loss: {avg_train_loss} | Validation Loss: {avg_val_loss}\")\n\n        num = 10000\n        \n        columns = []\n        \n        x = np.linspace(-6, 6, num).reshape(-1, 1)\n        y = np.linspace(-0, 0, num)\n        random_feature1 = np.linspace(-0, 0, num)\n        random_feature2 = np.linspace(-0, 0, num)\n\n        for bias in biases:\n            column = x + 0\n            columns.append(column)\n        columns.append(y)\n        columns.append(random_feature1)\n        \n        inputs = np.column_stack(columns)\n        inputs_tensor = torch.from_numpy(inputs).float().to(device)\n\n        model.eval()\n        with torch.no_grad():\n            y_pred_model = model(inputs_tensor).cpu().numpy()\n\n        a = x\n        y_pred_manual = np.sin(a) + 2 * np.cos(a + 3 * np.sin(a)) + 3 * np.cos(a) ** 2 * np.sin(a) ** 2 + 0.5 * np.cos(a)\n        \n        plt.figure(figsize=(10, 5))\n        plt.scatter(x, y_pred_model.flatten(), label='Model Output', s=1, alpha=0.1)\n        plt.scatter(x, y_pred_manual, label='Manual Calculation', s=1, alpha=0.1)\n        plt.xlabel('Input Feature 1')\n        plt.ylabel('Output')\n        plt.legend()\n        plt.grid(True)\n        plt.savefig(f\"{image_folder}/epoch_{epoch+1:04d}.png\")\n        plt.close()\n\n    images = []\n    for epoch in range(num_epochs):\n        filename = f\"{image_folder}/epoch_{epoch+1:04d}.png\"\n        images.append(imageio.imread(filename))\n    imageio.mimsave('training_progress.gif', images, duration=0.2)","metadata":{"execution":{"iopub.status.busy":"2024-06-22T18:33:28.429845Z","iopub.execute_input":"2024-06-22T18:33:28.430226Z","iopub.status.idle":"2024-06-22T18:33:28.453682Z","shell.execute_reply.started":"2024-06-22T18:33:28.430197Z","shell.execute_reply":"2024-06-22T18:33:28.452707Z"},"trusted":true},"execution_count":59,"outputs":[]},{"cell_type":"code","source":"import numpy as np\nnum_samples = 128 * 100\nimport math\n\na = (np.random.rand(num_samples, 1) - 0.5) * 4\nb = (np.random.rand(num_samples, 1) - 0.5) * 4\nc = (np.random.rand(num_samples, 1) - 0.5) * 4\nd = (np.random.rand(num_samples, 1) - 0.5) * 4\ne = (np.random.rand(num_samples, 1) - 0.5) * 4\n\nconst = 1\nconst_array = np.full((num_samples, 1), const)\n\ncolumns = []\nbiases = []\n\na = (np.random.rand(num_samples, 1) - 0.5) * 12\nb = np.random.choice([0, 1], size=(num_samples, 1))\n\nfor _ in range(1):\n    bias = np.random.uniform(-1, 1) \n    biases.append(bias)\n    column = a + 0\n    columns.append(column)\n\ncolumns.append(b)\ncolumns.append(c)\nbiases = np.array(biases)\n\nx_train = np.column_stack(columns)\ny_train = np.abs(a)\ny_train = np.sin(a) + 2 * np.cos(a + 3 * np.sin(a)) + 3 * np.cos(a) ** 2 * np.sin(a) ** 2 + 0.5 * np.cos(a)\ny_train = a * (1 / (1 + np.exp(-b)))\ny_train = np.where(b == 1, a*c, a*c)\n\ncustom_train_loader = CustomDataLoader(x_train, y_train, batch_size=128, validation_size=0.2, shuffle=True)","metadata":{"execution":{"iopub.status.busy":"2024-06-22T17:26:45.520107Z","iopub.execute_input":"2024-06-22T17:26:45.520959Z","iopub.status.idle":"2024-06-22T17:26:45.573762Z","shell.execute_reply.started":"2024-06-22T17:26:45.520929Z","shell.execute_reply":"2024-06-22T17:26:45.573045Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")","metadata":{"execution":{"iopub.status.busy":"2024-06-22T17:26:46.047972Z","iopub.execute_input":"2024-06-22T17:26:46.048833Z","iopub.status.idle":"2024-06-22T17:26:46.074106Z","shell.execute_reply.started":"2024-06-22T17:26:46.048801Z","shell.execute_reply":"2024-06-22T17:26:46.073084Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"class CustomLinearLayer(nn.Module):\n    def __init__(self, input_size, output_size, init_zero=False, split_inputs=False):\n        super(CustomLinearLayer, self).__init__()\n        self.linear = nn.Linear(input_size, output_size, bias=True)\n        \n        if init_zero:\n            nn.init.zeros_(self.linear.weight)\n            nn.init.zeros_(self.linear.bias)\n        elif split_inputs:\n            self.split_inputs()\n            nn.init.uniform_(self.linear.bias, a=-10, b=10)\n        else:\n            self.custom_weight_init()\n            nn.init.zeros_(self.linear.bias)\n\n    def custom_weight_init(self):\n        with torch.no_grad():\n            rows, cols = self.linear.weight.size()\n            weight = torch.zeros(rows, cols)\n            for i in range(min(rows, cols) // 2):\n                weight[2 * i, 2 * i] = 1\n                weight[2 * i, 2 * i + 1] = -1\n                if 2 * i + 1 < rows and 2 * i + 1 < cols:\n                    weight[2 * i + 1, 2 * i] = -1\n                    weight[2 * i + 1, 2 * i + 1] = 1\n            self.linear.weight.copy_(weight)\n            nn.init.zeros_(self.linear.bias)\n            \n    def split_inputs(self):\n        weight = torch.zeros((self.linear.out_features, self.linear.in_features))\n        \n        for i in range(self.linear.out_features):\n            if i % 2 == 0:\n                weight[i, (i // 2) % self.linear.in_features] = 1\n            else:\n                weight[i, (i - 1) // 2 % self.linear.in_features] = -1\n        \n        self.linear.weight = nn.Parameter(weight)\n\n    def forward(self, x):\n        return self.linear(x)","metadata":{"execution":{"iopub.status.busy":"2024-06-22T17:26:46.572021Z","iopub.execute_input":"2024-06-22T17:26:46.572839Z","iopub.status.idle":"2024-06-22T17:26:46.584689Z","shell.execute_reply.started":"2024-06-22T17:26:46.572806Z","shell.execute_reply":"2024-06-22T17:26:46.583765Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"class PairwiseCustomActivationNetwork(nn.Module):\n    def __init__(self, input_size, num_layers, output_size):\n        super(PairwiseCustomActivationNetwork, self).__init__()\n        self.layer1 = CustomLinearLayer(input_size, input_size * 2 * 256 , init_zero=False, split_inputs=True)\n#         self.layer2 = CustomLinearLayer(input_size * 2, input_size * 2 * 8)\n        self.layer2 = CustomLinearLayer(input_size * 2 * 256, 1, init_zero=True)\n#         self.num_layers = num_layers\n#         self.layers = nn.ModuleList()\n#         self.res_layers = nn.ModuleList()\n        self.relu = nn.ReLU()\n        self.softplus = nn.Softplus()\n        \n#         self.res_layers.append(CustomLinearLayer(input_size, input_size * 2, init_zero=False, split_inputs=True))\n#         self.layers.append(CustomLinearLayer(input_size * 2, 1))\n\n#         layer_size = input_size * 2\n#         self.res_layers.append(CustomLinearLayer(input_size, input_size * 2, init_zero=False, split_inputs=True))\n#         self.layers.append(CustomLinearLayer(layer_size, layer_size))\n        \n#         for i in range(1, num_layers):\n#             layer_size += input_size\n#             self.res_layers.append(CustomLinearLayer(input_size, input_size, init_zero=False, split_inputs=True))\n#             self.layers.append(CustomLinearLayer(layer_size, layer_size))\n            \n#         layer_size += input_size * 2\n#         self.res_layers.append(CustomLinearLayer(input_size, input_size * 2, init_zero=False, split_inputs=True))\n#         self.layers.append(CustomLinearLayer(layer_size, output_size, init_zero=True))\n\n    def forward(self, x):\n        x = self.relu(self.layer1(x))\n        x = self.layer2(x)\n        \n        return x\n#         original = x\n#         outputs = []\n        \n#         for i, (layer, res_layer) in enumerate(zip(self.layers, self.res_layers)):\n#             outputs.append(res_layer(original))            \n#             concatenated_outputs = torch.cat(outputs, dim=1)\n\n#             if i != len(self.layers) - 1:\n#                 outputs = [self.relu(layer(concatenated_outputs))]\n#             else:\n#                 outputs = [layer(concatenated_outputs)]\n                \n#         return outputs[0]\n    \nmodel = PairwiseCustomActivationNetwork(3, 1, 1).to(device)\ncriterion = nn.MSELoss()\nprint(summary(model, input_size=(1, 3)))","metadata":{"execution":{"iopub.status.busy":"2024-06-22T03:07:17.812135Z","iopub.execute_input":"2024-06-22T03:07:17.812928Z","iopub.status.idle":"2024-06-22T03:07:17.847464Z","shell.execute_reply.started":"2024-06-22T03:07:17.812885Z","shell.execute_reply":"2024-06-22T03:07:17.846438Z"},"trusted":true},"execution_count":361,"outputs":[{"name":"stdout","text":"==========================================================================================\nLayer (type:depth-idx)                   Output Shape              Param #\n==========================================================================================\nPairwiseCustomActivationNetwork          [1, 1]                    --\n├─CustomLinearLayer: 1-1                 [1, 1536]                 --\n│    └─Linear: 2-1                       [1, 1536]                 6,144\n├─ReLU: 1-2                              [1, 1536]                 --\n├─CustomLinearLayer: 1-3                 [1, 1]                    --\n│    └─Linear: 2-2                       [1, 1]                    1,537\n==========================================================================================\nTotal params: 7,681\nTrainable params: 7,681\nNon-trainable params: 0\nTotal mult-adds (M): 0.01\n==========================================================================================\nInput size (MB): 0.00\nForward/backward pass size (MB): 0.01\nParams size (MB): 0.03\nEstimated Total Size (MB): 0.04\n==========================================================================================\n","output_type":"stream"}]},{"cell_type":"code","source":"for name, param in model.named_parameters():\n    print(name, param)","metadata":{"execution":{"iopub.status.busy":"2024-06-22T03:03:08.984749Z","iopub.execute_input":"2024-06-22T03:03:08.986076Z","iopub.status.idle":"2024-06-22T03:03:08.995791Z","shell.execute_reply.started":"2024-06-22T03:03:08.986026Z","shell.execute_reply":"2024-06-22T03:03:08.994576Z"},"trusted":true},"execution_count":347,"outputs":[{"name":"stdout","text":"layer1.linear.weight Parameter containing:\ntensor([[ 1.,  0.,  0.],\n        [-1.,  0.,  0.],\n        [ 0.,  1.,  0.],\n        [ 0., -1.,  0.],\n        [ 0.,  0.,  1.],\n        [ 0.,  0., -1.]], device='cuda:0', requires_grad=True)\nlayer1.linear.bias Parameter containing:\ntensor([-7.6015, -9.0671,  9.7433,  7.5915, -7.0549,  9.0111], device='cuda:0',\n       requires_grad=True)\nlayer2.linear.weight Parameter containing:\ntensor([[0., 0., 0., 0., 0., 0.]], device='cuda:0', requires_grad=True)\nlayer2.linear.bias Parameter containing:\ntensor([0.], device='cuda:0', requires_grad=True)\n","output_type":"stream"}]},{"cell_type":"code","source":"model.eval()\n\ninput_vector = [0.01, 0, 0.5]\ninput_tensor = torch.tensor(input_vector, dtype=torch.float32).unsqueeze(0).to(device)\nuseless_var = np.where(b == 1, a**2, -a + c)\n\nwith torch.no_grad():\n    output = model(input_tensor)\n    print(output)","metadata":{"execution":{"iopub.status.busy":"2024-06-22T19:01:21.161478Z","iopub.execute_input":"2024-06-22T19:01:21.162084Z","iopub.status.idle":"2024-06-22T19:01:21.172294Z","shell.execute_reply.started":"2024-06-22T19:01:21.162050Z","shell.execute_reply":"2024-06-22T19:01:21.171255Z"},"trusted":true},"execution_count":98,"outputs":[{"name":"stdout","text":"tensor([[0.0023]], device='cuda:0')\n","output_type":"stream"}]},{"cell_type":"code","source":"class PairwiseCustomActivationNetwork(nn.Module):\n    def __init__(self, input_size, num_layers, output_size):\n        super(PairwiseCustomActivationNetwork, self).__init__()\n\n        self.num_layers = num_layers\n        self.layers = nn.ModuleList()\n        self.res_layers = nn.ModuleList()\n        self.relu = nn.ReLU()\n        self.softplus = nn.Softplus()\n\n        layer_size = input_size * 2\n        self.res_layers.append(CustomLinearLayer(input_size, input_size * 2, init_zero=False, split_inputs=True))\n        self.layers.append(CustomLinearLayer(layer_size, layer_size))\n        \n        for i in range(1, num_layers):\n            layer_size *= 2\n            self.res_layers.append(CustomLinearLayer(input_size, input_size, init_zero=False, split_inputs=True))\n            self.layers.append(CustomLinearLayer(layer_size, layer_size))\n            \n        layer_size *= 2\n        self.res_layers.append(CustomLinearLayer(input_size, input_size * 2, init_zero=False, split_inputs=True))\n        self.layers.append(CustomLinearLayer(layer_size, output_size, init_zero=True))\n\n    def forward(self, x):\n        original = x\n        outputs = [self.res_layers[0](original)]\n        \n        for i, (layer, res_layer) in enumerate(zip(self.layers, self.res_layers)):\n#             outputs.append(res_layer(original))            \n            concatenated_outputs = torch.cat(outputs, dim=1)\n            \n            if i != len(self.layers) - 1:\n                outputs.append(self.softplus(layer(concatenated_outputs)))\n            else:\n                outputs.append(layer(concatenated_outputs))\n                \n                \n        return outputs[-1]\n    \nmodel = PairwiseCustomActivationNetwork(3, 8, 1).to(device)\ncriterion = nn.MSELoss()\nprint(summary(model, input_size=(1, 3)))\n","metadata":{"execution":{"iopub.status.busy":"2024-06-22T18:08:30.437903Z","iopub.execute_input":"2024-06-22T18:08:30.438297Z","iopub.status.idle":"2024-06-22T18:08:30.503387Z","shell.execute_reply.started":"2024-06-22T18:08:30.438266Z","shell.execute_reply":"2024-06-22T18:08:30.502484Z"},"trusted":true},"execution_count":55,"outputs":[{"name":"stdout","text":"==========================================================================================\nLayer (type:depth-idx)                   Output Shape              Param #\n==========================================================================================\nPairwiseCustomActivationNetwork          [1, 1]                    --\n├─ModuleList: 1-1                        --                        108\n│    └─CustomLinearLayer: 2-1            [1, 6]                    --\n│    │    └─Linear: 3-1                  [1, 6]                    24\n├─ModuleList: 1-18                       --                        (recursive)\n│    └─CustomLinearLayer: 2-2            [1, 6]                    --\n│    │    └─Linear: 3-2                  [1, 6]                    42\n├─Softplus: 1-3                          [1, 6]                    --\n├─ModuleList: 1-18                       --                        (recursive)\n│    └─CustomLinearLayer: 2-3            [1, 12]                   --\n│    │    └─Linear: 3-3                  [1, 12]                   156\n├─Softplus: 1-5                          [1, 12]                   --\n├─ModuleList: 1-18                       --                        (recursive)\n│    └─CustomLinearLayer: 2-4            [1, 24]                   --\n│    │    └─Linear: 3-4                  [1, 24]                   600\n├─Softplus: 1-7                          [1, 24]                   --\n├─ModuleList: 1-18                       --                        (recursive)\n│    └─CustomLinearLayer: 2-5            [1, 48]                   --\n│    │    └─Linear: 3-5                  [1, 48]                   2,352\n├─Softplus: 1-9                          [1, 48]                   --\n├─ModuleList: 1-18                       --                        (recursive)\n│    └─CustomLinearLayer: 2-6            [1, 96]                   --\n│    │    └─Linear: 3-6                  [1, 96]                   9,312\n├─Softplus: 1-11                         [1, 96]                   --\n├─ModuleList: 1-18                       --                        (recursive)\n│    └─CustomLinearLayer: 2-7            [1, 192]                  --\n│    │    └─Linear: 3-7                  [1, 192]                  37,056\n├─Softplus: 1-13                         [1, 192]                  --\n├─ModuleList: 1-18                       --                        (recursive)\n│    └─CustomLinearLayer: 2-8            [1, 384]                  --\n│    │    └─Linear: 3-8                  [1, 384]                  147,840\n├─Softplus: 1-15                         [1, 384]                  --\n├─ModuleList: 1-18                       --                        (recursive)\n│    └─CustomLinearLayer: 2-9            [1, 768]                  --\n│    │    └─Linear: 3-9                  [1, 768]                  590,592\n├─Softplus: 1-17                         [1, 768]                  --\n├─ModuleList: 1-18                       --                        (recursive)\n│    └─CustomLinearLayer: 2-10           [1, 1]                    --\n│    │    └─Linear: 3-10                 [1, 1]                    1,537\n==========================================================================================\nTotal params: 789,619\nTrainable params: 789,619\nNon-trainable params: 0\nTotal mult-adds (M): 0.79\n==========================================================================================\nInput size (MB): 0.00\nForward/backward pass size (MB): 0.01\nParams size (MB): 3.16\nEstimated Total Size (MB): 3.17\n==========================================================================================\n","output_type":"stream"}]},{"cell_type":"code","source":"optimizer = optim.Adam(model.parameters(), lr=0.00000000001 * 1 * 1)\nevaluate_model(model, custom_train_loader, criterion, optimizer)","metadata":{"execution":{"iopub.status.busy":"2024-06-22T18:57:22.597981Z","iopub.execute_input":"2024-06-22T18:57:22.598353Z","iopub.status.idle":"2024-06-22T18:58:58.496374Z","shell.execute_reply.started":"2024-06-22T18:57:22.598324Z","shell.execute_reply":"2024-06-22T18:58:58.495052Z"},"trusted":true},"execution_count":64,"outputs":[{"name":"stdout","text":"Epoch    1 | Train Loss: 4.1761359352676666e-05 | Validation Loss: 3.729847758222604e-05\nEpoch    2 | Train Loss: 4.1761397687878345e-05 | Validation Loss: 3.729848094735644e-05\nEpoch    3 | Train Loss: 4.1761377383409125e-05 | Validation Loss: 3.729846666828962e-05\nEpoch    4 | Train Loss: 4.176138486400305e-05 | Validation Loss: 3.729852642209153e-05\nEpoch    5 | Train Loss: 4.176135346369847e-05 | Validation Loss: 3.7298500592442e-05\nEpoch    6 | Train Loss: 4.176135205398168e-05 | Validation Loss: 3.7298545248631856e-05\nEpoch    7 | Train Loss: 4.1761335432966006e-05 | Validation Loss: 3.7298547613318076e-05\nEpoch    8 | Train Loss: 4.176133068085619e-05 | Validation Loss: 3.7298540883057284e-05\nEpoch    9 | Train Loss: 4.1761336251511236e-05 | Validation Loss: 3.729858863152913e-05\nEpoch   10 | Train Loss: 4.176132949851308e-05 | Validation Loss: 3.7298549796105365e-05\nEpoch   11 | Train Loss: 4.17613283616447e-05 | Validation Loss: 3.729859936356661e-05\nEpoch   12 | Train Loss: 4.176133863893483e-05 | Validation Loss: 3.7298619099601636e-05\nEpoch   13 | Train Loss: 4.176136944806785e-05 | Validation Loss: 3.729863174157799e-05\nEpoch   14 | Train Loss: 4.176136058049451e-05 | Validation Loss: 3.7298637107596735e-05\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","Cell \u001b[0;32mIn[64], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m optimizer \u001b[38;5;241m=\u001b[39m optim\u001b[38;5;241m.\u001b[39mAdam(model\u001b[38;5;241m.\u001b[39mparameters(), lr\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.00000000001\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m----> 2\u001b[0m \u001b[43mevaluate_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcustom_train_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m)\u001b[49m\n","Cell \u001b[0;32mIn[59], line 51\u001b[0m, in \u001b[0;36mevaluate_model\u001b[0;34m(model, custom_train_loader, criterion, optimizer)\u001b[0m\n\u001b[1;32m     49\u001b[0m parameters \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m     50\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m name, param \u001b[38;5;129;01min\u001b[39;00m model\u001b[38;5;241m.\u001b[39mnamed_parameters():\n\u001b[0;32m---> 51\u001b[0m     parameters\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mparam\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     53\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[1;32m     54\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/_tensor.py:934\u001b[0m, in \u001b[0;36mTensor.__format__\u001b[0;34m(self, format_spec)\u001b[0m\n\u001b[1;32m    932\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdim() \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mis_meta \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;129;01mis\u001b[39;00m Tensor:\n\u001b[1;32m    933\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mitem()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__format__\u001b[39m(format_spec)\n\u001b[0;32m--> 934\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mobject\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__format__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mformat_spec\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/parameter.py:63\u001b[0m, in \u001b[0;36mParameter.__repr__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     62\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__repr__\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m---> 63\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mParameter containing:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__repr__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/_tensor.py:431\u001b[0m, in \u001b[0;36mTensor.__repr__\u001b[0;34m(self, tensor_contents)\u001b[0m\n\u001b[1;32m    427\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    428\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__repr__\u001b[39m, (\u001b[38;5;28mself\u001b[39m,), \u001b[38;5;28mself\u001b[39m, tensor_contents\u001b[38;5;241m=\u001b[39mtensor_contents\n\u001b[1;32m    429\u001b[0m     )\n\u001b[1;32m    430\u001b[0m \u001b[38;5;66;03m# All strings are unicode in Python 3.\u001b[39;00m\n\u001b[0;32m--> 431\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_tensor_str\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_str\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtensor_contents\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtensor_contents\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/_tensor_str.py:664\u001b[0m, in \u001b[0;36m_str\u001b[0;34m(self, tensor_contents)\u001b[0m\n\u001b[1;32m    662\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad(), torch\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39m_python_dispatch\u001b[38;5;241m.\u001b[39m_disable_current_modes():\n\u001b[1;32m    663\u001b[0m     guard \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39m_C\u001b[38;5;241m.\u001b[39m_DisableFuncTorch()\n\u001b[0;32m--> 664\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_str_intern\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtensor_contents\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtensor_contents\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/_tensor_str.py:595\u001b[0m, in \u001b[0;36m_str_intern\u001b[0;34m(inp, tensor_contents)\u001b[0m\n\u001b[1;32m    593\u001b[0m                     tensor_str \u001b[38;5;241m=\u001b[39m _tensor_str(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mto_dense(), indent)\n\u001b[1;32m    594\u001b[0m                 \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 595\u001b[0m                     tensor_str \u001b[38;5;241m=\u001b[39m \u001b[43m_tensor_str\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindent\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    597\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayout \u001b[38;5;241m!=\u001b[39m torch\u001b[38;5;241m.\u001b[39mstrided:\n\u001b[1;32m    598\u001b[0m     suffixes\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlayout=\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayout))\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/_tensor_str.py:347\u001b[0m, in \u001b[0;36m_tensor_str\u001b[0;34m(self, indent)\u001b[0m\n\u001b[1;32m    343\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _tensor_str_with_formatter(\n\u001b[1;32m    344\u001b[0m         \u001b[38;5;28mself\u001b[39m, indent, summarize, real_formatter, imag_formatter\n\u001b[1;32m    345\u001b[0m     )\n\u001b[1;32m    346\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 347\u001b[0m     formatter \u001b[38;5;241m=\u001b[39m \u001b[43m_Formatter\u001b[49m\u001b[43m(\u001b[49m\u001b[43mget_summarized_data\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43msummarize\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    348\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _tensor_str_with_formatter(\u001b[38;5;28mself\u001b[39m, indent, summarize, formatter)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/_tensor_str.py:183\u001b[0m, in \u001b[0;36m_Formatter.__init__\u001b[0;34m(self, tensor)\u001b[0m\n\u001b[1;32m    181\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    182\u001b[0m             \u001b[38;5;28;01mfor\u001b[39;00m value \u001b[38;5;129;01min\u001b[39;00m nonzero_finite_vals:\n\u001b[0;32m--> 183\u001b[0m                 value_str \u001b[38;5;241m=\u001b[39m \u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;130;43;01m{{\u001b[39;49;00m\u001b[38;5;124;43m:.\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mPRINT_OPTS\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprecision\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;130;43;01m}}\u001b[39;49;00m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mformat\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    184\u001b[0m                 \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmax_width \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmax\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmax_width, \u001b[38;5;28mlen\u001b[39m(value_str))\n\u001b[1;32m    186\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m PRINT_OPTS\u001b[38;5;241m.\u001b[39msci_mode \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/_tensor.py:932\u001b[0m, in \u001b[0;36mTensor.__format__\u001b[0;34m(self, format_spec)\u001b[0m\n\u001b[1;32m    930\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    931\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(Tensor\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__format__\u001b[39m, (\u001b[38;5;28mself\u001b[39m,), \u001b[38;5;28mself\u001b[39m, format_spec)\n\u001b[0;32m--> 932\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdim\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mis_meta \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;129;01mis\u001b[39;00m Tensor:\n\u001b[1;32m    933\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mitem()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__format__\u001b[39m(format_spec)\n\u001b[1;32m    934\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mobject\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__format__\u001b[39m(\u001b[38;5;28mself\u001b[39m, format_spec)\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "],"ename":"KeyboardInterrupt","evalue":"","output_type":"error"}]},{"cell_type":"code","source":"optimizer = optim.Adam(model.parameters(), lr=0.00000000001 * 1 * 1)\nevaluate_model(model, custom_train_loader, criterion, optimizer)","metadata":{"execution":{"iopub.status.busy":"2024-06-22T18:56:59.221468Z","iopub.execute_input":"2024-06-22T18:56:59.221930Z","iopub.status.idle":"2024-06-22T18:57:15.358806Z","shell.execute_reply.started":"2024-06-22T18:56:59.221892Z","shell.execute_reply":"2024-06-22T18:57:15.357436Z"},"trusted":true},"execution_count":63,"outputs":[{"name":"stdout","text":"Epoch    1 | Train Loss: 4.178831743502087e-05 | Validation Loss: 3.730434282260831e-05\nEpoch    2 | Train Loss: 4.1773178804760394e-05 | Validation Loss: 3.730439875653247e-05\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","Cell \u001b[0;32mIn[63], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m optimizer \u001b[38;5;241m=\u001b[39m optim\u001b[38;5;241m.\u001b[39mAdam(model\u001b[38;5;241m.\u001b[39mparameters(), lr\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.000000001\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m----> 2\u001b[0m \u001b[43mevaluate_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcustom_train_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m)\u001b[49m\n","Cell \u001b[0;32mIn[59], line 51\u001b[0m, in \u001b[0;36mevaluate_model\u001b[0;34m(model, custom_train_loader, criterion, optimizer)\u001b[0m\n\u001b[1;32m     49\u001b[0m parameters \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m     50\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m name, param \u001b[38;5;129;01min\u001b[39;00m model\u001b[38;5;241m.\u001b[39mnamed_parameters():\n\u001b[0;32m---> 51\u001b[0m     parameters\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mparam\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     53\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[1;32m     54\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/_tensor.py:934\u001b[0m, in \u001b[0;36mTensor.__format__\u001b[0;34m(self, format_spec)\u001b[0m\n\u001b[1;32m    932\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdim() \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mis_meta \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;129;01mis\u001b[39;00m Tensor:\n\u001b[1;32m    933\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mitem()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__format__\u001b[39m(format_spec)\n\u001b[0;32m--> 934\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mobject\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__format__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mformat_spec\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/parameter.py:63\u001b[0m, in \u001b[0;36mParameter.__repr__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     62\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__repr__\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m---> 63\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mParameter containing:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__repr__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/_tensor.py:431\u001b[0m, in \u001b[0;36mTensor.__repr__\u001b[0;34m(self, tensor_contents)\u001b[0m\n\u001b[1;32m    427\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    428\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__repr__\u001b[39m, (\u001b[38;5;28mself\u001b[39m,), \u001b[38;5;28mself\u001b[39m, tensor_contents\u001b[38;5;241m=\u001b[39mtensor_contents\n\u001b[1;32m    429\u001b[0m     )\n\u001b[1;32m    430\u001b[0m \u001b[38;5;66;03m# All strings are unicode in Python 3.\u001b[39;00m\n\u001b[0;32m--> 431\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_tensor_str\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_str\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtensor_contents\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtensor_contents\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/_tensor_str.py:664\u001b[0m, in \u001b[0;36m_str\u001b[0;34m(self, tensor_contents)\u001b[0m\n\u001b[1;32m    662\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad(), torch\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39m_python_dispatch\u001b[38;5;241m.\u001b[39m_disable_current_modes():\n\u001b[1;32m    663\u001b[0m     guard \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39m_C\u001b[38;5;241m.\u001b[39m_DisableFuncTorch()\n\u001b[0;32m--> 664\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_str_intern\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtensor_contents\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtensor_contents\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/_tensor_str.py:595\u001b[0m, in \u001b[0;36m_str_intern\u001b[0;34m(inp, tensor_contents)\u001b[0m\n\u001b[1;32m    593\u001b[0m                     tensor_str \u001b[38;5;241m=\u001b[39m _tensor_str(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mto_dense(), indent)\n\u001b[1;32m    594\u001b[0m                 \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 595\u001b[0m                     tensor_str \u001b[38;5;241m=\u001b[39m \u001b[43m_tensor_str\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindent\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    597\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayout \u001b[38;5;241m!=\u001b[39m torch\u001b[38;5;241m.\u001b[39mstrided:\n\u001b[1;32m    598\u001b[0m     suffixes\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlayout=\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayout))\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/_tensor_str.py:347\u001b[0m, in \u001b[0;36m_tensor_str\u001b[0;34m(self, indent)\u001b[0m\n\u001b[1;32m    343\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _tensor_str_with_formatter(\n\u001b[1;32m    344\u001b[0m         \u001b[38;5;28mself\u001b[39m, indent, summarize, real_formatter, imag_formatter\n\u001b[1;32m    345\u001b[0m     )\n\u001b[1;32m    346\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 347\u001b[0m     formatter \u001b[38;5;241m=\u001b[39m \u001b[43m_Formatter\u001b[49m\u001b[43m(\u001b[49m\u001b[43mget_summarized_data\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43msummarize\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    348\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _tensor_str_with_formatter(\u001b[38;5;28mself\u001b[39m, indent, summarize, formatter)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/_tensor_str.py:183\u001b[0m, in \u001b[0;36m_Formatter.__init__\u001b[0;34m(self, tensor)\u001b[0m\n\u001b[1;32m    181\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    182\u001b[0m             \u001b[38;5;28;01mfor\u001b[39;00m value \u001b[38;5;129;01min\u001b[39;00m nonzero_finite_vals:\n\u001b[0;32m--> 183\u001b[0m                 value_str \u001b[38;5;241m=\u001b[39m \u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;130;43;01m{{\u001b[39;49;00m\u001b[38;5;124;43m:.\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mPRINT_OPTS\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprecision\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;130;43;01m}}\u001b[39;49;00m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mformat\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    184\u001b[0m                 \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmax_width \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmax\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmax_width, \u001b[38;5;28mlen\u001b[39m(value_str))\n\u001b[1;32m    186\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m PRINT_OPTS\u001b[38;5;241m.\u001b[39msci_mode \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/_tensor.py:933\u001b[0m, in \u001b[0;36mTensor.__format__\u001b[0;34m(self, format_spec)\u001b[0m\n\u001b[1;32m    931\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(Tensor\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__format__\u001b[39m, (\u001b[38;5;28mself\u001b[39m,), \u001b[38;5;28mself\u001b[39m, format_spec)\n\u001b[1;32m    932\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdim() \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mis_meta \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;129;01mis\u001b[39;00m Tensor:\n\u001b[0;32m--> 933\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__format__\u001b[39m(format_spec)\n\u001b[1;32m    934\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mobject\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__format__\u001b[39m(\u001b[38;5;28mself\u001b[39m, format_spec)\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "],"ename":"KeyboardInterrupt","evalue":"","output_type":"error"}]},{"cell_type":"code","source":"optimizer = optim.Adam(model.parameters(), lr=0.00000001 * 1 * 1)\nevaluate_model(model, custom_train_loader, criterion, optimizer)","metadata":{"execution":{"iopub.status.busy":"2024-06-22T18:53:15.843012Z","iopub.execute_input":"2024-06-22T18:53:15.843391Z","iopub.status.idle":"2024-06-22T18:56:52.331393Z","shell.execute_reply.started":"2024-06-22T18:53:15.843361Z","shell.execute_reply":"2024-06-22T18:56:52.330166Z"},"trusted":true},"execution_count":62,"outputs":[{"name":"stdout","text":"Epoch    1 | Train Loss: 4.2122242371078754e-05 | Validation Loss: 3.74700440261222e-05\nEpoch    2 | Train Loss: 4.2075319515788576e-05 | Validation Loss: 3.7516467909881615e-05\nEpoch    3 | Train Loss: 4.205076124890184e-05 | Validation Loss: 3.748949820874259e-05\nEpoch    4 | Train Loss: 4.2070693166351705e-05 | Validation Loss: 3.7535088904405714e-05\nEpoch    5 | Train Loss: 4.20531376676081e-05 | Validation Loss: 3.7411842004075877e-05\nEpoch    6 | Train Loss: 4.198151807486283e-05 | Validation Loss: 3.745267640624661e-05\nEpoch    7 | Train Loss: 4.2055803669427404e-05 | Validation Loss: 3.7423850972118086e-05\nEpoch    8 | Train Loss: 4.1970083702835834e-05 | Validation Loss: 3.750649993889965e-05\nEpoch    9 | Train Loss: 4.194904599899019e-05 | Validation Loss: 3.746868278540205e-05\nEpoch   10 | Train Loss: 4.2001351425824394e-05 | Validation Loss: 3.750391097128159e-05\nEpoch   11 | Train Loss: 4.2038283299916654e-05 | Validation Loss: 3.7423305820993845e-05\nEpoch   12 | Train Loss: 4.200565981591353e-05 | Validation Loss: 3.742337003131979e-05\nEpoch   13 | Train Loss: 4.199600025458494e-05 | Validation Loss: 3.737780016308534e-05\nEpoch   14 | Train Loss: 4.1987839449575406e-05 | Validation Loss: 3.738971226994181e-05\nEpoch   15 | Train Loss: 4.200127123112907e-05 | Validation Loss: 3.739752237379435e-05\nEpoch   16 | Train Loss: 4.1939897937481876e-05 | Validation Loss: 3.741582340808236e-05\nEpoch   17 | Train Loss: 4.1998183314717606e-05 | Validation Loss: 3.736927974387072e-05\nEpoch   18 | Train Loss: 4.193467332243017e-05 | Validation Loss: 3.753358987523825e-05\nEpoch   19 | Train Loss: 4.196245629373152e-05 | Validation Loss: 3.7499812970054335e-05\nEpoch   20 | Train Loss: 4.20149464844144e-05 | Validation Loss: 3.7553270522039385e-05\nEpoch   21 | Train Loss: 4.195518833967071e-05 | Validation Loss: 3.7340878316172164e-05\nEpoch   22 | Train Loss: 4.1969735002567175e-05 | Validation Loss: 3.7443126166181175e-05\nEpoch   23 | Train Loss: 4.199963846076571e-05 | Validation Loss: 3.742286653505289e-05\nEpoch   24 | Train Loss: 4.1994081720986286e-05 | Validation Loss: 3.7435758440551584e-05\nEpoch   25 | Train Loss: 4.190551801457332e-05 | Validation Loss: 3.73732196749188e-05\nEpoch   26 | Train Loss: 4.1862854027385764e-05 | Validation Loss: 3.739466092156363e-05\nEpoch   27 | Train Loss: 4.202358934435324e-05 | Validation Loss: 3.756375845114235e-05\nEpoch   28 | Train Loss: 4.197427736016834e-05 | Validation Loss: 3.738622335731634e-05\nEpoch   29 | Train Loss: 4.189420301372593e-05 | Validation Loss: 3.7414888447528936e-05\nEpoch   30 | Train Loss: 4.192790320303175e-05 | Validation Loss: 3.7382590835477456e-05\nEpoch   31 | Train Loss: 4.19346930584652e-05 | Validation Loss: 3.733142475539353e-05\nEpoch   32 | Train Loss: 4.192499206965294e-05 | Validation Loss: 3.734416650331695e-05\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","Cell \u001b[0;32mIn[62], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m optimizer \u001b[38;5;241m=\u001b[39m optim\u001b[38;5;241m.\u001b[39mAdam(model\u001b[38;5;241m.\u001b[39mparameters(), lr\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.00000001\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m----> 2\u001b[0m \u001b[43mevaluate_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcustom_train_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m)\u001b[49m\n","Cell \u001b[0;32mIn[59], line 53\u001b[0m, in \u001b[0;36mevaluate_model\u001b[0;34m(model, custom_train_loader, criterion, optimizer)\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m name, param \u001b[38;5;129;01min\u001b[39;00m model\u001b[38;5;241m.\u001b[39mnamed_parameters():\n\u001b[1;32m     51\u001b[0m     parameters\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mparam\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 53\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     54\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m     56\u001b[0m running_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem()\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/_tensor.py:492\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    482\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    483\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    484\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    485\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    490\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    491\u001b[0m     )\n\u001b[0;32m--> 492\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    493\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    494\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/autograd/__init__.py:251\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    246\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    248\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    249\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    250\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 251\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    252\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    253\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    254\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    255\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    256\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    257\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    258\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    259\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "],"ename":"KeyboardInterrupt","evalue":"","output_type":"error"}]},{"cell_type":"code","source":"optimizer = optim.Adam(model.parameters(), lr=0.0000001 * 1 * 1)\nevaluate_model(model, custom_train_loader, criterion, optimizer)","metadata":{"execution":{"iopub.status.busy":"2024-06-22T18:37:04.976480Z","iopub.execute_input":"2024-06-22T18:37:04.976841Z","iopub.status.idle":"2024-06-22T18:53:08.047474Z","shell.execute_reply.started":"2024-06-22T18:37:04.976811Z","shell.execute_reply":"2024-06-22T18:53:08.045849Z"},"trusted":true},"execution_count":61,"outputs":[{"name":"stdout","text":"Epoch    1 | Train Loss: 4.932251197260484e-05 | Validation Loss: 4.361703377071535e-05\nEpoch    2 | Train Loss: 4.8983513920575206e-05 | Validation Loss: 4.3390861446823695e-05\nEpoch    3 | Train Loss: 4.8545198706051454e-05 | Validation Loss: 4.35760786785977e-05\nEpoch    4 | Train Loss: 4.917875296541752e-05 | Validation Loss: 4.342070560596767e-05\nEpoch    5 | Train Loss: 4.854464261825342e-05 | Validation Loss: 4.280488747099298e-05\nEpoch    6 | Train Loss: 4.8651249130671204e-05 | Validation Loss: 4.35042457866075e-05\nEpoch    7 | Train Loss: 4.848064788802731e-05 | Validation Loss: 4.36342121247435e-05\nEpoch    8 | Train Loss: 4.856405762438953e-05 | Validation Loss: 4.2978250712621956e-05\nEpoch    9 | Train Loss: 4.818977872673713e-05 | Validation Loss: 4.326902899265406e-05\nEpoch   10 | Train Loss: 4.8746057086646036e-05 | Validation Loss: 4.6133692740113476e-05\nEpoch   11 | Train Loss: 4.8721323310019216e-05 | Validation Loss: 4.2605574708431956e-05\nEpoch   12 | Train Loss: 4.814734631963802e-05 | Validation Loss: 4.307333774704603e-05\nEpoch   13 | Train Loss: 4.881281531652348e-05 | Validation Loss: 4.2932805899908996e-05\nEpoch   14 | Train Loss: 4.8302143204637106e-05 | Validation Loss: 4.3134516636200716e-05\nEpoch   15 | Train Loss: 4.8208297062046765e-05 | Validation Loss: 4.2628027495084096e-05\nEpoch   16 | Train Loss: 4.8283195178555614e-05 | Validation Loss: 4.3845474465342704e-05\nEpoch   17 | Train Loss: 4.836102539229614e-05 | Validation Loss: 4.430209319252754e-05\nEpoch   18 | Train Loss: 4.816983855562285e-05 | Validation Loss: 4.226644878144725e-05\nEpoch   19 | Train Loss: 4.873076907188079e-05 | Validation Loss: 4.2754231799335685e-05\nEpoch   20 | Train Loss: 4.8146389713110696e-05 | Validation Loss: 4.282743157091318e-05\nEpoch   21 | Train Loss: 4.8086613514897183e-05 | Validation Loss: 4.318725787015865e-05\nEpoch   22 | Train Loss: 4.800275969500945e-05 | Validation Loss: 4.195327665001969e-05\nEpoch   23 | Train Loss: 4.786567214978277e-05 | Validation Loss: 4.360449102023267e-05\nEpoch   24 | Train Loss: 4.858754673477961e-05 | Validation Loss: 4.200794255666551e-05\nEpoch   25 | Train Loss: 4.746932449961605e-05 | Validation Loss: 4.2444714290468254e-05\nEpoch   26 | Train Loss: 4.7765150452505625e-05 | Validation Loss: 4.368780546428752e-05\nEpoch   27 | Train Loss: 4.805802300325013e-05 | Validation Loss: 4.433314552443335e-05\nEpoch   28 | Train Loss: 4.74503930490755e-05 | Validation Loss: 4.297508767194813e-05\nEpoch   29 | Train Loss: 4.7725786839691867e-05 | Validation Loss: 4.565619829008938e-05\nEpoch   30 | Train Loss: 4.754037770453579e-05 | Validation Loss: 4.218827871227404e-05\nEpoch   31 | Train Loss: 4.774644723966048e-05 | Validation Loss: 4.368891759440885e-05\nEpoch   32 | Train Loss: 4.836164921471209e-05 | Validation Loss: 4.364829046608065e-05\nEpoch   33 | Train Loss: 4.72578347853414e-05 | Validation Loss: 4.202577256364748e-05\nEpoch   34 | Train Loss: 4.701931261479331e-05 | Validation Loss: 4.316301474318607e-05\nEpoch   35 | Train Loss: 4.726842978470813e-05 | Validation Loss: 4.198831920803059e-05\nEpoch   36 | Train Loss: 4.7242788900803136e-05 | Validation Loss: 4.172422050032765e-05\nEpoch   37 | Train Loss: 4.727694104076363e-05 | Validation Loss: 4.263177852408262e-05\nEpoch   38 | Train Loss: 4.7679446402071336e-05 | Validation Loss: 4.5608291839016604e-05\nEpoch   39 | Train Loss: 4.790539703662944e-05 | Validation Loss: 4.205762033961946e-05\nEpoch   40 | Train Loss: 4.769657887209178e-05 | Validation Loss: 4.395233327159076e-05\nEpoch   41 | Train Loss: 4.780799035870586e-05 | Validation Loss: 4.2462801229703476e-05\nEpoch   42 | Train Loss: 4.7463808732572944e-05 | Validation Loss: 4.227349591019447e-05\nEpoch   43 | Train Loss: 4.69184257553934e-05 | Validation Loss: 4.297738532841322e-05\nEpoch   44 | Train Loss: 4.6743055690967594e-05 | Validation Loss: 4.254848126947763e-05\nEpoch   45 | Train Loss: 4.703228025846329e-05 | Validation Loss: 4.349592982180184e-05\nEpoch   46 | Train Loss: 4.6665921536259704e-05 | Validation Loss: 4.14638158872549e-05\nEpoch   47 | Train Loss: 4.69048083459711e-05 | Validation Loss: 4.3413476669229566e-05\nEpoch   48 | Train Loss: 4.721318091469584e-05 | Validation Loss: 4.1437330219196154e-05\nEpoch   49 | Train Loss: 4.733826626761584e-05 | Validation Loss: 4.311757875257172e-05\nEpoch   50 | Train Loss: 4.6943243432906456e-05 | Validation Loss: 4.2101599865418395e-05\nEpoch   51 | Train Loss: 4.705136609572946e-05 | Validation Loss: 4.197697844574577e-05\nEpoch   52 | Train Loss: 4.640945846858813e-05 | Validation Loss: 4.293034917282057e-05\nEpoch   53 | Train Loss: 4.6982685103103e-05 | Validation Loss: 4.14288123465667e-05\nEpoch   54 | Train Loss: 4.745334431390802e-05 | Validation Loss: 4.285771165086771e-05\nEpoch   55 | Train Loss: 4.63076383994121e-05 | Validation Loss: 4.239275531290332e-05\nEpoch   56 | Train Loss: 4.705363353423309e-05 | Validation Loss: 4.152298979533953e-05\nEpoch   57 | Train Loss: 4.610771825355187e-05 | Validation Loss: 4.107158238184638e-05\nEpoch   58 | Train Loss: 4.660668907945365e-05 | Validation Loss: 4.073855770911905e-05\nEpoch   59 | Train Loss: 4.6462550017167814e-05 | Validation Loss: 4.115546444154461e-05\nEpoch   73 | Train Loss: 4.5638531378244804e-05 | Validation Loss: 4.038433689856902e-05\nEpoch   74 | Train Loss: 4.557399474833801e-05 | Validation Loss: 4.1553110440872844e-05\nEpoch   75 | Train Loss: 4.6048170315771134e-05 | Validation Loss: 4.186766245766194e-05\nEpoch   76 | Train Loss: 4.595517270900018e-05 | Validation Loss: 4.066350775246974e-05\nEpoch   77 | Train Loss: 4.5863510104027226e-05 | Validation Loss: 4.008443884231383e-05\nEpoch   78 | Train Loss: 4.561264779567864e-05 | Validation Loss: 4.0570109376858454e-05\nEpoch   79 | Train Loss: 4.521610144365695e-05 | Validation Loss: 3.9949786150828e-05\nEpoch   80 | Train Loss: 4.528065687736671e-05 | Validation Loss: 4.049153676533024e-05\nEpoch   81 | Train Loss: 4.565487695344928e-05 | Validation Loss: 4.019474135930068e-05\nEpoch   82 | Train Loss: 4.5435880952027216e-05 | Validation Loss: 4.171539858361939e-05\nEpoch   83 | Train Loss: 4.5175158425081464e-05 | Validation Loss: 4.0620391882839615e-05\nEpoch   84 | Train Loss: 4.540853408343537e-05 | Validation Loss: 4.080337266714196e-05\nEpoch   85 | Train Loss: 4.503045968249353e-05 | Validation Loss: 4.034205894640763e-05\nEpoch   86 | Train Loss: 4.5179425160313255e-05 | Validation Loss: 3.9856628518464277e-05\nEpoch   87 | Train Loss: 4.5455259828486307e-05 | Validation Loss: 4.027721151942387e-05\nEpoch   88 | Train Loss: 4.525345166257466e-05 | Validation Loss: 4.130758352403063e-05\nEpoch   89 | Train Loss: 4.5100849456503055e-05 | Validation Loss: 3.9786081288184505e-05\nEpoch   90 | Train Loss: 4.5226096631267865e-05 | Validation Loss: 4.011643859485048e-05\nEpoch   91 | Train Loss: 4.569116506445425e-05 | Validation Loss: 4.030489089927869e-05\nEpoch   92 | Train Loss: 4.500870782067068e-05 | Validation Loss: 3.978621507485514e-05\nEpoch   93 | Train Loss: 4.4999875058238104e-05 | Validation Loss: 3.981534882768756e-05\nEpoch   94 | Train Loss: 4.462988331397355e-05 | Validation Loss: 3.93629091377079e-05\nEpoch   95 | Train Loss: 4.491311296987988e-05 | Validation Loss: 4.004478005299461e-05\nEpoch   96 | Train Loss: 4.504810337948584e-05 | Validation Loss: 3.997072935817414e-05\nEpoch   97 | Train Loss: 4.504346163685114e-05 | Validation Loss: 3.940648812204017e-05\nEpoch   98 | Train Loss: 4.50702441185058e-05 | Validation Loss: 3.98224337004649e-05\nEpoch   99 | Train Loss: 4.472442219594086e-05 | Validation Loss: 3.960500871471595e-05\nEpoch  100 | Train Loss: 4.490753804020642e-05 | Validation Loss: 3.9612090040463956e-05\nEpoch  101 | Train Loss: 4.475591333630291e-05 | Validation Loss: 3.92789059333154e-05\nEpoch  102 | Train Loss: 4.504600613017828e-05 | Validation Loss: 4.0255811927636384e-05\nEpoch  103 | Train Loss: 4.473742485515686e-05 | Validation Loss: 3.992467491116258e-05\nEpoch  104 | Train Loss: 4.50839740324227e-05 | Validation Loss: 4.244772017045761e-05\nEpoch  105 | Train Loss: 4.445256954568322e-05 | Validation Loss: 3.9163542533060534e-05\nEpoch  106 | Train Loss: 4.4141211719761483e-05 | Validation Loss: 3.910087225449388e-05\nEpoch  107 | Train Loss: 4.394636662254925e-05 | Validation Loss: 3.926997660528286e-05\nEpoch  108 | Train Loss: 4.417958091380569e-05 | Validation Loss: 3.903105098288506e-05\nEpoch  109 | Train Loss: 4.436920446551085e-05 | Validation Loss: 3.952325414502411e-05\nEpoch  110 | Train Loss: 4.5022565291219505e-05 | Validation Loss: 3.94058472920733e-05\nEpoch  111 | Train Loss: 4.4532979904943205e-05 | Validation Loss: 3.8735985890525625e-05\nEpoch  112 | Train Loss: 4.426802706802846e-05 | Validation Loss: 3.891906571880099e-05\nEpoch  113 | Train Loss: 4.405019051318959e-05 | Validation Loss: 3.983821352449013e-05\nEpoch  114 | Train Loss: 4.4375286620379484e-05 | Validation Loss: 3.894825913448585e-05\nEpoch  115 | Train Loss: 4.418021062519983e-05 | Validation Loss: 3.9522618953924395e-05\nEpoch  116 | Train Loss: 4.403397390433384e-05 | Validation Loss: 3.905130324710626e-05\nEpoch  117 | Train Loss: 4.3677309417944346e-05 | Validation Loss: 3.90347669053881e-05\nEpoch  118 | Train Loss: 4.4126854641035605e-05 | Validation Loss: 3.890126126862015e-05\nEpoch  119 | Train Loss: 4.3812835360768074e-05 | Validation Loss: 3.850612220048788e-05\nEpoch  120 | Train Loss: 4.3792983615276174e-05 | Validation Loss: 3.967095453845104e-05\nEpoch  121 | Train Loss: 4.384932015000231e-05 | Validation Loss: 3.861601053358754e-05\nEpoch  122 | Train Loss: 4.341286914950615e-05 | Validation Loss: 3.8876770759088686e-05\nEpoch  123 | Train Loss: 4.355725025106949e-05 | Validation Loss: 3.942141274819733e-05\nEpoch  124 | Train Loss: 4.3375367795306374e-05 | Validation Loss: 3.906631236532121e-05\nEpoch  125 | Train Loss: 4.3611266778498245e-05 | Validation Loss: 3.903375682057231e-05\nEpoch  126 | Train Loss: 4.340891562151228e-05 | Validation Loss: 3.898629956893274e-05\nEpoch  127 | Train Loss: 4.375684100068611e-05 | Validation Loss: 3.949290276068495e-05\nEpoch  128 | Train Loss: 4.4187402522766206e-05 | Validation Loss: 3.829897132163751e-05\nEpoch  129 | Train Loss: 4.3567445277403746e-05 | Validation Loss: 3.9860802371549656e-05\nEpoch  130 | Train Loss: 4.3381801674513554e-05 | Validation Loss: 3.9301119340962035e-05\nEpoch  131 | Train Loss: 4.3404475104580345e-05 | Validation Loss: 3.858843238049303e-05\nEpoch  132 | Train Loss: 4.318085379964032e-05 | Validation Loss: 3.894931933245971e-05\nEpoch  133 | Train Loss: 4.36690527749306e-05 | Validation Loss: 3.7932905161142114e-05\nEpoch  134 | Train Loss: 4.295808016649971e-05 | Validation Loss: 3.8616253368672916e-05\nEpoch  135 | Train Loss: 4.3318278312654004e-05 | Validation Loss: 3.813965331573854e-05\nEpoch  136 | Train Loss: 4.302949976136006e-05 | Validation Loss: 3.8168756054801635e-05\nEpoch  137 | Train Loss: 4.37219398918387e-05 | Validation Loss: 3.8140145716170085e-05\nEpoch  138 | Train Loss: 4.330037797899422e-05 | Validation Loss: 3.893166167472373e-05\nEpoch  139 | Train Loss: 4.318703242915945e-05 | Validation Loss: 3.7918751695542595e-05\nEpoch  140 | Train Loss: 4.33265296578611e-05 | Validation Loss: 3.7583443099720173e-05\nEpoch  141 | Train Loss: 4.39328393440519e-05 | Validation Loss: 3.7727137805632085e-05\nEpoch  142 | Train Loss: 4.335258424816857e-05 | Validation Loss: 4.0589112177258355e-05\nEpoch  143 | Train Loss: 4.3172525556656184e-05 | Validation Loss: 3.797429208134418e-05\nEpoch  144 | Train Loss: 4.300982714084966e-05 | Validation Loss: 3.837482745439047e-05\nEpoch  145 | Train Loss: 4.3320375561961554e-05 | Validation Loss: 3.79795993467269e-05\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","Cell \u001b[0;32mIn[61], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m optimizer \u001b[38;5;241m=\u001b[39m optim\u001b[38;5;241m.\u001b[39mAdam(model\u001b[38;5;241m.\u001b[39mparameters(), lr\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.0000001\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m----> 2\u001b[0m \u001b[43mevaluate_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcustom_train_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m)\u001b[49m\n","Cell \u001b[0;32mIn[59], line 51\u001b[0m, in \u001b[0;36mevaluate_model\u001b[0;34m(model, custom_train_loader, criterion, optimizer)\u001b[0m\n\u001b[1;32m     49\u001b[0m parameters \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m     50\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m name, param \u001b[38;5;129;01min\u001b[39;00m model\u001b[38;5;241m.\u001b[39mnamed_parameters():\n\u001b[0;32m---> 51\u001b[0m     parameters\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mparam\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     53\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[1;32m     54\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/_tensor.py:934\u001b[0m, in \u001b[0;36mTensor.__format__\u001b[0;34m(self, format_spec)\u001b[0m\n\u001b[1;32m    932\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdim() \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mis_meta \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;129;01mis\u001b[39;00m Tensor:\n\u001b[1;32m    933\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mitem()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__format__\u001b[39m(format_spec)\n\u001b[0;32m--> 934\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mobject\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__format__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mformat_spec\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/parameter.py:63\u001b[0m, in \u001b[0;36mParameter.__repr__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     62\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__repr__\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m---> 63\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mParameter containing:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__repr__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/_tensor.py:431\u001b[0m, in \u001b[0;36mTensor.__repr__\u001b[0;34m(self, tensor_contents)\u001b[0m\n\u001b[1;32m    427\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    428\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__repr__\u001b[39m, (\u001b[38;5;28mself\u001b[39m,), \u001b[38;5;28mself\u001b[39m, tensor_contents\u001b[38;5;241m=\u001b[39mtensor_contents\n\u001b[1;32m    429\u001b[0m     )\n\u001b[1;32m    430\u001b[0m \u001b[38;5;66;03m# All strings are unicode in Python 3.\u001b[39;00m\n\u001b[0;32m--> 431\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_tensor_str\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_str\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtensor_contents\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtensor_contents\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/_tensor_str.py:664\u001b[0m, in \u001b[0;36m_str\u001b[0;34m(self, tensor_contents)\u001b[0m\n\u001b[1;32m    662\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad(), torch\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39m_python_dispatch\u001b[38;5;241m.\u001b[39m_disable_current_modes():\n\u001b[1;32m    663\u001b[0m     guard \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39m_C\u001b[38;5;241m.\u001b[39m_DisableFuncTorch()\n\u001b[0;32m--> 664\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_str_intern\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtensor_contents\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtensor_contents\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/_tensor_str.py:595\u001b[0m, in \u001b[0;36m_str_intern\u001b[0;34m(inp, tensor_contents)\u001b[0m\n\u001b[1;32m    593\u001b[0m                     tensor_str \u001b[38;5;241m=\u001b[39m _tensor_str(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mto_dense(), indent)\n\u001b[1;32m    594\u001b[0m                 \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 595\u001b[0m                     tensor_str \u001b[38;5;241m=\u001b[39m \u001b[43m_tensor_str\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindent\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    597\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayout \u001b[38;5;241m!=\u001b[39m torch\u001b[38;5;241m.\u001b[39mstrided:\n\u001b[1;32m    598\u001b[0m     suffixes\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlayout=\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayout))\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/_tensor_str.py:347\u001b[0m, in \u001b[0;36m_tensor_str\u001b[0;34m(self, indent)\u001b[0m\n\u001b[1;32m    343\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _tensor_str_with_formatter(\n\u001b[1;32m    344\u001b[0m         \u001b[38;5;28mself\u001b[39m, indent, summarize, real_formatter, imag_formatter\n\u001b[1;32m    345\u001b[0m     )\n\u001b[1;32m    346\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 347\u001b[0m     formatter \u001b[38;5;241m=\u001b[39m \u001b[43m_Formatter\u001b[49m\u001b[43m(\u001b[49m\u001b[43mget_summarized_data\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43msummarize\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    348\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _tensor_str_with_formatter(\u001b[38;5;28mself\u001b[39m, indent, summarize, formatter)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/_tensor_str.py:180\u001b[0m, in \u001b[0;36m_Formatter.__init__\u001b[0;34m(self, tensor)\u001b[0m\n\u001b[1;32m    178\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m value \u001b[38;5;129;01min\u001b[39;00m nonzero_finite_vals:\n\u001b[1;32m    179\u001b[0m         value_str \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m{{\u001b[39;00m\u001b[38;5;124m:.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mPRINT_OPTS\u001b[38;5;241m.\u001b[39mprecision\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124me\u001b[39m\u001b[38;5;130;01m}}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(value)\n\u001b[0;32m--> 180\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmax_width \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mmax\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_width\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mvalue_str\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    181\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    182\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m value \u001b[38;5;129;01min\u001b[39;00m nonzero_finite_vals:\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "],"ename":"KeyboardInterrupt","evalue":"","output_type":"error"}]},{"cell_type":"code","source":"evaluate_model(model, custom_train_loader, criterion, optimizer)","metadata":{"execution":{"iopub.status.busy":"2024-06-22T18:33:38.366169Z","iopub.execute_input":"2024-06-22T18:33:38.366531Z","iopub.status.idle":"2024-06-22T18:36:56.567193Z","shell.execute_reply.started":"2024-06-22T18:33:38.366503Z","shell.execute_reply":"2024-06-22T18:36:56.565884Z"},"trusted":true},"execution_count":60,"outputs":[{"name":"stdout","text":"Epoch    1 | Train Loss: 6.701514566884726e-05 | Validation Loss: 5.68784489587415e-05\nEpoch    2 | Train Loss: 6.500484996649903e-05 | Validation Loss: 5.715572806366254e-05\nEpoch    3 | Train Loss: 6.379183457738691e-05 | Validation Loss: 5.5473074098699726e-05\nEpoch    4 | Train Loss: 6.350125613607815e-05 | Validation Loss: 6.164491078379797e-05\nEpoch    5 | Train Loss: 6.316885774140246e-05 | Validation Loss: 5.6925460376078264e-05\nEpoch    6 | Train Loss: 6.201987080203252e-05 | Validation Loss: 5.212621672399109e-05\nEpoch    7 | Train Loss: 6.206905261478824e-05 | Validation Loss: 8.119083031488117e-05\nEpoch    8 | Train Loss: 6.365746876326738e-05 | Validation Loss: 5.104621159262024e-05\nEpoch    9 | Train Loss: 6.050487695574702e-05 | Validation Loss: 5.765892346971668e-05\nEpoch   10 | Train Loss: 6.210598685356672e-05 | Validation Loss: 5.255053056316683e-05\nEpoch   11 | Train Loss: 6.258985183649202e-05 | Validation Loss: 5.6532952839916106e-05\nEpoch   12 | Train Loss: 5.996096233502612e-05 | Validation Loss: 5.278794360492611e-05\nEpoch   13 | Train Loss: 6.517491110571427e-05 | Validation Loss: 5.056291120126843e-05\nEpoch   14 | Train Loss: 5.8497327017903444e-05 | Validation Loss: 5.5284594236582055e-05\nEpoch   15 | Train Loss: 6.059546749384026e-05 | Validation Loss: 6.103868036007043e-05\nEpoch   16 | Train Loss: 5.997538416977477e-05 | Validation Loss: 4.790971333932248e-05\nEpoch   17 | Train Loss: 6.0109375226602424e-05 | Validation Loss: 4.7815686139074386e-05\nEpoch   18 | Train Loss: 5.928739697083074e-05 | Validation Loss: 6.0927650883968455e-05\nEpoch   19 | Train Loss: 6.186764990161464e-05 | Validation Loss: 5.2794757175433914e-05\nEpoch   20 | Train Loss: 6.050355200386548e-05 | Validation Loss: 5.159279644431081e-05\nEpoch   21 | Train Loss: 5.5042519238668317e-05 | Validation Loss: 4.760170813824516e-05\nEpoch   22 | Train Loss: 6.156322845072282e-05 | Validation Loss: 5.0336282220087015e-05\nEpoch   23 | Train Loss: 5.685439725766628e-05 | Validation Loss: 4.840524088649545e-05\nEpoch   24 | Train Loss: 5.50940338143846e-05 | Validation Loss: 5.199580973567208e-05\nEpoch   25 | Train Loss: 5.9064689116894443e-05 | Validation Loss: 4.6293474497360876e-05\nEpoch   26 | Train Loss: 5.8540695204101215e-05 | Validation Loss: 5.140283838045434e-05\nEpoch   27 | Train Loss: 5.6472749315616964e-05 | Validation Loss: 6.541180500789779e-05\nEpoch   28 | Train Loss: 5.378825358093309e-05 | Validation Loss: 4.416552210386726e-05\nEpoch   29 | Train Loss: 5.6906977874859876e-05 | Validation Loss: 5.428067743196152e-05\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","Cell \u001b[0;32mIn[60], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mevaluate_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcustom_train_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m)\u001b[49m\n","Cell \u001b[0;32mIn[59], line 51\u001b[0m, in \u001b[0;36mevaluate_model\u001b[0;34m(model, custom_train_loader, criterion, optimizer)\u001b[0m\n\u001b[1;32m     49\u001b[0m parameters \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m     50\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m name, param \u001b[38;5;129;01min\u001b[39;00m model\u001b[38;5;241m.\u001b[39mnamed_parameters():\n\u001b[0;32m---> 51\u001b[0m     parameters\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mparam\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     53\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[1;32m     54\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/_tensor.py:934\u001b[0m, in \u001b[0;36mTensor.__format__\u001b[0;34m(self, format_spec)\u001b[0m\n\u001b[1;32m    932\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdim() \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mis_meta \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;129;01mis\u001b[39;00m Tensor:\n\u001b[1;32m    933\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mitem()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__format__\u001b[39m(format_spec)\n\u001b[0;32m--> 934\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mobject\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__format__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mformat_spec\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/parameter.py:63\u001b[0m, in \u001b[0;36mParameter.__repr__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     62\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__repr__\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m---> 63\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mParameter containing:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__repr__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/_tensor.py:431\u001b[0m, in \u001b[0;36mTensor.__repr__\u001b[0;34m(self, tensor_contents)\u001b[0m\n\u001b[1;32m    427\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    428\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__repr__\u001b[39m, (\u001b[38;5;28mself\u001b[39m,), \u001b[38;5;28mself\u001b[39m, tensor_contents\u001b[38;5;241m=\u001b[39mtensor_contents\n\u001b[1;32m    429\u001b[0m     )\n\u001b[1;32m    430\u001b[0m \u001b[38;5;66;03m# All strings are unicode in Python 3.\u001b[39;00m\n\u001b[0;32m--> 431\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_tensor_str\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_str\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtensor_contents\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtensor_contents\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/_tensor_str.py:664\u001b[0m, in \u001b[0;36m_str\u001b[0;34m(self, tensor_contents)\u001b[0m\n\u001b[1;32m    662\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad(), torch\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39m_python_dispatch\u001b[38;5;241m.\u001b[39m_disable_current_modes():\n\u001b[1;32m    663\u001b[0m     guard \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39m_C\u001b[38;5;241m.\u001b[39m_DisableFuncTorch()\n\u001b[0;32m--> 664\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_str_intern\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtensor_contents\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtensor_contents\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/_tensor_str.py:595\u001b[0m, in \u001b[0;36m_str_intern\u001b[0;34m(inp, tensor_contents)\u001b[0m\n\u001b[1;32m    593\u001b[0m                     tensor_str \u001b[38;5;241m=\u001b[39m _tensor_str(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mto_dense(), indent)\n\u001b[1;32m    594\u001b[0m                 \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 595\u001b[0m                     tensor_str \u001b[38;5;241m=\u001b[39m \u001b[43m_tensor_str\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindent\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    597\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayout \u001b[38;5;241m!=\u001b[39m torch\u001b[38;5;241m.\u001b[39mstrided:\n\u001b[1;32m    598\u001b[0m     suffixes\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlayout=\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayout))\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/_tensor_str.py:347\u001b[0m, in \u001b[0;36m_tensor_str\u001b[0;34m(self, indent)\u001b[0m\n\u001b[1;32m    343\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _tensor_str_with_formatter(\n\u001b[1;32m    344\u001b[0m         \u001b[38;5;28mself\u001b[39m, indent, summarize, real_formatter, imag_formatter\n\u001b[1;32m    345\u001b[0m     )\n\u001b[1;32m    346\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 347\u001b[0m     formatter \u001b[38;5;241m=\u001b[39m \u001b[43m_Formatter\u001b[49m\u001b[43m(\u001b[49m\u001b[43mget_summarized_data\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43msummarize\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    348\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _tensor_str_with_formatter(\u001b[38;5;28mself\u001b[39m, indent, summarize, formatter)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/_tensor_str.py:179\u001b[0m, in \u001b[0;36m_Formatter.__init__\u001b[0;34m(self, tensor)\u001b[0m\n\u001b[1;32m    177\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msci_mode \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    178\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m value \u001b[38;5;129;01min\u001b[39;00m nonzero_finite_vals:\n\u001b[0;32m--> 179\u001b[0m         value_str \u001b[38;5;241m=\u001b[39m \u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;130;43;01m{{\u001b[39;49;00m\u001b[38;5;124;43m:.\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mPRINT_OPTS\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprecision\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43me\u001b[39;49m\u001b[38;5;130;43;01m}}\u001b[39;49;00m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mformat\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    180\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmax_width \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmax\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmax_width, \u001b[38;5;28mlen\u001b[39m(value_str))\n\u001b[1;32m    181\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/_tensor.py:933\u001b[0m, in \u001b[0;36mTensor.__format__\u001b[0;34m(self, format_spec)\u001b[0m\n\u001b[1;32m    931\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(Tensor\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__format__\u001b[39m, (\u001b[38;5;28mself\u001b[39m,), \u001b[38;5;28mself\u001b[39m, format_spec)\n\u001b[1;32m    932\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdim() \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mis_meta \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;129;01mis\u001b[39;00m Tensor:\n\u001b[0;32m--> 933\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__format__\u001b[39m(format_spec)\n\u001b[1;32m    934\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mobject\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__format__\u001b[39m(\u001b[38;5;28mself\u001b[39m, format_spec)\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "],"ename":"KeyboardInterrupt","evalue":"","output_type":"error"}]},{"cell_type":"code","source":"optimizer = optim.Adam(model.parameters(), lr=0.0000001 * 100 * 1)\nevaluate_model(model, custom_train_loader, criterion, optimizer)","metadata":{"execution":{"iopub.status.busy":"2024-06-22T18:08:37.369989Z","iopub.execute_input":"2024-06-22T18:08:37.370910Z","iopub.status.idle":"2024-06-22T18:25:44.704546Z","shell.execute_reply.started":"2024-06-22T18:08:37.370876Z","shell.execute_reply":"2024-06-22T18:25:44.703181Z"},"trusted":true},"execution_count":56,"outputs":[{"name":"stdout","text":"Epoch    1 | Train Loss:    15.9797 | Validation Loss:    15.3399\nEpoch    2 | Train Loss:    15.1251 | Validation Loss:    13.6970\nEpoch    3 | Train Loss:    12.2845 | Validation Loss:     9.7600\nEpoch    4 | Train Loss:     7.9866 | Validation Loss:     6.2009\nEpoch    5 | Train Loss:     5.4810 | Validation Loss:     4.5194\nEpoch    6 | Train Loss:     4.1037 | Validation Loss:     3.4443\nEpoch    7 | Train Loss:     3.1998 | Validation Loss:     2.7696\nEpoch    8 | Train Loss:     2.6114 | Validation Loss:     2.3114\nEpoch    9 | Train Loss:     2.2190 | Validation Loss:     2.0217\nEpoch   10 | Train Loss:     1.9521 | Validation Loss:     1.8221\nEpoch   11 | Train Loss:     1.7594 | Validation Loss:     1.6778\nEpoch   12 | Train Loss:     1.6032 | Validation Loss:     1.5311\nEpoch   13 | Train Loss:     1.4597 | Validation Loss:     1.3871\nEpoch   14 | Train Loss:     1.3325 | Validation Loss:     1.2605\nEpoch   15 | Train Loss:     1.2078 | Validation Loss:     1.1481\nEpoch   16 | Train Loss:     1.0948 | Validation Loss:     1.0415\nEpoch   17 | Train Loss:     0.9845 | Validation Loss:     0.9250\nEpoch   18 | Train Loss:     0.8857 | Validation Loss:     0.8281\nEpoch   19 | Train Loss:     0.7911 | Validation Loss:     0.7378\nEpoch   20 | Train Loss:     0.7061 | Validation Loss:     0.6660\nEpoch   21 | Train Loss:     0.6255 | Validation Loss:     0.5787\nEpoch   22 | Train Loss:     0.5515 | Validation Loss:     0.5131\nEpoch   23 | Train Loss:     0.4807 | Validation Loss:     0.4398\nEpoch   24 | Train Loss:     0.4178 | Validation Loss:     0.3819\nEpoch   25 | Train Loss:     0.3588 | Validation Loss:     0.3211\nEpoch   26 | Train Loss:     0.3025 | Validation Loss:     0.2734\nEpoch   27 | Train Loss:     0.2548 | Validation Loss:     0.2278\nEpoch   28 | Train Loss:     0.2145 | Validation Loss:     0.1940\nEpoch   29 | Train Loss:     0.1807 | Validation Loss:     0.1610\nEpoch   30 | Train Loss:     0.1537 | Validation Loss:     0.1368\nEpoch   31 | Train Loss:     0.1309 | Validation Loss:     0.1218\nEpoch   32 | Train Loss:     0.1124 | Validation Loss:     0.1017\nEpoch   33 | Train Loss:     0.0980 | Validation Loss:     0.0894\nEpoch   34 | Train Loss:     0.0860 | Validation Loss:     0.0782\nEpoch   35 | Train Loss:     0.0752 | Validation Loss:     0.0693\nEpoch   36 | Train Loss:     0.0663 | Validation Loss:     0.0610\nEpoch   37 | Train Loss:     0.0584 | Validation Loss:     0.0552\nEpoch   38 | Train Loss:     0.0522 | Validation Loss:     0.0483\nEpoch   39 | Train Loss:     0.0469 | Validation Loss:     0.0427\nEpoch   40 | Train Loss:     0.0411 | Validation Loss:     0.0371\nEpoch   41 | Train Loss:     0.0361 | Validation Loss:     0.0335\nEpoch   42 | Train Loss:     0.0325 | Validation Loss:     0.0300\nEpoch   43 | Train Loss:     0.0291 | Validation Loss:     0.0260\nEpoch   44 | Train Loss:     0.0266 | Validation Loss:     0.0242\nEpoch   45 | Train Loss:     0.0230 | Validation Loss:     0.0218\nEpoch   46 | Train Loss:     0.0214 | Validation Loss:     0.0186\nEpoch   47 | Train Loss:     0.0188 | Validation Loss:     0.0178\nEpoch   48 | Train Loss:     0.0169 | Validation Loss:     0.0162\nEpoch   49 | Train Loss:     0.0152 | Validation Loss:     0.0144\nEpoch   50 | Train Loss:     0.0136 | Validation Loss:     0.0121\nEpoch   51 | Train Loss:     0.0128 | Validation Loss:     0.0113\nEpoch   52 | Train Loss:     0.0113 | Validation Loss:     0.0102\nEpoch   53 | Train Loss:     0.0101 | Validation Loss:     0.0092\nEpoch   54 | Train Loss:     0.0093 | Validation Loss:     0.0094\nEpoch   55 | Train Loss:     0.0084 | Validation Loss:     0.0073\nEpoch   56 | Train Loss:     0.0075 | Validation Loss:     0.0069\nEpoch   57 | Train Loss:     0.0073 | Validation Loss:     0.0063\nEpoch   58 | Train Loss:     0.0065 | Validation Loss:     0.0057\nEpoch   59 | Train Loss:     0.0059 | Validation Loss:     0.0055\nEpoch   60 | Train Loss:     0.0057 | Validation Loss:     0.0050\nEpoch   61 | Train Loss:     0.0051 | Validation Loss:     0.0044\nEpoch   62 | Train Loss:     0.0049 | Validation Loss:     0.0049\nEpoch   63 | Train Loss:     0.0048 | Validation Loss:     0.0043\nEpoch   64 | Train Loss:     0.0041 | Validation Loss:     0.0039\nEpoch   65 | Train Loss:     0.0041 | Validation Loss:     0.0034\nEpoch   66 | Train Loss:     0.0038 | Validation Loss:     0.0032\nEpoch   67 | Train Loss:     0.0034 | Validation Loss:     0.0030\nEpoch   68 | Train Loss:     0.0032 | Validation Loss:     0.0028\nEpoch   69 | Train Loss:     0.0031 | Validation Loss:     0.0032\nEpoch   70 | Train Loss:     0.0028 | Validation Loss:     0.0036\nEpoch   71 | Train Loss:     0.0028 | Validation Loss:     0.0027\nEpoch   72 | Train Loss:     0.0026 | Validation Loss:     0.0022\nEpoch   73 | Train Loss:     0.0025 | Validation Loss:     0.0024\nEpoch   74 | Train Loss:     0.0024 | Validation Loss:     0.0025\nEpoch   75 | Train Loss:     0.0021 | Validation Loss:     0.0018\nEpoch   76 | Train Loss:     0.0021 | Validation Loss:     0.0017\nEpoch   77 | Train Loss:     0.0020 | Validation Loss:     0.0016\nEpoch   78 | Train Loss:     0.0019 | Validation Loss:     0.0016\nEpoch   79 | Train Loss:     0.0018 | Validation Loss:     0.0029\nEpoch   80 | Train Loss:     0.0021 | Validation Loss:     0.0021\nEpoch   81 | Train Loss:     0.0016 | Validation Loss:     0.0012\nEpoch   82 | Train Loss:     0.0017 | Validation Loss:     0.0013\nEpoch   83 | Train Loss:     0.0013 | Validation Loss:     0.0015\nEpoch   84 | Train Loss:     0.0015 | Validation Loss:     0.0014\nEpoch   85 | Train Loss:     0.0013 | Validation Loss:     0.0010\nEpoch   86 | Train Loss:     0.0012 | Validation Loss:     0.0010\nEpoch   87 | Train Loss:     0.0012 | Validation Loss:     0.0012\nEpoch   88 | Train Loss:     0.0012 | Validation Loss:     0.0009\nEpoch   89 | Train Loss:     0.0014 | Validation Loss:     0.0009\nEpoch   90 | Train Loss:     0.0011 | Validation Loss:     0.0010\nEpoch   91 | Train Loss:     0.0011 | Validation Loss:     0.0009\nEpoch   92 | Train Loss:     0.0010 | Validation Loss:     0.0011\nEpoch   93 | Train Loss:     0.0010 | Validation Loss:     0.0014\nEpoch   94 | Train Loss:     0.0010 | Validation Loss:     0.0009\nEpoch   95 | Train Loss:     0.0009 | Validation Loss:     0.0007\nEpoch   96 | Train Loss:     0.0008 | Validation Loss:     0.0010\nEpoch   97 | Train Loss:     0.0009 | Validation Loss:     0.0007\nEpoch   98 | Train Loss:     0.0008 | Validation Loss:     0.0006\nEpoch   99 | Train Loss:     0.0007 | Validation Loss:     0.0007\nEpoch  100 | Train Loss:     0.0007 | Validation Loss:     0.0006\nEpoch  101 | Train Loss:     0.0007 | Validation Loss:     0.0007\nEpoch  102 | Train Loss:     0.0007 | Validation Loss:     0.0005\nEpoch  103 | Train Loss:     0.0006 | Validation Loss:     0.0013\nEpoch  104 | Train Loss:     0.0006 | Validation Loss:     0.0004\nEpoch  105 | Train Loss:     0.0006 | Validation Loss:     0.0006\nEpoch  106 | Train Loss:     0.0006 | Validation Loss:     0.0008\nEpoch  107 | Train Loss:     0.0007 | Validation Loss:     0.0004\nEpoch  108 | Train Loss:     0.0006 | Validation Loss:     0.0004\nEpoch  109 | Train Loss:     0.0005 | Validation Loss:     0.0004\nEpoch  110 | Train Loss:     0.0005 | Validation Loss:     0.0004\nEpoch  111 | Train Loss:     0.0006 | Validation Loss:     0.0005\nEpoch  112 | Train Loss:     0.0005 | Validation Loss:     0.0004\nEpoch  113 | Train Loss:     0.0005 | Validation Loss:     0.0004\nEpoch  114 | Train Loss:     0.0004 | Validation Loss:     0.0005\nEpoch  115 | Train Loss:     0.0005 | Validation Loss:     0.0003\nEpoch  116 | Train Loss:     0.0005 | Validation Loss:     0.0003\nEpoch  117 | Train Loss:     0.0004 | Validation Loss:     0.0006\nEpoch  118 | Train Loss:     0.0005 | Validation Loss:     0.0003\nEpoch  119 | Train Loss:     0.0004 | Validation Loss:     0.0003\nEpoch  120 | Train Loss:     0.0005 | Validation Loss:     0.0003\nEpoch  121 | Train Loss:     0.0004 | Validation Loss:     0.0003\nEpoch  122 | Train Loss:     0.0004 | Validation Loss:     0.0003\nEpoch  123 | Train Loss:     0.0003 | Validation Loss:     0.0002\nEpoch  124 | Train Loss:     0.0003 | Validation Loss:     0.0003\nEpoch  125 | Train Loss:     0.0003 | Validation Loss:     0.0004\nEpoch  126 | Train Loss:     0.0004 | Validation Loss:     0.0003\nEpoch  127 | Train Loss:     0.0003 | Validation Loss:     0.0002\nEpoch  128 | Train Loss:     0.0003 | Validation Loss:     0.0002\nEpoch  129 | Train Loss:     0.0003 | Validation Loss:     0.0003\nEpoch  130 | Train Loss:     0.0003 | Validation Loss:     0.0003\nEpoch  131 | Train Loss:     0.0003 | Validation Loss:     0.0003\nEpoch  132 | Train Loss:     0.0003 | Validation Loss:     0.0003\nEpoch  133 | Train Loss:     0.0002 | Validation Loss:     0.0002\nEpoch  134 | Train Loss:     0.0003 | Validation Loss:     0.0003\nEpoch  135 | Train Loss:     0.0003 | Validation Loss:     0.0003\nEpoch  136 | Train Loss:     0.0003 | Validation Loss:     0.0002\nEpoch  137 | Train Loss:     0.0003 | Validation Loss:     0.0002\nEpoch  138 | Train Loss:     0.0002 | Validation Loss:     0.0001\nEpoch  139 | Train Loss:     0.0003 | Validation Loss:     0.0004\nEpoch  140 | Train Loss:     0.0002 | Validation Loss:     0.0002\nEpoch  141 | Train Loss:     0.0003 | Validation Loss:     0.0012\nEpoch  142 | Train Loss:     0.0003 | Validation Loss:     0.0001\nEpoch  143 | Train Loss:     0.0002 | Validation Loss:     0.0001\nEpoch  144 | Train Loss:     0.0002 | Validation Loss:     0.0002\nEpoch  145 | Train Loss:     0.0002 | Validation Loss:     0.0001\nEpoch  146 | Train Loss:     0.0003 | Validation Loss:     0.0001\nEpoch  147 | Train Loss:     0.0002 | Validation Loss:     0.0001\nEpoch  148 | Train Loss:     0.0002 | Validation Loss:     0.0003\nEpoch  149 | Train Loss:     0.0002 | Validation Loss:     0.0001\nEpoch  150 | Train Loss:     0.0001 | Validation Loss:     0.0001\nEpoch  151 | Train Loss:     0.0002 | Validation Loss:     0.0002\nEpoch  152 | Train Loss:     0.0002 | Validation Loss:     0.0002\nEpoch  153 | Train Loss:     0.0002 | Validation Loss:     0.0001\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","Cell \u001b[0;32mIn[56], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m optimizer \u001b[38;5;241m=\u001b[39m optim\u001b[38;5;241m.\u001b[39mAdam(model\u001b[38;5;241m.\u001b[39mparameters(), lr\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.0000001\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m100\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m----> 2\u001b[0m \u001b[43mevaluate_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcustom_train_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m)\u001b[49m\n","Cell \u001b[0;32mIn[5], line 51\u001b[0m, in \u001b[0;36mevaluate_model\u001b[0;34m(model, custom_train_loader, criterion, optimizer)\u001b[0m\n\u001b[1;32m     49\u001b[0m parameters \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m     50\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m name, param \u001b[38;5;129;01min\u001b[39;00m model\u001b[38;5;241m.\u001b[39mnamed_parameters():\n\u001b[0;32m---> 51\u001b[0m     parameters\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mparam\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     53\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[1;32m     54\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/_tensor.py:934\u001b[0m, in \u001b[0;36mTensor.__format__\u001b[0;34m(self, format_spec)\u001b[0m\n\u001b[1;32m    932\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdim() \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mis_meta \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;129;01mis\u001b[39;00m Tensor:\n\u001b[1;32m    933\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mitem()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__format__\u001b[39m(format_spec)\n\u001b[0;32m--> 934\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mobject\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__format__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mformat_spec\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/parameter.py:63\u001b[0m, in \u001b[0;36mParameter.__repr__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     62\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__repr__\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m---> 63\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mParameter containing:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__repr__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/_tensor.py:431\u001b[0m, in \u001b[0;36mTensor.__repr__\u001b[0;34m(self, tensor_contents)\u001b[0m\n\u001b[1;32m    427\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    428\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__repr__\u001b[39m, (\u001b[38;5;28mself\u001b[39m,), \u001b[38;5;28mself\u001b[39m, tensor_contents\u001b[38;5;241m=\u001b[39mtensor_contents\n\u001b[1;32m    429\u001b[0m     )\n\u001b[1;32m    430\u001b[0m \u001b[38;5;66;03m# All strings are unicode in Python 3.\u001b[39;00m\n\u001b[0;32m--> 431\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_tensor_str\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_str\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtensor_contents\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtensor_contents\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/_tensor_str.py:664\u001b[0m, in \u001b[0;36m_str\u001b[0;34m(self, tensor_contents)\u001b[0m\n\u001b[1;32m    662\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad(), torch\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39m_python_dispatch\u001b[38;5;241m.\u001b[39m_disable_current_modes():\n\u001b[1;32m    663\u001b[0m     guard \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39m_C\u001b[38;5;241m.\u001b[39m_DisableFuncTorch()\n\u001b[0;32m--> 664\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_str_intern\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtensor_contents\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtensor_contents\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/_tensor_str.py:595\u001b[0m, in \u001b[0;36m_str_intern\u001b[0;34m(inp, tensor_contents)\u001b[0m\n\u001b[1;32m    593\u001b[0m                     tensor_str \u001b[38;5;241m=\u001b[39m _tensor_str(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mto_dense(), indent)\n\u001b[1;32m    594\u001b[0m                 \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 595\u001b[0m                     tensor_str \u001b[38;5;241m=\u001b[39m \u001b[43m_tensor_str\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindent\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    597\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayout \u001b[38;5;241m!=\u001b[39m torch\u001b[38;5;241m.\u001b[39mstrided:\n\u001b[1;32m    598\u001b[0m     suffixes\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlayout=\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayout))\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/_tensor_str.py:347\u001b[0m, in \u001b[0;36m_tensor_str\u001b[0;34m(self, indent)\u001b[0m\n\u001b[1;32m    343\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _tensor_str_with_formatter(\n\u001b[1;32m    344\u001b[0m         \u001b[38;5;28mself\u001b[39m, indent, summarize, real_formatter, imag_formatter\n\u001b[1;32m    345\u001b[0m     )\n\u001b[1;32m    346\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 347\u001b[0m     formatter \u001b[38;5;241m=\u001b[39m \u001b[43m_Formatter\u001b[49m\u001b[43m(\u001b[49m\u001b[43mget_summarized_data\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43msummarize\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    348\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _tensor_str_with_formatter(\u001b[38;5;28mself\u001b[39m, indent, summarize, formatter)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/_tensor_str.py:155\u001b[0m, in \u001b[0;36m_Formatter.__init__\u001b[0;34m(self, tensor)\u001b[0m\n\u001b[1;32m    152\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mint_mode \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m    153\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[0;32m--> 155\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mint_mode:\n\u001b[1;32m    156\u001b[0m     \u001b[38;5;66;03m# in int_mode for floats, all numbers are integers, and we append a decimal to nonfinites\u001b[39;00m\n\u001b[1;32m    157\u001b[0m     \u001b[38;5;66;03m# to indicate that the tensor is of floating type. add 1 to the len to account for this.\u001b[39;00m\n\u001b[1;32m    158\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m    159\u001b[0m         nonzero_finite_max \u001b[38;5;241m/\u001b[39m nonzero_finite_min \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1000.0\u001b[39m\n\u001b[1;32m    160\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m nonzero_finite_max \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1.0e8\u001b[39m\n\u001b[1;32m    161\u001b[0m     ):\n\u001b[1;32m    162\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msci_mode \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "],"ename":"KeyboardInterrupt","evalue":"","output_type":"error"}]},{"cell_type":"code","source":"optimizer = optim.Adam(model.parameters(), lr=0.0000001 * 10 * 1)\nevaluate_model(model, custom_train_loader, criterion, optimizer)","metadata":{"execution":{"iopub.status.busy":"2024-06-22T18:26:28.808411Z","iopub.execute_input":"2024-06-22T18:26:28.808810Z","iopub.status.idle":"2024-06-22T18:33:01.921444Z","shell.execute_reply.started":"2024-06-22T18:26:28.808781Z","shell.execute_reply":"2024-06-22T18:33:01.920023Z"},"trusted":true},"execution_count":57,"outputs":[{"name":"stdout","text":"Epoch    1 | Train Loss:     0.0001 | Validation Loss:     0.0001\nEpoch    2 | Train Loss:     0.0001 | Validation Loss:     0.0001\nEpoch    3 | Train Loss:     0.0001 | Validation Loss:     0.0001\nEpoch    4 | Train Loss:     0.0001 | Validation Loss:     0.0001\nEpoch    5 | Train Loss:     0.0001 | Validation Loss:     0.0001\nEpoch    6 | Train Loss:     0.0001 | Validation Loss:     0.0001\nEpoch    7 | Train Loss:     0.0001 | Validation Loss:     0.0001\nEpoch    8 | Train Loss:     0.0001 | Validation Loss:     0.0001\nEpoch    9 | Train Loss:     0.0001 | Validation Loss:     0.0001\nEpoch   10 | Train Loss:     0.0001 | Validation Loss:     0.0001\nEpoch   11 | Train Loss:     0.0001 | Validation Loss:     0.0001\nEpoch   12 | Train Loss:     0.0001 | Validation Loss:     0.0001\nEpoch   13 | Train Loss:     0.0001 | Validation Loss:     0.0001\nEpoch   14 | Train Loss:     0.0001 | Validation Loss:     0.0001\nEpoch   15 | Train Loss:     0.0001 | Validation Loss:     0.0001\nEpoch   16 | Train Loss:     0.0001 | Validation Loss:     0.0001\nEpoch   17 | Train Loss:     0.0001 | Validation Loss:     0.0001\nEpoch   18 | Train Loss:     0.0001 | Validation Loss:     0.0001\nEpoch   19 | Train Loss:     0.0001 | Validation Loss:     0.0001\nEpoch   20 | Train Loss:     0.0001 | Validation Loss:     0.0001\nEpoch   21 | Train Loss:     0.0001 | Validation Loss:     0.0001\nEpoch   22 | Train Loss:     0.0001 | Validation Loss:     0.0001\nEpoch   23 | Train Loss:     0.0001 | Validation Loss:     0.0001\nEpoch   24 | Train Loss:     0.0001 | Validation Loss:     0.0001\nEpoch   25 | Train Loss:     0.0001 | Validation Loss:     0.0001\nEpoch   26 | Train Loss:     0.0001 | Validation Loss:     0.0001\nEpoch   27 | Train Loss:     0.0001 | Validation Loss:     0.0001\nEpoch   28 | Train Loss:     0.0001 | Validation Loss:     0.0001\nEpoch   29 | Train Loss:     0.0001 | Validation Loss:     0.0001\nEpoch   30 | Train Loss:     0.0001 | Validation Loss:     0.0001\nEpoch   31 | Train Loss:     0.0001 | Validation Loss:     0.0001\nEpoch   32 | Train Loss:     0.0001 | Validation Loss:     0.0001\nEpoch   33 | Train Loss:     0.0001 | Validation Loss:     0.0001\nEpoch   34 | Train Loss:     0.0001 | Validation Loss:     0.0001\nEpoch   35 | Train Loss:     0.0001 | Validation Loss:     0.0001\nEpoch   36 | Train Loss:     0.0001 | Validation Loss:     0.0001\nEpoch   37 | Train Loss:     0.0001 | Validation Loss:     0.0001\nEpoch   38 | Train Loss:     0.0001 | Validation Loss:     0.0001\nEpoch   39 | Train Loss:     0.0001 | Validation Loss:     0.0001\nEpoch   40 | Train Loss:     0.0001 | Validation Loss:     0.0001\nEpoch   41 | Train Loss:     0.0001 | Validation Loss:     0.0001\nEpoch   42 | Train Loss:     0.0001 | Validation Loss:     0.0001\nEpoch   43 | Train Loss:     0.0001 | Validation Loss:     0.0001\nEpoch   44 | Train Loss:     0.0001 | Validation Loss:     0.0001\nEpoch   45 | Train Loss:     0.0001 | Validation Loss:     0.0001\nEpoch   46 | Train Loss:     0.0001 | Validation Loss:     0.0001\nEpoch   47 | Train Loss:     0.0001 | Validation Loss:     0.0001\nEpoch   48 | Train Loss:     0.0001 | Validation Loss:     0.0001\nEpoch   49 | Train Loss:     0.0001 | Validation Loss:     0.0001\nEpoch   50 | Train Loss:     0.0001 | Validation Loss:     0.0001\nEpoch   51 | Train Loss:     0.0001 | Validation Loss:     0.0001\nEpoch   52 | Train Loss:     0.0001 | Validation Loss:     0.0001\nEpoch   53 | Train Loss:     0.0001 | Validation Loss:     0.0001\nEpoch   54 | Train Loss:     0.0001 | Validation Loss:     0.0001\nEpoch   55 | Train Loss:     0.0001 | Validation Loss:     0.0001\nEpoch   56 | Train Loss:     0.0001 | Validation Loss:     0.0001\nEpoch   57 | Train Loss:     0.0001 | Validation Loss:     0.0001\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","Cell \u001b[0;32mIn[57], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m optimizer \u001b[38;5;241m=\u001b[39m optim\u001b[38;5;241m.\u001b[39mAdam(model\u001b[38;5;241m.\u001b[39mparameters(), lr\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.0000001\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m10\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m----> 2\u001b[0m \u001b[43mevaluate_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcustom_train_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m)\u001b[49m\n","Cell \u001b[0;32mIn[5], line 51\u001b[0m, in \u001b[0;36mevaluate_model\u001b[0;34m(model, custom_train_loader, criterion, optimizer)\u001b[0m\n\u001b[1;32m     49\u001b[0m parameters \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m     50\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m name, param \u001b[38;5;129;01min\u001b[39;00m model\u001b[38;5;241m.\u001b[39mnamed_parameters():\n\u001b[0;32m---> 51\u001b[0m     parameters\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mparam\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     53\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[1;32m     54\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/_tensor.py:934\u001b[0m, in \u001b[0;36mTensor.__format__\u001b[0;34m(self, format_spec)\u001b[0m\n\u001b[1;32m    932\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdim() \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mis_meta \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;129;01mis\u001b[39;00m Tensor:\n\u001b[1;32m    933\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mitem()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__format__\u001b[39m(format_spec)\n\u001b[0;32m--> 934\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mobject\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__format__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mformat_spec\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/parameter.py:63\u001b[0m, in \u001b[0;36mParameter.__repr__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     62\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__repr__\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m---> 63\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mParameter containing:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__repr__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/_tensor.py:431\u001b[0m, in \u001b[0;36mTensor.__repr__\u001b[0;34m(self, tensor_contents)\u001b[0m\n\u001b[1;32m    427\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    428\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__repr__\u001b[39m, (\u001b[38;5;28mself\u001b[39m,), \u001b[38;5;28mself\u001b[39m, tensor_contents\u001b[38;5;241m=\u001b[39mtensor_contents\n\u001b[1;32m    429\u001b[0m     )\n\u001b[1;32m    430\u001b[0m \u001b[38;5;66;03m# All strings are unicode in Python 3.\u001b[39;00m\n\u001b[0;32m--> 431\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_tensor_str\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_str\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtensor_contents\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtensor_contents\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/_tensor_str.py:664\u001b[0m, in \u001b[0;36m_str\u001b[0;34m(self, tensor_contents)\u001b[0m\n\u001b[1;32m    662\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad(), torch\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39m_python_dispatch\u001b[38;5;241m.\u001b[39m_disable_current_modes():\n\u001b[1;32m    663\u001b[0m     guard \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39m_C\u001b[38;5;241m.\u001b[39m_DisableFuncTorch()\n\u001b[0;32m--> 664\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_str_intern\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtensor_contents\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtensor_contents\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/_tensor_str.py:595\u001b[0m, in \u001b[0;36m_str_intern\u001b[0;34m(inp, tensor_contents)\u001b[0m\n\u001b[1;32m    593\u001b[0m                     tensor_str \u001b[38;5;241m=\u001b[39m _tensor_str(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mto_dense(), indent)\n\u001b[1;32m    594\u001b[0m                 \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 595\u001b[0m                     tensor_str \u001b[38;5;241m=\u001b[39m \u001b[43m_tensor_str\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindent\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    597\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayout \u001b[38;5;241m!=\u001b[39m torch\u001b[38;5;241m.\u001b[39mstrided:\n\u001b[1;32m    598\u001b[0m     suffixes\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlayout=\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayout))\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/_tensor_str.py:348\u001b[0m, in \u001b[0;36m_tensor_str\u001b[0;34m(self, indent)\u001b[0m\n\u001b[1;32m    346\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    347\u001b[0m     formatter \u001b[38;5;241m=\u001b[39m _Formatter(get_summarized_data(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;28;01mif\u001b[39;00m summarize \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m)\n\u001b[0;32m--> 348\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_tensor_str_with_formatter\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindent\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msummarize\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mformatter\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/_tensor_str.py:272\u001b[0m, in \u001b[0;36m_tensor_str_with_formatter\u001b[0;34m(self, indent, summarize, formatter1, formatter2)\u001b[0m\n\u001b[1;32m    269\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _scalar_str(\u001b[38;5;28mself\u001b[39m, formatter1, formatter2)\n\u001b[1;32m    271\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m dim \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m--> 272\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_vector_str\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindent\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msummarize\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mformatter1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mformatter2\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    274\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m summarize \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m0\u001b[39m) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m2\u001b[39m \u001b[38;5;241m*\u001b[39m PRINT_OPTS\u001b[38;5;241m.\u001b[39medgeitems:\n\u001b[1;32m    275\u001b[0m     slices \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    276\u001b[0m         [\n\u001b[1;32m    277\u001b[0m             _tensor_str_with_formatter(\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    288\u001b[0m         ]\n\u001b[1;32m    289\u001b[0m     )\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/_tensor_str.py:253\u001b[0m, in \u001b[0;36m_vector_str\u001b[0;34m(self, indent, summarize, formatter1, formatter2)\u001b[0m\n\u001b[1;32m    247\u001b[0m     data \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    248\u001b[0m         [_val_formatter(val) \u001b[38;5;28;01mfor\u001b[39;00m val \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m[: PRINT_OPTS\u001b[38;5;241m.\u001b[39medgeitems]\u001b[38;5;241m.\u001b[39mtolist()]\n\u001b[1;32m    249\u001b[0m         \u001b[38;5;241m+\u001b[39m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m ...\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m    250\u001b[0m         \u001b[38;5;241m+\u001b[39m [_val_formatter(val) \u001b[38;5;28;01mfor\u001b[39;00m val \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m[\u001b[38;5;241m-\u001b[39mPRINT_OPTS\u001b[38;5;241m.\u001b[39medgeitems :]\u001b[38;5;241m.\u001b[39mtolist()]\n\u001b[1;32m    251\u001b[0m     )\n\u001b[1;32m    252\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 253\u001b[0m     data \u001b[38;5;241m=\u001b[39m [_val_formatter(val) \u001b[38;5;28;01mfor\u001b[39;00m val \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtolist\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m]\n\u001b[1;32m    255\u001b[0m data_lines \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m    256\u001b[0m     data[i : i \u001b[38;5;241m+\u001b[39m elements_per_line] \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;28mlen\u001b[39m(data), elements_per_line)\n\u001b[1;32m    257\u001b[0m ]\n\u001b[1;32m    258\u001b[0m lines \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(line) \u001b[38;5;28;01mfor\u001b[39;00m line \u001b[38;5;129;01min\u001b[39;00m data_lines]\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "],"ename":"KeyboardInterrupt","evalue":"","output_type":"error"}]},{"cell_type":"code","source":"optimizer = optim.Adam(model.parameters(), lr=0.0000001 * 100 * 1)\nevaluate_model(model, custom_train_loader, criterion, optimizer)","metadata":{"execution":{"iopub.status.busy":"2024-06-22T18:03:28.740271Z","iopub.execute_input":"2024-06-22T18:03:28.741131Z","iopub.status.idle":"2024-06-22T18:07:44.767575Z","shell.execute_reply.started":"2024-06-22T18:03:28.741081Z","shell.execute_reply":"2024-06-22T18:07:44.766185Z"},"trusted":true},"execution_count":54,"outputs":[{"name":"stdout","text":"Epoch    1 | Train Loss:    16.0331 | Validation Loss:    15.5599\nEpoch    2 | Train Loss:    16.0332 | Validation Loss:    15.5571\nEpoch    3 | Train Loss:    16.0341 | Validation Loss:    15.5662\nEpoch    4 | Train Loss:    16.0464 | Validation Loss:    15.5801\nEpoch    5 | Train Loss:    16.0263 | Validation Loss:    15.5635\nEpoch    6 | Train Loss:    16.0488 | Validation Loss:    15.5995\nEpoch    7 | Train Loss:    16.0297 | Validation Loss:    15.5734\nEpoch    8 | Train Loss:    16.0246 | Validation Loss:    15.5519\nEpoch    9 | Train Loss:    15.9835 | Validation Loss:    15.4684\nEpoch   10 | Train Loss:    15.7837 | Validation Loss:    15.1572\nEpoch   11 | Train Loss:    15.1635 | Validation Loss:    14.2238\nEpoch   12 | Train Loss:    13.1517 | Validation Loss:    10.8894\nEpoch   13 | Train Loss:     8.8288 | Validation Loss:     6.5896\nEpoch   14 | Train Loss:     5.6189 | Validation Loss:     4.9263\nEpoch   15 | Train Loss:     4.7493 | Validation Loss:     4.6685\nEpoch   16 | Train Loss:     4.5796 | Validation Loss:     4.5948\nEpoch   17 | Train Loss:     4.5237 | Validation Loss:     4.5420\nEpoch   18 | Train Loss:     4.4868 | Validation Loss:     4.5269\nEpoch   19 | Train Loss:     4.4608 | Validation Loss:     4.4849\nEpoch   20 | Train Loss:     4.4458 | Validation Loss:     4.4808\nEpoch   21 | Train Loss:     4.4182 | Validation Loss:     4.4478\nEpoch   22 | Train Loss:     4.4021 | Validation Loss:     4.4488\nEpoch   23 | Train Loss:     4.3838 | Validation Loss:     4.4070\nEpoch   24 | Train Loss:     4.3585 | Validation Loss:     4.3936\nEpoch   25 | Train Loss:     4.3424 | Validation Loss:     4.3684\nEpoch   26 | Train Loss:     4.3212 | Validation Loss:     4.3442\nEpoch   27 | Train Loss:     4.2975 | Validation Loss:     4.3406\nEpoch   28 | Train Loss:     4.2789 | Validation Loss:     4.2947\nEpoch   29 | Train Loss:     4.2417 | Validation Loss:     4.2704\nEpoch   30 | Train Loss:     4.2238 | Validation Loss:     4.2481\nEpoch   31 | Train Loss:     4.1910 | Validation Loss:     4.2184\nEpoch   32 | Train Loss:     4.1723 | Validation Loss:     4.2154\nEpoch   33 | Train Loss:     4.1368 | Validation Loss:     4.1845\nEpoch   34 | Train Loss:     4.1128 | Validation Loss:     4.1465\nEpoch   35 | Train Loss:     4.0859 | Validation Loss:     4.1153\nEpoch   36 | Train Loss:     4.0387 | Validation Loss:     4.0680\nEpoch   37 | Train Loss:     4.0088 | Validation Loss:     4.0364\nEpoch   38 | Train Loss:     3.9684 | Validation Loss:     3.9988\nEpoch   39 | Train Loss:     3.9373 | Validation Loss:     3.9777\nEpoch   40 | Train Loss:     3.8966 | Validation Loss:     3.9250\nEpoch   41 | Train Loss:     3.8572 | Validation Loss:     3.8792\nEpoch   42 | Train Loss:     3.8158 | Validation Loss:     3.8458\nEpoch   43 | Train Loss:     3.7771 | Validation Loss:     3.8161\nEpoch   44 | Train Loss:     3.7349 | Validation Loss:     3.7481\nEpoch   45 | Train Loss:     3.6755 | Validation Loss:     3.7141\nEpoch   46 | Train Loss:     3.6294 | Validation Loss:     3.6601\nEpoch   47 | Train Loss:     3.5696 | Validation Loss:     3.5881\nEpoch   48 | Train Loss:     3.5122 | Validation Loss:     3.5237\nEpoch   49 | Train Loss:     3.4530 | Validation Loss:     3.4772\nEpoch   50 | Train Loss:     3.3922 | Validation Loss:     3.3831\nEpoch   51 | Train Loss:     3.3147 | Validation Loss:     3.3098\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","Cell \u001b[0;32mIn[54], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m optimizer \u001b[38;5;241m=\u001b[39m optim\u001b[38;5;241m.\u001b[39mAdam(model\u001b[38;5;241m.\u001b[39mparameters(), lr\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.0000001\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m100\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m----> 2\u001b[0m \u001b[43mevaluate_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcustom_train_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m)\u001b[49m\n","Cell \u001b[0;32mIn[5], line 54\u001b[0m, in \u001b[0;36mevaluate_model\u001b[0;34m(model, custom_train_loader, criterion, optimizer)\u001b[0m\n\u001b[1;32m     51\u001b[0m         parameters\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mparam\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     53\u001b[0m     loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[0;32m---> 54\u001b[0m     \u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     56\u001b[0m     running_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem()\n\u001b[1;32m     57\u001b[0m avg_train_loss \u001b[38;5;241m=\u001b[39m running_loss \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mlen\u001b[39m(custom_train_loader\u001b[38;5;241m.\u001b[39mget_train_loader())\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/optim/optimizer.py:373\u001b[0m, in \u001b[0;36mOptimizer.profile_hook_step.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    368\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    369\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m    370\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must return None or a tuple of (new_args, new_kwargs), but got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresult\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    371\u001b[0m             )\n\u001b[0;32m--> 373\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    374\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_optimizer_step_code()\n\u001b[1;32m    376\u001b[0m \u001b[38;5;66;03m# call optimizer step post hooks\u001b[39;00m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/optim/optimizer.py:76\u001b[0m, in \u001b[0;36m_use_grad_for_differentiable.<locals>._use_grad\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     74\u001b[0m     torch\u001b[38;5;241m.\u001b[39mset_grad_enabled(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdefaults[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdifferentiable\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m     75\u001b[0m     torch\u001b[38;5;241m.\u001b[39m_dynamo\u001b[38;5;241m.\u001b[39mgraph_break()\n\u001b[0;32m---> 76\u001b[0m     ret \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     77\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     78\u001b[0m     torch\u001b[38;5;241m.\u001b[39m_dynamo\u001b[38;5;241m.\u001b[39mgraph_break()\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/optim/adam.py:154\u001b[0m, in \u001b[0;36mAdam.step\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    151\u001b[0m     state_steps \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m    152\u001b[0m     beta1, beta2 \u001b[38;5;241m=\u001b[39m group[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbetas\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m--> 154\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_init_group\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    155\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    156\u001b[0m \u001b[43m        \u001b[49m\u001b[43mparams_with_grad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    157\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    158\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexp_avgs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    159\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    160\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_exp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    161\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstate_steps\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    163\u001b[0m     adam(\n\u001b[1;32m    164\u001b[0m         params_with_grad,\n\u001b[1;32m    165\u001b[0m         grads,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    182\u001b[0m         found_inf\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfound_inf\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[1;32m    183\u001b[0m     )\n\u001b[1;32m    185\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m loss\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/optim/adam.py:92\u001b[0m, in \u001b[0;36mAdam._init_group\u001b[0;34m(self, group, params_with_grad, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps)\u001b[0m\n\u001b[1;32m     90\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m group[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mparams\u001b[39m\u001b[38;5;124m'\u001b[39m]:\n\u001b[1;32m     91\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m p\u001b[38;5;241m.\u001b[39mgrad \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m---> 92\u001b[0m         \u001b[43mparams_with_grad\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mappend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mp\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     93\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m p\u001b[38;5;241m.\u001b[39mgrad\u001b[38;5;241m.\u001b[39mis_sparse:\n\u001b[1;32m     94\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mAdam does not support sparse gradients, please consider SparseAdam instead\u001b[39m\u001b[38;5;124m'\u001b[39m)\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "],"ename":"KeyboardInterrupt","evalue":"","output_type":"error"}]},{"cell_type":"code","source":"evaluate_model(model, custom_train_loader, criterion, optimizer)","metadata":{"execution":{"iopub.status.busy":"2024-06-22T03:36:26.316336Z","iopub.execute_input":"2024-06-22T03:36:26.316721Z"},"trusted":true},"execution_count":null,"outputs":[{"name":"stdout","text":"Epoch    1 | Train Loss:     0.0067 | Validation Loss:     0.0068\nEpoch    2 | Train Loss:     0.0076 | Validation Loss:     0.0069\nEpoch    3 | Train Loss:     0.0072 | Validation Loss:     0.0066\nEpoch    4 | Train Loss:     0.0069 | Validation Loss:     0.0066\nEpoch    5 | Train Loss:     0.0071 | Validation Loss:     0.0080\nEpoch    6 | Train Loss:     0.0090 | Validation Loss:     0.0060\nEpoch    7 | Train Loss:     0.0074 | Validation Loss:     0.0057\nEpoch    8 | Train Loss:     0.0064 | Validation Loss:     0.0060\nEpoch    9 | Train Loss:     0.0065 | Validation Loss:     0.0059\nEpoch   10 | Train Loss:     0.0063 | Validation Loss:     0.0071\nEpoch   11 | Train Loss:     0.0061 | Validation Loss:     0.0071\nEpoch   12 | Train Loss:     0.0069 | Validation Loss:     0.0078\nEpoch   13 | Train Loss:     0.0069 | Validation Loss:     0.0072\nEpoch   14 | Train Loss:     0.0064 | Validation Loss:     0.0056\nEpoch   15 | Train Loss:     0.0066 | Validation Loss:     0.0071\nEpoch   16 | Train Loss:     0.0068 | Validation Loss:     0.0080\nEpoch   17 | Train Loss:     0.0076 | Validation Loss:     0.0059\nEpoch   18 | Train Loss:     0.0060 | Validation Loss:     0.0056\nEpoch   19 | Train Loss:     0.0064 | Validation Loss:     0.0059\nEpoch   20 | Train Loss:     0.0060 | Validation Loss:     0.0062\nEpoch   21 | Train Loss:     0.0061 | Validation Loss:     0.0063\n","output_type":"stream"}]},{"cell_type":"code","source":"optimizer = optim.Adam(model.parameters(), lr=0.0000001 * 1000 * 1)\nevaluate_model(model, custom_train_loader, criterion, optimizer)","metadata":{"execution":{"iopub.status.busy":"2024-06-22T03:10:53.802331Z","iopub.execute_input":"2024-06-22T03:10:53.802768Z","iopub.status.idle":"2024-06-22T03:32:47.753291Z","shell.execute_reply.started":"2024-06-22T03:10:53.802735Z","shell.execute_reply":"2024-06-22T03:32:47.751339Z"},"trusted":true},"execution_count":364,"outputs":[{"name":"stdout","text":"Epoch    1 | Train Loss:    16.2018 | Validation Loss:    15.6782\nEpoch    2 | Train Loss:    16.1983 | Validation Loss:    15.6765\nEpoch    3 | Train Loss:    16.1977 | Validation Loss:    15.6730\nEpoch    4 | Train Loss:    16.1967 | Validation Loss:    15.6714\nEpoch    5 | Train Loss:    16.1919 | Validation Loss:    15.6658\nEpoch    6 | Train Loss:    16.1776 | Validation Loss:    15.6321\nEpoch    7 | Train Loss:    16.1310 | Validation Loss:    15.5671\nEpoch    8 | Train Loss:    15.9455 | Validation Loss:    15.2049\nEpoch    9 | Train Loss:    14.8920 | Validation Loss:    13.2720\nEpoch   10 | Train Loss:    11.9144 | Validation Loss:     9.7941\nEpoch   11 | Train Loss:     9.1096 | Validation Loss:     7.6368\nEpoch   12 | Train Loss:     7.5952 | Validation Loss:     6.5399\nEpoch   13 | Train Loss:     6.6632 | Validation Loss:     5.6619\nEpoch   14 | Train Loss:     5.7258 | Validation Loss:     4.5282\nEpoch   15 | Train Loss:     3.9493 | Validation Loss:     2.4194\nEpoch   16 | Train Loss:     1.7144 | Validation Loss:     0.9831\nEpoch   17 | Train Loss:     0.8193 | Validation Loss:     0.5409\nEpoch   18 | Train Loss:     0.4934 | Validation Loss:     0.3703\nEpoch   19 | Train Loss:     0.3689 | Validation Loss:     0.3017\nEpoch   20 | Train Loss:     0.3187 | Validation Loss:     0.2785\nEpoch   21 | Train Loss:     0.2856 | Validation Loss:     0.2579\nEpoch   22 | Train Loss:     0.2536 | Validation Loss:     0.2273\nEpoch   23 | Train Loss:     0.2381 | Validation Loss:     0.2157\nEpoch   24 | Train Loss:     0.2119 | Validation Loss:     0.1989\nEpoch   25 | Train Loss:     0.1982 | Validation Loss:     0.2020\nEpoch   26 | Train Loss:     0.1866 | Validation Loss:     0.1685\nEpoch   27 | Train Loss:     0.1710 | Validation Loss:     0.1541\nEpoch   28 | Train Loss:     0.1582 | Validation Loss:     0.1486\nEpoch   29 | Train Loss:     0.1440 | Validation Loss:     0.1353\nEpoch   30 | Train Loss:     0.1324 | Validation Loss:     0.1219\nEpoch   31 | Train Loss:     0.1231 | Validation Loss:     0.1157\nEpoch   32 | Train Loss:     0.1135 | Validation Loss:     0.1103\nEpoch   33 | Train Loss:     0.1091 | Validation Loss:     0.1011\nEpoch   34 | Train Loss:     0.1019 | Validation Loss:     0.0962\nEpoch   35 | Train Loss:     0.0988 | Validation Loss:     0.0960\nEpoch   36 | Train Loss:     0.0911 | Validation Loss:     0.0869\nEpoch   37 | Train Loss:     0.0844 | Validation Loss:     0.0809\nEpoch   38 | Train Loss:     0.0801 | Validation Loss:     0.0777\nEpoch   39 | Train Loss:     0.0774 | Validation Loss:     0.0755\nEpoch   40 | Train Loss:     0.0745 | Validation Loss:     0.0689\nEpoch   41 | Train Loss:     0.0697 | Validation Loss:     0.0674\nEpoch   42 | Train Loss:     0.0674 | Validation Loss:     0.0642\nEpoch   43 | Train Loss:     0.0655 | Validation Loss:     0.0653\nEpoch   44 | Train Loss:     0.0613 | Validation Loss:     0.0720\nEpoch   45 | Train Loss:     0.0589 | Validation Loss:     0.0547\nEpoch   46 | Train Loss:     0.0554 | Validation Loss:     0.0547\nEpoch   47 | Train Loss:     0.0550 | Validation Loss:     0.0531\nEpoch   48 | Train Loss:     0.0500 | Validation Loss:     0.0587\nEpoch   49 | Train Loss:     0.0493 | Validation Loss:     0.0482\nEpoch   50 | Train Loss:     0.0463 | Validation Loss:     0.0452\nEpoch   51 | Train Loss:     0.0440 | Validation Loss:     0.0440\nEpoch   52 | Train Loss:     0.0429 | Validation Loss:     0.0458\nEpoch   53 | Train Loss:     0.0439 | Validation Loss:     0.0410\nEpoch   54 | Train Loss:     0.0388 | Validation Loss:     0.0386\nEpoch   55 | Train Loss:     0.0381 | Validation Loss:     0.0387\nEpoch   56 | Train Loss:     0.0374 | Validation Loss:     0.0359\nEpoch   57 | Train Loss:     0.0372 | Validation Loss:     0.0346\nEpoch   58 | Train Loss:     0.0338 | Validation Loss:     0.0330\nEpoch   59 | Train Loss:     0.0329 | Validation Loss:     0.0354\nEpoch   60 | Train Loss:     0.0319 | Validation Loss:     0.0330\nEpoch   61 | Train Loss:     0.0304 | Validation Loss:     0.0298\nEpoch   62 | Train Loss:     0.0297 | Validation Loss:     0.0313\nEpoch   63 | Train Loss:     0.0281 | Validation Loss:     0.0280\nEpoch   64 | Train Loss:     0.0267 | Validation Loss:     0.0284\nEpoch   65 | Train Loss:     0.0262 | Validation Loss:     0.0278\nEpoch   66 | Train Loss:     0.0258 | Validation Loss:     0.0287\nEpoch   67 | Train Loss:     0.0258 | Validation Loss:     0.0253\nEpoch   68 | Train Loss:     0.0243 | Validation Loss:     0.0240\nEpoch   69 | Train Loss:     0.0241 | Validation Loss:     0.0238\nEpoch   70 | Train Loss:     0.0228 | Validation Loss:     0.0221\nEpoch   71 | Train Loss:     0.0226 | Validation Loss:     0.0241\nEpoch   72 | Train Loss:     0.0224 | Validation Loss:     0.0207\nEpoch   73 | Train Loss:     0.0217 | Validation Loss:     0.0236\nEpoch   74 | Train Loss:     0.0230 | Validation Loss:     0.0201\nEpoch   86 | Train Loss:     0.0185 | Validation Loss:     0.0220\nEpoch   87 | Train Loss:     0.0186 | Validation Loss:     0.0158\nEpoch   88 | Train Loss:     0.0159 | Validation Loss:     0.0154\nEpoch   89 | Train Loss:     0.0166 | Validation Loss:     0.0169\nEpoch   90 | Train Loss:     0.0162 | Validation Loss:     0.0150\nEpoch   91 | Train Loss:     0.0160 | Validation Loss:     0.0153\nEpoch   92 | Train Loss:     0.0153 | Validation Loss:     0.0158\nEpoch   93 | Train Loss:     0.0161 | Validation Loss:     0.0151\nEpoch   94 | Train Loss:     0.0157 | Validation Loss:     0.0158\nEpoch   95 | Train Loss:     0.0150 | Validation Loss:     0.0139\nEpoch   96 | Train Loss:     0.0142 | Validation Loss:     0.0152\nEpoch   97 | Train Loss:     0.0135 | Validation Loss:     0.0167\nEpoch   98 | Train Loss:     0.0140 | Validation Loss:     0.0135\nEpoch   99 | Train Loss:     0.0135 | Validation Loss:     0.0149\nEpoch  100 | Train Loss:     0.0130 | Validation Loss:     0.0147\nEpoch  101 | Train Loss:     0.0134 | Validation Loss:     0.0126\nEpoch  102 | Train Loss:     0.0130 | Validation Loss:     0.0128\nEpoch  103 | Train Loss:     0.0140 | Validation Loss:     0.0133\nEpoch  104 | Train Loss:     0.0134 | Validation Loss:     0.0122\nEpoch  105 | Train Loss:     0.0129 | Validation Loss:     0.0140\nEpoch  106 | Train Loss:     0.0134 | Validation Loss:     0.0147\nEpoch  107 | Train Loss:     0.0120 | Validation Loss:     0.0115\nEpoch  108 | Train Loss:     0.0120 | Validation Loss:     0.0157\nEpoch  109 | Train Loss:     0.0127 | Validation Loss:     0.0110\nEpoch  110 | Train Loss:     0.0114 | Validation Loss:     0.0120\nEpoch  111 | Train Loss:     0.0119 | Validation Loss:     0.0124\nEpoch  112 | Train Loss:     0.0125 | Validation Loss:     0.0141\nEpoch  113 | Train Loss:     0.0115 | Validation Loss:     0.0123\nEpoch  114 | Train Loss:     0.0135 | Validation Loss:     0.0139\nEpoch  115 | Train Loss:     0.0105 | Validation Loss:     0.0103\nEpoch  116 | Train Loss:     0.0102 | Validation Loss:     0.0101\nEpoch  117 | Train Loss:     0.0114 | Validation Loss:     0.0101\nEpoch  118 | Train Loss:     0.0104 | Validation Loss:     0.0106\nEpoch  119 | Train Loss:     0.0104 | Validation Loss:     0.0108\nEpoch  120 | Train Loss:     0.0101 | Validation Loss:     0.0123\nEpoch  121 | Train Loss:     0.0099 | Validation Loss:     0.0102\nEpoch  122 | Train Loss:     0.0110 | Validation Loss:     0.0104\nEpoch  123 | Train Loss:     0.0097 | Validation Loss:     0.0124\nEpoch  124 | Train Loss:     0.0097 | Validation Loss:     0.0121\nEpoch  125 | Train Loss:     0.0096 | Validation Loss:     0.0106\nEpoch  126 | Train Loss:     0.0096 | Validation Loss:     0.0096\nEpoch  127 | Train Loss:     0.0099 | Validation Loss:     0.0097\nEpoch  128 | Train Loss:     0.0100 | Validation Loss:     0.0106\nEpoch  129 | Train Loss:     0.0112 | Validation Loss:     0.0095\nEpoch  130 | Train Loss:     0.0098 | Validation Loss:     0.0090\nEpoch  131 | Train Loss:     0.0085 | Validation Loss:     0.0104\nEpoch  132 | Train Loss:     0.0091 | Validation Loss:     0.0086\nEpoch  133 | Train Loss:     0.0086 | Validation Loss:     0.0094\nEpoch  134 | Train Loss:     0.0105 | Validation Loss:     0.0130\nEpoch  135 | Train Loss:     0.0095 | Validation Loss:     0.0084\nEpoch  136 | Train Loss:     0.0086 | Validation Loss:     0.0080\nEpoch  137 | Train Loss:     0.0104 | Validation Loss:     0.0087\nEpoch  138 | Train Loss:     0.0092 | Validation Loss:     0.0112\nEpoch  139 | Train Loss:     0.0091 | Validation Loss:     0.0083\nEpoch  140 | Train Loss:     0.0085 | Validation Loss:     0.0087\nEpoch  141 | Train Loss:     0.0087 | Validation Loss:     0.0096\nEpoch  142 | Train Loss:     0.0094 | Validation Loss:     0.0097\nEpoch  143 | Train Loss:     0.0081 | Validation Loss:     0.0079\nEpoch  144 | Train Loss:     0.0081 | Validation Loss:     0.0110\nEpoch  145 | Train Loss:     0.0078 | Validation Loss:     0.0076\nEpoch  146 | Train Loss:     0.0094 | Validation Loss:     0.0102\nEpoch  147 | Train Loss:     0.0085 | Validation Loss:     0.0082\nEpoch  148 | Train Loss:     0.0074 | Validation Loss:     0.0084\nEpoch  149 | Train Loss:     0.0076 | Validation Loss:     0.0070\nEpoch  150 | Train Loss:     0.0078 | Validation Loss:     0.0071\nEpoch  151 | Train Loss:     0.0081 | Validation Loss:     0.0077\nEpoch  152 | Train Loss:     0.0076 | Validation Loss:     0.0081\nEpoch  153 | Train Loss:     0.0072 | Validation Loss:     0.0084\nEpoch  154 | Train Loss:     0.0069 | Validation Loss:     0.0073\nEpoch  155 | Train Loss:     0.0074 | Validation Loss:     0.0103\nEpoch  156 | Train Loss:     0.0086 | Validation Loss:     0.0077\nEpoch  157 | Train Loss:     0.0075 | Validation Loss:     0.0076\nEpoch  158 | Train Loss:     0.0076 | Validation Loss:     0.0113\nEpoch  159 | Train Loss:     0.0079 | Validation Loss:     0.0070\nEpoch  160 | Train Loss:     0.0074 | Validation Loss:     0.0072\nEpoch  161 | Train Loss:     0.0073 | Validation Loss:     0.0071\nEpoch  162 | Train Loss:     0.0069 | Validation Loss:     0.0081\nEpoch  163 | Train Loss:     0.0077 | Validation Loss:     0.0067\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","Cell \u001b[0;32mIn[364], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m optimizer \u001b[38;5;241m=\u001b[39m optim\u001b[38;5;241m.\u001b[39mAdam(model\u001b[38;5;241m.\u001b[39mparameters(), lr\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.0000001\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m1000\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m----> 2\u001b[0m \u001b[43mevaluate_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcustom_train_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m)\u001b[49m\n","Cell \u001b[0;32mIn[173], line 51\u001b[0m, in \u001b[0;36mevaluate_model\u001b[0;34m(model, custom_train_loader, criterion, optimizer)\u001b[0m\n\u001b[1;32m     49\u001b[0m parameters \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m     50\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m name, param \u001b[38;5;129;01min\u001b[39;00m model\u001b[38;5;241m.\u001b[39mnamed_parameters():\n\u001b[0;32m---> 51\u001b[0m     parameters\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mparam\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     53\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[1;32m     54\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/_tensor.py:934\u001b[0m, in \u001b[0;36mTensor.__format__\u001b[0;34m(self, format_spec)\u001b[0m\n\u001b[1;32m    932\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdim() \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mis_meta \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;129;01mis\u001b[39;00m Tensor:\n\u001b[1;32m    933\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mitem()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__format__\u001b[39m(format_spec)\n\u001b[0;32m--> 934\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mobject\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__format__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mformat_spec\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/parameter.py:63\u001b[0m, in \u001b[0;36mParameter.__repr__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     62\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__repr__\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m---> 63\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mParameter containing:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__repr__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/_tensor.py:431\u001b[0m, in \u001b[0;36mTensor.__repr__\u001b[0;34m(self, tensor_contents)\u001b[0m\n\u001b[1;32m    427\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    428\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__repr__\u001b[39m, (\u001b[38;5;28mself\u001b[39m,), \u001b[38;5;28mself\u001b[39m, tensor_contents\u001b[38;5;241m=\u001b[39mtensor_contents\n\u001b[1;32m    429\u001b[0m     )\n\u001b[1;32m    430\u001b[0m \u001b[38;5;66;03m# All strings are unicode in Python 3.\u001b[39;00m\n\u001b[0;32m--> 431\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_tensor_str\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_str\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtensor_contents\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtensor_contents\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/_tensor_str.py:664\u001b[0m, in \u001b[0;36m_str\u001b[0;34m(self, tensor_contents)\u001b[0m\n\u001b[1;32m    662\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad(), torch\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39m_python_dispatch\u001b[38;5;241m.\u001b[39m_disable_current_modes():\n\u001b[1;32m    663\u001b[0m     guard \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39m_C\u001b[38;5;241m.\u001b[39m_DisableFuncTorch()\n\u001b[0;32m--> 664\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_str_intern\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtensor_contents\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtensor_contents\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/_tensor_str.py:595\u001b[0m, in \u001b[0;36m_str_intern\u001b[0;34m(inp, tensor_contents)\u001b[0m\n\u001b[1;32m    593\u001b[0m                     tensor_str \u001b[38;5;241m=\u001b[39m _tensor_str(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mto_dense(), indent)\n\u001b[1;32m    594\u001b[0m                 \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 595\u001b[0m                     tensor_str \u001b[38;5;241m=\u001b[39m \u001b[43m_tensor_str\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindent\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    597\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayout \u001b[38;5;241m!=\u001b[39m torch\u001b[38;5;241m.\u001b[39mstrided:\n\u001b[1;32m    598\u001b[0m     suffixes\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlayout=\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayout))\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/_tensor_str.py:347\u001b[0m, in \u001b[0;36m_tensor_str\u001b[0;34m(self, indent)\u001b[0m\n\u001b[1;32m    343\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _tensor_str_with_formatter(\n\u001b[1;32m    344\u001b[0m         \u001b[38;5;28mself\u001b[39m, indent, summarize, real_formatter, imag_formatter\n\u001b[1;32m    345\u001b[0m     )\n\u001b[1;32m    346\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 347\u001b[0m     formatter \u001b[38;5;241m=\u001b[39m \u001b[43m_Formatter\u001b[49m\u001b[43m(\u001b[49m\u001b[43mget_summarized_data\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43msummarize\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    348\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _tensor_str_with_formatter(\u001b[38;5;28mself\u001b[39m, indent, summarize, formatter)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/_tensor_str.py:178\u001b[0m, in \u001b[0;36m_Formatter.__init__\u001b[0;34m(self, tensor)\u001b[0m\n\u001b[1;32m    172\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m    173\u001b[0m     nonzero_finite_max \u001b[38;5;241m/\u001b[39m nonzero_finite_min \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1000.0\u001b[39m\n\u001b[1;32m    174\u001b[0m     \u001b[38;5;129;01mor\u001b[39;00m nonzero_finite_max \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1.0e8\u001b[39m\n\u001b[1;32m    175\u001b[0m     \u001b[38;5;129;01mor\u001b[39;00m nonzero_finite_min \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m1.0e-4\u001b[39m\n\u001b[1;32m    176\u001b[0m ):\n\u001b[1;32m    177\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msci_mode \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m--> 178\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m value \u001b[38;5;129;01min\u001b[39;00m nonzero_finite_vals:\n\u001b[1;32m    179\u001b[0m         value_str \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m{{\u001b[39;00m\u001b[38;5;124m:.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mPRINT_OPTS\u001b[38;5;241m.\u001b[39mprecision\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124me\u001b[39m\u001b[38;5;130;01m}}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(value)\n\u001b[1;32m    180\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmax_width \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmax\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmax_width, \u001b[38;5;28mlen\u001b[39m(value_str))\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/_tensor.py:1000\u001b[0m, in \u001b[0;36mTensor.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    991\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_C\u001b[38;5;241m.\u001b[39m_get_tracing_state():\n\u001b[1;32m    992\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[1;32m    993\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIterating over a tensor might cause the trace to be incorrect. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    994\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPassing a tensor of different shape won\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt change the number of \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    998\u001b[0m         stacklevel\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m,\n\u001b[1;32m    999\u001b[0m     )\n\u001b[0;32m-> 1000\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28miter\u001b[39m(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43munbind\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m)\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "],"ename":"KeyboardInterrupt","evalue":"","output_type":"error"}]},{"cell_type":"code","source":"optimizer = optim.Adam(model.parameters(), lr=0.0000001 * 1000 * 1)\nevaluate_model(model, custom_train_loader, criterion, optimizer)","metadata":{"execution":{"iopub.status.busy":"2024-06-22T03:07:20.199183Z","iopub.execute_input":"2024-06-22T03:07:20.199567Z","iopub.status.idle":"2024-06-22T03:10:34.943405Z","shell.execute_reply.started":"2024-06-22T03:07:20.199537Z","shell.execute_reply":"2024-06-22T03:10:34.941771Z"},"trusted":true},"execution_count":362,"outputs":[{"name":"stdout","text":"Epoch    1 | Train Loss:    16.3214 | Validation Loss:    15.7638\nEpoch    2 | Train Loss:    16.2479 | Validation Loss:    15.6506\nEpoch    3 | Train Loss:    16.2096 | Validation Loss:    15.6601\nEpoch    4 | Train Loss:    16.0590 | Validation Loss:    15.4282\nEpoch    5 | Train Loss:    15.7726 | Validation Loss:    15.0790\nEpoch    6 | Train Loss:    15.2782 | Validation Loss:    14.6372\nEpoch    7 | Train Loss:    14.6292 | Validation Loss:    13.7000\nEpoch    8 | Train Loss:    13.6994 | Validation Loss:    12.8070\nEpoch    9 | Train Loss:    12.6405 | Validation Loss:    11.7299\nEpoch   10 | Train Loss:    11.4711 | Validation Loss:    10.5386\nEpoch   11 | Train Loss:    10.1810 | Validation Loss:     9.3904\nEpoch   12 | Train Loss:     8.8768 | Validation Loss:     7.9216\nEpoch   13 | Train Loss:     7.5419 | Validation Loss:     6.7129\nEpoch   14 | Train Loss:     6.2650 | Validation Loss:     5.4831\nEpoch   15 | Train Loss:     5.0917 | Validation Loss:     4.5048\nEpoch   16 | Train Loss:     4.0551 | Validation Loss:     3.4501\nEpoch   17 | Train Loss:     3.1340 | Validation Loss:     2.6523\nEpoch   18 | Train Loss:     2.3844 | Validation Loss:     2.0095\nEpoch   19 | Train Loss:     1.7796 | Validation Loss:     1.4735\nEpoch   20 | Train Loss:     1.3096 | Validation Loss:     1.0822\nEpoch   21 | Train Loss:     0.9639 | Validation Loss:     0.8075\nEpoch   22 | Train Loss:     0.7167 | Validation Loss:     0.6045\nEpoch   23 | Train Loss:     0.5427 | Validation Loss:     0.4692\nEpoch   24 | Train Loss:     0.4281 | Validation Loss:     0.3846\nEpoch   25 | Train Loss:     0.3551 | Validation Loss:     0.3401\nEpoch   26 | Train Loss:     0.3098 | Validation Loss:     0.2893\nEpoch   27 | Train Loss:     0.2776 | Validation Loss:     0.2647\nEpoch   28 | Train Loss:     0.2555 | Validation Loss:     0.2499\nEpoch   29 | Train Loss:     0.2417 | Validation Loss:     0.2356\nEpoch   30 | Train Loss:     0.2295 | Validation Loss:     0.2247\nEpoch   31 | Train Loss:     0.2204 | Validation Loss:     0.2196\nEpoch   32 | Train Loss:     0.2107 | Validation Loss:     0.2092\nEpoch   33 | Train Loss:     0.2031 | Validation Loss:     0.2071\nEpoch   34 | Train Loss:     0.1950 | Validation Loss:     0.1955\nEpoch   35 | Train Loss:     0.1880 | Validation Loss:     0.1882\nEpoch   36 | Train Loss:     0.1830 | Validation Loss:     0.1937\nEpoch   37 | Train Loss:     0.1760 | Validation Loss:     0.1709\nEpoch   38 | Train Loss:     0.1677 | Validation Loss:     0.1666\nEpoch   39 | Train Loss:     0.1617 | Validation Loss:     0.1581\nEpoch   40 | Train Loss:     0.1558 | Validation Loss:     0.1540\nEpoch   41 | Train Loss:     0.1489 | Validation Loss:     0.1468\nEpoch   42 | Train Loss:     0.1434 | Validation Loss:     0.1422\nEpoch   43 | Train Loss:     0.1396 | Validation Loss:     0.1343\nEpoch   44 | Train Loss:     0.1327 | Validation Loss:     0.1311\nEpoch   45 | Train Loss:     0.1288 | Validation Loss:     0.1281\nEpoch   46 | Train Loss:     0.1247 | Validation Loss:     0.1214\nEpoch   47 | Train Loss:     0.1179 | Validation Loss:     0.1149\nEpoch   48 | Train Loss:     0.1133 | Validation Loss:     0.1098\nEpoch   49 | Train Loss:     0.1096 | Validation Loss:     0.1072\nEpoch   50 | Train Loss:     0.1046 | Validation Loss:     0.1133\nEpoch   51 | Train Loss:     0.1035 | Validation Loss:     0.0970\nEpoch   52 | Train Loss:     0.0967 | Validation Loss:     0.0934\nEpoch   53 | Train Loss:     0.0937 | Validation Loss:     0.0898\nEpoch   54 | Train Loss:     0.0892 | Validation Loss:     0.0874\nEpoch   55 | Train Loss:     0.0863 | Validation Loss:     0.0851\nEpoch   56 | Train Loss:     0.0834 | Validation Loss:     0.0805\nEpoch   57 | Train Loss:     0.0804 | Validation Loss:     0.0775\nEpoch   58 | Train Loss:     0.0771 | Validation Loss:     0.0793\nEpoch   59 | Train Loss:     0.0740 | Validation Loss:     0.0741\nEpoch   60 | Train Loss:     0.0711 | Validation Loss:     0.0686\nEpoch   61 | Train Loss:     0.0681 | Validation Loss:     0.0652\nEpoch   62 | Train Loss:     0.0655 | Validation Loss:     0.0631\nEpoch   63 | Train Loss:     0.0661 | Validation Loss:     0.0657\nEpoch   64 | Train Loss:     0.0611 | Validation Loss:     0.0582\nEpoch   65 | Train Loss:     0.0579 | Validation Loss:     0.0583\nEpoch   66 | Train Loss:     0.0568 | Validation Loss:     0.0551\nEpoch   67 | Train Loss:     0.0545 | Validation Loss:     0.0523\nEpoch   68 | Train Loss:     0.0523 | Validation Loss:     0.0504\nEpoch   69 | Train Loss:     0.0502 | Validation Loss:     0.0489\nEpoch   70 | Train Loss:     0.0493 | Validation Loss:     0.0478\nEpoch   71 | Train Loss:     0.0469 | Validation Loss:     0.0454\nEpoch   72 | Train Loss:     0.0455 | Validation Loss:     0.0439\nEpoch   73 | Train Loss:     0.0437 | Validation Loss:     0.0426\nEpoch   74 | Train Loss:     0.0426 | Validation Loss:     0.0411\nEpoch   75 | Train Loss:     0.0409 | Validation Loss:     0.0394\nEpoch   76 | Train Loss:     0.0398 | Validation Loss:     0.0391\nEpoch   77 | Train Loss:     0.0383 | Validation Loss:     0.0404\nEpoch   78 | Train Loss:     0.0379 | Validation Loss:     0.0361\nEpoch   79 | Train Loss:     0.0373 | Validation Loss:     0.0418\nEpoch   80 | Train Loss:     0.0360 | Validation Loss:     0.0341\nEpoch   81 | Train Loss:     0.0355 | Validation Loss:     0.0329\nEpoch   82 | Train Loss:     0.0324 | Validation Loss:     0.0332\nEpoch   83 | Train Loss:     0.0321 | Validation Loss:     0.0309\nEpoch   84 | Train Loss:     0.0315 | Validation Loss:     0.0306\nEpoch   85 | Train Loss:     0.0298 | Validation Loss:     0.0306\nEpoch   86 | Train Loss:     0.0287 | Validation Loss:     0.0285\nEpoch   87 | Train Loss:     0.0284 | Validation Loss:     0.0274\nEpoch   88 | Train Loss:     0.0273 | Validation Loss:     0.0268\nEpoch   89 | Train Loss:     0.0263 | Validation Loss:     0.0264\nEpoch   90 | Train Loss:     0.0258 | Validation Loss:     0.0272\nEpoch   91 | Train Loss:     0.0253 | Validation Loss:     0.0248\nEpoch   92 | Train Loss:     0.0251 | Validation Loss:     0.0249\nEpoch   93 | Train Loss:     0.0240 | Validation Loss:     0.0239\nEpoch   94 | Train Loss:     0.0229 | Validation Loss:     0.0257\nEpoch   95 | Train Loss:     0.0226 | Validation Loss:     0.0231\nEpoch   96 | Train Loss:     0.0221 | Validation Loss:     0.0277\nEpoch   97 | Train Loss:     0.0221 | Validation Loss:     0.0242\nEpoch   98 | Train Loss:     0.0210 | Validation Loss:     0.0248\nEpoch   99 | Train Loss:     0.0195 | Validation Loss:     0.0192\nEpoch  100 | Train Loss:     0.0194 | Validation Loss:     0.0218\nEpoch  101 | Train Loss:     0.0191 | Validation Loss:     0.0183\nEpoch  102 | Train Loss:     0.0178 | Validation Loss:     0.0174\nEpoch  103 | Train Loss:     0.0182 | Validation Loss:     0.0191\nEpoch  104 | Train Loss:     0.0171 | Validation Loss:     0.0198\nEpoch  105 | Train Loss:     0.0163 | Validation Loss:     0.0161\nEpoch  106 | Train Loss:     0.0155 | Validation Loss:     0.0153\nEpoch  107 | Train Loss:     0.0162 | Validation Loss:     0.0149\nEpoch  108 | Train Loss:     0.0152 | Validation Loss:     0.0147\nEpoch  109 | Train Loss:     0.0146 | Validation Loss:     0.0141\nEpoch  110 | Train Loss:     0.0148 | Validation Loss:     0.0171\nEpoch  111 | Train Loss:     0.0134 | Validation Loss:     0.0138\nEpoch  112 | Train Loss:     0.0129 | Validation Loss:     0.0127\nEpoch  113 | Train Loss:     0.0127 | Validation Loss:     0.0127\nEpoch  114 | Train Loss:     0.0125 | Validation Loss:     0.0119\nEpoch  115 | Train Loss:     0.0119 | Validation Loss:     0.0115\nEpoch  116 | Train Loss:     0.0121 | Validation Loss:     0.0110\nEpoch  117 | Train Loss:     0.0110 | Validation Loss:     0.0107\nEpoch  118 | Train Loss:     0.0107 | Validation Loss:     0.0102\nEpoch  119 | Train Loss:     0.0106 | Validation Loss:     0.0098\nEpoch  120 | Train Loss:     0.0109 | Validation Loss:     0.0104\nEpoch  121 | Train Loss:     0.0113 | Validation Loss:     0.0103\nEpoch  122 | Train Loss:     0.0094 | Validation Loss:     0.0091\nEpoch  123 | Train Loss:     0.0093 | Validation Loss:     0.0086\nEpoch  124 | Train Loss:     0.0090 | Validation Loss:     0.0084\nEpoch  125 | Train Loss:     0.0089 | Validation Loss:     0.0083\nEpoch  126 | Train Loss:     0.0084 | Validation Loss:     0.0077\nEpoch  127 | Train Loss:     0.0083 | Validation Loss:     0.0076\nEpoch  128 | Train Loss:     0.0080 | Validation Loss:     0.0108\nEpoch  129 | Train Loss:     0.0084 | Validation Loss:     0.0071\nEpoch  130 | Train Loss:     0.0074 | Validation Loss:     0.0095\nEpoch  131 | Train Loss:     0.0075 | Validation Loss:     0.0065\nEpoch  132 | Train Loss:     0.0069 | Validation Loss:     0.0064\nEpoch  133 | Train Loss:     0.0065 | Validation Loss:     0.0063\nEpoch  134 | Train Loss:     0.0064 | Validation Loss:     0.0065\nEpoch  135 | Train Loss:     0.0063 | Validation Loss:     0.0059\nEpoch  136 | Train Loss:     0.0063 | Validation Loss:     0.0092\nEpoch  137 | Train Loss:     0.0062 | Validation Loss:     0.0054\nEpoch  138 | Train Loss:     0.0057 | Validation Loss:     0.0073\nEpoch  139 | Train Loss:     0.0059 | Validation Loss:     0.0053\nEpoch  140 | Train Loss:     0.0057 | Validation Loss:     0.0051\nEpoch  141 | Train Loss:     0.0052 | Validation Loss:     0.0050\nEpoch  142 | Train Loss:     0.0050 | Validation Loss:     0.0047\nEpoch  143 | Train Loss:     0.0057 | Validation Loss:     0.0062\nEpoch  144 | Train Loss:     0.0053 | Validation Loss:     0.0053\nEpoch  145 | Train Loss:     0.0049 | Validation Loss:     0.0043\nEpoch  146 | Train Loss:     0.0046 | Validation Loss:     0.0041\nEpoch  147 | Train Loss:     0.0047 | Validation Loss:     0.0040\nEpoch  148 | Train Loss:     0.0044 | Validation Loss:     0.0039\nEpoch  149 | Train Loss:     0.0044 | Validation Loss:     0.0039\nEpoch  150 | Train Loss:     0.0044 | Validation Loss:     0.0039\nEpoch  151 | Train Loss:     0.0042 | Validation Loss:     0.0037\nEpoch  152 | Train Loss:     0.0039 | Validation Loss:     0.0039\nEpoch  153 | Train Loss:     0.0045 | Validation Loss:     0.0043\nEpoch  154 | Train Loss:     0.0041 | Validation Loss:     0.0042\nEpoch  155 | Train Loss:     0.0036 | Validation Loss:     0.0054\nEpoch  156 | Train Loss:     0.0037 | Validation Loss:     0.0043\nEpoch  157 | Train Loss:     0.0036 | Validation Loss:     0.0034\nEpoch  158 | Train Loss:     0.0035 | Validation Loss:     0.0030\nEpoch  159 | Train Loss:     0.0039 | Validation Loss:     0.0055\nEpoch  160 | Train Loss:     0.0033 | Validation Loss:     0.0030\nEpoch  161 | Train Loss:     0.0034 | Validation Loss:     0.0035\nEpoch  162 | Train Loss:     0.0031 | Validation Loss:     0.0027\nEpoch  163 | Train Loss:     0.0034 | Validation Loss:     0.0034\nEpoch  164 | Train Loss:     0.0031 | Validation Loss:     0.0027\nEpoch  165 | Train Loss:     0.0030 | Validation Loss:     0.0026\nEpoch  166 | Train Loss:     0.0030 | Validation Loss:     0.0027\nEpoch  167 | Train Loss:     0.0028 | Validation Loss:     0.0028\nEpoch  168 | Train Loss:     0.0028 | Validation Loss:     0.0029\nEpoch  169 | Train Loss:     0.0027 | Validation Loss:     0.0036\nEpoch  170 | Train Loss:     0.0028 | Validation Loss:     0.0023\nEpoch  171 | Train Loss:     0.0026 | Validation Loss:     0.0023\nEpoch  172 | Train Loss:     0.0026 | Validation Loss:     0.0021\nEpoch  173 | Train Loss:     0.0029 | Validation Loss:     0.0026\nEpoch  174 | Train Loss:     0.0028 | Validation Loss:     0.0022\nEpoch  175 | Train Loss:     0.0023 | Validation Loss:     0.0023\nEpoch  176 | Train Loss:     0.0025 | Validation Loss:     0.0021\nEpoch  177 | Train Loss:     0.0022 | Validation Loss:     0.0021\nEpoch  178 | Train Loss:     0.0024 | Validation Loss:     0.0020\nEpoch  179 | Train Loss:     0.0022 | Validation Loss:     0.0019\nEpoch  180 | Train Loss:     0.0025 | Validation Loss:     0.0022\nEpoch  181 | Train Loss:     0.0024 | Validation Loss:     0.0022\nEpoch  182 | Train Loss:     0.0023 | Validation Loss:     0.0019\nEpoch  183 | Train Loss:     0.0021 | Validation Loss:     0.0018\nEpoch  184 | Train Loss:     0.0021 | Validation Loss:     0.0018\nEpoch  185 | Train Loss:     0.0020 | Validation Loss:     0.0017\nEpoch  186 | Train Loss:     0.0020 | Validation Loss:     0.0017\nEpoch  187 | Train Loss:     0.0018 | Validation Loss:     0.0016\nEpoch  188 | Train Loss:     0.0018 | Validation Loss:     0.0016\nEpoch  189 | Train Loss:     0.0020 | Validation Loss:     0.0016\nEpoch  190 | Train Loss:     0.0021 | Validation Loss:     0.0016\nEpoch  191 | Train Loss:     0.0017 | Validation Loss:     0.0017\nEpoch  192 | Train Loss:     0.0024 | Validation Loss:     0.0016\nEpoch  193 | Train Loss:     0.0016 | Validation Loss:     0.0016\nEpoch  194 | Train Loss:     0.0016 | Validation Loss:     0.0014\nEpoch  195 | Train Loss:     0.0016 | Validation Loss:     0.0015\nEpoch  196 | Train Loss:     0.0018 | Validation Loss:     0.0014\nEpoch  197 | Train Loss:     0.0017 | Validation Loss:     0.0017\nEpoch  198 | Train Loss:     0.0016 | Validation Loss:     0.0019\nEpoch  199 | Train Loss:     0.0016 | Validation Loss:     0.0013\nEpoch  200 | Train Loss:     0.0016 | Validation Loss:     0.0020\nEpoch  201 | Train Loss:     0.0015 | Validation Loss:     0.0014\nEpoch  202 | Train Loss:     0.0015 | Validation Loss:     0.0013\nEpoch  203 | Train Loss:     0.0015 | Validation Loss:     0.0012\nEpoch  204 | Train Loss:     0.0015 | Validation Loss:     0.0018\nEpoch  205 | Train Loss:     0.0014 | Validation Loss:     0.0012\nEpoch  206 | Train Loss:     0.0014 | Validation Loss:     0.0013\nEpoch  207 | Train Loss:     0.0014 | Validation Loss:     0.0011\nEpoch  208 | Train Loss:     0.0015 | Validation Loss:     0.0012\nEpoch  209 | Train Loss:     0.0013 | Validation Loss:     0.0013\nEpoch  210 | Train Loss:     0.0015 | Validation Loss:     0.0014\nEpoch  211 | Train Loss:     0.0014 | Validation Loss:     0.0020\nEpoch  212 | Train Loss:     0.0014 | Validation Loss:     0.0010\nEpoch  213 | Train Loss:     0.0012 | Validation Loss:     0.0011\nEpoch  214 | Train Loss:     0.0013 | Validation Loss:     0.0013\nEpoch  215 | Train Loss:     0.0012 | Validation Loss:     0.0014\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","Cell \u001b[0;32mIn[362], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m optimizer \u001b[38;5;241m=\u001b[39m optim\u001b[38;5;241m.\u001b[39mAdam(model\u001b[38;5;241m.\u001b[39mparameters(), lr\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.0000001\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m1000\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m----> 2\u001b[0m \u001b[43mevaluate_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcustom_train_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m)\u001b[49m\n","Cell \u001b[0;32mIn[173], line 51\u001b[0m, in \u001b[0;36mevaluate_model\u001b[0;34m(model, custom_train_loader, criterion, optimizer)\u001b[0m\n\u001b[1;32m     49\u001b[0m parameters \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m     50\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m name, param \u001b[38;5;129;01min\u001b[39;00m model\u001b[38;5;241m.\u001b[39mnamed_parameters():\n\u001b[0;32m---> 51\u001b[0m     parameters\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mparam\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     53\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[1;32m     54\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/_tensor.py:934\u001b[0m, in \u001b[0;36mTensor.__format__\u001b[0;34m(self, format_spec)\u001b[0m\n\u001b[1;32m    932\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdim() \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mis_meta \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;129;01mis\u001b[39;00m Tensor:\n\u001b[1;32m    933\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mitem()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__format__\u001b[39m(format_spec)\n\u001b[0;32m--> 934\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mobject\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__format__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mformat_spec\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/parameter.py:63\u001b[0m, in \u001b[0;36mParameter.__repr__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     62\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__repr__\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m---> 63\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mParameter containing:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__repr__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/_tensor.py:431\u001b[0m, in \u001b[0;36mTensor.__repr__\u001b[0;34m(self, tensor_contents)\u001b[0m\n\u001b[1;32m    427\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    428\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__repr__\u001b[39m, (\u001b[38;5;28mself\u001b[39m,), \u001b[38;5;28mself\u001b[39m, tensor_contents\u001b[38;5;241m=\u001b[39mtensor_contents\n\u001b[1;32m    429\u001b[0m     )\n\u001b[1;32m    430\u001b[0m \u001b[38;5;66;03m# All strings are unicode in Python 3.\u001b[39;00m\n\u001b[0;32m--> 431\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_tensor_str\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_str\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtensor_contents\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtensor_contents\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/_tensor_str.py:664\u001b[0m, in \u001b[0;36m_str\u001b[0;34m(self, tensor_contents)\u001b[0m\n\u001b[1;32m    662\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad(), torch\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39m_python_dispatch\u001b[38;5;241m.\u001b[39m_disable_current_modes():\n\u001b[1;32m    663\u001b[0m     guard \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39m_C\u001b[38;5;241m.\u001b[39m_DisableFuncTorch()\n\u001b[0;32m--> 664\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_str_intern\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtensor_contents\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtensor_contents\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/_tensor_str.py:595\u001b[0m, in \u001b[0;36m_str_intern\u001b[0;34m(inp, tensor_contents)\u001b[0m\n\u001b[1;32m    593\u001b[0m                     tensor_str \u001b[38;5;241m=\u001b[39m _tensor_str(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mto_dense(), indent)\n\u001b[1;32m    594\u001b[0m                 \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 595\u001b[0m                     tensor_str \u001b[38;5;241m=\u001b[39m \u001b[43m_tensor_str\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindent\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    597\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayout \u001b[38;5;241m!=\u001b[39m torch\u001b[38;5;241m.\u001b[39mstrided:\n\u001b[1;32m    598\u001b[0m     suffixes\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlayout=\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayout))\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/_tensor_str.py:347\u001b[0m, in \u001b[0;36m_tensor_str\u001b[0;34m(self, indent)\u001b[0m\n\u001b[1;32m    343\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _tensor_str_with_formatter(\n\u001b[1;32m    344\u001b[0m         \u001b[38;5;28mself\u001b[39m, indent, summarize, real_formatter, imag_formatter\n\u001b[1;32m    345\u001b[0m     )\n\u001b[1;32m    346\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 347\u001b[0m     formatter \u001b[38;5;241m=\u001b[39m \u001b[43m_Formatter\u001b[49m\u001b[43m(\u001b[49m\u001b[43mget_summarized_data\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43msummarize\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    348\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _tensor_str_with_formatter(\u001b[38;5;28mself\u001b[39m, indent, summarize, formatter)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/_tensor_str.py:128\u001b[0m, in \u001b[0;36m_Formatter.__init__\u001b[0;34m(self, tensor)\u001b[0m\n\u001b[1;32m    125\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msci_mode \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m    126\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmax_width \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m--> 128\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mno_grad\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[1;32m    129\u001b[0m     tensor_view \u001b[38;5;241m=\u001b[39m tensor\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m    131\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfloating_dtype:\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/autograd/grad_mode.py:75\u001b[0m, in \u001b[0;36mno_grad.__init__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     73\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     74\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_jit_internal\u001b[38;5;241m.\u001b[39mis_scripting():\n\u001b[0;32m---> 75\u001b[0m         \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     76\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprev \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "],"ename":"KeyboardInterrupt","evalue":"","output_type":"error"}]},{"cell_type":"code","source":"optimizer = optim.Adam(model.parameters(), lr=0.0000001 * 1000 * 100)\nevaluate_model(model, custom_train_loader, criterion, optimizer)","metadata":{"execution":{"iopub.status.busy":"2024-06-22T02:46:48.890086Z","iopub.execute_input":"2024-06-22T02:46:48.890465Z","iopub.status.idle":"2024-06-22T02:52:09.272925Z","shell.execute_reply.started":"2024-06-22T02:46:48.890432Z","shell.execute_reply":"2024-06-22T02:52:09.271062Z"},"trusted":true},"execution_count":316,"outputs":[{"name":"stdout","text":"Epoch    1 | Train Loss:    13.2357 | Validation Loss:     7.5224\nEpoch    2 | Train Loss:     4.8987 | Validation Loss:     2.9569\nEpoch    3 | Train Loss:     2.9797 | Validation Loss:     2.3361\nEpoch    4 | Train Loss:     2.4879 | Validation Loss:     1.9432\nEpoch    5 | Train Loss:     2.0585 | Validation Loss:     1.5535\nEpoch    6 | Train Loss:     1.6818 | Validation Loss:     1.2518\nEpoch    7 | Train Loss:     1.4028 | Validation Loss:     1.0520\nEpoch    8 | Train Loss:     1.2043 | Validation Loss:     0.9144\nEpoch    9 | Train Loss:     1.0586 | Validation Loss:     0.8148\nEpoch   10 | Train Loss:     0.9475 | Validation Loss:     0.7359\nEpoch   11 | Train Loss:     0.8605 | Validation Loss:     0.6722\nEpoch   12 | Train Loss:     0.7930 | Validation Loss:     0.6380\nEpoch   13 | Train Loss:     0.7366 | Validation Loss:     0.6010\nEpoch   14 | Train Loss:     0.6900 | Validation Loss:     0.5699\nEpoch   15 | Train Loss:     0.6568 | Validation Loss:     0.5475\nEpoch   16 | Train Loss:     0.6262 | Validation Loss:     0.5387\nEpoch   17 | Train Loss:     0.6004 | Validation Loss:     0.5199\nEpoch   18 | Train Loss:     0.5787 | Validation Loss:     0.5043\nEpoch   19 | Train Loss:     0.5581 | Validation Loss:     0.4928\nEpoch   20 | Train Loss:     0.5403 | Validation Loss:     0.4908\nEpoch   21 | Train Loss:     0.5226 | Validation Loss:     0.4685\nEpoch   22 | Train Loss:     0.5114 | Validation Loss:     0.4696\nEpoch   23 | Train Loss:     0.4986 | Validation Loss:     0.4495\nEpoch   24 | Train Loss:     0.4873 | Validation Loss:     0.4437\nEpoch   25 | Train Loss:     0.4813 | Validation Loss:     0.4437\nEpoch   26 | Train Loss:     0.4769 | Validation Loss:     0.4336\nEpoch   27 | Train Loss:     0.4679 | Validation Loss:     0.4287\nEpoch   28 | Train Loss:     0.4620 | Validation Loss:     0.4442\nEpoch   29 | Train Loss:     0.4556 | Validation Loss:     0.4214\nEpoch   30 | Train Loss:     0.4548 | Validation Loss:     0.4304\nEpoch   31 | Train Loss:     0.4540 | Validation Loss:     0.4193\nEpoch   32 | Train Loss:     0.4440 | Validation Loss:     0.4208\nEpoch   33 | Train Loss:     0.4422 | Validation Loss:     0.4106\nEpoch   34 | Train Loss:     0.4393 | Validation Loss:     0.4243\nEpoch   35 | Train Loss:     0.4370 | Validation Loss:     0.4161\nEpoch   36 | Train Loss:     0.4338 | Validation Loss:     0.4272\nEpoch   37 | Train Loss:     0.4356 | Validation Loss:     0.4174\nEpoch   38 | Train Loss:     0.4349 | Validation Loss:     0.4147\nEpoch   39 | Train Loss:     0.4282 | Validation Loss:     0.4205\nEpoch   40 | Train Loss:     0.4291 | Validation Loss:     0.4088\nEpoch   41 | Train Loss:     0.4315 | Validation Loss:     0.4208\nEpoch   42 | Train Loss:     0.4293 | Validation Loss:     0.4176\nEpoch   43 | Train Loss:     0.4262 | Validation Loss:     0.4146\nEpoch   44 | Train Loss:     0.4294 | Validation Loss:     0.4205\nEpoch   45 | Train Loss:     0.4302 | Validation Loss:     0.4078\nEpoch   46 | Train Loss:     0.4260 | Validation Loss:     0.4120\nEpoch   47 | Train Loss:     0.4264 | Validation Loss:     0.4083\nEpoch   48 | Train Loss:     0.4291 | Validation Loss:     0.4349\nEpoch   49 | Train Loss:     0.4246 | Validation Loss:     0.4237\nEpoch   50 | Train Loss:     0.4349 | Validation Loss:     0.4113\nEpoch   51 | Train Loss:     0.4330 | Validation Loss:     0.4306\nEpoch   52 | Train Loss:     0.4252 | Validation Loss:     0.4250\nEpoch   53 | Train Loss:     0.4244 | Validation Loss:     0.4183\nEpoch   54 | Train Loss:     0.4225 | Validation Loss:     0.4067\nEpoch   55 | Train Loss:     0.4232 | Validation Loss:     0.4049\nEpoch   56 | Train Loss:     0.4251 | Validation Loss:     0.4033\nEpoch   57 | Train Loss:     0.4239 | Validation Loss:     0.4147\nEpoch   58 | Train Loss:     0.4256 | Validation Loss:     0.4103\nEpoch   59 | Train Loss:     0.4275 | Validation Loss:     0.4113\nEpoch   60 | Train Loss:     0.4241 | Validation Loss:     0.4150\nEpoch   61 | Train Loss:     0.4292 | Validation Loss:     0.4043\nEpoch   62 | Train Loss:     0.4232 | Validation Loss:     0.4148\nEpoch   63 | Train Loss:     0.4232 | Validation Loss:     0.4072\nEpoch   64 | Train Loss:     0.4254 | Validation Loss:     0.4257\nEpoch   65 | Train Loss:     0.4320 | Validation Loss:     0.4080\nEpoch   66 | Train Loss:     0.4265 | Validation Loss:     0.4135\nEpoch   67 | Train Loss:     0.4373 | Validation Loss:     0.4156\nEpoch   68 | Train Loss:     0.4241 | Validation Loss:     0.4315\nEpoch   69 | Train Loss:     0.4330 | Validation Loss:     0.4113\nEpoch   70 | Train Loss:     0.4309 | Validation Loss:     0.4070\nEpoch   71 | Train Loss:     0.4251 | Validation Loss:     0.4041\nEpoch   72 | Train Loss:     0.4267 | Validation Loss:     0.4145\nEpoch   73 | Train Loss:     0.4337 | Validation Loss:     0.4149\nEpoch   74 | Train Loss:     0.4222 | Validation Loss:     0.4106\nEpoch   75 | Train Loss:     0.4258 | Validation Loss:     0.4091\nEpoch   76 | Train Loss:     0.4267 | Validation Loss:     0.4116\nEpoch   77 | Train Loss:     0.4256 | Validation Loss:     0.4243\nEpoch   78 | Train Loss:     0.4295 | Validation Loss:     0.4047\nEpoch   79 | Train Loss:     0.4267 | Validation Loss:     0.4076\nEpoch   80 | Train Loss:     0.4258 | Validation Loss:     0.4217\nEpoch   81 | Train Loss:     0.4253 | Validation Loss:     0.4044\nEpoch   82 | Train Loss:     0.4255 | Validation Loss:     0.4103\nEpoch   83 | Train Loss:     0.4282 | Validation Loss:     0.4056\nEpoch   84 | Train Loss:     0.4266 | Validation Loss:     0.4171\nEpoch   85 | Train Loss:     0.4238 | Validation Loss:     0.4163\nEpoch   86 | Train Loss:     0.4256 | Validation Loss:     0.4039\nEpoch   87 | Train Loss:     0.4237 | Validation Loss:     0.4050\nEpoch   88 | Train Loss:     0.4232 | Validation Loss:     0.4097\nEpoch   89 | Train Loss:     0.4236 | Validation Loss:     0.4503\nEpoch   90 | Train Loss:     0.4272 | Validation Loss:     0.4107\nEpoch   91 | Train Loss:     0.4308 | Validation Loss:     0.4121\nEpoch   92 | Train Loss:     0.4247 | Validation Loss:     0.4142\nEpoch   93 | Train Loss:     0.4216 | Validation Loss:     0.4248\nEpoch   94 | Train Loss:     0.4258 | Validation Loss:     0.4070\nEpoch   95 | Train Loss:     0.4251 | Validation Loss:     0.4082\nEpoch   96 | Train Loss:     0.4225 | Validation Loss:     0.4082\nEpoch   97 | Train Loss:     0.4236 | Validation Loss:     0.4123\nEpoch   98 | Train Loss:     0.4247 | Validation Loss:     0.4035\nEpoch   99 | Train Loss:     0.4246 | Validation Loss:     0.4201\nEpoch  100 | Train Loss:     0.4250 | Validation Loss:     0.4326\nEpoch  101 | Train Loss:     0.4300 | Validation Loss:     0.4174\nEpoch  102 | Train Loss:     0.4265 | Validation Loss:     0.4126\nEpoch  103 | Train Loss:     0.4251 | Validation Loss:     0.4065\nEpoch  104 | Train Loss:     0.4249 | Validation Loss:     0.4079\nEpoch  105 | Train Loss:     0.4264 | Validation Loss:     0.4056\nEpoch  106 | Train Loss:     0.4247 | Validation Loss:     0.4338\nEpoch  107 | Train Loss:     0.4265 | Validation Loss:     0.4206\nEpoch  108 | Train Loss:     0.4253 | Validation Loss:     0.4137\nEpoch  109 | Train Loss:     0.4263 | Validation Loss:     0.4062\nEpoch  110 | Train Loss:     0.4257 | Validation Loss:     0.4106\nEpoch  111 | Train Loss:     0.4312 | Validation Loss:     0.4082\nEpoch  112 | Train Loss:     0.4236 | Validation Loss:     0.4434\nEpoch  113 | Train Loss:     0.4236 | Validation Loss:     0.4202\nEpoch  114 | Train Loss:     0.4256 | Validation Loss:     0.4176\nEpoch  115 | Train Loss:     0.4237 | Validation Loss:     0.4176\nEpoch  116 | Train Loss:     0.4242 | Validation Loss:     0.4025\nEpoch  117 | Train Loss:     0.4264 | Validation Loss:     0.4226\nEpoch  118 | Train Loss:     0.4245 | Validation Loss:     0.4144\nEpoch  119 | Train Loss:     0.4226 | Validation Loss:     0.4147\nEpoch  120 | Train Loss:     0.4212 | Validation Loss:     0.4095\nEpoch  121 | Train Loss:     0.4261 | Validation Loss:     0.4157\nEpoch  122 | Train Loss:     0.4236 | Validation Loss:     0.4144\nEpoch  123 | Train Loss:     0.4224 | Validation Loss:     0.4285\nEpoch  124 | Train Loss:     0.4239 | Validation Loss:     0.4061\nEpoch  125 | Train Loss:     0.4227 | Validation Loss:     0.4132\nEpoch  126 | Train Loss:     0.4222 | Validation Loss:     0.4096\nEpoch  127 | Train Loss:     0.4245 | Validation Loss:     0.4061\nEpoch  128 | Train Loss:     0.4280 | Validation Loss:     0.4241\nEpoch  129 | Train Loss:     0.4341 | Validation Loss:     0.4116\nEpoch  130 | Train Loss:     0.4268 | Validation Loss:     0.4244\nEpoch  131 | Train Loss:     0.4259 | Validation Loss:     0.4405\nEpoch  132 | Train Loss:     0.4293 | Validation Loss:     0.4429\nEpoch  133 | Train Loss:     0.4240 | Validation Loss:     0.4055\nEpoch  134 | Train Loss:     0.4249 | Validation Loss:     0.4128\nEpoch  135 | Train Loss:     0.4277 | Validation Loss:     0.4171\nEpoch  136 | Train Loss:     0.4248 | Validation Loss:     0.4090\nEpoch  137 | Train Loss:     0.4243 | Validation Loss:     0.4114\nEpoch  138 | Train Loss:     0.4220 | Validation Loss:     0.4092\nEpoch  139 | Train Loss:     0.4248 | Validation Loss:     0.4110\nEpoch  140 | Train Loss:     0.4221 | Validation Loss:     0.4097\nEpoch  141 | Train Loss:     0.4276 | Validation Loss:     0.4098\nEpoch  142 | Train Loss:     0.4353 | Validation Loss:     0.4176\nEpoch  143 | Train Loss:     0.4253 | Validation Loss:     0.4142\nEpoch  144 | Train Loss:     0.4223 | Validation Loss:     0.4089\nEpoch  145 | Train Loss:     0.4268 | Validation Loss:     0.4122\nEpoch  146 | Train Loss:     0.4255 | Validation Loss:     0.4200\nEpoch  147 | Train Loss:     0.4266 | Validation Loss:     0.4066\nEpoch  148 | Train Loss:     0.4219 | Validation Loss:     0.4086\nEpoch  149 | Train Loss:     0.4250 | Validation Loss:     0.4206\nEpoch  150 | Train Loss:     0.4240 | Validation Loss:     0.4116\nEpoch  151 | Train Loss:     0.4264 | Validation Loss:     0.4079\nEpoch  152 | Train Loss:     0.4229 | Validation Loss:     0.4031\nEpoch  153 | Train Loss:     0.4294 | Validation Loss:     0.4430\nEpoch  154 | Train Loss:     0.4261 | Validation Loss:     0.4158\nEpoch  155 | Train Loss:     0.4283 | Validation Loss:     0.4100\nEpoch  156 | Train Loss:     0.4249 | Validation Loss:     0.4126\nEpoch  157 | Train Loss:     0.4216 | Validation Loss:     0.4126\nEpoch  158 | Train Loss:     0.4262 | Validation Loss:     0.4362\nEpoch  159 | Train Loss:     0.4319 | Validation Loss:     0.4125\nEpoch  160 | Train Loss:     0.4253 | Validation Loss:     0.4043\nEpoch  161 | Train Loss:     0.4227 | Validation Loss:     0.4092\nEpoch  162 | Train Loss:     0.4243 | Validation Loss:     0.4120\nEpoch  163 | Train Loss:     0.4220 | Validation Loss:     0.4220\nEpoch  164 | Train Loss:     0.4230 | Validation Loss:     0.4101\nEpoch  165 | Train Loss:     0.4283 | Validation Loss:     0.4162\nEpoch  166 | Train Loss:     0.4235 | Validation Loss:     0.4090\nEpoch  167 | Train Loss:     0.4227 | Validation Loss:     0.4365\nEpoch  168 | Train Loss:     0.4226 | Validation Loss:     0.4062\nEpoch  169 | Train Loss:     0.4232 | Validation Loss:     0.4168\nEpoch  170 | Train Loss:     0.4271 | Validation Loss:     0.4390\nEpoch  171 | Train Loss:     0.4260 | Validation Loss:     0.4065\nEpoch  172 | Train Loss:     0.4221 | Validation Loss:     0.4035\nEpoch  173 | Train Loss:     0.4225 | Validation Loss:     0.4254\nEpoch  174 | Train Loss:     0.4235 | Validation Loss:     0.4082\nEpoch  175 | Train Loss:     0.4229 | Validation Loss:     0.4174\nEpoch  176 | Train Loss:     0.4250 | Validation Loss:     0.4106\nEpoch  177 | Train Loss:     0.4224 | Validation Loss:     0.4087\nEpoch  178 | Train Loss:     0.4252 | Validation Loss:     0.4205\nEpoch  179 | Train Loss:     0.4248 | Validation Loss:     0.4042\nEpoch  180 | Train Loss:     0.4245 | Validation Loss:     0.4062\nEpoch  181 | Train Loss:     0.4242 | Validation Loss:     0.4135\nEpoch  182 | Train Loss:     0.4241 | Validation Loss:     0.4089\nEpoch  183 | Train Loss:     0.4257 | Validation Loss:     0.4086\nEpoch  184 | Train Loss:     0.4236 | Validation Loss:     0.4177\nEpoch  185 | Train Loss:     0.4234 | Validation Loss:     0.4236\nEpoch  186 | Train Loss:     0.4277 | Validation Loss:     0.4183\nEpoch  187 | Train Loss:     0.4283 | Validation Loss:     0.4092\nEpoch  188 | Train Loss:     0.4236 | Validation Loss:     0.4086\nEpoch  189 | Train Loss:     0.4227 | Validation Loss:     0.4088\nEpoch  190 | Train Loss:     0.4300 | Validation Loss:     0.4225\nEpoch  191 | Train Loss:     0.4246 | Validation Loss:     0.4045\nEpoch  192 | Train Loss:     0.4233 | Validation Loss:     0.4093\nEpoch  193 | Train Loss:     0.4265 | Validation Loss:     0.4151\nEpoch  194 | Train Loss:     0.4245 | Validation Loss:     0.4071\nEpoch  195 | Train Loss:     0.4227 | Validation Loss:     0.4089\nEpoch  196 | Train Loss:     0.4230 | Validation Loss:     0.4051\nEpoch  197 | Train Loss:     0.4241 | Validation Loss:     0.4179\nEpoch  198 | Train Loss:     0.4252 | Validation Loss:     0.4227\nEpoch  199 | Train Loss:     0.4238 | Validation Loss:     0.4042\nEpoch  200 | Train Loss:     0.4227 | Validation Loss:     0.4163\nEpoch  201 | Train Loss:     0.4281 | Validation Loss:     0.4167\nEpoch  202 | Train Loss:     0.4245 | Validation Loss:     0.4110\nEpoch  203 | Train Loss:     0.4267 | Validation Loss:     0.4348\nEpoch  204 | Train Loss:     0.4271 | Validation Loss:     0.4117\nEpoch  205 | Train Loss:     0.4287 | Validation Loss:     0.4127\nEpoch  206 | Train Loss:     0.4256 | Validation Loss:     0.4230\nEpoch  207 | Train Loss:     0.4246 | Validation Loss:     0.4110\nEpoch  208 | Train Loss:     0.4245 | Validation Loss:     0.4148\nEpoch  209 | Train Loss:     0.4280 | Validation Loss:     0.4127\nEpoch  210 | Train Loss:     0.4267 | Validation Loss:     0.4227\nEpoch  211 | Train Loss:     0.4248 | Validation Loss:     0.4107\nEpoch  212 | Train Loss:     0.4222 | Validation Loss:     0.4220\nEpoch  213 | Train Loss:     0.4219 | Validation Loss:     0.4073\nEpoch  214 | Train Loss:     0.4221 | Validation Loss:     0.4138\nEpoch  215 | Train Loss:     0.4234 | Validation Loss:     0.4377\nEpoch  216 | Train Loss:     0.4315 | Validation Loss:     0.4087\nEpoch  217 | Train Loss:     0.4281 | Validation Loss:     0.4217\nEpoch  218 | Train Loss:     0.4237 | Validation Loss:     0.4149\nEpoch  219 | Train Loss:     0.4260 | Validation Loss:     0.4186\nEpoch  220 | Train Loss:     0.4226 | Validation Loss:     0.4194\nEpoch  221 | Train Loss:     0.4239 | Validation Loss:     0.4064\nEpoch  222 | Train Loss:     0.4212 | Validation Loss:     0.4097\nEpoch  223 | Train Loss:     0.4280 | Validation Loss:     0.4160\nEpoch  224 | Train Loss:     0.4296 | Validation Loss:     0.4216\nEpoch  225 | Train Loss:     0.4220 | Validation Loss:     0.4127\nEpoch  226 | Train Loss:     0.4280 | Validation Loss:     0.4135\nEpoch  227 | Train Loss:     0.4261 | Validation Loss:     0.4107\nEpoch  228 | Train Loss:     0.4300 | Validation Loss:     0.4240\nEpoch  229 | Train Loss:     0.4239 | Validation Loss:     0.4149\nEpoch  230 | Train Loss:     0.4257 | Validation Loss:     0.4114\nEpoch  231 | Train Loss:     0.4227 | Validation Loss:     0.4188\nEpoch  232 | Train Loss:     0.4297 | Validation Loss:     0.4327\nEpoch  233 | Train Loss:     0.4306 | Validation Loss:     0.4039\nEpoch  234 | Train Loss:     0.4242 | Validation Loss:     0.4061\nEpoch  235 | Train Loss:     0.4227 | Validation Loss:     0.4156\nEpoch  236 | Train Loss:     0.4304 | Validation Loss:     0.4293\nEpoch  237 | Train Loss:     0.4236 | Validation Loss:     0.4061\nEpoch  238 | Train Loss:     0.4226 | Validation Loss:     0.4114\nEpoch  239 | Train Loss:     0.4220 | Validation Loss:     0.4084\nEpoch  240 | Train Loss:     0.4244 | Validation Loss:     0.4070\nEpoch  241 | Train Loss:     0.4216 | Validation Loss:     0.4100\nEpoch  242 | Train Loss:     0.4233 | Validation Loss:     0.4150\nEpoch  243 | Train Loss:     0.4235 | Validation Loss:     0.4186\nEpoch  244 | Train Loss:     0.4271 | Validation Loss:     0.4060\nEpoch  245 | Train Loss:     0.4244 | Validation Loss:     0.4072\nEpoch  246 | Train Loss:     0.4241 | Validation Loss:     0.4068\nEpoch  247 | Train Loss:     0.4239 | Validation Loss:     0.4119\nEpoch  248 | Train Loss:     0.4218 | Validation Loss:     0.4094\nEpoch  249 | Train Loss:     0.4244 | Validation Loss:     0.4094\nEpoch  250 | Train Loss:     0.4250 | Validation Loss:     0.4183\nEpoch  251 | Train Loss:     0.4295 | Validation Loss:     0.4066\nEpoch  252 | Train Loss:     0.4270 | Validation Loss:     0.4114\nEpoch  253 | Train Loss:     0.4224 | Validation Loss:     0.4175\nEpoch  254 | Train Loss:     0.4271 | Validation Loss:     0.4304\nEpoch  255 | Train Loss:     0.4280 | Validation Loss:     0.4343\nEpoch  256 | Train Loss:     0.4289 | Validation Loss:     0.4114\nEpoch  257 | Train Loss:     0.4251 | Validation Loss:     0.4211\nEpoch  258 | Train Loss:     0.4254 | Validation Loss:     0.4102\nEpoch  259 | Train Loss:     0.4219 | Validation Loss:     0.4089\nEpoch  260 | Train Loss:     0.4215 | Validation Loss:     0.4073\nEpoch  261 | Train Loss:     0.4247 | Validation Loss:     0.4069\nEpoch  262 | Train Loss:     0.4219 | Validation Loss:     0.4168\nEpoch  263 | Train Loss:     0.4256 | Validation Loss:     0.4079\nEpoch  264 | Train Loss:     0.4241 | Validation Loss:     0.4166\nEpoch  265 | Train Loss:     0.4241 | Validation Loss:     0.4115\nEpoch  266 | Train Loss:     0.4226 | Validation Loss:     0.4093\nEpoch  267 | Train Loss:     0.4268 | Validation Loss:     0.4230\nEpoch  268 | Train Loss:     0.4258 | Validation Loss:     0.4143\nEpoch  269 | Train Loss:     0.4223 | Validation Loss:     0.4122\nEpoch  270 | Train Loss:     0.4213 | Validation Loss:     0.4072\nEpoch  271 | Train Loss:     0.4254 | Validation Loss:     0.4078\nEpoch  272 | Train Loss:     0.4242 | Validation Loss:     0.4087\nEpoch  273 | Train Loss:     0.4238 | Validation Loss:     0.4255\nEpoch  274 | Train Loss:     0.4237 | Validation Loss:     0.4058\nEpoch  275 | Train Loss:     0.4231 | Validation Loss:     0.4069\nEpoch  276 | Train Loss:     0.4218 | Validation Loss:     0.4111\nEpoch  277 | Train Loss:     0.4217 | Validation Loss:     0.4150\nEpoch  278 | Train Loss:     0.4223 | Validation Loss:     0.4131\nEpoch  279 | Train Loss:     0.4221 | Validation Loss:     0.4135\nEpoch  280 | Train Loss:     0.4238 | Validation Loss:     0.4082\nEpoch  281 | Train Loss:     0.4220 | Validation Loss:     0.4198\nEpoch  282 | Train Loss:     0.4217 | Validation Loss:     0.4126\nEpoch  283 | Train Loss:     0.4259 | Validation Loss:     0.4494\nEpoch  284 | Train Loss:     0.4259 | Validation Loss:     0.4131\nEpoch  285 | Train Loss:     0.4236 | Validation Loss:     0.4097\nEpoch  286 | Train Loss:     0.4234 | Validation Loss:     0.4082\nEpoch  287 | Train Loss:     0.4290 | Validation Loss:     0.4062\nEpoch  288 | Train Loss:     0.4249 | Validation Loss:     0.4371\nEpoch  289 | Train Loss:     0.4233 | Validation Loss:     0.4143\nEpoch  290 | Train Loss:     0.4284 | Validation Loss:     0.4198\nEpoch  291 | Train Loss:     0.4220 | Validation Loss:     0.4183\nEpoch  292 | Train Loss:     0.4261 | Validation Loss:     0.4196\nEpoch  293 | Train Loss:     0.4234 | Validation Loss:     0.4064\nEpoch  294 | Train Loss:     0.4253 | Validation Loss:     0.4239\nEpoch  295 | Train Loss:     0.4233 | Validation Loss:     0.4410\nEpoch  296 | Train Loss:     0.4285 | Validation Loss:     0.4090\nEpoch  297 | Train Loss:     0.4193 | Validation Loss:     0.4144\nEpoch  298 | Train Loss:     0.4240 | Validation Loss:     0.4081\nEpoch  299 | Train Loss:     0.4244 | Validation Loss:     0.4175\nEpoch  300 | Train Loss:     0.4277 | Validation Loss:     0.4080\nEpoch  301 | Train Loss:     0.4252 | Validation Loss:     0.4132\nEpoch  302 | Train Loss:     0.4240 | Validation Loss:     0.4187\nEpoch  303 | Train Loss:     0.4216 | Validation Loss:     0.4109\nEpoch  304 | Train Loss:     0.4287 | Validation Loss:     0.4192\nEpoch  305 | Train Loss:     0.4253 | Validation Loss:     0.4085\nEpoch  306 | Train Loss:     0.4242 | Validation Loss:     0.4102\nEpoch  307 | Train Loss:     0.4261 | Validation Loss:     0.4117\nEpoch  308 | Train Loss:     0.4232 | Validation Loss:     0.4107\nEpoch  309 | Train Loss:     0.4235 | Validation Loss:     0.4067\nEpoch  310 | Train Loss:     0.4262 | Validation Loss:     0.4129\nEpoch  311 | Train Loss:     0.4259 | Validation Loss:     0.4194\nEpoch  312 | Train Loss:     0.4259 | Validation Loss:     0.4097\nEpoch  313 | Train Loss:     0.4237 | Validation Loss:     0.4374\nEpoch  314 | Train Loss:     0.4265 | Validation Loss:     0.4163\nEpoch  315 | Train Loss:     0.4223 | Validation Loss:     0.4111\nEpoch  316 | Train Loss:     0.4293 | Validation Loss:     0.4073\nEpoch  317 | Train Loss:     0.4227 | Validation Loss:     0.4082\nEpoch  318 | Train Loss:     0.4219 | Validation Loss:     0.4216\nEpoch  319 | Train Loss:     0.4257 | Validation Loss:     0.4060\nEpoch  320 | Train Loss:     0.4261 | Validation Loss:     0.4191\nEpoch  321 | Train Loss:     0.4241 | Validation Loss:     0.4112\nEpoch  322 | Train Loss:     0.4263 | Validation Loss:     0.4268\nEpoch  323 | Train Loss:     0.4252 | Validation Loss:     0.4130\nEpoch  324 | Train Loss:     0.4211 | Validation Loss:     0.4111\nEpoch  325 | Train Loss:     0.4228 | Validation Loss:     0.4089\nEpoch  326 | Train Loss:     0.4335 | Validation Loss:     0.4278\nEpoch  327 | Train Loss:     0.4282 | Validation Loss:     0.4070\nEpoch  328 | Train Loss:     0.4226 | Validation Loss:     0.4068\nEpoch  329 | Train Loss:     0.4223 | Validation Loss:     0.4057\nEpoch  330 | Train Loss:     0.4243 | Validation Loss:     0.4054\nEpoch  331 | Train Loss:     0.4216 | Validation Loss:     0.4117\nEpoch  332 | Train Loss:     0.4271 | Validation Loss:     0.4342\nEpoch  333 | Train Loss:     0.4236 | Validation Loss:     0.4076\nEpoch  334 | Train Loss:     0.4228 | Validation Loss:     0.4132\nEpoch  335 | Train Loss:     0.4284 | Validation Loss:     0.4138\nEpoch  336 | Train Loss:     0.4208 | Validation Loss:     0.4300\nEpoch  337 | Train Loss:     0.4227 | Validation Loss:     0.4275\nEpoch  338 | Train Loss:     0.4358 | Validation Loss:     0.4091\nEpoch  339 | Train Loss:     0.4219 | Validation Loss:     0.4096\nEpoch  340 | Train Loss:     0.4225 | Validation Loss:     0.4039\nEpoch  341 | Train Loss:     0.4243 | Validation Loss:     0.4149\nEpoch  342 | Train Loss:     0.4228 | Validation Loss:     0.4051\nEpoch  343 | Train Loss:     0.4222 | Validation Loss:     0.4337\nEpoch  344 | Train Loss:     0.4252 | Validation Loss:     0.4123\nEpoch  345 | Train Loss:     0.4252 | Validation Loss:     0.4081\nEpoch  346 | Train Loss:     0.4221 | Validation Loss:     0.4047\nEpoch  347 | Train Loss:     0.4228 | Validation Loss:     0.4156\nEpoch  348 | Train Loss:     0.4261 | Validation Loss:     0.4059\nEpoch  349 | Train Loss:     0.4227 | Validation Loss:     0.4095\nEpoch  350 | Train Loss:     0.4246 | Validation Loss:     0.4311\nEpoch  351 | Train Loss:     0.4270 | Validation Loss:     0.4111\nEpoch  352 | Train Loss:     0.4236 | Validation Loss:     0.4039\nEpoch  353 | Train Loss:     0.4253 | Validation Loss:     0.4044\nEpoch  354 | Train Loss:     0.4285 | Validation Loss:     0.4082\nEpoch  355 | Train Loss:     0.4245 | Validation Loss:     0.4116\nEpoch  356 | Train Loss:     0.4319 | Validation Loss:     0.4086\nEpoch  357 | Train Loss:     0.4220 | Validation Loss:     0.4142\nEpoch  358 | Train Loss:     0.4237 | Validation Loss:     0.4304\nEpoch  359 | Train Loss:     0.4283 | Validation Loss:     0.4065\nEpoch  360 | Train Loss:     0.4222 | Validation Loss:     0.4192\nEpoch  361 | Train Loss:     0.4216 | Validation Loss:     0.4309\nEpoch  362 | Train Loss:     0.4223 | Validation Loss:     0.4053\nEpoch  363 | Train Loss:     0.4227 | Validation Loss:     0.4101\nEpoch  364 | Train Loss:     0.4232 | Validation Loss:     0.4162\nEpoch  365 | Train Loss:     0.4221 | Validation Loss:     0.4124\nEpoch  366 | Train Loss:     0.4278 | Validation Loss:     0.4140\nEpoch  367 | Train Loss:     0.4248 | Validation Loss:     0.4062\nEpoch  368 | Train Loss:     0.4294 | Validation Loss:     0.4082\nEpoch  369 | Train Loss:     0.4261 | Validation Loss:     0.4146\nEpoch  370 | Train Loss:     0.4237 | Validation Loss:     0.4064\nEpoch  371 | Train Loss:     0.4236 | Validation Loss:     0.4086\nEpoch  372 | Train Loss:     0.4258 | Validation Loss:     0.4117\nEpoch  373 | Train Loss:     0.4302 | Validation Loss:     0.4097\nEpoch  374 | Train Loss:     0.4258 | Validation Loss:     0.4059\nEpoch  375 | Train Loss:     0.4209 | Validation Loss:     0.4128\nEpoch  376 | Train Loss:     0.4246 | Validation Loss:     0.4179\nEpoch  377 | Train Loss:     0.4277 | Validation Loss:     0.4093\nEpoch  378 | Train Loss:     0.4257 | Validation Loss:     0.4097\nEpoch  379 | Train Loss:     0.4301 | Validation Loss:     0.4229\nEpoch  380 | Train Loss:     0.4209 | Validation Loss:     0.4047\nEpoch  381 | Train Loss:     0.4242 | Validation Loss:     0.4078\nEpoch  382 | Train Loss:     0.4257 | Validation Loss:     0.4407\nEpoch  383 | Train Loss:     0.4258 | Validation Loss:     0.4040\nEpoch  384 | Train Loss:     0.4253 | Validation Loss:     0.4211\nEpoch  385 | Train Loss:     0.4264 | Validation Loss:     0.4111\nEpoch  386 | Train Loss:     0.4259 | Validation Loss:     0.4119\nEpoch  387 | Train Loss:     0.4241 | Validation Loss:     0.4125\nEpoch  388 | Train Loss:     0.4237 | Validation Loss:     0.4065\nEpoch  389 | Train Loss:     0.4206 | Validation Loss:     0.4346\nEpoch  390 | Train Loss:     0.4255 | Validation Loss:     0.4074\nEpoch  391 | Train Loss:     0.4229 | Validation Loss:     0.4223\nEpoch  392 | Train Loss:     0.4245 | Validation Loss:     0.4101\nEpoch  393 | Train Loss:     0.4227 | Validation Loss:     0.4078\nEpoch  394 | Train Loss:     0.4259 | Validation Loss:     0.4127\nEpoch  395 | Train Loss:     0.4224 | Validation Loss:     0.4120\nEpoch  396 | Train Loss:     0.4237 | Validation Loss:     0.4098\nEpoch  397 | Train Loss:     0.4290 | Validation Loss:     0.4106\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","Cell \u001b[0;32mIn[316], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m optimizer \u001b[38;5;241m=\u001b[39m optim\u001b[38;5;241m.\u001b[39mAdam(model\u001b[38;5;241m.\u001b[39mparameters(), lr\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.0000001\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m1000\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m100\u001b[39m)\n\u001b[0;32m----> 2\u001b[0m \u001b[43mevaluate_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcustom_train_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m)\u001b[49m\n","Cell \u001b[0;32mIn[173], line 103\u001b[0m, in \u001b[0;36mevaluate_model\u001b[0;34m(model, custom_train_loader, criterion, optimizer)\u001b[0m\n\u001b[1;32m    101\u001b[0m     plt\u001b[38;5;241m.\u001b[39mlegend()\n\u001b[1;32m    102\u001b[0m     plt\u001b[38;5;241m.\u001b[39mgrid(\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m--> 103\u001b[0m     \u001b[43mplt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msavefig\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mimage_folder\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m/epoch_\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mepoch\u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[38;5;132;43;01m:\u001b[39;49;00m\u001b[38;5;124;43m04d\u001b[39;49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m.png\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    104\u001b[0m     plt\u001b[38;5;241m.\u001b[39mclose()\n\u001b[1;32m    106\u001b[0m images \u001b[38;5;241m=\u001b[39m []\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/matplotlib/pyplot.py:1023\u001b[0m, in \u001b[0;36msavefig\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m   1020\u001b[0m \u001b[38;5;129m@_copy_docstring_and_deprecators\u001b[39m(Figure\u001b[38;5;241m.\u001b[39msavefig)\n\u001b[1;32m   1021\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msavefig\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m   1022\u001b[0m     fig \u001b[38;5;241m=\u001b[39m gcf()\n\u001b[0;32m-> 1023\u001b[0m     res \u001b[38;5;241m=\u001b[39m \u001b[43mfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msavefig\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1024\u001b[0m     fig\u001b[38;5;241m.\u001b[39mcanvas\u001b[38;5;241m.\u001b[39mdraw_idle()  \u001b[38;5;66;03m# Need this if 'transparent=True', to reset colors.\u001b[39;00m\n\u001b[1;32m   1025\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m res\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/matplotlib/figure.py:3378\u001b[0m, in \u001b[0;36mFigure.savefig\u001b[0;34m(self, fname, transparent, **kwargs)\u001b[0m\n\u001b[1;32m   3374\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m ax \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maxes:\n\u001b[1;32m   3375\u001b[0m         stack\u001b[38;5;241m.\u001b[39menter_context(\n\u001b[1;32m   3376\u001b[0m             ax\u001b[38;5;241m.\u001b[39mpatch\u001b[38;5;241m.\u001b[39m_cm_set(facecolor\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnone\u001b[39m\u001b[38;5;124m'\u001b[39m, edgecolor\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnone\u001b[39m\u001b[38;5;124m'\u001b[39m))\n\u001b[0;32m-> 3378\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcanvas\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprint_figure\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/matplotlib/backend_bases.py:2366\u001b[0m, in \u001b[0;36mFigureCanvasBase.print_figure\u001b[0;34m(self, filename, dpi, facecolor, edgecolor, orientation, format, bbox_inches, pad_inches, bbox_extra_artists, backend, **kwargs)\u001b[0m\n\u001b[1;32m   2362\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   2363\u001b[0m     \u001b[38;5;66;03m# _get_renderer may change the figure dpi (as vector formats\u001b[39;00m\n\u001b[1;32m   2364\u001b[0m     \u001b[38;5;66;03m# force the figure dpi to 72), so we need to set it again here.\u001b[39;00m\n\u001b[1;32m   2365\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m cbook\u001b[38;5;241m.\u001b[39m_setattr_cm(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfigure, dpi\u001b[38;5;241m=\u001b[39mdpi):\n\u001b[0;32m-> 2366\u001b[0m         result \u001b[38;5;241m=\u001b[39m \u001b[43mprint_method\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2367\u001b[0m \u001b[43m            \u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2368\u001b[0m \u001b[43m            \u001b[49m\u001b[43mfacecolor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfacecolor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2369\u001b[0m \u001b[43m            \u001b[49m\u001b[43medgecolor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43medgecolor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2370\u001b[0m \u001b[43m            \u001b[49m\u001b[43morientation\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43morientation\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2371\u001b[0m \u001b[43m            \u001b[49m\u001b[43mbbox_inches_restore\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_bbox_inches_restore\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2372\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2373\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m   2374\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m bbox_inches \u001b[38;5;129;01mand\u001b[39;00m restore_bbox:\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/matplotlib/backend_bases.py:2232\u001b[0m, in \u001b[0;36mFigureCanvasBase._switch_canvas_and_return_print_method.<locals>.<lambda>\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m   2228\u001b[0m     optional_kws \u001b[38;5;241m=\u001b[39m {  \u001b[38;5;66;03m# Passed by print_figure for other renderers.\u001b[39;00m\n\u001b[1;32m   2229\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdpi\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfacecolor\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124medgecolor\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124morientation\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   2230\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbbox_inches_restore\u001b[39m\u001b[38;5;124m\"\u001b[39m}\n\u001b[1;32m   2231\u001b[0m     skip \u001b[38;5;241m=\u001b[39m optional_kws \u001b[38;5;241m-\u001b[39m {\u001b[38;5;241m*\u001b[39minspect\u001b[38;5;241m.\u001b[39msignature(meth)\u001b[38;5;241m.\u001b[39mparameters}\n\u001b[0;32m-> 2232\u001b[0m     print_method \u001b[38;5;241m=\u001b[39m functools\u001b[38;5;241m.\u001b[39mwraps(meth)(\u001b[38;5;28;01mlambda\u001b[39;00m \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: \u001b[43mmeth\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2233\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m{\u001b[49m\u001b[43mk\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mv\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mv\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitems\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mk\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mskip\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m   2234\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:  \u001b[38;5;66;03m# Let third-parties do as they see fit.\u001b[39;00m\n\u001b[1;32m   2235\u001b[0m     print_method \u001b[38;5;241m=\u001b[39m meth\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/matplotlib/backends/backend_agg.py:509\u001b[0m, in \u001b[0;36mFigureCanvasAgg.print_png\u001b[0;34m(self, filename_or_obj, metadata, pil_kwargs)\u001b[0m\n\u001b[1;32m    462\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mprint_png\u001b[39m(\u001b[38;5;28mself\u001b[39m, filename_or_obj, \u001b[38;5;241m*\u001b[39m, metadata\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, pil_kwargs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m    463\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    464\u001b[0m \u001b[38;5;124;03m    Write the figure to a PNG file.\u001b[39;00m\n\u001b[1;32m    465\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    507\u001b[0m \u001b[38;5;124;03m        *metadata*, including the default 'Software' key.\u001b[39;00m\n\u001b[1;32m    508\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 509\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_print_pil\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilename_or_obj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpng\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpil_kwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/matplotlib/backends/backend_agg.py:457\u001b[0m, in \u001b[0;36mFigureCanvasAgg._print_pil\u001b[0;34m(self, filename_or_obj, fmt, pil_kwargs, metadata)\u001b[0m\n\u001b[1;32m    452\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_print_pil\u001b[39m(\u001b[38;5;28mself\u001b[39m, filename_or_obj, fmt, pil_kwargs, metadata\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m    453\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    454\u001b[0m \u001b[38;5;124;03m    Draw the canvas, then save it using `.image.imsave` (to which\u001b[39;00m\n\u001b[1;32m    455\u001b[0m \u001b[38;5;124;03m    *pil_kwargs* and *metadata* are forwarded).\u001b[39;00m\n\u001b[1;32m    456\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 457\u001b[0m     \u001b[43mFigureCanvasAgg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdraw\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    458\u001b[0m     mpl\u001b[38;5;241m.\u001b[39mimage\u001b[38;5;241m.\u001b[39mimsave(\n\u001b[1;32m    459\u001b[0m         filename_or_obj, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuffer_rgba(), \u001b[38;5;28mformat\u001b[39m\u001b[38;5;241m=\u001b[39mfmt, origin\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mupper\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    460\u001b[0m         dpi\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfigure\u001b[38;5;241m.\u001b[39mdpi, metadata\u001b[38;5;241m=\u001b[39mmetadata, pil_kwargs\u001b[38;5;241m=\u001b[39mpil_kwargs)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/matplotlib/backends/backend_agg.py:400\u001b[0m, in \u001b[0;36mFigureCanvasAgg.draw\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    396\u001b[0m \u001b[38;5;66;03m# Acquire a lock on the shared font cache.\u001b[39;00m\n\u001b[1;32m    397\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m RendererAgg\u001b[38;5;241m.\u001b[39mlock, \\\n\u001b[1;32m    398\u001b[0m      (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtoolbar\u001b[38;5;241m.\u001b[39m_wait_cursor_for_draw_cm() \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtoolbar\n\u001b[1;32m    399\u001b[0m       \u001b[38;5;28;01melse\u001b[39;00m nullcontext()):\n\u001b[0;32m--> 400\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfigure\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdraw\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrenderer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    401\u001b[0m     \u001b[38;5;66;03m# A GUI class may be need to update a window using this draw, so\u001b[39;00m\n\u001b[1;32m    402\u001b[0m     \u001b[38;5;66;03m# don't forget to call the superclass.\u001b[39;00m\n\u001b[1;32m    403\u001b[0m     \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mdraw()\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/matplotlib/artist.py:95\u001b[0m, in \u001b[0;36m_finalize_rasterization.<locals>.draw_wrapper\u001b[0;34m(artist, renderer, *args, **kwargs)\u001b[0m\n\u001b[1;32m     93\u001b[0m \u001b[38;5;129m@wraps\u001b[39m(draw)\n\u001b[1;32m     94\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdraw_wrapper\u001b[39m(artist, renderer, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m---> 95\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mdraw\u001b[49m\u001b[43m(\u001b[49m\u001b[43martist\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrenderer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     96\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m renderer\u001b[38;5;241m.\u001b[39m_rasterizing:\n\u001b[1;32m     97\u001b[0m         renderer\u001b[38;5;241m.\u001b[39mstop_rasterizing()\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/matplotlib/artist.py:72\u001b[0m, in \u001b[0;36mallow_rasterization.<locals>.draw_wrapper\u001b[0;34m(artist, renderer)\u001b[0m\n\u001b[1;32m     69\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m artist\u001b[38;5;241m.\u001b[39mget_agg_filter() \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     70\u001b[0m         renderer\u001b[38;5;241m.\u001b[39mstart_filter()\n\u001b[0;32m---> 72\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mdraw\u001b[49m\u001b[43m(\u001b[49m\u001b[43martist\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrenderer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     73\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     74\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m artist\u001b[38;5;241m.\u001b[39mget_agg_filter() \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/matplotlib/figure.py:3175\u001b[0m, in \u001b[0;36mFigure.draw\u001b[0;34m(self, renderer)\u001b[0m\n\u001b[1;32m   3172\u001b[0m         \u001b[38;5;66;03m# ValueError can occur when resizing a window.\u001b[39;00m\n\u001b[1;32m   3174\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpatch\u001b[38;5;241m.\u001b[39mdraw(renderer)\n\u001b[0;32m-> 3175\u001b[0m \u001b[43mmimage\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_draw_list_compositing_images\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   3176\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrenderer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43martists\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msuppressComposite\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3178\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m sfig \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msubfigs:\n\u001b[1;32m   3179\u001b[0m     sfig\u001b[38;5;241m.\u001b[39mdraw(renderer)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/matplotlib/image.py:131\u001b[0m, in \u001b[0;36m_draw_list_compositing_images\u001b[0;34m(renderer, parent, artists, suppress_composite)\u001b[0m\n\u001b[1;32m    129\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m not_composite \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m has_images:\n\u001b[1;32m    130\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m a \u001b[38;5;129;01min\u001b[39;00m artists:\n\u001b[0;32m--> 131\u001b[0m         \u001b[43ma\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdraw\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrenderer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    132\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    133\u001b[0m     \u001b[38;5;66;03m# Composite any adjacent images together\u001b[39;00m\n\u001b[1;32m    134\u001b[0m     image_group \u001b[38;5;241m=\u001b[39m []\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/matplotlib/artist.py:72\u001b[0m, in \u001b[0;36mallow_rasterization.<locals>.draw_wrapper\u001b[0;34m(artist, renderer)\u001b[0m\n\u001b[1;32m     69\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m artist\u001b[38;5;241m.\u001b[39mget_agg_filter() \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     70\u001b[0m         renderer\u001b[38;5;241m.\u001b[39mstart_filter()\n\u001b[0;32m---> 72\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mdraw\u001b[49m\u001b[43m(\u001b[49m\u001b[43martist\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrenderer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     73\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     74\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m artist\u001b[38;5;241m.\u001b[39mget_agg_filter() \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/matplotlib/axes/_base.py:3064\u001b[0m, in \u001b[0;36m_AxesBase.draw\u001b[0;34m(self, renderer)\u001b[0m\n\u001b[1;32m   3061\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m artists_rasterized:\n\u001b[1;32m   3062\u001b[0m     _draw_rasterized(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfigure, artists_rasterized, renderer)\n\u001b[0;32m-> 3064\u001b[0m \u001b[43mmimage\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_draw_list_compositing_images\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   3065\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrenderer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43martists\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfigure\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msuppressComposite\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3067\u001b[0m renderer\u001b[38;5;241m.\u001b[39mclose_group(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124maxes\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m   3068\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstale \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/matplotlib/image.py:131\u001b[0m, in \u001b[0;36m_draw_list_compositing_images\u001b[0;34m(renderer, parent, artists, suppress_composite)\u001b[0m\n\u001b[1;32m    129\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m not_composite \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m has_images:\n\u001b[1;32m    130\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m a \u001b[38;5;129;01min\u001b[39;00m artists:\n\u001b[0;32m--> 131\u001b[0m         \u001b[43ma\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdraw\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrenderer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    132\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    133\u001b[0m     \u001b[38;5;66;03m# Composite any adjacent images together\u001b[39;00m\n\u001b[1;32m    134\u001b[0m     image_group \u001b[38;5;241m=\u001b[39m []\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/matplotlib/artist.py:72\u001b[0m, in \u001b[0;36mallow_rasterization.<locals>.draw_wrapper\u001b[0;34m(artist, renderer)\u001b[0m\n\u001b[1;32m     69\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m artist\u001b[38;5;241m.\u001b[39mget_agg_filter() \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     70\u001b[0m         renderer\u001b[38;5;241m.\u001b[39mstart_filter()\n\u001b[0;32m---> 72\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mdraw\u001b[49m\u001b[43m(\u001b[49m\u001b[43martist\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrenderer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     73\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     74\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m artist\u001b[38;5;241m.\u001b[39mget_agg_filter() \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/matplotlib/axis.py:1395\u001b[0m, in \u001b[0;36mAxis.draw\u001b[0;34m(self, renderer, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1392\u001b[0m     tick\u001b[38;5;241m.\u001b[39mdraw(renderer)\n\u001b[1;32m   1394\u001b[0m \u001b[38;5;66;03m# Shift label away from axes to avoid overlapping ticklabels.\u001b[39;00m\n\u001b[0;32m-> 1395\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_update_label_position\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrenderer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1396\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlabel\u001b[38;5;241m.\u001b[39mdraw(renderer)\n\u001b[1;32m   1398\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_update_offset_text_position(tlb1, tlb2)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/matplotlib/axis.py:2614\u001b[0m, in \u001b[0;36mYAxis._update_label_position\u001b[0;34m(self, renderer)\u001b[0m\n\u001b[1;32m   2610\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[1;32m   2612\u001b[0m \u001b[38;5;66;03m# get bounding boxes for this axis and any siblings\u001b[39;00m\n\u001b[1;32m   2613\u001b[0m \u001b[38;5;66;03m# that have been set by `fig.align_ylabels()`\u001b[39;00m\n\u001b[0;32m-> 2614\u001b[0m bboxes, bboxes2 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_tick_boxes_siblings\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrenderer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrenderer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2615\u001b[0m x, y \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlabel\u001b[38;5;241m.\u001b[39mget_position()\n\u001b[1;32m   2616\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlabel_position \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mleft\u001b[39m\u001b[38;5;124m'\u001b[39m:\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/matplotlib/axis.py:2148\u001b[0m, in \u001b[0;36mAxis._get_tick_boxes_siblings\u001b[0;34m(self, renderer)\u001b[0m\n\u001b[1;32m   2146\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m ax \u001b[38;5;129;01min\u001b[39;00m grouper\u001b[38;5;241m.\u001b[39mget_siblings(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maxes):\n\u001b[1;32m   2147\u001b[0m     axis \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(ax, \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00maxis_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124maxis\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m-> 2148\u001b[0m     ticks_to_draw \u001b[38;5;241m=\u001b[39m \u001b[43maxis\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_update_ticks\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2149\u001b[0m     tlb, tlb2 \u001b[38;5;241m=\u001b[39m axis\u001b[38;5;241m.\u001b[39m_get_ticklabel_bboxes(ticks_to_draw, renderer)\n\u001b[1;32m   2150\u001b[0m     bboxes\u001b[38;5;241m.\u001b[39mextend(tlb)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/matplotlib/axis.py:1277\u001b[0m, in \u001b[0;36mAxis._update_ticks\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1275\u001b[0m major_labels \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmajor\u001b[38;5;241m.\u001b[39mformatter\u001b[38;5;241m.\u001b[39mformat_ticks(major_locs)\n\u001b[1;32m   1276\u001b[0m major_ticks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_major_ticks(\u001b[38;5;28mlen\u001b[39m(major_locs))\n\u001b[0;32m-> 1277\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmajor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mformatter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mset_locs\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmajor_locs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1278\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m tick, loc, label \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(major_ticks, major_locs, major_labels):\n\u001b[1;32m   1279\u001b[0m     tick\u001b[38;5;241m.\u001b[39mupdate_position(loc)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/matplotlib/ticker.py:702\u001b[0m, in \u001b[0;36mScalarFormatter.set_locs\u001b[0;34m(self, locs)\u001b[0m\n\u001b[1;32m    700\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_useOffset:\n\u001b[1;32m    701\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compute_offset()\n\u001b[0;32m--> 702\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_set_order_of_magnitude\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    703\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_set_format()\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/matplotlib/ticker.py:766\u001b[0m, in \u001b[0;36mScalarFormatter._set_order_of_magnitude\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    764\u001b[0m     oom \u001b[38;5;241m=\u001b[39m math\u001b[38;5;241m.\u001b[39mfloor(math\u001b[38;5;241m.\u001b[39mlog10(vmax \u001b[38;5;241m-\u001b[39m vmin))\n\u001b[1;32m    765\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 766\u001b[0m     val \u001b[38;5;241m=\u001b[39m \u001b[43mlocs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    767\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m val \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    768\u001b[0m         oom \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/numpy/core/_methods.py:41\u001b[0m, in \u001b[0;36m_amax\u001b[0;34m(a, axis, out, keepdims, initial, where)\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_amax\u001b[39m(a, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, out\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, keepdims\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m     40\u001b[0m           initial\u001b[38;5;241m=\u001b[39m_NoValue, where\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[0;32m---> 41\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mumr_maximum\u001b[49m\u001b[43m(\u001b[49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkeepdims\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minitial\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwhere\u001b[49m\u001b[43m)\u001b[49m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "],"ename":"KeyboardInterrupt","evalue":"","output_type":"error"},{"output_type":"display_data","data":{"text/plain":"<Figure size 1000x500 with 1 Axes>","image/png":"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"},"metadata":{}}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}