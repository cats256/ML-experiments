{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":84894,"databundleVersionId":9709193,"sourceType":"competition"},{"sourceId":408,"sourceType":"datasetVersion","datasetId":180},{"sourceId":668,"sourceType":"datasetVersion","datasetId":308},{"sourceId":7949759,"sourceType":"datasetVersion","datasetId":4675026},{"sourceId":9738619,"sourceType":"datasetVersion","datasetId":5960716}],"dockerImageVersionId":30787,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import sys\nimport time\nimport math\nimport itertools\n\nimport numpy as np\nimport pandas as pd\n\nimport matplotlib.pyplot as plt\nfrom scipy.special import logit\nfrom scipy.stats import norm\n\nimport tensorflow as tf\nfrom keras import layers, models, datasets\n\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport torch.nn.functional as F\nfrom torch.utils.data import TensorDataset, DataLoader\nfrom torchinfo import summary\nfrom torch.optim.lr_scheduler import StepLR, LambdaLR\nimport torch.autograd.profiler as profiler\n\nfrom sklearn.model_selection import train_test_split, LeaveOneOut, StratifiedKFold, cross_val_predict\nfrom sklearn.preprocessing import StandardScaler, MinMaxScaler, LabelEncoder, PowerTransformer\nfrom sklearn.metrics import f1_score, log_loss, accuracy_score\nfrom sklearn.linear_model import LogisticRegression\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(device)","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2024-12-04T02:36:54.962610Z","iopub.execute_input":"2024-12-04T02:36:54.962950Z","iopub.status.idle":"2024-12-04T02:36:54.970655Z","shell.execute_reply.started":"2024-12-04T02:36:54.962920Z","shell.execute_reply":"2024-12-04T02:36:54.969543Z"}},"outputs":[{"name":"stdout","text":"cuda\n","output_type":"stream"}],"execution_count":13},{"cell_type":"code","source":"def calculate_metrics(model, data_tensor, labels_tensor, batch_size=1024, num_features=22):\n    model.eval()\n    all_preds = []\n    all_labels = []\n\n    with torch.no_grad():\n        for start_idx in range(0, len(data_tensor), batch_size):\n            end_idx = min(start_idx + batch_size, len(data_tensor))\n            inputs = data_tensor[start_idx:end_idx].view(-1, num_features)\n            labels = labels_tensor[start_idx:end_idx]\n\n            outputs = model(inputs)\n            _, preds = torch.max(outputs, 1)\n            all_preds.extend(preds.cpu().numpy())\n            all_labels.extend(labels.cpu().numpy())\n\n    accuracy = accuracy_score(all_labels, all_preds)\n    f1 = f1_score(all_labels, all_preds, average='weighted')\n    return accuracy, f1","metadata":{"execution":{"iopub.status.busy":"2024-12-04T02:36:55.741986Z","iopub.execute_input":"2024-12-04T02:36:55.743008Z","iopub.status.idle":"2024-12-04T02:36:55.751068Z","shell.execute_reply.started":"2024-12-04T02:36:55.742956Z","shell.execute_reply":"2024-12-04T02:36:55.750310Z"},"trusted":true},"outputs":[],"execution_count":14},{"cell_type":"code","source":"class CustomDataLoader:\n    def __init__(self, features, labels, validation_size=0.2, random_state=42, classification=True):        \n        if validation_size > 0.0:\n            stratify = labels if classification else None\n            train_data, val_data, train_labels, val_labels = train_test_split(\n                features, labels, test_size=validation_size, stratify=stratify, random_state=random_state\n            )\n            \n            self.val_data_tensor = torch.tensor(val_data).float().to(device)\n            \n            if classification:\n                self.val_labels_tensor = torch.tensor(val_labels).long().to(device)\n\n            else:\n                self.val_labels_tensor =torch.tensor(val_labels).float().to(device)\n        else:\n            train_data, train_labels = features, labels\n            self.val_data_tensor, self.val_labels_tensor = None, None\n        \n        self.train_data_tensor = torch.tensor(train_data).float().to(device)\n\n        if classification:\n            self.train_labels_tensor = torch.tensor(train_labels).long().to(device)\n        else:\n            self.train_labels_tensor = torch.tensor(train_labels).float().to(device)\n\n        torch.manual_seed(random_state)\n        indices = torch.randperm(len(self.train_data_tensor))\n\n        self.train_data_tensor = self.train_data_tensor[indices]\n        self.train_labels_tensor = self.train_labels_tensor[indices]","metadata":{"execution":{"iopub.status.busy":"2024-12-04T02:36:56.334376Z","iopub.execute_input":"2024-12-04T02:36:56.335322Z","iopub.status.idle":"2024-12-04T02:36:56.345880Z","shell.execute_reply.started":"2024-12-04T02:36:56.335276Z","shell.execute_reply":"2024-12-04T02:36:56.344569Z"},"trusted":true},"outputs":[],"execution_count":15},{"cell_type":"code","source":"def evaluate_model(model, custom_train_loader, criterion, optimizer, num_epochs, scheduler, batch_size=1024, num_features=22, early_stopping_patience=10):\n    best_val_loss = float('inf')\n    best_epoch = 0\n    patience_counter = 0\n\n    last_epoch_y_pred = None\n    last_epoch_y_t = None\n    epoch_gamma = 0.20\n\n    for epoch in range(num_epochs):\n        running_loss = 0.0\n        model.train()\n        i = 0\n        total_loss = 0\n        num_items = 0\n        epoch_y_pred=[]\n        epoch_y_t=[]\n\n        for start_idx in range(0, len(custom_train_loader.train_data_tensor), batch_size):\n            end_idx = min(start_idx + batch_size, len(custom_train_loader.train_data_tensor))\n            inputs = custom_train_loader.train_data_tensor[start_idx:end_idx].view(-1, num_features)\n            labels = custom_train_loader.train_labels_tensor[start_idx:end_idx]\n\n            optimizer.zero_grad()\n            outputs = model(inputs)\n\n            if epoch == 0:\n                loss = criterion(outputs, labels, model)\n                outputs = outputs[:, 0]\n            else:\n                outputs = outputs[:, 0]                \n                loss = roc_star_loss(labels, outputs, epoch_gamma, last_epoch_y_t, last_epoch_y_pred)\n                \n            loss.backward()\n            optimizer.step()\n            scheduler.step()\n            running_loss += loss.item() * len(labels)\n            total_loss += loss.item() * len(labels)\n            num_items += len(labels)\n\n            epoch_y_pred.extend(outputs)\n            epoch_y_t.extend(labels)\n\n            i += 1\n\n        last_epoch_y_pred = torch.tensor(epoch_y_pred).cuda()\n        last_epoch_y_t = torch.tensor(epoch_y_t).cuda()\n        epoch_gamma = epoch_update_gamma(last_epoch_y_t, last_epoch_y_pred, epoch)\n    \n        if epoch % 10 == 0:\n            for param_group in optimizer.param_groups:\n                print(\"Learning Rate:\", param_group['lr'])\n\n            # model.eval()\n            # val_loss = 0.0\n            # with torch.no_grad():\n            #     for start_idx in range(0, len(custom_train_loader.val_data_tensor), batch_size):\n            #         end_idx = min(start_idx + batch_size, len(custom_train_loader.val_data_tensor))\n            #         val_inputs = custom_train_loader.val_data_tensor[start_idx:end_idx].view(-1, num_features)\n            #         val_labels = custom_train_loader.val_labels_tensor[start_idx:end_idx]\n    \n            #         val_outputs = model(val_inputs)\n            #         val_loss += roc_star_loss(val_outputs, val_labels, epoch_gamma, last_epoch_y_t, last_epoch_y_pred).item() * len(val_labels)\n    \n            avg_train_loss = running_loss / len(custom_train_loader.train_data_tensor)\n            # avg_val_loss = val_loss / len(custom_train_loader.val_data_tensor)\n    \n            train_accuracy, train_f1 = calculate_metrics(model, custom_train_loader.train_data_tensor, custom_train_loader.train_labels_tensor, batch_size, num_features)\n            val_accuracy, val_f1 = calculate_metrics(model, custom_train_loader.val_data_tensor, custom_train_loader.val_labels_tensor, batch_size, num_features)\n\n            print(f'Epoch {epoch + 1}, Training Loss: {avg_train_loss}')\n\n            # print(f'Epoch {epoch + 1}, Training Loss: {avg_train_loss}, Validation Loss: {avg_val_loss}')\n            print(f'Training Accuracy: {train_accuracy}, Training F1 Score: {train_f1}')\n            print(f'Validation Accuracy: {val_accuracy}, Validation F1 Score: {val_f1}')\n            print()\n            \n            # if avg_val_loss < best_val_loss:\n            #     best_val_loss = avg_val_loss\n            #     best_epoch = epoch + 1\n            #     patience_counter = 0\n            # else:\n            #     patience_counter += 1\n            #     if patience_counter >= early_stopping_patience:\n            #         print(f'Early stopping triggered after {epoch + 1} epochs.')\n            #         print(f'Best Validation Loss: {best_val_loss} from Epoch {best_epoch}')\n            #         break\n\n    if patience_counter < early_stopping_patience:\n        print(f'Best Validation Loss after {num_epochs} epochs: {best_val_loss} from Epoch {best_epoch}')","metadata":{"execution":{"iopub.status.busy":"2024-12-04T03:24:16.742829Z","iopub.execute_input":"2024-12-04T03:24:16.743780Z","iopub.status.idle":"2024-12-04T03:24:16.754670Z","shell.execute_reply.started":"2024-12-04T03:24:16.743742Z","shell.execute_reply":"2024-12-04T03:24:16.753804Z"},"trusted":true},"outputs":[],"execution_count":336},{"cell_type":"code","source":"data_dl = pd.read_csv('/kaggle/input/playground-series-s4e10/train.csv')\ndata_og = pd.read_csv('/kaggle/input/loan-approval-prediction/credit_risk_dataset.csv')\n\ndata_dl = data_dl.drop([\"id\"], axis=1)\n\nmedian_emp_length = data_og['person_emp_length'].median()\nmedian_int_rate = data_og['loan_int_rate'].median()\n\ndata_dl['source'] = 0\ndata_og['source'] = 1\n\ndata = pd.concat([data_dl, data_og], ignore_index=True)\n\ndata['person_emp_length_missing'] = data['person_emp_length'].isna().astype(int)\ndata['loan_int_rate_missing'] = data['loan_int_rate'].isna().astype(int)\n\ndata['person_emp_length'] = data['person_emp_length'].fillna(median_emp_length)\ndata['loan_int_rate'] = data['loan_int_rate'].fillna(median_int_rate)\n\ngrade_mapping = {'A': 7, 'B': 6, 'C': 5, 'D': 4, 'E': 3, 'F': 2, 'G': 1}\ndata['loan_grade'] = data['loan_grade'].map(grade_mapping)\n\npurpose_mapping = {\n    'DEBTCONSOLIDATION': 1,\n    'HOMEIMPROVEMENT': 2,\n    'MEDICAL': 3,\n    'PERSONAL': 4,\n    'EDUCATION': 5,\n    'VENTURE': 6\n}\ndata['loan_intent'] = data['loan_intent'].map(purpose_mapping)\n\nhome_ownership_mapping = {\n    'OWN': 1,\n    'MORTGAGE': 2,\n    'OTHER': 3,\n    'RENT': 4\n}\ndata['person_home_ownership'] = data['person_home_ownership'].map(home_ownership_mapping)\n\nX = data.drop([\"loan_status\"], axis=1)\nX = pd.get_dummies(X, drop_first=True)\ny = data[\"loan_status\"]\n\ncolumn_to_log = [\n    'person_age',\n    'person_income',\n]\n\ncolumn_to_sqrt = [\n    'person_emp_length',\n    'loan_percent_income',\n]\n\nfor col in column_to_log:\n    if (X[col] <= 0).any():\n        print(f\"Column '{col}' contains non-positive values. Adding 1 to avoid log of non-positive numbers.\")\n        X[col] = np.log(X[col] + 1)\n    else:\n        X[col] = np.log(X[col])\n\nfor col in column_to_sqrt:\n    if (X[col] < 0).any():\n        print(f\"Column '{col}' contains negative values. Setting negative values to NaN before applying sqrt.\")\n        X[col] = np.sqrt(X[col].clip(lower=0))\n    else:\n        X[col] = np.sqrt(X[col])\n\nprint(data.isnull().sum())\nprint(X.columns)\nprint(X.shape, y.shape)\nprint(X.columns.get_loc('source'))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-04T03:24:18.014301Z","iopub.execute_input":"2024-12-04T03:24:18.015115Z","iopub.status.idle":"2024-12-04T03:24:18.178148Z","shell.execute_reply.started":"2024-12-04T03:24:18.015084Z","shell.execute_reply":"2024-12-04T03:24:18.177142Z"}},"outputs":[{"name":"stdout","text":"person_age                    0\nperson_income                 0\nperson_home_ownership         0\nperson_emp_length             0\nloan_intent                   0\nloan_grade                    0\nloan_amnt                     0\nloan_int_rate                 0\nloan_percent_income           0\ncb_person_default_on_file     0\ncb_person_cred_hist_length    0\nloan_status                   0\nsource                        0\nperson_emp_length_missing     0\nloan_int_rate_missing         0\ndtype: int64\nIndex(['person_age', 'person_income', 'person_home_ownership',\n       'person_emp_length', 'loan_intent', 'loan_grade', 'loan_amnt',\n       'loan_int_rate', 'loan_percent_income', 'cb_person_cred_hist_length',\n       'source', 'person_emp_length_missing', 'loan_int_rate_missing',\n       'cb_person_default_on_file_Y'],\n      dtype='object')\n(91226, 14) (91226,)\n10\n","output_type":"stream"}],"execution_count":337},{"cell_type":"code","source":"classes, counts = np.unique(y, return_counts=True)\nfor cls, count in zip(classes, counts):\n    print(f\"Class {cls}: {count} samples\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-04T03:24:18.361285Z","iopub.execute_input":"2024-12-04T03:24:18.361633Z","iopub.status.idle":"2024-12-04T03:24:18.370006Z","shell.execute_reply.started":"2024-12-04T03:24:18.361601Z","shell.execute_reply":"2024-12-04T03:24:18.369128Z"}},"outputs":[{"name":"stdout","text":"Class 0: 75768 samples\nClass 1: 15458 samples\n","output_type":"stream"}],"execution_count":338},{"cell_type":"code","source":"x_scaler = StandardScaler()\nx_scaled = x_scaler.fit_transform(X)\n\nlabel_encoder = LabelEncoder()\ny_encoded = label_encoder.fit_transform(y)\n\nprint(x_scaled.shape)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-04T03:24:18.670848Z","iopub.execute_input":"2024-12-04T03:24:18.671534Z","iopub.status.idle":"2024-12-04T03:24:18.739489Z","shell.execute_reply.started":"2024-12-04T03:24:18.671485Z","shell.execute_reply":"2024-12-04T03:24:18.738545Z"}},"outputs":[{"name":"stdout","text":"(91226, 14)\n","output_type":"stream"}],"execution_count":339},{"cell_type":"code","source":"feature_means = x_scaled.mean(axis=0)\nfeature_variances = x_scaled.var(axis=0)\nfeature_mins = x_scaled.min(axis=0)\nfeature_maxs = x_scaled.max(axis=0)\n\nfeature_stats_scaled_full = pd.DataFrame({\n    'Mean': feature_means,\n    'Variance': feature_variances,\n    'Min': feature_mins,\n    'Max': feature_maxs\n})\n\nprint(\"Mean, Variance, Min, and Max of Scaled Features:\")\nprint(feature_stats_scaled_full)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-04T03:24:18.956070Z","iopub.execute_input":"2024-12-04T03:24:18.956399Z","iopub.status.idle":"2024-12-04T03:24:18.972814Z","shell.execute_reply.started":"2024-12-04T03:24:18.956368Z","shell.execute_reply":"2024-12-04T03:24:18.971838Z"}},"outputs":[{"name":"stdout","text":"Mean, Variance, Min, and Max of Scaled Features:\n            Mean  Variance       Min        Max\n0  -3.289997e-16       1.0 -1.552712   8.591255\n1  -1.420680e-16       1.0 -5.315235   9.344027\n2  -1.370832e-17       1.0 -1.810229   0.945472\n3   3.987875e-17       1.0 -1.859550   8.913061\n4   6.153166e-17       1.0 -1.591801   1.383254\n5  -1.183900e-17       1.0 -4.464004   1.025387\n6  -1.333446e-16       1.0 -1.513249   4.385625\n7   9.327889e-16       1.0 -1.759487   4.065809\n8  -5.358707e-16       1.0 -3.245038   4.413749\n9   6.480297e-17       1.0 -0.943500   5.989958\n10 -1.944089e-16       1.0 -0.745361   1.341632\n11 -4.610980e-17       1.0 -0.099539  10.046317\n12  6.729539e-17       1.0 -0.188056   5.317578\n13  2.928596e-17       1.0 -0.433778   2.305326\n","output_type":"stream"}],"execution_count":340},{"cell_type":"code","source":"class CustomLoss(nn.Module):\n    def __init__(self, criterion, f1_lambda, f2_lambda, l1_lambda, l2_lambda):\n        super(CustomLoss, self).__init__()\n        self.criterion = criterion\n        self.f1_lambda = f1_lambda\n        self.f2_lambda = f2_lambda\n        self.l1_lambda = l1_lambda\n        self.l2_lambda = l2_lambda\n\n    def forward(self, outputs, labels, model): \n        f1_loss = 0.0\n        f2_loss = 0.0\n        l1_loss = 0.0\n        l2_loss = 0.0\n\n        for name, module in model.named_modules():\n            if isinstance(module, CustomActivation):\n                f1_loss += (module.a ** 2).sum() + (module.b ** 2).sum()\n                f2_loss += ((module.a - module.b) ** 2).sum()\n\n            if isinstance(module, nn.Linear):\n                l1_loss += torch.norm(module.weight, 1)\n                l2_loss += torch.norm(module.weight, 2) ** 2\n\n        total_loss = (self.criterion(outputs, labels)\n                      + self.f1_lambda * f1_loss\n                      + self.f2_lambda * f2_loss\n                      + self.l1_lambda * l1_loss\n                      + self.l2_lambda * l2_loss)\n\n        return total_loss\n\n    def regular_loss(self, outputs, labels):\n        return self.criterion(outputs, labels)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-04T03:24:19.284653Z","iopub.execute_input":"2024-12-04T03:24:19.284990Z","iopub.status.idle":"2024-12-04T03:24:19.292711Z","shell.execute_reply.started":"2024-12-04T03:24:19.284959Z","shell.execute_reply":"2024-12-04T03:24:19.291778Z"}},"outputs":[],"execution_count":341},{"cell_type":"code","source":"class CustomActivation(nn.Module):\n    def __init__(self, num_features, num_control_points, bias_tensor, init_identity=False):\n        super(CustomActivation, self).__init__()\n        self.a = nn.Parameter(torch.zeros(num_features, num_control_points))\n        self.b = nn.Parameter(torch.zeros(num_features, num_control_points))\n        \n        self.local_bias = nn.Parameter(torch.zeros(num_features, num_control_points))\n        self.global_bias = nn.Parameter(torch.zeros(1, num_features))\n        # global_bias may not even be needed\n\n        with torch.no_grad():\n            repeated_bias = bias_tensor.repeat(num_features // bias_tensor.shape[0], 1)\n            self.local_bias.copy_(repeated_bias)\n\n            if init_identity:\n                middle_index = num_control_points // 2\n                self.a[:, middle_index] = 1.0\n                self.b[:, middle_index] = 1.0\n\n    def forward(self, x):\n        x = x.unsqueeze(-1) + self.local_bias\n        x = torch.where(x < 0, self.a * x, self.b * x)\n        return x.sum(dim=-1) + self.global_bias","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-04T03:24:19.663456Z","iopub.execute_input":"2024-12-04T03:24:19.663856Z","iopub.status.idle":"2024-12-04T03:24:19.671354Z","shell.execute_reply.started":"2024-12-04T03:24:19.663826Z","shell.execute_reply":"2024-12-04T03:24:19.670389Z"}},"outputs":[],"execution_count":342},{"cell_type":"code","source":"class CustomLinear(nn.Module):\n    def __init__(self, num_features, num_outputs, init_identity=False):\n        super(CustomLinear, self).__init__()\n        \n        if init_identity and num_features != num_outputs:\n            raise ValueError(\"For identity initialization, num_features must equal num_outputs.\")\n\n        self.linear = nn.Linear(num_features, num_outputs, bias=True)\n        \n        with torch.no_grad():\n            self.linear.bias.zero_()\n\n            if init_identity:\n                self.linear.weight.copy_(torch.eye(num_features, num_outputs))\n            else:\n                self.linear.weight.zero_()\n                            \n    def forward(self, x):\n        return self.linear(x)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-04T03:24:19.999617Z","iopub.execute_input":"2024-12-04T03:24:19.999964Z","iopub.status.idle":"2024-12-04T03:24:20.005982Z","shell.execute_reply.started":"2024-12-04T03:24:19.999934Z","shell.execute_reply":"2024-12-04T03:24:20.005046Z"}},"outputs":[],"execution_count":343},{"cell_type":"code","source":"class TabularDenseNet(nn.Module):\n    def __init__(self, input_size, output_size, num_control_points, num_layers, corr_comb_indices):\n        super(TabularDenseNet, self).__init__()\n        self.layers = nn.ModuleList()\n        self.corr_comb_indices = corr_comb_indices\n\n        quantiles = np.quantile(x_scaled, q=np.linspace(0, 1, num_control_points), axis=0).T\n        bias_tensor = torch.tensor(quantiles)\n        \n        index_expanded = corr_comb_indices.unsqueeze(1).expand(-1, bias_tensor.size(1))\n        bias_tensor_comb = torch.gather(bias_tensor, dim=0, index=index_expanded)\n        \n        if num_layers % 2 == 1:\n            self.layers.append(CustomActivation(input_size, num_control_points, bias_tensor_comb, init_identity=True))\n            num_layers -= 1\n            input_size *= 2\n            \n        for i in range(num_layers):\n            if i % 2 == 0:\n                self.layers.append(CustomLinear(input_size, input_size, init_identity=True))\n            else:\n                self.layers.append(CustomActivation(input_size, num_control_points, bias_tensor_comb, init_identity=True))\n\n            input_size *= 2\n            \n        self.final = CustomLinear(input_size, output_size, init_identity=False)\n        \n    def forward(self, x):\n        combinations_tensor = x[:, self.corr_comb_indices]\n        outputs = [combinations_tensor]\n\n        for layer in self.layers:\n            concatenated_outputs = torch.cat(outputs, dim=-1)\n            outputs.append(layer(concatenated_outputs))\n\n        concatenated_outputs = torch.cat(outputs, dim=-1)\n        x = self.final(concatenated_outputs)\n        return x","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-04T03:24:20.364216Z","iopub.execute_input":"2024-12-04T03:24:20.365351Z","iopub.status.idle":"2024-12-04T03:24:20.373411Z","shell.execute_reply.started":"2024-12-04T03:24:20.365313Z","shell.execute_reply":"2024-12-04T03:24:20.372515Z"}},"outputs":[],"execution_count":344},{"cell_type":"code","source":"def roc_star_loss( _y_true, y_pred, gamma, _epoch_true, epoch_pred):\n    \"\"\"\n    Nearly direct loss function for AUC.\n    See article,\n    C. Reiss, \"Roc-star : An objective function for ROC-AUC that actually works.\"\n    https://github.com/iridiumblue/articles/blob/master/roc_star.md\n        _y_true: `Tensor`. Targets (labels).  Float either 0.0 or 1.0 .\n        y_pred: `Tensor` . Predictions.\n        gamma  : `Float` Gamma, as derived from last epoch.\n        _epoch_true: `Tensor`.  Targets (labels) from last epoch.\n        epoch_pred : `Tensor`.  Predicions from last epoch.\n    \"\"\"\n    #convert labels to boolean\n    y_true = (_y_true>=0.50)\n    epoch_true = (_epoch_true>=0.50)\n\n    # if batch is either all true or false return small random stub value.\n    if torch.sum(y_true)==0 or torch.sum(y_true) == y_true.shape[0]: return torch.sum(y_pred)*1e-8\n\n    pos = y_pred[y_true]\n    neg = y_pred[~y_true]\n\n    epoch_pos = epoch_pred[epoch_true]\n    epoch_neg = epoch_pred[~epoch_true]\n\n    max_pos = 15458 // 1000\n    max_neg = 75768 // 1000\n    # max_pos = 1000\n    # max_neg = 1000\n\n    cap_pos = epoch_pos.shape[0]\n    cap_neg = epoch_neg.shape[0]\n    epoch_pos = epoch_pos[torch.rand_like(epoch_pos) < max_pos/cap_pos]\n    epoch_neg = epoch_neg[torch.rand_like(epoch_neg) < max_neg/cap_pos]\n\n    ln_pos = pos.shape[0]\n    ln_neg = neg.shape[0]\n\n    if ln_pos>0 :\n        pos_expand = pos.view(-1,1).expand(-1,epoch_neg.shape[0]).reshape(-1)\n        neg_expand = epoch_neg.repeat(ln_pos)\n        # print(y_pred.shape)\n        # print(y_true.shape)\n        # print(pos.shape)\n        # print(ln_pos)\n        # print(ln_neg)\n        # print(pos_expand.shape)\n        # print(epoch_neg.shape)\n        # print(neg_expand.shape)\n\n        diff2 = neg_expand - pos_expand + gamma\n        l2 = diff2[diff2>0]\n        m2 = l2 * l2\n        len2 = l2.shape[0]\n    else:\n        m2 = torch.tensor([0], dtype=torch.float).cuda()\n        len2 = 0\n\n    if ln_neg>0 :\n        pos_expand = epoch_pos.view(-1,1).expand(-1, ln_neg).reshape(-1)\n        neg_expand = neg.repeat(epoch_pos.shape[0])\n\n        diff3 = neg_expand - pos_expand + gamma\n        l3 = diff3[diff3>0]\n        m3 = l3*l3\n        len3 = l3.shape[0]\n    else:\n        m3 = torch.tensor([0], dtype=torch.float).cuda()\n        len3=0\n\n    if (torch.sum(m2)+torch.sum(m3))!=0 :\n       res2 = torch.sum(m2)/max_pos+torch.sum(m3)/max_neg\n    else:\n       res2 = torch.sum(m2)+torch.sum(m3)\n\n    res2 = torch.where(torch.isnan(res2), torch.zeros_like(res2), res2)\n\n    return res2","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-04T03:24:31.223137Z","iopub.execute_input":"2024-12-04T03:24:31.223612Z","iopub.status.idle":"2024-12-04T03:24:31.240202Z","shell.execute_reply.started":"2024-12-04T03:24:31.223561Z","shell.execute_reply":"2024-12-04T03:24:31.238968Z"}},"outputs":[],"execution_count":346},{"cell_type":"code","source":"def epoch_update_gamma(y_true,y_pred, epoch=-1,delta=2):\n    \"\"\"\n    Calculate gamma from last epoch's targets and predictions.\n    Gamma is updated at the end of each epoch.\n    y_true: `Tensor`. Targets (labels).  Float either 0.0 or 1.0 .\n    y_pred: `Tensor` . Predictions.\n    \"\"\"\n    DELTA = delta\n    SUB_SAMPLE_SIZE = 2000.0\n    pos = y_pred[y_true==1]\n    neg = y_pred[y_true==0] # yo pytorch, no boolean tensors or operators?  Wassap?\n    # subsample the training set for performance\n    cap_pos = pos.shape[0]\n    cap_neg = neg.shape[0]\n    pos = pos[torch.rand_like(pos) < SUB_SAMPLE_SIZE/cap_pos]\n    neg = neg[torch.rand_like(neg) < SUB_SAMPLE_SIZE/cap_neg]\n    ln_pos = pos.shape[0]\n    ln_neg = neg.shape[0]\n    pos_expand = pos.view(-1,1).expand(-1,ln_neg).reshape(-1)\n    neg_expand = neg.repeat(ln_pos)\n    diff = neg_expand - pos_expand\n    ln_All = diff.shape[0]\n    Lp = diff[diff>0] # because we're taking positive diffs, we got pos and neg flipped.\n    ln_Lp = Lp.shape[0]-1\n    diff_neg = -1.0 * diff[diff<0]\n    diff_neg = diff_neg.sort()[0]\n    ln_neg = diff_neg.shape[0]-1\n    ln_neg = max([ln_neg, 0])\n    left_wing = int(ln_Lp*DELTA)\n    left_wing = max([0,left_wing])\n    left_wing = min([ln_neg,left_wing])\n    default_gamma=torch.tensor(0.2, dtype=torch.float).cuda()\n    if diff_neg.shape[0] > 0 :\n       gamma = diff_neg[left_wing]\n    else:\n       gamma = default_gamma # default=torch.tensor(0.2, dtype=torch.float).cuda() #zoink\n    L1 = diff[diff>-1.0*gamma]\n    ln_L1 = L1.shape[0]\n    if epoch > -1 :\n        return gamma\n    else :\n        return default_gamma","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-04T03:24:31.491125Z","iopub.execute_input":"2024-12-04T03:24:31.491637Z","iopub.status.idle":"2024-12-04T03:24:31.503855Z","shell.execute_reply.started":"2024-12-04T03:24:31.491580Z","shell.execute_reply":"2024-12-04T03:24:31.502863Z"}},"outputs":[],"execution_count":347},{"cell_type":"code","source":"custom_train_loader = CustomDataLoader(x_scaled, y_encoded, validation_size=0.2, random_state=0, classification=True)\nprint(custom_train_loader.train_data_tensor.shape)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-04T03:24:31.762480Z","iopub.execute_input":"2024-12-04T03:24:31.762858Z","iopub.status.idle":"2024-12-04T03:24:31.810905Z","shell.execute_reply.started":"2024-12-04T03:24:31.762827Z","shell.execute_reply":"2024-12-04T03:24:31.809959Z"}},"outputs":[{"name":"stdout","text":"torch.Size([72980, 14])\n","output_type":"stream"}],"execution_count":348},{"cell_type":"code","source":"interaction_order = 14\nx = custom_train_loader.train_data_tensor\nbatch_size = x.size(0)\nnum_features = x.size(1)\n\ncomb_indices = list(itertools.combinations(range(num_features), interaction_order))\ncomb_indices = torch.tensor(comb_indices, dtype=torch.long)\ncorr_comb_indices = comb_indices[0]\n\nprint(corr_comb_indices.shape)\nprint(corr_comb_indices)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-04T03:24:32.053204Z","iopub.execute_input":"2024-12-04T03:24:32.053624Z","iopub.status.idle":"2024-12-04T03:24:32.060583Z","shell.execute_reply.started":"2024-12-04T03:24:32.053591Z","shell.execute_reply":"2024-12-04T03:24:32.059519Z"}},"outputs":[{"name":"stdout","text":"torch.Size([14])\ntensor([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13])\n","output_type":"stream"}],"execution_count":349},{"cell_type":"code","source":"models = []","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-04T03:24:32.401155Z","iopub.execute_input":"2024-12-04T03:24:32.401491Z","iopub.status.idle":"2024-12-04T03:24:32.405852Z","shell.execute_reply.started":"2024-12-04T03:24:32.401462Z","shell.execute_reply":"2024-12-04T03:24:32.404946Z"}},"outputs":[],"execution_count":350},{"cell_type":"code","source":"feature_tensor = torch.empty((custom_train_loader.train_data_tensor.size(0), 0))\nprint(feature_tensor.shape)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-04T03:24:32.795165Z","iopub.execute_input":"2024-12-04T03:24:32.795546Z","iopub.status.idle":"2024-12-04T03:24:32.800892Z","shell.execute_reply.started":"2024-12-04T03:24:32.795485Z","shell.execute_reply":"2024-12-04T03:24:32.799943Z"}},"outputs":[{"name":"stdout","text":"torch.Size([72980, 0])\n","output_type":"stream"}],"execution_count":351},{"cell_type":"code","source":"num_og_features = 14\nnum_features = interaction_order\nnum_classes = 2\nnum_control_points = 15\nnum_epochs = 1000\nbatch_size = 7298","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-04T03:24:33.168960Z","iopub.execute_input":"2024-12-04T03:24:33.169323Z","iopub.status.idle":"2024-12-04T03:24:33.174319Z","shell.execute_reply.started":"2024-12-04T03:24:33.169291Z","shell.execute_reply":"2024-12-04T03:24:33.173356Z"}},"outputs":[],"execution_count":352},{"cell_type":"code","source":"torch.cuda.empty_cache()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-04T03:24:33.523377Z","iopub.execute_input":"2024-12-04T03:24:33.523850Z","iopub.status.idle":"2024-12-04T03:24:33.556246Z","shell.execute_reply.started":"2024-12-04T03:24:33.523806Z","shell.execute_reply":"2024-12-04T03:24:33.555197Z"}},"outputs":[],"execution_count":353},{"cell_type":"code","source":"def custom_lr_lambda(step):\n    num_step_threshold = 100\n\n    if step < num_step_threshold:\n        return step / num_step_threshold\n    if step == num_step_threshold:\n        print(\"here\")\n    return 0.9999 ** (step - num_step_threshold)\n\nstart_time = time.time()\nmodel = TabularDenseNet(num_features, num_classes, num_control_points, 4, corr_comb_indices).to(device)\n\noptimizer = optim.Adam(model.parameters(), lr=0.001 * 5 * 0.00001 * 0.01)\nscheduler = LambdaLR(optimizer, lr_lambda=custom_lr_lambda)\n\ncriterion = CustomLoss(nn.CrossEntropyLoss(), 0.0, 0.0, 0.0001 * 0.0, 0.0001 * 0.0)\nevaluate_model(model, custom_train_loader, criterion, optimizer, num_epochs, scheduler, batch_size, num_og_features, early_stopping_patience=100)\n\nmodels.append(model)\nprint(f\"Execution time: {(time.time() - start_time):.6f} seconds\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-04T03:33:30.574632Z","iopub.execute_input":"2024-12-04T03:33:30.575419Z"}},"outputs":[{"name":"stdout","text":"Learning Rate: 5.000000000000001e-11\nEpoch 1, Training Loss: 0.6931459903717041\nTraining Accuracy: 0.8064949301178405, Training F1 Score: 0.8122415151345942\nValidation Accuracy: 0.8089992327085389, Validation F1 Score: 0.8140481026508355\n\nhere\nLearning Rate: 4.995002249400106e-10\nEpoch 11, Training Loss: 4.507810402731849e-10\nTraining Accuracy: 0.8150178130994793, Training F1 Score: 0.7471134511891488\nValidation Accuracy: 0.8133837553436369, Validation F1 Score: 0.7457248172508865\n\nLearning Rate: 4.94529867378049e-10\nEpoch 21, Training Loss: 7.298563142743575e-10\nTraining Accuracy: 0.8188407782954235, Training F1 Score: 0.7482208659955182\nValidation Accuracy: 0.8182067302422449, Validation F1 Score: 0.7475940072891669\n\nLearning Rate: 4.896089681607693e-10\nEpoch 31, Training Loss: 1.5208984072145881e-09\nTraining Accuracy: 0.8176760756371608, Training F1 Score: 0.7474882286025458\nValidation Accuracy: 0.8176586649128577, Validation F1 Score: 0.7472214327206057\n\n","output_type":"stream"}],"execution_count":null},{"cell_type":"code","source":"for name, param in model.named_parameters():\n    print(name)\n    print(param)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-13T22:32:10.903045Z","iopub.execute_input":"2024-11-13T22:32:10.903746Z","iopub.status.idle":"2024-11-13T22:32:10.924583Z","shell.execute_reply.started":"2024-11-13T22:32:10.903706Z","shell.execute_reply":"2024-11-13T22:32:10.923709Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"num_og_features = 14\nnum_features = interaction_order\nnum_classes = 2\nnum_control_points = 15\nnum_epochs = 1000\nbatch_size = 7298\n\ndef custom_lr_lambda(step):\n    num_step_threshold = 100\n\n    if step < num_step_threshold:\n        return step / num_step_threshold\n    if step == num_step_threshold:\n        print(\"here\")\n    return 0.9999 ** (step - num_step_threshold)\n\nstart_time = time.time()\nmodel = TabularDenseNet(num_features, num_classes, num_control_points, 4, corr_comb_indices).to(device)\n\noptimizer = optim.Adam(model.parameters(), lr=0.01 * 0.02 * 10)\nscheduler = LambdaLR(optimizer, lr_lambda=custom_lr_lambda)\n\ncriterion = CustomLoss(nn.CrossEntropyLoss(), 0.0, 0.0, 0.0001 * 0.0, 0.0001 * 0.0)\nevaluate_model(model, custom_train_loader, criterion, optimizer, num_epochs, scheduler, batch_size, num_og_features, early_stopping_patience=100)\n\nmodels.append(model)\nprint(f\"Execution time: {(time.time() - start_time):.6f} seconds\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-13T21:48:22.897263Z","iopub.execute_input":"2024-11-13T21:48:22.89806Z","iopub.status.idle":"2024-11-13T21:49:16.711233Z","shell.execute_reply.started":"2024-11-13T21:48:22.89802Z","shell.execute_reply":"2024-11-13T21:49:16.709955Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"data = pd.read_csv('/kaggle/input/playground-series-s4e10/test.csv')\n\ndata = data.drop([\"id\"], axis=1)\ndata['source'] = 0\n\ngrade_mapping = {'A': 7, 'B': 6, 'C': 5, 'D': 4, 'E': 3, 'F': 2, 'G': 1}\ndata['loan_grade'] = data['loan_grade'].map(grade_mapping)\n\npurpose_mapping = {\n    'DEBTCONSOLIDATION': 1,\n    'HOMEIMPROVEMENT': 2,\n    'MEDICAL': 3,\n    'PERSONAL': 4,\n    'EDUCATION': 5,\n    'VENTURE': 6\n}\ndata['loan_intent'] = data['loan_intent'].map(purpose_mapping)\n\nhome_ownership_mapping = {\n    'OWN': 1,\n    'MORTGAGE': 2,\n    'OTHER': 3,\n    'RENT': 4\n}\ndata['person_home_ownership'] = data['person_home_ownership'].map(home_ownership_mapping)\n\nprint(data.columns)\nprint(data.isnull().sum())\n\nX = data.drop([], axis=1)\n\nX = pd.get_dummies(X, drop_first=True)\n\ncolumn_to_log = [\n    'person_age',\n    'person_income',\n]\n\ncolumn_to_sqrt = [\n    'person_emp_length',\n    'loan_percent_income',\n]\n\nfor col in column_to_log:\n    if (X[col] <= 0).any():\n        print(f\"Column '{col}' contains non-positive values. Adding 1 to avoid log of non-positive numbers.\")\n        X[col] = np.log(X[col] + 1)\n    else:\n        X[col] = np.log(X[col])\n\nfor col in column_to_sqrt:\n    if (X[col] < 0).any():\n        print(f\"Column '{col}' contains negative values. Setting negative values to NaN before applying sqrt.\")\n        X[col] = np.sqrt(X[col].clip(lower=0))\n    else:\n        X[col] = np.sqrt(X[col])\n\nprint(X.shape)\nprint(X.columns)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-12T05:47:47.512892Z","iopub.execute_input":"2024-11-12T05:47:47.513707Z","iopub.status.idle":"2024-11-12T05:47:47.636662Z","shell.execute_reply.started":"2024-11-12T05:47:47.513664Z","shell.execute_reply":"2024-11-12T05:47:47.635475Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(x_scaled)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-12T05:48:01.72192Z","iopub.execute_input":"2024-11-12T05:48:01.722289Z","iopub.status.idle":"2024-11-12T05:48:01.728672Z","shell.execute_reply.started":"2024-11-12T05:48:01.722256Z","shell.execute_reply":"2024-11-12T05:48:01.727476Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(X.shape)\nX_scaled_test = x_scaler.transform(X)\nprint(X_scaled_test.shape)\nprint(X_scaled_test)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-12T05:48:11.357669Z","iopub.execute_input":"2024-11-12T05:48:11.358046Z","iopub.status.idle":"2024-11-12T05:48:11.392927Z","shell.execute_reply.started":"2024-11-12T05:48:11.358011Z","shell.execute_reply":"2024-11-12T05:48:11.391959Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"X_scaled_test_tensor = torch.tensor(X_scaled_test).float().to(device)\noutputs = models[-1](X_scaled_test_tensor)\nprint(outputs)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-11T21:19:52.781054Z","iopub.execute_input":"2024-11-11T21:19:52.781762Z","iopub.status.idle":"2024-11-11T21:19:52.864006Z","shell.execute_reply.started":"2024-11-11T21:19:52.781722Z","shell.execute_reply":"2024-11-11T21:19:52.863029Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"probabilities = F.softmax(outputs, dim=1)\nprint(probabilities)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-11T21:19:53.638494Z","iopub.execute_input":"2024-11-11T21:19:53.639135Z","iopub.status.idle":"2024-11-11T21:19:53.647569Z","shell.execute_reply.started":"2024-11-11T21:19:53.639093Z","shell.execute_reply":"2024-11-11T21:19:53.646456Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"positive_class_probs = probabilities[:, 1]\nprint(positive_class_probs)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-11T21:20:12.694515Z","iopub.execute_input":"2024-11-11T21:20:12.695268Z","iopub.status.idle":"2024-11-11T21:20:12.70197Z","shell.execute_reply.started":"2024-11-11T21:20:12.695225Z","shell.execute_reply":"2024-11-11T21:20:12.700992Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import pandas as pd\n\ntest_df = pd.read_csv('/kaggle/input/playground-series-s4e10/test.csv')\nids = test_df['id']\n\npositive_class_probs = positive_class_probs.cpu().detach().numpy()\n\nsubmission_df = pd.DataFrame({\n    'id': ids,\n    'loan_status': positive_class_probs\n})\n\nsubmission_df.to_csv('submission.csv', index=False)\nprint(\"Submission file created successfully.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-11T21:20:17.211548Z","iopub.execute_input":"2024-11-11T21:20:17.21195Z","iopub.status.idle":"2024-11-11T21:20:17.359435Z","shell.execute_reply.started":"2024-11-11T21:20:17.211911Z","shell.execute_reply":"2024-11-11T21:20:17.358447Z"}},"outputs":[],"execution_count":null}]}