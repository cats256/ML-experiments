{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":84894,"databundleVersionId":9709193,"sourceType":"competition"},{"sourceId":408,"sourceType":"datasetVersion","datasetId":180},{"sourceId":668,"sourceType":"datasetVersion","datasetId":308},{"sourceId":7949759,"sourceType":"datasetVersion","datasetId":4675026},{"sourceId":9738619,"sourceType":"datasetVersion","datasetId":5960716}],"dockerImageVersionId":30787,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import sys\nimport time\nimport math\nimport itertools\n\nimport numpy as np\nimport pandas as pd\n\nimport matplotlib.pyplot as plt\nfrom scipy.special import logit\nfrom scipy.stats import norm\n\nimport tensorflow as tf\nfrom keras import layers, models, datasets\n\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport torch.nn.functional as F\nfrom torch.utils.data import TensorDataset, DataLoader\nfrom torch.optim.lr_scheduler import StepLR, LambdaLR\nimport torch.autograd.profiler as profiler\n\nfrom sklearn.model_selection import train_test_split, LeaveOneOut, StratifiedKFold, cross_val_predict\nfrom sklearn.preprocessing import StandardScaler, MinMaxScaler, LabelEncoder, PowerTransformer\nfrom sklearn.metrics import f1_score, log_loss, accuracy_score\nfrom sklearn.linear_model import LogisticRegression\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(device)","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2024-12-15T00:31:22.017954Z","iopub.execute_input":"2024-12-15T00:31:22.018263Z","iopub.status.idle":"2024-12-15T00:31:36.873169Z","shell.execute_reply.started":"2024-12-15T00:31:22.018237Z","shell.execute_reply":"2024-12-15T00:31:36.872118Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def calculate_metrics(model, data_tensor, labels_tensor, batch_size=1024, num_features=22):\n    model.eval()\n    all_preds = []\n    all_labels = []\n\n    with torch.no_grad():\n        for start_idx in range(0, len(data_tensor), batch_size):\n            end_idx = min(start_idx + batch_size, len(data_tensor))\n            inputs = data_tensor[start_idx:end_idx].view(-1, num_features)\n            labels = labels_tensor[start_idx:end_idx]\n\n            outputs = model(inputs)\n            _, preds = torch.max(outputs, 1)\n            all_preds.extend(preds.cpu().numpy())\n            all_labels.extend(labels.cpu().numpy())\n\n    accuracy = accuracy_score(all_labels, all_preds)\n    f1 = f1_score(all_labels, all_preds, average='weighted')\n    return accuracy, f1","metadata":{"execution":{"iopub.status.busy":"2024-12-15T00:31:36.874691Z","iopub.execute_input":"2024-12-15T00:31:36.875251Z","iopub.status.idle":"2024-12-15T00:31:36.881821Z","shell.execute_reply.started":"2024-12-15T00:31:36.875203Z","shell.execute_reply":"2024-12-15T00:31:36.880817Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class CustomDataLoader:\n    def __init__(self, features, labels, validation_size=0.2, random_state=42, classification=True):        \n        if validation_size > 0.0:\n            stratify = labels if classification else None\n            train_data, val_data, train_labels, val_labels = train_test_split(\n                features, labels, test_size=validation_size, stratify=stratify, random_state=random_state\n            )\n            \n            self.val_data_tensor = torch.tensor(val_data).float().to(device)\n            \n            if classification:\n                self.val_labels_tensor = torch.tensor(val_labels).long().to(device)\n\n            else:\n                self.val_labels_tensor =torch.tensor(val_labels).float().to(device)\n        else:\n            train_data, train_labels = features, labels\n            self.val_data_tensor, self.val_labels_tensor = None, None\n        \n        self.train_data_tensor = torch.tensor(train_data).float().to(device)\n\n        if classification:\n            self.train_labels_tensor = torch.tensor(train_labels).long().to(device)\n        else:\n            self.train_labels_tensor = torch.tensor(train_labels).float().to(device)\n\n        torch.manual_seed(random_state)\n        indices = torch.randperm(len(self.train_data_tensor))\n\n        self.train_data_tensor = self.train_data_tensor[indices]\n        self.train_labels_tensor = self.train_labels_tensor[indices]","metadata":{"execution":{"iopub.status.busy":"2024-12-15T00:31:36.884750Z","iopub.execute_input":"2024-12-15T00:31:36.885180Z","iopub.status.idle":"2024-12-15T00:31:36.910933Z","shell.execute_reply.started":"2024-12-15T00:31:36.885133Z","shell.execute_reply":"2024-12-15T00:31:36.910234Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def evaluate_model(model, custom_train_loader, criterion, optimizer, num_epochs, scheduler, batch_size=1024, num_features=22, early_stopping_patience=10):\n    best_val_loss = float('inf')\n    best_epoch = 0\n    patience_counter = 0\n    \n\n    for epoch in range(num_epochs):\n        running_loss = 0.0\n        model.train()\n        i = 0\n        total_loss = 0\n        num_items = 0\n\n        for start_idx in range(0, len(custom_train_loader.train_data_tensor), batch_size):\n            end_idx = min(start_idx + batch_size, len(custom_train_loader.train_data_tensor))\n            inputs = custom_train_loader.train_data_tensor[start_idx:end_idx].view(-1, num_features)\n            labels = custom_train_loader.train_labels_tensor[start_idx:end_idx]\n\n            optimizer.zero_grad()\n            outputs = model(inputs)\n            loss = criterion(outputs, labels, model)\n            loss.backward()\n            optimizer.step()\n            scheduler.step()\n            running_loss += loss.item() * len(labels)\n            total_loss += loss.item() * len(labels)\n            num_items += len(labels)\n\n            i += 1\n\n        if epoch % 10 == 0:\n            model.eval()\n            for param_group in optimizer.param_groups:\n                print(\"Learning Rate:\", param_group['lr'])\n\n            train_reg_loss = 0.0\n            val_loss = 0.0\n            with torch.no_grad():\n                for start_idx in range(0, len(custom_train_loader.train_data_tensor), batch_size):\n                    end_idx = min(start_idx + batch_size, len(custom_train_loader.train_data_tensor))\n                    inputs = custom_train_loader.train_data_tensor[start_idx:end_idx].view(-1, num_features)\n                    labels = custom_train_loader.train_labels_tensor[start_idx:end_idx]\n        \n                    outputs = model(inputs)\n                    train_reg_loss += criterion.regular_loss(outputs, labels).item() * len(labels)\n\n                for start_idx in range(0, len(custom_train_loader.val_data_tensor), batch_size):\n                    end_idx = min(start_idx + batch_size, len(custom_train_loader.val_data_tensor))\n                    val_inputs = custom_train_loader.val_data_tensor[start_idx:end_idx].view(-1, num_features)\n                    val_labels = custom_train_loader.val_labels_tensor[start_idx:end_idx]\n    \n                    val_outputs = model(val_inputs)\n                    val_loss += criterion.regular_loss(val_outputs, val_labels).item() * len(val_labels)\n    \n            avg_train_loss = running_loss / len(custom_train_loader.train_data_tensor)\n            avg_val_loss = val_loss / len(custom_train_loader.val_data_tensor)\n    \n            train_accuracy, train_f1 = calculate_metrics(model, custom_train_loader.train_data_tensor, custom_train_loader.train_labels_tensor, batch_size, num_features)\n            val_accuracy, val_f1 = calculate_metrics(model, custom_train_loader.val_data_tensor, custom_train_loader.val_labels_tensor, batch_size, num_features)\n    \n            print(f'Epoch {epoch + 1}, Training Loss: {avg_train_loss}, Validation Loss: {avg_val_loss}')\n            print(f'Epoch {epoch + 1}, Training Loss: {train_reg_loss / len(custom_train_loader.train_data_tensor)}, Validation Loss: {avg_val_loss}')\n            print(f'Training Accuracy: {train_accuracy}, Training F1 Score: {train_f1}')\n            print(f'Validation Accuracy: {val_accuracy}, Validation F1 Score: {val_f1}')\n            print()\n            \n            if avg_val_loss < best_val_loss:\n                best_val_loss = avg_val_loss\n                best_epoch = epoch + 1\n                patience_counter = 0\n            else:\n                patience_counter += 10\n                if patience_counter >= early_stopping_patience:\n                    print(f'Early stopping triggered after {epoch + 1} epochs.')\n                    print(f'Best Validation Loss: {best_val_loss} from Epoch {best_epoch}')\n                    break\n\n    if patience_counter < early_stopping_patience:\n        print(f'Best Validation Loss after {num_epochs} epochs: {best_val_loss} from Epoch {best_epoch}')","metadata":{"execution":{"iopub.status.busy":"2024-12-15T00:31:39.991110Z","iopub.execute_input":"2024-12-15T00:31:39.991453Z","iopub.status.idle":"2024-12-15T00:31:40.003757Z","shell.execute_reply.started":"2024-12-15T00:31:39.991423Z","shell.execute_reply":"2024-12-15T00:31:40.002885Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"data_dl = pd.read_csv('/kaggle/input/playground-series-s4e10/train.csv')\ndata_og = pd.read_csv('/kaggle/input/loan-approval-prediction/credit_risk_dataset.csv')\n\ndata_dl = data_dl.drop([\"id\"], axis=1)\n\nmedian_emp_length = data_og['person_emp_length'].median()\nmedian_int_rate = data_og['loan_int_rate'].median()\n\ndata_dl['source'] = 0\ndata_og['source'] = 1\n\ndata = pd.concat([data_dl, data_og], ignore_index=True)\n\ndata['person_emp_length_missing'] = data['person_emp_length'].isna().astype(int)\ndata['loan_int_rate_missing'] = data['loan_int_rate'].isna().astype(int)\n\ndata['person_emp_length'] = data['person_emp_length'].fillna(median_emp_length)\ndata['loan_int_rate'] = data['loan_int_rate'].fillna(median_int_rate)\n\ngrade_mapping = {'A': 7, 'B': 6, 'C': 5, 'D': 4, 'E': 3, 'F': 2, 'G': 1}\ndata['loan_grade'] = data['loan_grade'].map(grade_mapping)\n\npurpose_mapping = {\n    'DEBTCONSOLIDATION': 1,\n    'HOMEIMPROVEMENT': 2,\n    'MEDICAL': 3,\n    'PERSONAL': 4,\n    'EDUCATION': 5,\n    'VENTURE': 6\n}\ndata['loan_intent'] = data['loan_intent'].map(purpose_mapping)\n\nhome_ownership_mapping = {\n    'OWN': 1,\n    'MORTGAGE': 2,\n    'OTHER': 3,\n    'RENT': 4\n}\ndata['person_home_ownership'] = data['person_home_ownership'].map(home_ownership_mapping)\n\nX = data.drop([\"loan_status\"], axis=1)\nX = pd.get_dummies(X, drop_first=True)\ny = data[\"loan_status\"]\n\ncolumn_to_log = [\n    'person_age',\n    'person_income',\n]\n\ncolumn_to_sqrt = [\n    'person_emp_length',\n    'loan_percent_income',\n]\n\nfor col in column_to_log:\n    if (X[col] <= 0).any():\n        print(f\"Column '{col}' contains non-positive values. Adding 1 to avoid log of non-positive numbers.\")\n        X[col] = np.log(X[col] + 1)\n    else:\n        X[col] = np.log(X[col])\n\nfor col in column_to_sqrt:\n    if (X[col] < 0).any():\n        print(f\"Column '{col}' contains negative values. Setting negative values to NaN before applying sqrt.\")\n        X[col] = np.sqrt(X[col].clip(lower=0))\n    else:\n        X[col] = np.sqrt(X[col])\n\nprint(data.isnull().sum())\nprint(X.columns)\nprint(X.shape, y.shape)\nprint(X.columns.get_loc('source'))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-15T00:32:11.709668Z","iopub.execute_input":"2024-12-15T00:32:11.710277Z","iopub.status.idle":"2024-12-15T00:32:12.005836Z","shell.execute_reply.started":"2024-12-15T00:32:11.710242Z","shell.execute_reply":"2024-12-15T00:32:12.004928Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"x_scaler = StandardScaler()\nx_scaled = x_scaler.fit_transform(X)\n\nlabel_encoder = LabelEncoder()\ny_encoded = label_encoder.fit_transform(y)\n\nprint(x_scaled.shape)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-15T00:32:12.540946Z","iopub.execute_input":"2024-12-15T00:32:12.541297Z","iopub.status.idle":"2024-12-15T00:32:12.624922Z","shell.execute_reply.started":"2024-12-15T00:32:12.541267Z","shell.execute_reply":"2024-12-15T00:32:12.623933Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"feature_means = x_scaled.mean(axis=0)\nfeature_variances = x_scaled.var(axis=0)\nfeature_mins = x_scaled.min(axis=0)\nfeature_maxs = x_scaled.max(axis=0)\n\nfeature_stats_scaled_full = pd.DataFrame({\n    'Mean': feature_means,\n    'Variance': feature_variances,\n    'Min': feature_mins,\n    'Max': feature_maxs\n})\n\nprint(\"Mean, Variance, Min, and Max of Scaled Features:\")\nprint(feature_stats_scaled_full)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-15T00:32:13.490462Z","iopub.execute_input":"2024-12-15T00:32:13.490766Z","iopub.status.idle":"2024-12-15T00:32:13.507980Z","shell.execute_reply.started":"2024-12-15T00:32:13.490741Z","shell.execute_reply":"2024-12-15T00:32:13.507138Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class CustomLoss(nn.Module):\n    def __init__(self, criterion, f1_lambda, f2_lambda, l1_lambda, l2_lambda, wa_lambda):\n        super(CustomLoss, self).__init__()\n        self.criterion = criterion\n        self.f1_lambda = f1_lambda\n        self.f2_lambda = f2_lambda\n        self.l1_lambda = l1_lambda\n        self.l2_lambda = l2_lambda\n        self.wa_lambda = wa_lambda\n        self.i = 0\n\n    def forward(self, outputs, labels, model): \n        f1_loss = 0.0\n        f2_loss = 0.0\n        l1_loss = 0.0\n        l2_loss = 0.0\n\n        for name, module in model.named_modules():\n            if isinstance(module, CustomActivation):\n                f1_loss += (module.a ** 2).sum() + (module.b ** 2).sum()\n                f2_loss += ((module.a - module.b) ** 2).sum()\n\n            if isinstance(module, nn.Linear):\n                l1_loss += torch.norm(module.weight, 1)\n                l2_loss += torch.norm(module.weight, 2) ** 2\n\n        total_loss = (self.criterion(outputs, labels)\n                      + self.f1_lambda * f1_loss\n                      + self.f2_lambda * f2_loss\n                      + self.l1_lambda * l1_loss\n                      + self.l2_lambda * l2_loss)\n        self.i += 1\n\n        return total_loss\n\n    def compute_gradient_magnitude(self, model):\n        total_abs_sum = 0.0\n        for param in model.parameters():\n            if param.grad is not None:\n                total_abs_sum += param.grad.abs().sum().item()\n        self.grad_magnitude = total_abs_sum\n\n    def regular_loss(self, outputs, labels):\n        return self.criterion(outputs, labels)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-15T00:32:16.474443Z","iopub.execute_input":"2024-12-15T00:32:16.475189Z","iopub.status.idle":"2024-12-15T00:32:16.482937Z","shell.execute_reply.started":"2024-12-15T00:32:16.475157Z","shell.execute_reply":"2024-12-15T00:32:16.482097Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class CustomActivation(nn.Module):\n    def __init__(self, num_features, buffer_size=10, num_control_points=9, init_identity=True):\n        super(CustomActivation, self).__init__()\n        self.i = 0\n        self.buffer_size = buffer_size\n        \n        self.input_buffer = [torch.tensor(0) for _ in range(buffer_size)]\n        self.quantiles = nn.Parameter(torch.linspace(0, 1, num_control_points + 2)[1:-1], requires_grad=False)\n        \n        self.a = nn.Parameter(torch.zeros(num_features, num_control_points))\n        self.b = nn.Parameter(torch.zeros(num_features, num_control_points))\n        \n        self.global_bias = nn.Parameter(torch.zeros(1, num_features))\n                \n        with torch.no_grad():\n            if init_identity:\n                middle_index = num_control_points // 2\n                self.a[:, middle_index] = 1.0\n                self.b[:, middle_index] = 1.0\n\n    def forward(self, x):\n        if self.training:\n            index = self.i % self.buffer_size\n            self.input_buffer[index - 1] = self.input_buffer[index - 1].detach()            \n            self.input_buffer[index] = x\n            \n            all_inputs = torch.cat(self.input_buffer[:min(self.i + 1, self.buffer_size)], dim=0)\n            quantiles_values = torch.quantile(all_inputs, self.quantiles, dim=0)\n            self.local_bias = quantiles_values.transpose(0, 1)\n            \n            self.i += 1\n                \n        x = x.unsqueeze(-1) + self.local_bias\n        x = torch.where(x < 0, self.a * x, self.b * x)\n        x = x.sum(dim=-1) + self.global_bias            \n        return x","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-15T01:47:10.802376Z","iopub.execute_input":"2024-12-15T01:47:10.803227Z","iopub.status.idle":"2024-12-15T01:47:10.815146Z","shell.execute_reply.started":"2024-12-15T01:47:10.803180Z","shell.execute_reply":"2024-12-15T01:47:10.814188Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class CustomLinear(nn.Module):\n    def __init__(self, num_features, num_outputs, init_identity=False):\n        super(CustomLinear, self).__init__()\n        \n        if init_identity and num_features != num_outputs:\n            raise ValueError(\"For identity initialization, num_features must equal num_outputs.\")\n\n        self.linear = nn.Linear(num_features, num_outputs, bias=True)\n        \n        with torch.no_grad():\n            self.linear.bias.zero_()\n\n            if init_identity:\n                self.linear.weight.copy_(torch.eye(num_features, num_outputs))\n            else:\n                self.linear.weight.zero_()\n\n    def forward(self, x):\n        return self.linear(x)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-15T01:47:11.017527Z","iopub.execute_input":"2024-12-15T01:47:11.017885Z","iopub.status.idle":"2024-12-15T01:47:11.023781Z","shell.execute_reply.started":"2024-12-15T01:47:11.017853Z","shell.execute_reply":"2024-12-15T01:47:11.022932Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class TabularDenseNet(nn.Module):\n    def __init__(self, input_size, output_size, num_control_points, num_layers, window_size):\n        super(TabularDenseNet, self).__init__()\n        self.layers = nn.ModuleList()\n        \n        if num_layers % 2 == 1:\n            self.layers.append(CustomLinear(input_size, input_size, init_identity=True))\n            # self.layers.append(CustomActivation(input_size, window_size, num_control_points, init_identity=True))\n            num_layers -= 1\n            input_size *= 2\n            \n        for i in range(num_layers):\n            if i % 2 == 0:\n                self.layers.append(CustomLinear(input_size, input_size, init_identity=True))\n            else:\n                # self.layers.append(CustomActivation(input_size, window_size, num_control_points, init_identity=True))\n                self.layers.append(CustomLinear(input_size, input_size, init_identity=True))\n\n            input_size *= 2\n\n        self.final = CustomLinear(input_size, output_size, init_identity=False)\n        self.final_act = CustomActivation(output_size, window_size, num_control_points, init_identity=True)\n        \n    def forward(self, x):\n        outputs = [x]\n\n        for layer in self.layers:\n            concatenated_outputs = torch.cat(outputs, dim=-1)\n            outputs.append(F.relu(layer(concatenated_outputs)))\n\n        concatenated_outputs = torch.cat(outputs, dim=-1)\n        x = self.final(concatenated_outputs)\n        x = self.final_act(x)\n        return x","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-15T01:47:18.861975Z","iopub.execute_input":"2024-12-15T01:47:18.862308Z","iopub.status.idle":"2024-12-15T01:47:18.869382Z","shell.execute_reply.started":"2024-12-15T01:47:18.862280Z","shell.execute_reply":"2024-12-15T01:47:18.868515Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"custom_train_loader = CustomDataLoader(x_scaled, y_encoded, validation_size=0.2, random_state=0, classification=True)\nprint(custom_train_loader.train_data_tensor.shape)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-15T01:47:19.369458Z","iopub.execute_input":"2024-12-15T01:47:19.370338Z","iopub.status.idle":"2024-12-15T01:47:19.420720Z","shell.execute_reply.started":"2024-12-15T01:47:19.370304Z","shell.execute_reply":"2024-12-15T01:47:19.419931Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"models = []","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-15T01:47:20.533538Z","iopub.execute_input":"2024-12-15T01:47:20.534097Z","iopub.status.idle":"2024-12-15T01:47:20.537823Z","shell.execute_reply.started":"2024-12-15T01:47:20.534037Z","shell.execute_reply":"2024-12-15T01:47:20.536922Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"feature_tensor = torch.empty((custom_train_loader.train_data_tensor.size(0), 0))\nprint(feature_tensor.shape)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-15T01:47:20.761410Z","iopub.execute_input":"2024-12-15T01:47:20.762246Z","iopub.status.idle":"2024-12-15T01:47:20.766741Z","shell.execute_reply.started":"2024-12-15T01:47:20.762212Z","shell.execute_reply":"2024-12-15T01:47:20.765874Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"num_features = 14\nnum_classes = 2\nnum_control_points = 21\nnum_epochs = 10000\nbatch_size = 7298 * 1","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-15T01:47:21.061545Z","iopub.execute_input":"2024-12-15T01:47:21.061905Z","iopub.status.idle":"2024-12-15T01:47:21.065996Z","shell.execute_reply.started":"2024-12-15T01:47:21.061873Z","shell.execute_reply":"2024-12-15T01:47:21.065119Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"torch.cuda.empty_cache()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-15T01:47:21.397351Z","iopub.execute_input":"2024-12-15T01:47:21.398001Z","iopub.status.idle":"2024-12-15T01:47:21.402255Z","shell.execute_reply.started":"2024-12-15T01:47:21.397969Z","shell.execute_reply":"2024-12-15T01:47:21.401321Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def custom_lr_lambda(step):\n    num_step_threshold = 100\n\n    if step < num_step_threshold:\n        return step / num_step_threshold\n    if step == num_step_threshold:\n        print(\"here\")\n    return 0.99995 ** (step - num_step_threshold)\n\nstart_time = time.time()\nmodel = TabularDenseNet(num_features, num_classes, num_control_points, 11, 10).to(device)\n\noptimizer = optim.Adam(model.parameters(), lr=0.001 * 1.0)\nscheduler = LambdaLR(optimizer, lr_lambda=custom_lr_lambda)\n\ncriterion = CustomLoss(nn.CrossEntropyLoss(), 0.0, 0.0, 0.0, 0.0, 0.0)\nevaluate_model(model, custom_train_loader, criterion, optimizer, 10000, scheduler, batch_size, num_features, early_stopping_patience=10000)\n\nmodels.append(model)\nprint(f\"Execution time: {(time.time() - start_time):.6f} seconds\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-15T02:00:29.050277Z","iopub.execute_input":"2024-12-15T02:00:29.050616Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(len(model.final.linear.weight[0]))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-15T01:59:10.585459Z","iopub.execute_input":"2024-12-15T01:59:10.586273Z","iopub.status.idle":"2024-12-15T01:59:10.591049Z","shell.execute_reply.started":"2024-12-15T01:59:10.586242Z","shell.execute_reply":"2024-12-15T01:59:10.590011Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import matplotlib.pyplot as plt\n\ntensor_data = model.final.linear.weight[0]\n\ntensor_data_numpy = tensor_data.detach().cpu().numpy()\n\nplt.hist(tensor_data_numpy, bins=100, alpha=0.75)\nplt.title(\"Histogram of Tensor Values\")\nplt.xlabel(\"Value\")\nplt.ylabel(\"Frequency\")\nplt.grid(True)\nplt.show()\n","metadata":{"trusted":true,"scrolled":true,"execution":{"iopub.status.busy":"2024-12-15T01:59:13.306445Z","iopub.execute_input":"2024-12-15T01:59:13.306790Z","iopub.status.idle":"2024-12-15T01:59:13.645964Z","shell.execute_reply.started":"2024-12-15T01:59:13.306759Z","shell.execute_reply":"2024-12-15T01:59:13.645277Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy.stats import kstest, norm, laplace\n\ntensor_data = model.final.linear.weight[0]\ndata = tensor_data.detach().cpu().numpy()\n\nnormal_test = kstest(data, 'norm', args=(np.mean(data), np.std(data)))\nlaplace_test = kstest(data, 'laplace', args=(np.mean(data), np.std(data)))\n\nprint(\"Normal distribution p-value:\", normal_test.pvalue)\nprint(\"Laplacian distribution p-value:\", laplace_test.pvalue)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-15T01:59:28.979347Z","iopub.execute_input":"2024-12-15T01:59:28.979715Z","iopub.status.idle":"2024-12-15T01:59:28.995188Z","shell.execute_reply.started":"2024-12-15T01:59:28.979682Z","shell.execute_reply":"2024-12-15T01:59:28.994131Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def custom_lr_lambda(step):\n    num_step_threshold = 100\n\n    if step < num_step_threshold:\n        return step / num_step_threshold\n    if step == num_step_threshold:\n        print(\"here\")\n    return 0.99995 ** (step - num_step_threshold)\n\nstart_time = time.time()\nmodel = TabularDenseNet(num_features, num_classes, num_control_points, 3, 10).to(device)\n\noptimizer = optim.Adam(model.parameters(), lr=0.001 * 1.0)\nscheduler = LambdaLR(optimizer, lr_lambda=custom_lr_lambda)\n\ncriterion = CustomLoss(nn.CrossEntropyLoss(), 0.0, 0.0, 0.0, 0.0, 0.0)\nevaluate_model(model, custom_train_loader, criterion, optimizer, 10000, scheduler, batch_size, num_features, early_stopping_patience=10000)\n\nmodels.append(model)\nprint(f\"Execution time: {(time.time() - start_time):.6f} seconds\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-12T21:56:58.231347Z","iopub.execute_input":"2024-12-12T21:56:58.231696Z","iopub.status.idle":"2024-12-12T22:00:02.143567Z","shell.execute_reply.started":"2024-12-12T21:56:58.231664Z","shell.execute_reply":"2024-12-12T22:00:02.141920Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def custom_lr_lambda(step):\n    num_step_threshold = 100\n\n    if step < num_step_threshold:\n        return step / num_step_threshold\n    if step == num_step_threshold:\n        print(\"here\")\n    return 0.99995 ** (step - num_step_threshold)\n\nstart_time = time.time()\nmodel = TabularDenseNet(num_features, num_classes, num_control_points, 3, corr_comb_indices).to(device)\n\noptimizer = optim.Adam(model.parameters(), lr=0.001 * 1.0)\nscheduler = LambdaLR(optimizer, lr_lambda=custom_lr_lambda)\n\ncriterion = CustomLoss(nn.CrossEntropyLoss(), 0.0, 0.0, 0.0, 0.0, 0.0)\nevaluate_model(model, custom_train_loader, criterion, optimizer, 10000, scheduler, batch_size, num_og_features, early_stopping_patience=10000)\n\nmodels.append(model)\nprint(f\"Execution time: {(time.time() - start_time):.6f} seconds\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-10T19:03:15.111423Z","iopub.execute_input":"2024-12-10T19:03:15.112269Z","iopub.status.idle":"2024-12-10T19:05:48.226504Z","shell.execute_reply.started":"2024-12-10T19:03:15.112236Z","shell.execute_reply":"2024-12-10T19:05:48.225289Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def custom_lr_lambda(step):\n    num_step_threshold = 100\n\n    if step < num_step_threshold:\n        return step / num_step_threshold\n    if step == num_step_threshold:\n        print(\"here\")\n    return 0.99995 ** (step - num_step_threshold)\n\nstart_time = time.time()\nmodel = TabularDenseNet(num_features, num_classes, num_control_points, 3, corr_comb_indices).to(device)\n\noptimizer = optim.Adam(model.parameters(), lr=0.001 * 1.0)\nscheduler = LambdaLR(optimizer, lr_lambda=custom_lr_lambda)\n\ncriterion = CustomLoss(nn.CrossEntropyLoss(), 0.0, 0.0, 0.0, 0.0, 0.0)\nevaluate_model(model, custom_train_loader, criterion, optimizer, 10000, scheduler, batch_size, num_og_features, early_stopping_patience=10000)\n\nmodels.append(model)\nprint(f\"Execution time: {(time.time() - start_time):.6f} seconds\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-10T18:53:05.805977Z","iopub.execute_input":"2024-12-10T18:53:05.806335Z","iopub.status.idle":"2024-12-10T19:02:13.313177Z","shell.execute_reply.started":"2024-12-10T18:53:05.806303Z","shell.execute_reply":"2024-12-10T19:02:13.311710Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def custom_lr_lambda(step):\n    num_step_threshold = 100\n\n    if step < num_step_threshold:\n        return step / num_step_threshold\n    if step == num_step_threshold:\n        print(\"here\")\n    return 0.99995 ** (step - num_step_threshold)\n\nstart_time = time.time()\nmodel = TabularDenseNet(num_features, num_classes, num_control_points, 3, corr_comb_indices).to(device)\n\noptimizer = optim.Adam(model.parameters(), lr=0.001 * 1.0)\nscheduler = LambdaLR(optimizer, lr_lambda=custom_lr_lambda)\n\ncriterion = CustomLoss(nn.CrossEntropyLoss(), 0.0, 0.0, 0.0, 0.0, 0.0)\nevaluate_model(model, custom_train_loader, criterion, optimizer, 10000, scheduler, batch_size, num_og_features, early_stopping_patience=10000)\n\nmodels.append(model)\nprint(f\"Execution time: {(time.time() - start_time):.6f} seconds\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-10T18:15:15.209191Z","iopub.execute_input":"2024-12-10T18:15:15.209537Z","iopub.status.idle":"2024-12-10T18:42:25.512105Z","shell.execute_reply.started":"2024-12-10T18:15:15.209505Z","shell.execute_reply":"2024-12-10T18:42:25.511164Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"data = pd.read_csv('/kaggle/input/playground-series-s4e10/test.csv')\n\ndata = data.drop([\"id\"], axis=1)\ndata['source'] = 0\n\ngrade_mapping = {'A': 7, 'B': 6, 'C': 5, 'D': 4, 'E': 3, 'F': 2, 'G': 1}\ndata['loan_grade'] = data['loan_grade'].map(grade_mapping)\n\npurpose_mapping = {\n    'DEBTCONSOLIDATION': 1,\n    'HOMEIMPROVEMENT': 2,\n    'MEDICAL': 3,\n    'PERSONAL': 4,\n    'EDUCATION': 5,\n    'VENTURE': 6\n}\ndata['loan_intent'] = data['loan_intent'].map(purpose_mapping)\n\nhome_ownership_mapping = {\n    'OWN': 1,\n    'MORTGAGE': 2,\n    'OTHER': 3,\n    'RENT': 4\n}\ndata['person_home_ownership'] = data['person_home_ownership'].map(home_ownership_mapping)\n\nprint(data.columns)\nprint(data.isnull().sum())\n\nX = data.drop([], axis=1)\n\nX = pd.get_dummies(X, drop_first=True)\n\ncolumn_to_log = [\n    'person_age',\n    'person_income',\n]\n\ncolumn_to_sqrt = [\n    'person_emp_length',\n    'loan_percent_income',\n]\n\nfor col in column_to_log:\n    if (X[col] <= 0).any():\n        print(f\"Column '{col}' contains non-positive values. Adding 1 to avoid log of non-positive numbers.\")\n        X[col] = np.log(X[col] + 1)\n    else:\n        X[col] = np.log(X[col])\n\nfor col in column_to_sqrt:\n    if (X[col] < 0).any():\n        print(f\"Column '{col}' contains negative values. Setting negative values to NaN before applying sqrt.\")\n        X[col] = np.sqrt(X[col].clip(lower=0))\n    else:\n        X[col] = np.sqrt(X[col])\n\nprint(X.shape)\nprint(X.columns)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-12T05:47:47.512892Z","iopub.execute_input":"2024-11-12T05:47:47.513707Z","iopub.status.idle":"2024-11-12T05:47:47.636662Z","shell.execute_reply.started":"2024-11-12T05:47:47.513664Z","shell.execute_reply":"2024-11-12T05:47:47.635475Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(x_scaled)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-12T05:48:01.72192Z","iopub.execute_input":"2024-11-12T05:48:01.722289Z","iopub.status.idle":"2024-11-12T05:48:01.728672Z","shell.execute_reply.started":"2024-11-12T05:48:01.722256Z","shell.execute_reply":"2024-11-12T05:48:01.727476Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(X.shape)\nX_scaled_test = x_scaler.transform(X)\nprint(X_scaled_test.shape)\nprint(X_scaled_test)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-12T05:48:11.357669Z","iopub.execute_input":"2024-11-12T05:48:11.358046Z","iopub.status.idle":"2024-11-12T05:48:11.392927Z","shell.execute_reply.started":"2024-11-12T05:48:11.358011Z","shell.execute_reply":"2024-11-12T05:48:11.391959Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"X_scaled_test_tensor = torch.tensor(X_scaled_test).float().to(device)\noutputs = models[-1](X_scaled_test_tensor)\nprint(outputs)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-11T21:19:52.781054Z","iopub.execute_input":"2024-11-11T21:19:52.781762Z","iopub.status.idle":"2024-11-11T21:19:52.864006Z","shell.execute_reply.started":"2024-11-11T21:19:52.781722Z","shell.execute_reply":"2024-11-11T21:19:52.863029Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"probabilities = F.softmax(outputs, dim=1)\nprint(probabilities)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-11T21:19:53.638494Z","iopub.execute_input":"2024-11-11T21:19:53.639135Z","iopub.status.idle":"2024-11-11T21:19:53.647569Z","shell.execute_reply.started":"2024-11-11T21:19:53.639093Z","shell.execute_reply":"2024-11-11T21:19:53.646456Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"positive_class_probs = probabilities[:, 1]\nprint(positive_class_probs)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-11T21:20:12.694515Z","iopub.execute_input":"2024-11-11T21:20:12.695268Z","iopub.status.idle":"2024-11-11T21:20:12.70197Z","shell.execute_reply.started":"2024-11-11T21:20:12.695225Z","shell.execute_reply":"2024-11-11T21:20:12.700992Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import pandas as pd\n\ntest_df = pd.read_csv('/kaggle/input/playground-series-s4e10/test.csv')\nids = test_df['id']\n\npositive_class_probs = positive_class_probs.cpu().detach().numpy()\n\nsubmission_df = pd.DataFrame({\n    'id': ids,\n    'loan_status': positive_class_probs\n})\n\nsubmission_df.to_csv('submission.csv', index=False)\nprint(\"Submission file created successfully.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-11T21:20:17.211548Z","iopub.execute_input":"2024-11-11T21:20:17.21195Z","iopub.status.idle":"2024-11-11T21:20:17.359435Z","shell.execute_reply.started":"2024-11-11T21:20:17.211911Z","shell.execute_reply":"2024-11-11T21:20:17.358447Z"}},"outputs":[],"execution_count":null}]}