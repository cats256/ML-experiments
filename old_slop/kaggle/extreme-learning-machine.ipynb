{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":84894,"databundleVersionId":9709193,"sourceType":"competition"},{"sourceId":408,"sourceType":"datasetVersion","datasetId":180},{"sourceId":668,"sourceType":"datasetVersion","datasetId":308},{"sourceId":7949759,"sourceType":"datasetVersion","datasetId":4675026},{"sourceId":9738619,"sourceType":"datasetVersion","datasetId":5960716}],"dockerImageVersionId":30787,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import sys\nimport time\nimport math\nimport itertools\n\nimport numpy as np\nimport pandas as pd\n\nimport matplotlib.pyplot as plt\nfrom scipy.special import logit\nfrom scipy.stats import norm\n\nimport tensorflow as tf\nfrom keras import layers, models, datasets\n\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport torch.nn.functional as F\nfrom torch.utils.data import TensorDataset, DataLoader\nfrom torchinfo import summary\nfrom torch.optim.lr_scheduler import StepLR, LambdaLR\nimport torch.autograd.profiler as profiler\n\nfrom sklearn.model_selection import train_test_split, LeaveOneOut, StratifiedKFold, cross_val_predict\nfrom sklearn.preprocessing import StandardScaler, MinMaxScaler, LabelEncoder, PowerTransformer\nfrom sklearn.metrics import f1_score, log_loss, accuracy_score\nfrom sklearn.linear_model import LogisticRegression\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(device)","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-12-14T03:20:09.930915Z","iopub.execute_input":"2024-12-14T03:20:09.931243Z","iopub.status.idle":"2024-12-14T03:20:26.125156Z","shell.execute_reply.started":"2024-12-14T03:20:09.931215Z","shell.execute_reply":"2024-12-14T03:20:26.124168Z"},"trusted":true},"outputs":[{"name":"stdout","text":"cuda\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"def calculate_metrics(model, data_tensor, labels_tensor, batch_size=1024, num_features=22):\n    model.eval()\n    all_preds = []\n    all_labels = []\n\n    with torch.no_grad():\n        for start_idx in range(0, len(data_tensor), batch_size):\n            end_idx = min(start_idx + batch_size, len(data_tensor))\n            inputs = data_tensor[start_idx:end_idx].view(-1, num_features)\n            labels = labels_tensor[start_idx:end_idx]\n\n            outputs = model(inputs)\n            _, preds = torch.max(outputs, 1)\n            all_preds.extend(preds.cpu().numpy())\n            all_labels.extend(labels.cpu().numpy())\n\n    accuracy = accuracy_score(all_labels, all_preds)\n    f1 = f1_score(all_labels, all_preds, average='weighted')\n    return accuracy, f1","metadata":{"execution":{"iopub.status.busy":"2024-12-14T03:20:26.126631Z","iopub.execute_input":"2024-12-14T03:20:26.127168Z","iopub.status.idle":"2024-12-14T03:20:26.133705Z","shell.execute_reply.started":"2024-12-14T03:20:26.127140Z","shell.execute_reply":"2024-12-14T03:20:26.132692Z"},"trusted":true},"outputs":[],"execution_count":2},{"cell_type":"code","source":"class CustomDataLoader:\n    def __init__(self, features, labels, validation_size=0.2, random_state=42, classification=True):        \n        if validation_size > 0.0:\n            stratify = labels if classification else None\n            train_data, val_data, train_labels, val_labels = train_test_split(\n                features, labels, test_size=validation_size, stratify=stratify, random_state=random_state\n            )\n            \n            self.val_data_tensor = torch.tensor(val_data).float().to(device)\n            \n            if classification:\n                self.val_labels_tensor = torch.tensor(val_labels).long().to(device)\n\n            else:\n                self.val_labels_tensor =torch.tensor(val_labels).float().to(device)\n        else:\n            train_data, train_labels = features, labels\n            self.val_data_tensor, self.val_labels_tensor = None, None\n        \n        self.train_data_tensor = torch.tensor(train_data).float().to(device)\n\n        if classification:\n            self.train_labels_tensor = torch.tensor(train_labels).long().to(device)\n        else:\n            self.train_labels_tensor = torch.tensor(train_labels).float().to(device)\n\n        torch.manual_seed(random_state)\n        indices = torch.randperm(len(self.train_data_tensor))\n\n        self.train_data_tensor = self.train_data_tensor[indices]\n        self.train_labels_tensor = self.train_labels_tensor[indices]","metadata":{"execution":{"iopub.status.busy":"2024-12-14T03:20:26.134622Z","iopub.execute_input":"2024-12-14T03:20:26.134911Z","iopub.status.idle":"2024-12-14T03:20:26.167029Z","shell.execute_reply.started":"2024-12-14T03:20:26.134886Z","shell.execute_reply":"2024-12-14T03:20:26.166054Z"},"trusted":true},"outputs":[],"execution_count":3},{"cell_type":"code","source":"def evaluate_model(model, custom_train_loader, criterion, optimizer, num_epochs, scheduler, batch_size=1024, num_features=22, early_stopping_patience=10):\n    best_val_loss = float('inf')\n    best_epoch = 0\n    patience_counter = 0\n    \n\n    for epoch in range(num_epochs):\n        running_loss = 0.0\n        model.train()\n        i = 0\n        total_loss = 0\n        num_items = 0\n\n        for start_idx in range(0, len(custom_train_loader.train_data_tensor), batch_size):\n            end_idx = min(start_idx + batch_size, len(custom_train_loader.train_data_tensor))\n            inputs = custom_train_loader.train_data_tensor[start_idx:end_idx].view(-1, num_features)\n            labels = custom_train_loader.train_labels_tensor[start_idx:end_idx]\n\n            optimizer.zero_grad()\n            outputs = model(inputs)\n            loss = criterion(outputs, labels, model)\n            loss.backward()\n            optimizer.step()\n            scheduler.step()\n            running_loss += loss.item() * len(labels)\n            total_loss += loss.item() * len(labels)\n            num_items += len(labels)\n\n            i += 1\n\n        if epoch % 100 == 0:\n            model.eval()\n            for param_group in optimizer.param_groups:\n                print(\"Learning Rate:\", param_group['lr'])\n\n            train_reg_loss = 0.0\n            val_loss = 0.0\n            with torch.no_grad():\n                for start_idx in range(0, len(custom_train_loader.train_data_tensor), batch_size):\n                    end_idx = min(start_idx + batch_size, len(custom_train_loader.train_data_tensor))\n                    inputs = custom_train_loader.train_data_tensor[start_idx:end_idx].view(-1, num_features)\n                    labels = custom_train_loader.train_labels_tensor[start_idx:end_idx]\n        \n                    outputs = model(inputs)\n                    train_reg_loss += criterion.regular_loss(outputs, labels).item() * len(labels)\n\n                val_iter = 0\n                for start_idx in range(0, len(custom_train_loader.val_data_tensor), batch_size):\n                    end_idx = min(start_idx + batch_size, len(custom_train_loader.val_data_tensor))\n                    val_inputs = custom_train_loader.val_data_tensor[start_idx:end_idx].view(-1, num_features)\n                    val_labels = custom_train_loader.val_labels_tensor[start_idx:end_idx]\n    \n                    val_outputs = model(val_inputs, val_iter)\n                    val_loss += criterion.regular_loss(val_outputs, val_labels).item() * len(val_labels)\n                    val_iter += 1\n    \n            avg_train_loss = running_loss / len(custom_train_loader.train_data_tensor)\n            avg_val_loss = val_loss / len(custom_train_loader.val_data_tensor)\n    \n            # train_accuracy, train_f1 = calculate_metrics(model, custom_train_loader.train_data_tensor, custom_train_loader.train_labels_tensor, batch_size, num_features)\n            # val_accuracy, val_f1 = calculate_metrics(model, custom_train_loader.val_data_tensor, custom_train_loader.val_labels_tensor, batch_size, num_features)\n    \n            print(f'Epoch {epoch + 1}, Training Loss: {avg_train_loss}, Validation Loss: {avg_val_loss}')\n            print(f'Epoch {epoch + 1}, Training Loss: {train_reg_loss / len(custom_train_loader.train_data_tensor)}, Validation Loss: {avg_val_loss}')\n            # print(f'Training Accuracy: {train_accuracy}, Training F1 Score: {train_f1}')\n            # print(f'Validation Accuracy: {val_accuracy}, Validation F1 Score: {val_f1}')\n            print()\n            \n            if avg_val_loss < best_val_loss:\n                best_val_loss = avg_val_loss\n                best_epoch = epoch + 1\n                patience_counter = 0\n            else:\n                patience_counter += 10\n                if patience_counter >= early_stopping_patience:\n                    print(f'Early stopping triggered after {epoch + 1} epochs.')\n                    print(f'Best Validation Loss: {best_val_loss} from Epoch {best_epoch}')\n                    break\n\n    if patience_counter < early_stopping_patience:\n        print(f'Best Validation Loss after {num_epochs} epochs: {best_val_loss} from Epoch {best_epoch}')","metadata":{"execution":{"iopub.status.busy":"2024-12-14T03:20:26.168792Z","iopub.execute_input":"2024-12-14T03:20:26.169078Z","iopub.status.idle":"2024-12-14T03:20:26.187597Z","shell.execute_reply.started":"2024-12-14T03:20:26.169036Z","shell.execute_reply":"2024-12-14T03:20:26.186980Z"},"trusted":true},"outputs":[],"execution_count":4},{"cell_type":"code","source":"data_dl = pd.read_csv('/kaggle/input/playground-series-s4e10/train.csv')\ndata_og = pd.read_csv('/kaggle/input/loan-approval-prediction/credit_risk_dataset.csv')\n\ndata_dl = data_dl.drop([\"id\"], axis=1)\n\nmedian_emp_length = data_og['person_emp_length'].median()\nmedian_int_rate = data_og['loan_int_rate'].median()\n\ndata_dl['source'] = 0\ndata_og['source'] = 1\n\ndata = pd.concat([data_dl, data_og], ignore_index=True)\n\ndata['person_emp_length_missing'] = data['person_emp_length'].isna().astype(int)\ndata['loan_int_rate_missing'] = data['loan_int_rate'].isna().astype(int)\n\ndata['person_emp_length'] = data['person_emp_length'].fillna(median_emp_length)\ndata['loan_int_rate'] = data['loan_int_rate'].fillna(median_int_rate)\n\ngrade_mapping = {'A': 7, 'B': 6, 'C': 5, 'D': 4, 'E': 3, 'F': 2, 'G': 1}\ndata['loan_grade'] = data['loan_grade'].map(grade_mapping)\n\npurpose_mapping = {\n    'DEBTCONSOLIDATION': 1,\n    'HOMEIMPROVEMENT': 2,\n    'MEDICAL': 3,\n    'PERSONAL': 4,\n    'EDUCATION': 5,\n    'VENTURE': 6\n}\ndata['loan_intent'] = data['loan_intent'].map(purpose_mapping)\n\nhome_ownership_mapping = {\n    'OWN': 1,\n    'MORTGAGE': 2,\n    'OTHER': 3,\n    'RENT': 4\n}\ndata['person_home_ownership'] = data['person_home_ownership'].map(home_ownership_mapping)\n\nX = data.drop([\"loan_status\"], axis=1)\nX = pd.get_dummies(X, drop_first=True)\ny = data[\"loan_status\"]\n\ncolumn_to_log = [\n    'person_age',\n    'person_income',\n]\n\ncolumn_to_sqrt = [\n    'person_emp_length',\n    'loan_percent_income',\n]\n\nfor col in column_to_log:\n    if (X[col] <= 0).any():\n        print(f\"Column '{col}' contains non-positive values. Adding 1 to avoid log of non-positive numbers.\")\n        X[col] = np.log(X[col] + 1)\n    else:\n        X[col] = np.log(X[col])\n\nfor col in column_to_sqrt:\n    if (X[col] < 0).any():\n        print(f\"Column '{col}' contains negative values. Setting negative values to NaN before applying sqrt.\")\n        X[col] = np.sqrt(X[col].clip(lower=0))\n    else:\n        X[col] = np.sqrt(X[col])\n\nprint(data.isnull().sum())\nprint(X.columns)\nprint(X.shape, y.shape)\nprint(X.columns.get_loc('source'))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-14T03:20:26.188557Z","iopub.execute_input":"2024-12-14T03:20:26.188827Z","iopub.status.idle":"2024-12-14T03:20:26.478137Z","shell.execute_reply.started":"2024-12-14T03:20:26.188802Z","shell.execute_reply":"2024-12-14T03:20:26.477173Z"}},"outputs":[{"name":"stdout","text":"person_age                    0\nperson_income                 0\nperson_home_ownership         0\nperson_emp_length             0\nloan_intent                   0\nloan_grade                    0\nloan_amnt                     0\nloan_int_rate                 0\nloan_percent_income           0\ncb_person_default_on_file     0\ncb_person_cred_hist_length    0\nloan_status                   0\nsource                        0\nperson_emp_length_missing     0\nloan_int_rate_missing         0\ndtype: int64\nIndex(['person_age', 'person_income', 'person_home_ownership',\n       'person_emp_length', 'loan_intent', 'loan_grade', 'loan_amnt',\n       'loan_int_rate', 'loan_percent_income', 'cb_person_cred_hist_length',\n       'source', 'person_emp_length_missing', 'loan_int_rate_missing',\n       'cb_person_default_on_file_Y'],\n      dtype='object')\n(91226, 14) (91226,)\n10\n","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"x_scaler = StandardScaler()\nx_scaled = x_scaler.fit_transform(X)\n\nlabel_encoder = LabelEncoder()\ny_encoded = label_encoder.fit_transform(y)\n\nprint(x_scaled.shape)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-14T03:20:26.479435Z","iopub.execute_input":"2024-12-14T03:20:26.479851Z","iopub.status.idle":"2024-12-14T03:20:26.560309Z","shell.execute_reply.started":"2024-12-14T03:20:26.479808Z","shell.execute_reply":"2024-12-14T03:20:26.559397Z"}},"outputs":[{"name":"stdout","text":"(91226, 14)\n","output_type":"stream"}],"execution_count":6},{"cell_type":"code","source":"feature_means = x_scaled.mean(axis=0)\nfeature_variances = x_scaled.var(axis=0)\nfeature_mins = x_scaled.min(axis=0)\nfeature_maxs = x_scaled.max(axis=0)\n\nfeature_stats_scaled_full = pd.DataFrame({\n    'Mean': feature_means,\n    'Variance': feature_variances,\n    'Min': feature_mins,\n    'Max': feature_maxs\n})\n\nprint(\"Mean, Variance, Min, and Max of Scaled Features:\")\nprint(feature_stats_scaled_full)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-14T03:20:26.561235Z","iopub.execute_input":"2024-12-14T03:20:26.561517Z","iopub.status.idle":"2024-12-14T03:20:26.576902Z","shell.execute_reply.started":"2024-12-14T03:20:26.561490Z","shell.execute_reply":"2024-12-14T03:20:26.576043Z"}},"outputs":[{"name":"stdout","text":"Mean, Variance, Min, and Max of Scaled Features:\n            Mean  Variance       Min        Max\n0  -3.289997e-16       1.0 -1.552712   8.591255\n1  -1.420680e-16       1.0 -5.315235   9.344027\n2  -1.370832e-17       1.0 -1.810229   0.945472\n3   3.987875e-17       1.0 -1.859550   8.913061\n4   6.153166e-17       1.0 -1.591801   1.383254\n5  -1.183900e-17       1.0 -4.464004   1.025387\n6  -1.333446e-16       1.0 -1.513249   4.385625\n7   9.327889e-16       1.0 -1.759487   4.065809\n8  -5.358707e-16       1.0 -3.245038   4.413749\n9   6.480297e-17       1.0 -0.943500   5.989958\n10 -1.944089e-16       1.0 -0.745361   1.341632\n11 -4.610980e-17       1.0 -0.099539  10.046317\n12  6.729539e-17       1.0 -0.188056   5.317578\n13  2.928596e-17       1.0 -0.433778   2.305326\n","output_type":"stream"}],"execution_count":7},{"cell_type":"code","source":"class CustomLoss(nn.Module):\n    def __init__(self, criterion, f1_lambda, f2_lambda, l1_lambda, l2_lambda, wa_lambda):\n        super(CustomLoss, self).__init__()\n        self.criterion = criterion\n        self.f1_lambda = f1_lambda\n        self.f2_lambda = f2_lambda\n        self.l1_lambda = l1_lambda\n        self.l2_lambda = l2_lambda\n        self.wa_lambda = wa_lambda\n        self.i = 0\n\n    def forward(self, outputs, labels, model): \n        f1_loss = 0.0\n        f2_loss = 0.0\n        l1_loss = 0.0\n        l2_loss = 0.0\n\n        for name, module in model.named_modules():\n            if isinstance(module, CustomActivation):\n                f1_loss += (module.a ** 2).sum() + (module.b ** 2).sum()\n                f2_loss += ((module.a - module.b) ** 2).sum()\n\n            if isinstance(module, nn.Linear):\n                l1_loss += torch.norm(module.weight, 1)\n                l2_loss += torch.norm(module.weight, 2) ** 2\n\n        total_loss = (self.criterion(outputs, labels)\n                      + self.f1_lambda * f1_loss\n                      + self.f2_lambda * f2_loss\n                      + self.l1_lambda * l1_loss\n                      + self.l2_lambda * l2_loss)\n        self.i += 1\n\n        return total_loss\n\n    def compute_gradient_magnitude(self, model):\n        total_abs_sum = 0.0\n        for param in model.parameters():\n            if param.grad is not None:\n                total_abs_sum += param.grad.abs().sum().item()\n        self.grad_magnitude = total_abs_sum\n\n    def regular_loss(self, outputs, labels):\n        return self.criterion(outputs, labels)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-14T03:20:29.946417Z","iopub.execute_input":"2024-12-14T03:20:29.946794Z","iopub.status.idle":"2024-12-14T03:20:29.955137Z","shell.execute_reply.started":"2024-12-14T03:20:29.946759Z","shell.execute_reply":"2024-12-14T03:20:29.954252Z"}},"outputs":[],"execution_count":8},{"cell_type":"code","source":"class CustomActivation(nn.Module):\n    def __init__(self, num_features, buffer_size=10, num_control_points=9, init_identity=True):\n        super(CustomActivation, self).__init__()\n        self.i = 0\n        self.buffer_size = buffer_size\n                \n        self.a = nn.Parameter(torch.zeros(num_features, num_control_points))\n        self.b = nn.Parameter(torch.zeros(num_features, num_control_points))\n\n        self.local_bias = nn.Parameter(torch.zeros(num_features, num_control_points))\n        self.global_bias = nn.Parameter(torch.zeros(1, num_features))\n\n        self.a = torch.randn(num_features, num_control_points)\n        self.b = torch.randn(num_features, num_control_points)\n\n        with torch.no_grad():\n            if init_identity:\n                middle_index = num_control_points // 2\n                self.a[:, middle_index] = 1.0\n                self.b[:, middle_index] = 1.0\n\n    def forward(self, x):\n        \n        x = x.unsqueeze(-1) + self.local_bias\n        x = torch.where(x < 0, self.a * x, self.b * x)\n        x = x.sum(dim=-1) + self.global_bias            \n        return x","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-14T03:20:31.022356Z","iopub.execute_input":"2024-12-14T03:20:31.022944Z","iopub.status.idle":"2024-12-14T03:20:31.029761Z","shell.execute_reply.started":"2024-12-14T03:20:31.022914Z","shell.execute_reply":"2024-12-14T03:20:31.028857Z"}},"outputs":[],"execution_count":9},{"cell_type":"code","source":"class CustomActivation(nn.Module):\n    def __init__(self, num_features, buffer_size=10, num_control_points=9, width=50):\n        super(CustomActivation, self).__init__()\n        self.a = torch.randn(num_features, num_control_points).to(device)\n        self.b = torch.randn(num_features, num_control_points).to(device)\n\n        self.local_bias = torch.linspace(-width, width, num_control_points).repeat(num_features, 1).to(device)\n\n    def forward(self, x):\n        x = x.unsqueeze(-1) + self.local_bias\n        x = torch.where(x < 0, self.a * x, self.b * x)\n        x = x.sum(dim=-1)\n        return x","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-14T03:28:35.422809Z","iopub.execute_input":"2024-12-14T03:28:35.423148Z","iopub.status.idle":"2024-12-14T03:28:35.429163Z","shell.execute_reply.started":"2024-12-14T03:28:35.423119Z","shell.execute_reply":"2024-12-14T03:28:35.428301Z"}},"outputs":[],"execution_count":38},{"cell_type":"code","source":"# import torch\n# import torch.nn as nn\n\n# class CustomActivation(nn.Module):\n#     def __init__(self, num_features, buffer_size=10, num_control_points=9, width=50):\n#         super(CustomActivation, self).__init__()\n#         self.num_features = num_features\n#         num_control_points = 41\n#         self.num_control_points = num_control_points\n\n#         self.a = nn.Parameter(torch.randn(num_features, num_control_points))\n#         self.b = nn.Parameter(torch.randn(num_features, num_control_points))\n\n#         self.local_bias = nn.Parameter(torch.linspace(-width, width, num_control_points))\n\n#     def forward(self, x):\n#         output = torch.zeros_like(x)\n\n#         for i in range(self.num_control_points):\n#             shifted_x = x + self.local_bias[i]\n\n#             output += torch.where(\n#                 shifted_x < 0, \n#                 self.a[:, i] * shifted_x, \n#                 self.b[:, i] * shifted_x\n#             )\n\n#         return output","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-14T03:28:35.834460Z","iopub.execute_input":"2024-12-14T03:28:35.834831Z","iopub.status.idle":"2024-12-14T03:28:35.839631Z","shell.execute_reply.started":"2024-12-14T03:28:35.834799Z","shell.execute_reply":"2024-12-14T03:28:35.838584Z"}},"outputs":[],"execution_count":39},{"cell_type":"code","source":"class CustomLinear(nn.Module):\n    def __init__(self, num_features, num_outputs, init_identity=False):\n        super(CustomLinear, self).__init__()\n        \n        # if init_identity and num_features != num_outputs:\n        #     raise ValueError(\"For identity initialization, num_features must equal num_outputs.\")\n\n        self.linear = nn.Linear(num_features, num_outputs, bias=True)\n        \n        with torch.no_grad():\n            self.linear.bias.zero_()\n\n            # if init_identity:\n            #     self.linear.weight.copy_(torch.eye(num_features, num_outputs))\n            # else:\n            #     self.linear.weight.zero_()\n\n            if not init_identity:\n                self.linear.weight.zero_()\n\n    def forward(self, x):\n        return self.linear(x)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-14T03:28:36.226068Z","iopub.execute_input":"2024-12-14T03:28:36.226423Z","iopub.status.idle":"2024-12-14T03:28:36.232162Z","shell.execute_reply.started":"2024-12-14T03:28:36.226393Z","shell.execute_reply":"2024-12-14T03:28:36.231283Z"}},"outputs":[],"execution_count":40},{"cell_type":"code","source":"class TabularDenseNet(nn.Module):\n    def __init__(self, input_size, output_size, num_control_points, num_layers, window_size):\n        super(TabularDenseNet, self).__init__()\n        self.num_group = 64\n        self.layer_1 = CustomActivation(input_size * self.num_group, 5, 20)\n        self.layer_2 = CustomActivation(self.num_group, 5, 20)\n        self.final = CustomLinear(self.num_group, output_size, init_identity=False)\n\n        self.val_x = [None for i in range(100)]\n        self.train_x = [None for i in range(100)]\n        self.i = 0\n        \n    def forward(self, x, val_iter=None):\n        x = x.repeat(1, self.num_group)\n        x = self.layer_1(x)\n        x = x.unfold(1, 14, 14).sum(dim=2)\n        x = self.layer_2(x)\n\n        return self.final(x)\n        # if val_iter is not None:\n        #     if self.val_x[val_iter] is not None:\n        #         x = self.val_x[val_iter]\n        #         return self.final(x)\n        #     else:\n        #         x = x.repeat(1, self.num_group)\n        #         x = self.layer_1(x)\n        #         x = x.unfold(1, 14, 14).sum(dim=2)\n        #         x = self.layer_2(x)\n\n        #         self.val_x[val_iter] = x.detach()\n        #         x = self.final(x)\n        #         return x\n\n        # if self.i < 10:\n        #     x = x.repeat(1, self.num_group)\n        #     x = self.layer_1(x)\n        #     x = x.unfold(1, 14, 14).sum(dim=2)\n        #     x = self.layer_2(x)\n\n        #     self.train_x[self.i % 10] = x.detach()\n        # else:\n        #     x = self.train_x[self.i % 10]\n            \n        # self.i += 1\n        # x = self.final(x)\n        # return x","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-14T03:29:16.832931Z","iopub.execute_input":"2024-12-14T03:29:16.833287Z","iopub.status.idle":"2024-12-14T03:29:16.840583Z","shell.execute_reply.started":"2024-12-14T03:29:16.833259Z","shell.execute_reply":"2024-12-14T03:29:16.839494Z"}},"outputs":[],"execution_count":47},{"cell_type":"code","source":"# import torch.nn.functional as F\n\n# class TabularDenseNet(nn.Module):\n#     def __init__(self, input_size, output_size, num_control_points, num_layers, window_size):\n#         super(TabularDenseNet, self).__init__()\n#         self.num_group = 256 * 6\n#         self.layer_1 = CustomLinear(input_size, input_size * 16, init_identity=True)\n#         self.layer_2 = CustomLinear(input_size * 16, input_size * self.num_group, init_identity=True)\n#         self.final = CustomLinear(input_size * self.num_group, output_size, init_identity=False)\n\n#         self.i = 0\n#         self.x_list = [None for i in range(10)]\n#         self.x_val_list = [None for i in range(3)]\n\n#         for param in self.layer_1.parameters():\n#             param.requires_grad = False\n#         for param in self.layer_2.parameters():\n#             param.requires_grad = False\n\n#         self.stored_x = None\n#         self.stored_x_val = None\n        \n#     def forward(self, x, val_iter=None):\n#         if self.stored_x is not None and val_iter is None:\n#             return self.final(self.stored_x)\n#         if self.stored_x_val is not None and val_iter is not None:\n#             return self.final(self.stored_x_val)\n\n#         x = self.layer_1(x)\n#         x = F.relu(x)\n#         x = self.layer_2(x)\n#         x = F.relu(x)\n#         print(x.shape)\n        \n#         if val_iter is None:\n#             self.stored_x = x\n#         else:\n#             self.stored_x_val = x\n\n#         return self.final(x)\n\n#         # if val_iter:\n#         #     if self.x_val_list[val_iter] is not None:\n#         #         x = self.x_val_list[val_iter]\n#         #         return self.final(x)\n#         #     else:\n#         #         x = self.layer_1(x)\n#         #         x = F.relu(x)\n#         #         x = self.layer_2(x)\n#         #         x = F.relu(x)\n#         #         self.x_val_list[val_iter] = x.detach()\n#         #         return self.final(x)\n\n#         # if self.i < 10:\n#         #     x = self.layer_1(x)\n#         #     x = F.relu(x)\n#         #     x = self.layer_2(x)\n#         #     x = F.relu(x)\n#         #     self.x_list[self.i] = x.detach()\n#         # else:\n#         #     x = self.x_list[self.i % 10]\n\n#         # self.i += 1\n#         # x = self.final(x)\n#         # return x","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-14T00:26:42.760758Z","iopub.execute_input":"2024-12-14T00:26:42.761698Z","iopub.status.idle":"2024-12-14T00:26:42.774678Z","shell.execute_reply.started":"2024-12-14T00:26:42.761646Z","shell.execute_reply":"2024-12-14T00:26:42.773617Z"},"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":17},{"cell_type":"code","source":"print(x_scaled.shape)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-14T05:24:05.853618Z","iopub.execute_input":"2024-12-14T05:24:05.854023Z","iopub.status.idle":"2024-12-14T05:24:05.858775Z","shell.execute_reply.started":"2024-12-14T05:24:05.853993Z","shell.execute_reply":"2024-12-14T05:24:05.857855Z"}},"outputs":[{"name":"stdout","text":"(91226, 14)\n","output_type":"stream"}],"execution_count":53},{"cell_type":"code","source":"custom_train_loader = CustomDataLoader(x_scaled, y_encoded, validation_size=0.2, random_state=0, classification=True)\nprint(custom_train_loader.train_data_tensor.shape)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-14T03:29:19.270016Z","iopub.execute_input":"2024-12-14T03:29:19.270355Z","iopub.status.idle":"2024-12-14T03:29:19.318414Z","shell.execute_reply.started":"2024-12-14T03:29:19.270328Z","shell.execute_reply":"2024-12-14T03:29:19.317465Z"}},"outputs":[{"name":"stdout","text":"torch.Size([72980, 14])\n","output_type":"stream"}],"execution_count":48},{"cell_type":"code","source":"models = []","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-14T03:29:19.731099Z","iopub.execute_input":"2024-12-14T03:29:19.731963Z","iopub.status.idle":"2024-12-14T03:29:19.736360Z","shell.execute_reply.started":"2024-12-14T03:29:19.731922Z","shell.execute_reply":"2024-12-14T03:29:19.735349Z"}},"outputs":[],"execution_count":49},{"cell_type":"code","source":"feature_tensor = torch.empty((custom_train_loader.train_data_tensor.size(0), 0))\nprint(feature_tensor.shape)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-14T03:29:20.021614Z","iopub.execute_input":"2024-12-14T03:29:20.022383Z","iopub.status.idle":"2024-12-14T03:29:20.027008Z","shell.execute_reply.started":"2024-12-14T03:29:20.022347Z","shell.execute_reply":"2024-12-14T03:29:20.026101Z"}},"outputs":[{"name":"stdout","text":"torch.Size([72980, 0])\n","output_type":"stream"}],"execution_count":50},{"cell_type":"code","source":"num_features = 14\nnum_classes = 2\nnum_control_points = 41\nnum_epochs = 10000\nbatch_size = 7298 * 1","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-14T03:29:21.091146Z","iopub.execute_input":"2024-12-14T03:29:21.091858Z","iopub.status.idle":"2024-12-14T03:29:21.095799Z","shell.execute_reply.started":"2024-12-14T03:29:21.091813Z","shell.execute_reply":"2024-12-14T03:29:21.094864Z"}},"outputs":[],"execution_count":51},{"cell_type":"code","source":"for name, param in model.named_parameters():\n    print(name)\n    print(param)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-14T03:24:39.993878Z","iopub.execute_input":"2024-12-14T03:24:39.994207Z","iopub.status.idle":"2024-12-14T03:24:40.011937Z","shell.execute_reply.started":"2024-12-14T03:24:39.994180Z","shell.execute_reply":"2024-12-14T03:24:40.011111Z"}},"outputs":[{"name":"stdout","text":"layer_1.a\nParameter containing:\ntensor([[-0.6702,  0.2361,  0.8290,  ..., -0.2312, -0.0621,  0.6341],\n        [ 0.0468,  1.7785, -0.2693,  ...,  0.5128,  0.4180,  0.5795],\n        [-0.1065,  1.3127, -1.0797,  ..., -0.3545, -1.4691, -1.6942],\n        ...,\n        [ 2.1793,  2.4107, -1.6754,  ...,  1.0180,  1.6123, -1.2275],\n        [ 0.4764,  0.4330,  0.6436,  ..., -0.6466,  0.1749, -0.0196],\n        [ 0.4806, -1.4551, -2.5456,  ...,  0.9486, -2.1704,  0.7256]],\n       device='cuda:0', requires_grad=True)\nlayer_1.b\nParameter containing:\ntensor([[ 1.5328,  0.5896, -0.4796,  ..., -0.6579,  1.9220,  0.5372],\n        [ 2.1024,  0.9821, -0.1678,  ...,  0.2924, -1.8483, -2.5776],\n        [ 0.7518,  0.5171, -0.5488,  ..., -0.0913,  0.7806,  0.1588],\n        ...,\n        [ 0.4202, -1.8980,  0.1207,  ...,  1.2828, -1.9447,  0.2433],\n        [ 0.1236,  0.4643, -1.4194,  ...,  0.3370,  0.4962,  0.4576],\n        [ 0.2270,  1.7739, -0.0756,  ..., -1.9056, -0.8622,  1.9557]],\n       device='cuda:0', requires_grad=True)\nlayer_1.local_bias\nParameter containing:\ntensor([-5.0000e+01, -4.7500e+01, -4.5000e+01, -4.2500e+01, -4.0000e+01,\n        -3.7500e+01, -3.5000e+01, -3.2500e+01, -3.0000e+01, -2.7500e+01,\n        -2.5000e+01, -2.2500e+01, -2.0000e+01, -1.7500e+01, -1.5000e+01,\n        -1.2500e+01, -1.0000e+01, -7.5000e+00, -5.0000e+00, -2.5000e+00,\n         3.8326e-07,  2.5000e+00,  5.0000e+00,  7.5000e+00,  1.0000e+01,\n         1.2500e+01,  1.5000e+01,  1.7500e+01,  2.0000e+01,  2.2500e+01,\n         2.5000e+01,  2.7500e+01,  3.0000e+01,  3.2500e+01,  3.5000e+01,\n         3.7500e+01,  4.0000e+01,  4.2500e+01,  4.5000e+01,  4.7500e+01,\n         5.0000e+01], device='cuda:0', requires_grad=True)\nlayer_2.a\nParameter containing:\ntensor([[-0.8720, -1.4274,  0.9928,  ...,  1.6025, -0.1102, -0.1926],\n        [-2.4241,  1.8985, -0.4344,  ...,  2.1138, -1.1994,  1.4129],\n        [-0.0195, -0.4926,  0.1449,  ..., -1.9571,  1.2869, -1.5901],\n        ...,\n        [ 0.0337,  1.3077,  0.3889,  ...,  1.5226,  0.4674, -1.1254],\n        [ 1.5976, -0.1079,  0.2220,  ..., -0.5886, -0.6830, -1.1147],\n        [ 0.0323, -0.5586, -0.9696,  ..., -0.4522,  1.4745,  0.1202]],\n       device='cuda:0', requires_grad=True)\nlayer_2.b\nParameter containing:\ntensor([[-0.3579,  2.4908, -1.0543,  ..., -1.3908, -0.2583,  0.1942],\n        [-1.7666, -1.3113,  0.7821,  ...,  0.5526,  0.6935,  0.9259],\n        [ 0.8425,  0.3413,  0.1332,  ...,  0.6809,  0.6130,  1.0933],\n        ...,\n        [-0.1121, -1.1703,  1.0889,  ...,  0.2069,  0.5178,  0.9134],\n        [ 0.9583, -0.6623, -1.0851,  ..., -0.6184, -1.2312,  0.1347],\n        [ 0.6446, -0.6269, -1.1928,  ..., -1.2414, -0.2565,  0.8481]],\n       device='cuda:0', requires_grad=True)\nlayer_2.local_bias\nParameter containing:\ntensor([-5.0000e+01, -4.7500e+01, -4.5000e+01, -4.2500e+01, -4.0000e+01,\n        -3.7500e+01, -3.5000e+01, -3.2500e+01, -3.0000e+01, -2.7500e+01,\n        -2.5000e+01, -2.2500e+01, -2.0000e+01, -1.7500e+01, -1.5000e+01,\n        -1.2500e+01, -1.0000e+01, -7.5000e+00, -5.0000e+00, -2.5000e+00,\n         3.3810e-07,  2.5000e+00,  5.0000e+00,  7.5000e+00,  1.0000e+01,\n         1.2500e+01,  1.5000e+01,  1.7500e+01,  2.0000e+01,  2.2500e+01,\n         2.5000e+01,  2.7500e+01,  3.0000e+01,  3.2500e+01,  3.5000e+01,\n         3.7500e+01,  4.0000e+01,  4.2500e+01,  4.5000e+01,  4.7500e+01,\n         5.0000e+01], device='cuda:0', requires_grad=True)\nfinal.linear.weight\nParameter containing:\ntensor([[ 2.9266e-05,  1.0379e-04, -8.4078e-06,  ..., -2.9096e-05,\n         -1.5227e-05, -1.1291e-04],\n        [-2.9267e-05, -1.0379e-04,  8.4060e-06,  ...,  2.9098e-05,\n          1.5229e-05,  1.1291e-04]], device='cuda:0', requires_grad=True)\nfinal.linear.bias\nParameter containing:\ntensor([-1.4262e-06,  1.4245e-06], device='cuda:0', requires_grad=True)\n","output_type":"stream"}],"execution_count":35},{"cell_type":"code","source":"import torch\nimport matplotlib.pyplot as plt\n\nfinal_linear_weight = model.final.linear.weight\n\nweights = final_linear_weight.detach().cpu().numpy().flatten()\n\nplt.hist(weights, bins=200, alpha=0.75)\nplt.title('Histogram of final.linear.weight')\nplt.xlabel('Weight values')\nplt.ylabel('Frequency')\nplt.grid(True)\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-14T03:24:40.167022Z","iopub.execute_input":"2024-12-14T03:24:40.167359Z","iopub.status.idle":"2024-12-14T03:24:40.660101Z","shell.execute_reply.started":"2024-12-14T03:24:40.167334Z","shell.execute_reply":"2024-12-14T03:24:40.659160Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"<Figure size 640x480 with 1 Axes>","image/png":"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"},"metadata":{}}],"execution_count":36},{"cell_type":"code","source":"import torch\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy.stats import norm, laplace\n\n# Assuming final.linear.weight is your tensor\nweights = model.final.linear.weight.detach().cpu().numpy().flatten()\n\n# Fit normal and Laplacian distributions to the data\nmean, std = norm.fit(weights)\nloc, scale = laplace.fit(weights)\n\n# Create a range of values for the x-axis\nx = np.linspace(min(weights), max(weights), 1000)\n\n# Calculate the PDFs\nnormal_pdf = norm.pdf(x, mean, std)\nlaplacian_pdf = laplace.pdf(x, loc, scale)\n\n# Plot the histogram and fitted distributions\nplt.hist(weights, bins=200, density=True, alpha=0.6, label='Histogram')\nplt.plot(x, normal_pdf, label='Normal PDF', linestyle='--')\nplt.plot(x, laplacian_pdf, label='Laplacian PDF', linestyle='--')\nplt.title('Fit Comparison: Normal vs. Laplacian')\nplt.xlabel('Weight values')\nplt.ylabel('Density')\nplt.legend()\nplt.grid(True)\nplt.show()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-14T03:16:48.543845Z","iopub.execute_input":"2024-12-14T03:16:48.544193Z","iopub.status.idle":"2024-12-14T03:16:48.578382Z","shell.execute_reply.started":"2024-12-14T03:16:48.544162Z","shell.execute_reply":"2024-12-14T03:16:48.577304Z"}},"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","Cell \u001b[0;32mIn[20], line 7\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mscipy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mstats\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m norm, laplace\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m# Assuming final.linear.weight is your tensor\u001b[39;00m\n\u001b[0;32m----> 7\u001b[0m weights \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241m.\u001b[39mfinal\u001b[38;5;241m.\u001b[39mlinear\u001b[38;5;241m.\u001b[39mweight\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mnumpy()\u001b[38;5;241m.\u001b[39mflatten()\n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m# Fit normal and Laplacian distributions to the data\u001b[39;00m\n\u001b[1;32m     10\u001b[0m mean, std \u001b[38;5;241m=\u001b[39m norm\u001b[38;5;241m.\u001b[39mfit(weights)\n","\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"],"ename":"NameError","evalue":"name 'model' is not defined","output_type":"error"}],"execution_count":20},{"cell_type":"code","source":"from scipy.stats import kstest\n\n# Test against normal distribution\nks_normal = kstest(weights, 'norm', args=(mean, std))\n\n# Test against Laplacian distribution\nks_laplacian = kstest(weights, 'laplace', args=(loc, scale))\n\nprint(\"K-S Test for Normal Distribution:\", ks_normal)\nprint(\"K-S Test for Laplacian Distribution:\", ks_laplacian)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-14T03:06:43.943401Z","iopub.execute_input":"2024-12-14T03:06:43.943705Z","iopub.status.idle":"2024-12-14T03:06:43.954433Z","shell.execute_reply.started":"2024-12-14T03:06:43.943681Z","shell.execute_reply":"2024-12-14T03:06:43.953587Z"}},"outputs":[{"name":"stdout","text":"K-S Test for Normal Distribution: KstestResult(statistic=0.3538434206906831, pvalue=0.0, statistic_location=-2.7208716e-05, statistic_sign=-1)\nK-S Test for Laplacian Distribution: KstestResult(statistic=0.2176632993794887, pvalue=0.0, statistic_location=1.1941935e-05, statistic_sign=1)\n","output_type":"stream"}],"execution_count":42},{"cell_type":"code","source":"from scipy.stats import anderson\n\nad_result = anderson(weights, dist='norm')\nprint(\"Anderson-Darling Test for Normal Distribution:\", ad_result)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-14T03:06:55.623930Z","iopub.execute_input":"2024-12-14T03:06:55.624742Z","iopub.status.idle":"2024-12-14T03:06:55.633438Z","shell.execute_reply.started":"2024-12-14T03:06:55.624708Z","shell.execute_reply":"2024-12-14T03:06:55.632540Z"}},"outputs":[{"name":"stdout","text":"Anderson-Darling Test for Normal Distribution: AndersonResult(statistic=1988.462997823679, critical_values=array([0.576, 0.656, 0.787, 0.918, 1.091]), significance_level=array([15. , 10. ,  5. ,  2.5,  1. ]), fit_result=  params: FitParams(loc=2.2737368e-12, scale=0.00016291751)\n success: True\n message: '`anderson` successfully fit the distribution to the data.')\n","output_type":"stream"}],"execution_count":43},{"cell_type":"code","source":"torch.cuda.empty_cache()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-14T02:45:43.435789Z","iopub.execute_input":"2024-12-14T02:45:43.436550Z","iopub.status.idle":"2024-12-14T02:45:43.440436Z","shell.execute_reply.started":"2024-12-14T02:45:43.436514Z","shell.execute_reply":"2024-12-14T02:45:43.439560Z"}},"outputs":[],"execution_count":33},{"cell_type":"code","source":"import torch\n\n# Tensor of shape (batch_size=3, num_features=28)\nx = torch.arange(1, 85).view(3, 28).float()  # 3 samples, each with 28 features\n\nprint(\"Original tensor shape:\", x.shape)\nprint(x)\n\n# Unfold the second dimension (num_features) into sliding windows of size 14 with a stride of 14\nx_unfolded = x.unfold(1, 14, 14)\n\nprint(\"\\nUnfolded tensor shape:\", x_unfolded.shape)\nprint(x_unfolded)\n\n# Sum over the new dimension (dim=2) created by the unfold\nx_summed = x_unfolded.sum(dim=2)\n\nprint(\"\\nSummed tensor shape:\", x_summed.shape)\nprint(x_summed)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-14T02:39:40.590563Z","iopub.execute_input":"2024-12-14T02:39:40.590898Z","iopub.status.idle":"2024-12-14T02:39:40.600976Z","shell.execute_reply.started":"2024-12-14T02:39:40.590866Z","shell.execute_reply":"2024-12-14T02:39:40.600043Z"},"jupyter":{"source_hidden":true}},"outputs":[{"name":"stdout","text":"Original tensor shape: torch.Size([3, 28])\ntensor([[ 1.,  2.,  3.,  4.,  5.,  6.,  7.,  8.,  9., 10., 11., 12., 13., 14.,\n         15., 16., 17., 18., 19., 20., 21., 22., 23., 24., 25., 26., 27., 28.],\n        [29., 30., 31., 32., 33., 34., 35., 36., 37., 38., 39., 40., 41., 42.,\n         43., 44., 45., 46., 47., 48., 49., 50., 51., 52., 53., 54., 55., 56.],\n        [57., 58., 59., 60., 61., 62., 63., 64., 65., 66., 67., 68., 69., 70.,\n         71., 72., 73., 74., 75., 76., 77., 78., 79., 80., 81., 82., 83., 84.]])\n\nUnfolded tensor shape: torch.Size([3, 2, 14])\ntensor([[[ 1.,  2.,  3.,  4.,  5.,  6.,  7.,  8.,  9., 10., 11., 12., 13., 14.],\n         [15., 16., 17., 18., 19., 20., 21., 22., 23., 24., 25., 26., 27., 28.]],\n\n        [[29., 30., 31., 32., 33., 34., 35., 36., 37., 38., 39., 40., 41., 42.],\n         [43., 44., 45., 46., 47., 48., 49., 50., 51., 52., 53., 54., 55., 56.]],\n\n        [[57., 58., 59., 60., 61., 62., 63., 64., 65., 66., 67., 68., 69., 70.],\n         [71., 72., 73., 74., 75., 76., 77., 78., 79., 80., 81., 82., 83., 84.]]])\n\nSummed tensor shape: torch.Size([3, 2])\ntensor([[ 105.,  301.],\n        [ 497.,  693.],\n        [ 889., 1085.]])\n","output_type":"stream"}],"execution_count":87},{"cell_type":"code","source":"def custom_lr_lambda(step):\n    num_step_threshold = 100\n\n    if step < num_step_threshold:\n        return step / num_step_threshold\n    if step == num_step_threshold:\n        print(\"here\")\n    return 0.99995 ** (step - num_step_threshold)\n\nstart_time = time.time()\nmodel = TabularDenseNet(num_features, num_classes, num_control_points, 3, 10).to(device)\n\nfor name, param in model.named_parameters():\n    print(name)\n    print(param)\n    \noptimizer = optim.SGD(model.parameters(), lr=0.001 * 10.0)\noptimizer = optim.Adam(model.parameters(), lr=0.001 * 1.0 * 0.001)\nscheduler = LambdaLR(optimizer, lr_lambda=custom_lr_lambda)\n\ncriterion = CustomLoss(nn.CrossEntropyLoss(), 0.0, 0.0, 0.0, 0.0, 0.0)\nevaluate_model(model, custom_train_loader, criterion, optimizer, 100000, scheduler, batch_size, num_features, early_stopping_patience=10000)\n\nmodels.append(model)\nprint(f\"Execution time: {(time.time() - start_time):.6f} seconds\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-14T03:29:25.162488Z","iopub.execute_input":"2024-12-14T03:29:25.162887Z","iopub.status.idle":"2024-12-14T05:14:13.868368Z","shell.execute_reply.started":"2024-12-14T03:29:25.162853Z","shell.execute_reply":"2024-12-14T05:14:13.867124Z"}},"outputs":[{"name":"stdout","text":"final.linear.weight\nParameter containing:\ntensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]],\n       device='cuda:0', requires_grad=True)\nfinal.linear.bias\nParameter containing:\ntensor([0., 0.], device='cuda:0', requires_grad=True)\nLearning Rate: 1e-07\nEpoch 1, Training Loss: 0.6855927467346191, Validation Loss: 0.6652988691569341\nEpoch 1, Training Loss: 0.6653014302253724, Validation Loss: 0.6652988691569341\n\nhere\nLearning Rate: 9.555185156327981e-07\nEpoch 101, Training Loss: 0.4176750540733337, Validation Loss: 0.4159214579522002\nEpoch 101, Training Loss: 0.41741337776184084, Validation Loss: 0.4159214579522002\n\nLearning Rate: 9.089161915413455e-07\nEpoch 201, Training Loss: 0.3740170896053314, Validation Loss: 0.37055801127767873\nEpoch 201, Training Loss: 0.37381079196929934, Validation Loss: 0.37055801127767873\n\nLearning Rate: 8.64586745029127e-07\nEpoch 301, Training Loss: 0.34359329342842104, Validation Loss: 0.339374001642519\nEpoch 301, Training Loss: 0.3434559524059296, Validation Loss: 0.339374001642519\n\nLearning Rate: 8.224193238459403e-07\nEpoch 401, Training Loss: 0.3237167239189148, Validation Loss: 0.3195703061352458\nEpoch 401, Training Loss: 0.32363014221191405, Validation Loss: 0.3195703061352458\n\nLearning Rate: 7.823084822014332e-07\nEpoch 501, Training Loss: 0.3116495668888092, Validation Loss: 0.3078688459707093\nEpoch 501, Training Loss: 0.31160004138946534, Validation Loss: 0.3078688459707093\n\nLearning Rate: 7.441539170825151e-07\nEpoch 601, Training Loss: 0.30533368289470675, Validation Loss: 0.3018511230879207\nEpoch 601, Training Loss: 0.3053001970052719, Validation Loss: 0.3018511230879207\n\nLearning Rate: 7.078602174310367e-07\nEpoch 701, Training Loss: 0.3020508259534836, Validation Loss: 0.29875724801470493\nEpoch 701, Training Loss: 0.30202611088752745, Validation Loss: 0.29875724801470493\n\nLearning Rate: 6.733366255545142e-07\nEpoch 801, Training Loss: 0.3002067029476166, Validation Loss: 0.29702572515564085\nEpoch 801, Training Loss: 0.3001857966184616, Validation Loss: 0.29702572515564085\n\nLearning Rate: 6.404968101732754e-07\nEpoch 901, Training Loss: 0.2990921676158905, Validation Loss: 0.29597753700908963\nEpoch 901, Training Loss: 0.2990730553865433, Validation Loss: 0.29597753700908963\n\nLearning Rate: 6.092586505364953e-07\nEpoch 1001, Training Loss: 0.2983736664056778, Validation Loss: 0.29529745168073335\nEpoch 1001, Training Loss: 0.2983555167913437, Validation Loss: 0.29529745168073335\n\nLearning Rate: 5.79544031067275e-07\nEpoch 1101, Training Loss: 0.2978821098804474, Validation Loss: 0.2948274626375669\nEpoch 1101, Training Loss: 0.2978646159172058, Validation Loss: 0.2948274626375669\n\nLearning Rate: 5.512786460232419e-07\nEpoch 1201, Training Loss: 0.2975269705057144, Validation Loss: 0.2944836699490901\nEpoch 1201, Training Loss: 0.2975099891424179, Validation Loss: 0.2944836699490901\n\nLearning Rate: 5.243918136841969e-07\nEpoch 1301, Training Loss: 0.2972575813531876, Validation Loss: 0.29421917039823026\nEpoch 1301, Training Loss: 0.29724101424217225, Validation Loss: 0.29421917039823026\n\nLearning Rate: 4.988162996021579e-07\nEpoch 1401, Training Loss: 0.297044375538826, Validation Loss: 0.29400690322472156\nEpoch 1401, Training Loss: 0.2970282733440399, Validation Loss: 0.29400690322472156\n\nLearning Rate: 4.744881484718116e-07\nEpoch 1501, Training Loss: 0.2968697249889374, Validation Loss: 0.2938305067804369\nEpoch 1501, Training Loss: 0.29685401916503906, Validation Loss: 0.2938305067804369\n\nLearning Rate: 4.5134652420093847e-07\nEpoch 1601, Training Loss: 0.29672256410121917, Validation Loss: 0.2936799554243091\nEpoch 1601, Training Loss: 0.2967072516679764, Validation Loss: 0.2936799554243091\n\nLearning Rate: 4.2933355778088643e-07\nEpoch 1701, Training Loss: 0.2965958148241043, Validation Loss: 0.29354881769887164\nEpoch 1701, Training Loss: 0.29658092856407164, Validation Loss: 0.29354881769887164\n\nLearning Rate: 4.083942025766695e-07\nEpoch 1801, Training Loss: 0.29648478627204894, Validation Loss: 0.29343270066732324\nEpoch 1801, Training Loss: 0.2964703440666199, Validation Loss: 0.29343270066732324\n\nLearning Rate: 3.884760966748241e-07\nEpoch 1901, Training Loss: 0.2963863223791122, Validation Loss: 0.29332877304696936\nEpoch 1901, Training Loss: 0.2963722884654999, Validation Loss: 0.29332877304696936\n\nLearning Rate: 3.6952943194480236e-07\nEpoch 2001, Training Loss: 0.2962981045246124, Validation Loss: 0.2932349128588817\nEpoch 2001, Training Loss: 0.296284493803978, Validation Loss: 0.2932349128588817\n\nLearning Rate: 3.5150682948647393e-07\nEpoch 2101, Training Loss: 0.2962184906005859, Validation Loss: 0.29314961802746453\nEpoch 2101, Training Loss: 0.2962053120136261, Validation Loss: 0.29314961802746453\n\nLearning Rate: 3.34363221152271e-07\nEpoch 2201, Training Loss: 0.29614620804786684, Validation Loss: 0.2930716606532614\nEpoch 2201, Training Loss: 0.2961334615945816, Validation Loss: 0.2930716606532614\n\nLearning Rate: 3.180557368477091e-07\nEpoch 2301, Training Loss: 0.2960802882909775, Validation Loss: 0.2930002062614453\nEpoch 2301, Training Loss: 0.29606798887252805, Validation Loss: 0.2930002062614453\n\nLearning Rate: 3.0254359732845903e-07\nEpoch 2401, Training Loss: 0.296019983291626, Validation Loss: 0.29293449188577597\nEpoch 2401, Training Loss: 0.29600809812545775, Validation Loss: 0.29293449188577597\n\nLearning Rate: 2.8778801222589564e-07\nEpoch 2501, Training Loss: 0.2959646135568619, Validation Loss: 0.29287390358469195\nEpoch 2501, Training Loss: 0.2959531545639038, Validation Loss: 0.29287390358469195\n\nLearning Rate: 2.7375208304612027e-07\nEpoch 2601, Training Loss: 0.2959136515855789, Validation Loss: 0.29281791084876096\nEpoch 2601, Training Loss: 0.295902618765831, Validation Loss: 0.29281791084876096\n\nLearning Rate: 2.6040071089989157e-07\nEpoch 2701, Training Loss: 0.29586667120456694, Validation Loss: 0.2927661620151522\nEpoch 2701, Training Loss: 0.29585607051849366, Validation Loss: 0.2927661620151522\n\nLearning Rate: 2.477005087327313e-07\nEpoch 2801, Training Loss: 0.2958232700824738, Validation Loss: 0.292718168313365\nEpoch 2801, Training Loss: 0.295813050866127, Validation Loss: 0.292718168313365\n\nLearning Rate: 2.3561971783572217e-07\nEpoch 2901, Training Loss: 0.29578312039375304, Validation Loss: 0.29267367939147776\nEpoch 2901, Training Loss: 0.2957733005285263, Validation Loss: 0.29267367939147776\n\nLearning Rate: 2.2412812842822118e-07\nEpoch 3001, Training Loss: 0.29574590623378755, Validation Loss: 0.29263227802024366\nEpoch 3001, Training Loss: 0.29573649168014526, Validation Loss: 0.29263227802024366\n\nLearning Rate: 2.1319700411389484e-07\nEpoch 3101, Training Loss: 0.2957113981246948, Validation Loss: 0.2925938330537641\nEpoch 3101, Training Loss: 0.29570235013961793, Validation Loss: 0.2925938330537641\n\nLearning Rate: 2.0279901002116645e-07\nEpoch 3201, Training Loss: 0.2956793218851089, Validation Loss: 0.292558070312633\nEpoch 3201, Training Loss: 0.2956706464290619, Validation Loss: 0.292558070312633\n\nLearning Rate: 1.9290814444838032e-07\nEpoch 3301, Training Loss: 0.2956494867801666, Validation Loss: 0.29252472156941545\nEpoch 3301, Training Loss: 0.2956411957740784, Validation Loss: 0.29252472156941545\n\nLearning Rate: 1.8349967384275268e-07\nEpoch 3401, Training Loss: 0.2956217288970947, Validation Loss: 0.292493602048406\nEpoch 3401, Training Loss: 0.2956137925386429, Validation Loss: 0.292493602048406\n\nLearning Rate: 1.7455007095051306e-07\nEpoch 3501, Training Loss: 0.29559583961963654, Validation Loss: 0.2924645806200395\nEpoch 3501, Training Loss: 0.2955882787704468, Validation Loss: 0.2924645806200395\n\nLearning Rate: 1.6603695598357306e-07\nEpoch 3601, Training Loss: 0.29557174146175386, Validation Loss: 0.2924374188592033\nEpoch 3601, Training Loss: 0.2955644756555557, Validation Loss: 0.2924374188592033\n\nLearning Rate: 1.579390406556002e-07\nEpoch 3701, Training Loss: 0.29554919302463534, Validation Loss: 0.2924120810050707\nEpoch 3701, Training Loss: 0.29554225504398346, Validation Loss: 0.2924120810050707\n\nLearning Rate: 1.5023607494755114e-07\nEpoch 3801, Training Loss: 0.29552816450595853, Validation Loss: 0.29238835247961376\nEpoch 3801, Training Loss: 0.2955215334892273, Validation Loss: 0.29238835247961376\n\nLearning Rate: 1.4290879646954398e-07\nEpoch 3901, Training Loss: 0.2955084890127182, Validation Loss: 0.2923661557928742\nEpoch 3901, Training Loss: 0.29550215005874636, Validation Loss: 0.2923661557928742\n\nLearning Rate: 1.3593888229244136e-07\nEpoch 4001, Training Loss: 0.29549006521701815, Validation Loss: 0.2923453419332401\nEpoch 4001, Training Loss: 0.29548401534557345, Validation Loss: 0.2923453419332401\n\nLearning Rate: 1.2930890312869197e-07\nEpoch 4101, Training Loss: 0.2954728215932846, Validation Loss: 0.29232586321634224\nEpoch 4101, Training Loss: 0.2954670339822769, Validation Loss: 0.29232586321634224\n\nLearning Rate: 1.2300227974785387e-07\nEpoch 4201, Training Loss: 0.29545666873455045, Validation Loss: 0.29230760043942455\nEpoch 4201, Training Loss: 0.2954511523246765, Validation Loss: 0.29230760043942455\n\nLearning Rate: 1.1700324151780886e-07\nEpoch 4301, Training Loss: 0.2954415112733841, Validation Loss: 0.2922903509349346\nEpoch 4301, Training Loss: 0.29543623328208923, Validation Loss: 0.2922903509349346\n\nLearning Rate: 1.1129678696799579e-07\nEpoch 4401, Training Loss: 0.2954272896051407, Validation Loss: 0.29227428756483576\nEpoch 4401, Training Loss: 0.29542227983474734, Validation Loss: 0.29227428756483576\n\nLearning Rate: 1.0586864627604386e-07\nEpoch 4501, Training Loss: 0.29541394114494324, Validation Loss: 0.2922590884588194\nEpoch 4501, Training Loss: 0.2954091548919678, Validation Loss: 0.2922590884588194\n\nLearning Rate: 1.0070524558399953e-07\nEpoch 4601, Training Loss: 0.2954014092683792, Validation Loss: 0.2922448489823572\nEpoch 4601, Training Loss: 0.29539682567119596, Validation Loss: 0.2922448489823572\n\nLearning Rate: 9.579367305491378e-08\nEpoch 4701, Training Loss: 0.2953896075487137, Validation Loss: 0.2922314976137954\nEpoch 4701, Training Loss: 0.2953852444887161, Validation Loss: 0.2922314976137954\n\nLearning Rate: 9.112164658490938e-08\nEpoch 4801, Training Loss: 0.29537852108478546, Validation Loss: 0.2922188317019153\nEpoch 4801, Training Loss: 0.2953743726015091, Validation Loss: 0.2922188317019153\n\nLearning Rate: 8.6677483089988e-08\nEpoch 4901, Training Loss: 0.29536809027194977, Validation Loss: 0.2922069823632151\nEpoch 4901, Training Loss: 0.295364111661911, Validation Loss: 0.2922069823632151\n\nLearning Rate: 8.245006929077353e-08\nEpoch 5001, Training Loss: 0.29535825550556183, Validation Loss: 0.29219575887655175\nEpoch 5001, Training Loss: 0.2953544735908508, Validation Loss: 0.29219575887655175\n\nLearning Rate: 7.842883392213525e-08\nEpoch 5101, Training Loss: 0.2953489989042282, Validation Loss: 0.29218526852440574\nEpoch 5101, Training Loss: 0.2953453898429871, Validation Loss: 0.29218526852440574\n\nLearning Rate: 7.46037212981967e-08\nEpoch 5201, Training Loss: 0.29534026980400085, Validation Loss: 0.29217531461079604\nEpoch 5201, Training Loss: 0.29533683955669404, Validation Loss: 0.29217531461079604\n\nLearning Rate: 7.096516616662554e-08\nEpoch 5301, Training Loss: 0.2953320652246475, Validation Loss: 0.2921659030974937\nEpoch 5301, Training Loss: 0.29532878994941714, Validation Loss: 0.2921659030974937\n\nLearning Rate: 6.750406978932439e-08\nEpoch 5401, Training Loss: 0.29532428085803986, Validation Loss: 0.2921570876241057\nEpoch 5401, Training Loss: 0.29532118141651154, Validation Loss: 0.2921570876241057\n\nLearning Rate: 6.421177718970816e-08\nEpoch 5501, Training Loss: 0.29531697928905487, Validation Loss: 0.2921487311058291\nEpoch 5501, Training Loss: 0.29531402289867403, Validation Loss: 0.2921487311058291\n\nLearning Rate: 6.10800555096723e-08\nEpoch 5601, Training Loss: 0.2953100800514221, Validation Loss: 0.29214085142797747\nEpoch 5601, Training Loss: 0.295307257771492, Validation Loss: 0.29214085142797747\n\nLearning Rate: 5.810107342212943e-08\nEpoch 5701, Training Loss: 0.295303550362587, Validation Loss: 0.2921334008963814\nEpoch 5701, Training Loss: 0.2953008562326431, Validation Loss: 0.2921334008963814\n\nLearning Rate: 5.5267381547633204e-08\nEpoch 5801, Training Loss: 0.295297372341156, Validation Loss: 0.2921263616355276\nEpoch 5801, Training Loss: 0.2952948272228241, Validation Loss: 0.2921263616355276\n\nLearning Rate: 5.257189382611787e-08\nEpoch 5901, Training Loss: 0.29529154896736143, Validation Loss: 0.29211970383982694\nEpoch 5901, Training Loss: 0.29528912901878357, Validation Loss: 0.29211970383982694\n\nLearning Rate: 5.000786979717096e-08\nEpoch 6001, Training Loss: 0.29528604745864867, Validation Loss: 0.2921133917582528\nEpoch 6001, Training Loss: 0.29528374075889585, Validation Loss: 0.2921133917582528\n\nLearning Rate: 4.756889774452837e-08\nEpoch 6101, Training Loss: 0.2952808439731598, Validation Loss: 0.29210747305557394\nEpoch 6101, Training Loss: 0.2952786356210709, Validation Loss: 0.29210747305557394\n\nLearning Rate: 4.524887866264216e-08\nEpoch 6201, Training Loss: 0.29527592062950136, Validation Loss: 0.2921018583378901\nEpoch 6201, Training Loss: 0.29527383744716645, Validation Loss: 0.2921018583378901\n\nLearning Rate: 4.3042011005227116e-08\nEpoch 6301, Training Loss: 0.29527128040790557, Validation Loss: 0.292096493959061\nEpoch 6301, Training Loss: 0.2952692866325378, Validation Loss: 0.292096493959061\n\nLearning Rate: 4.0942776177647584e-08\nEpoch 6401, Training Loss: 0.29526686668395996, Validation Loss: 0.29209148124306267\nEpoch 6401, Training Loss: 0.2952649652957916, Validation Loss: 0.29209148124306267\n\nLearning Rate: 3.8945924736866306e-08\nEpoch 6501, Training Loss: 0.2952627092599869, Validation Loss: 0.2920866831018255\nEpoch 6501, Training Loss: 0.29526090919971465, Validation Loss: 0.2920866831018255\n\nLearning Rate: 3.704646326444598e-08\nEpoch 6601, Training Loss: 0.29525876343250274, Validation Loss: 0.29208216510176543\nEpoch 6601, Training Loss: 0.295257031917572, Validation Loss: 0.29208216510176543\n\nLearning Rate: 3.523964187977773e-08\nEpoch 6701, Training Loss: 0.2952550083398819, Validation Loss: 0.2920778795585133\nEpoch 6701, Training Loss: 0.29525339007377627, Validation Loss: 0.2920778795585133\n\nLearning Rate: 3.3520942362311515e-08\nEpoch 6801, Training Loss: 0.2952514737844467, Validation Loss: 0.2920738681979338\nEpoch 6801, Training Loss: 0.29524990618228913, Validation Loss: 0.2920738681979338\n\nLearning Rate: 3.188606685308625e-08\nEpoch 6901, Training Loss: 0.2952481180429459, Validation Loss: 0.2920699939254239\nEpoch 6901, Training Loss: 0.2952466279268265, Validation Loss: 0.2920699939254239\n\nLearning Rate: 3.033092710730627e-08\nEpoch 7001, Training Loss: 0.29524493515491484, Validation Loss: 0.29206632230739954\nEpoch 7001, Training Loss: 0.295243513584137, Validation Loss: 0.29206632230739954\n\nLearning Rate: 2.8851634271088635e-08\nEpoch 7101, Training Loss: 0.295241916179657, Validation Loss: 0.29206293678579004\nEpoch 7101, Training Loss: 0.2952405959367752, Validation Loss: 0.29206293678579004\n\nLearning Rate: 2.744448915681642e-08\nEpoch 7201, Training Loss: 0.29523906409740447, Validation Loss: 0.29205962279563435\nEpoch 7201, Training Loss: 0.2952377885580063, Validation Loss: 0.29205962279563435\n\nLearning Rate: 2.6105972992780285e-08\nEpoch 7301, Training Loss: 0.29523636400699615, Validation Loss: 0.29205655317929546\nEpoch 7301, Training Loss: 0.2952351480722427, Validation Loss: 0.29205655317929546\n\nLearning Rate: 2.4832738623976294e-08\nEpoch 7401, Training Loss: 0.2952337771654129, Validation Loss: 0.29205353720909694\nEpoch 7401, Training Loss: 0.2952326416969299, Validation Loss: 0.29205353720909694\n\nLearning Rate: 2.3621602142056346e-08\nEpoch 7501, Training Loss: 0.295231357216835, Validation Loss: 0.2920507894565332\nEpoch 7501, Training Loss: 0.2952302634716034, Validation Loss: 0.2920507894565332\n\nLearning Rate: 2.2469534923500738e-08\nEpoch 7601, Training Loss: 0.29522906243801117, Validation Loss: 0.292048137069441\nEpoch 7601, Training Loss: 0.29522799551486967, Validation Loss: 0.292048137069441\n\nLearning Rate: 2.1373656056103048e-08\nEpoch 7701, Training Loss: 0.29522685408592225, Validation Loss: 0.29204559793313395\nEpoch 7701, Training Loss: 0.2952258616685867, Validation Loss: 0.29204559793313395\n\nLearning Rate: 2.033122513482875e-08\nEpoch 7801, Training Loss: 0.2952247768640518, Validation Loss: 0.2920432554895413\nEpoch 7801, Training Loss: 0.29522384107112887, Validation Loss: 0.2920432554895413\n\nLearning Rate: 1.9339635409032487e-08\nEpoch 7901, Training Loss: 0.2952228128910065, Validation Loss: 0.292040984570869\nEpoch 7901, Training Loss: 0.29522189795970916, Validation Loss: 0.292040984570869\n\nLearning Rate: 1.839640726389771e-08\nEpoch 8001, Training Loss: 0.29522094428539275, Validation Loss: 0.2920388328582195\nEpoch 8001, Training Loss: 0.2952201008796692, Validation Loss: 0.2920388328582195\n\nLearning Rate: 1.749918201979791e-08\nEpoch 8101, Training Loss: 0.2952191710472107, Validation Loss: 0.2920367943963552\nEpoch 8101, Training Loss: 0.2952183723449707, Validation Loss: 0.2920367943963552\n\nLearning Rate: 1.6645716034074052e-08\nEpoch 8201, Training Loss: 0.2952174633741379, Validation Loss: 0.29203488110555154\nEpoch 8201, Training Loss: 0.2952166885137558, Validation Loss: 0.29203488110555154\n\nLearning Rate: 1.583387509047865e-08\nEpoch 8301, Training Loss: 0.29521587789058684, Validation Loss: 0.29203305125667717\nEpoch 8301, Training Loss: 0.2952151417732239, Validation Loss: 0.29203305125667717\n\nLearning Rate: 1.5061629062256598e-08\nEpoch 8401, Training Loss: 0.295214381814003, Validation Loss: 0.29203130485626555\nEpoch 8401, Training Loss: 0.29521364569664, Validation Loss: 0.29203130485626555\n\nLearning Rate: 1.4327046835516928e-08\nEpoch 8501, Training Loss: 0.29521292448043823, Validation Loss: 0.2920296299742408\nEpoch 8501, Training Loss: 0.2952122509479523, Validation Loss: 0.2920296299742408\n\nLearning Rate: 1.3628291480200754e-08\nEpoch 8601, Training Loss: 0.2952115833759308, Validation Loss: 0.29202805642599233\nEpoch 8601, Training Loss: 0.2952109336853027, Validation Loss: 0.29202805642599233\n\nLearning Rate: 1.296361565656955e-08\nEpoch 8701, Training Loss: 0.29521026015281676, Validation Loss: 0.29202661996581347\nEpoch 8701, Training Loss: 0.29520965218544004, Validation Loss: 0.29202661996581347\n\nLearning Rate: 1.233135724572715e-08\nEpoch 8801, Training Loss: 0.2952090293169022, Validation Loss: 0.2920251537000455\nEpoch 8801, Training Loss: 0.29520846605300904, Validation Loss: 0.2920251537000455\n\nLearning Rate: 1.1729935193248887e-08\nEpoch 8901, Training Loss: 0.2952078700065613, Validation Loss: 0.2920238364458894\nEpoch 8901, Training Loss: 0.29520732164382935, Validation Loss: 0.2920238364458894\n\nLearning Rate: 1.1157845555524284e-08\nEpoch 9001, Training Loss: 0.29520673751831056, Validation Loss: 0.29202257879637816\nEpoch 9001, Training Loss: 0.29520622491836546, Validation Loss: 0.29202257879637816\n\nLearning Rate: 1.0613657738926554e-08\nEpoch 9101, Training Loss: 0.295205694437027, Validation Loss: 0.29202134499068483\nEpoch 9101, Training Loss: 0.2952052056789398, Validation Loss: 0.29202134499068483\n\nLearning Rate: 1.0096010922404486e-08\nEpoch 9201, Training Loss: 0.2952047049999237, Validation Loss: 0.2920202244357766\nEpoch 9201, Training Loss: 0.29520423412323, Validation Loss: 0.2920202244357766\n\nLearning Rate: 9.603610654550809e-09\nEpoch 9301, Training Loss: 0.2952037274837494, Validation Loss: 0.29201912771815286\nEpoch 9301, Training Loss: 0.29520330131053923, Validation Loss: 0.29201912771815286\n\nLearning Rate: 9.135225616637535e-09\nEpoch 9401, Training Loss: 0.29520281553268435, Validation Loss: 0.29201809657021177\nEpoch 9401, Training Loss: 0.2952024072408676, Validation Loss: 0.29201809657021177\n\nLearning Rate: 8.689684543523798e-09\nEpoch 9501, Training Loss: 0.29520197212696075, Validation Loss: 0.29201705350199514\nEpoch 9501, Training Loss: 0.29520157277584075, Validation Loss: 0.29201705350199514\n\nLearning Rate: 8.265873294736437e-09\nEpoch 9601, Training Loss: 0.29520115852355955, Validation Loss: 0.2920161117544878\nEpoch 9601, Training Loss: 0.29520074725151063, Validation Loss: 0.2920161117544878\n\nLearning Rate: 7.86273206839915e-09\nEpoch 9701, Training Loss: 0.29520038068294524, Validation Loss: 0.29201521173284517\nEpoch 9701, Training Loss: 0.2951999992132187, Validation Loss: 0.29201521173284517\n\nLearning Rate: 7.479252751043244e-09\nEpoch 9801, Training Loss: 0.2951996445655823, Validation Loss: 0.2920144130384453\nEpoch 9801, Training Loss: 0.2951992839574814, Validation Loss: 0.2920144130384453\n\nLearning Rate: 7.114476396672783e-09\nEpoch 9901, Training Loss: 0.29519895315170286, Validation Loss: 0.2920136262610543\nEpoch 9901, Training Loss: 0.2951986104249954, Validation Loss: 0.2920136262610543\n\nLearning Rate: 6.76749082878019e-09\nEpoch 10001, Training Loss: 0.29519826769828794, Validation Loss: 0.29201291100858373\nEpoch 10001, Training Loss: 0.2951979458332062, Validation Loss: 0.29201291100858373\n\nLearning Rate: 6.43742835931576e-09\nEpoch 10101, Training Loss: 0.2951976388692856, Validation Loss: 0.29201220171788433\nEpoch 10101, Training Loss: 0.29519733786582947, Validation Loss: 0.29201220171788433\n\nLearning Rate: 6.123463618907078e-09\nEpoch 10201, Training Loss: 0.2951970398426056, Validation Loss: 0.2920115222295073\nEpoch 10201, Training Loss: 0.29519674777984617, Validation Loss: 0.2920115222295073\n\nLearning Rate: 5.824811492902445e-09\nEpoch 10301, Training Loss: 0.29519646763801577, Validation Loss: 0.292010842744397\nEpoch 10301, Training Loss: 0.2951961994171143, Validation Loss: 0.292010842744397\n\nLearning Rate: 5.54072515807712e-09\nEpoch 10401, Training Loss: 0.2951959192752838, Validation Loss: 0.29201022882243594\nEpoch 10401, Training Loss: 0.295195659995079, Validation Loss: 0.29201022882243594\n\nLearning Rate: 5.270494215092856e-09\nEpoch 10501, Training Loss: 0.29519540071487427, Validation Loss: 0.2920096327759882\nEpoch 10501, Training Loss: 0.29519514739513397, Validation Loss: 0.2920096327759882\n\nLearning Rate: 5.013442912040689e-09\nEpoch 10601, Training Loss: 0.2951949179172516, Validation Loss: 0.2920090188540271\nEpoch 10601, Training Loss: 0.2951946914196014, Validation Loss: 0.2920090188540271\n\nLearning Rate: 4.768928454624668e-09\nEpoch 10701, Training Loss: 0.2951944380998611, Validation Loss: 0.2920084824154908\nEpoch 10701, Training Loss: 0.2951942205429077, Validation Loss: 0.2920084824154908\n\nLearning Rate: 4.536339398760913e-09\nEpoch 10801, Training Loss: 0.29519397020339966, Validation Loss: 0.2920079698175058\nEpoch 10801, Training Loss: 0.29519377648830414, Validation Loss: 0.2920079698175058\n\nLearning Rate: 4.315094121572456e-09\nEpoch 10901, Training Loss: 0.2951935440301895, Validation Loss: 0.29200747510483427\nEpoch 10901, Training Loss: 0.2951933592557907, Validation Loss: 0.29200747510483427\n\nLearning Rate: 4.104639366956356e-09\nEpoch 11001, Training Loss: 0.29519314765930177, Validation Loss: 0.29200706383735875\nEpoch 11001, Training Loss: 0.29519295394420625, Validation Loss: 0.29200706383735875\n\nLearning Rate: 3.904448862086071e-09\nEpoch 11101, Training Loss: 0.2951927691698074, Validation Loss: 0.2920066466048453\nEpoch 11101, Training Loss: 0.29519256949424744, Validation Loss: 0.2920066466048453\n\nLearning Rate: 3.7140220013894614e-09\nEpoch 11201, Training Loss: 0.2951923727989197, Validation Loss: 0.29200621745858973\nEpoch 11201, Training Loss: 0.29519220292568205, Validation Loss: 0.29200621745858973\n\nLearning Rate: 3.5328825947114943e-09\nEpoch 11301, Training Loss: 0.29519203305244446, Validation Loss: 0.292005841951941\nEpoch 11301, Training Loss: 0.29519187211990355, Validation Loss: 0.292005841951941\n\nLearning Rate: 3.360577676531272e-09\nEpoch 11401, Training Loss: 0.29519171714782716, Validation Loss: 0.2920055260532038\nEpoch 11401, Training Loss: 0.295191553235054, Validation Loss: 0.2920055260532038\n\nLearning Rate: 3.1966763732556418e-09\nEpoch 11501, Training Loss: 0.2951913833618164, Validation Loss: 0.29200511478572827\nEpoch 11501, Training Loss: 0.2951912432909012, Validation Loss: 0.29200511478572827\n\nLearning Rate: 3.0407688257568993e-09\nEpoch 11601, Training Loss: 0.2951911002397537, Validation Loss: 0.2920048048422288\nEpoch 11601, Training Loss: 0.2951909452676773, Validation Loss: 0.2920048048422288\n\nLearning Rate: 2.892465164460224e-09\nEpoch 11701, Training Loss: 0.2951908320188522, Validation Loss: 0.29200447702321597\nEpoch 11701, Training Loss: 0.2951906830072403, Validation Loss: 0.29200447702321597\n\nLearning Rate: 2.7513945344179134e-09\nEpoch 11801, Training Loss: 0.29519055783748627, Validation Loss: 0.2920041372773941\nEpoch 11801, Training Loss: 0.2951904237270355, Validation Loss: 0.2920041372773941\n\nLearning Rate: 2.6172041679324676e-09\nEpoch 11901, Training Loss: 0.295190292596817, Validation Loss: 0.2920037975381056\nEpoch 11901, Training Loss: 0.29519015550613403, Validation Loss: 0.2920037975381056\n\nLearning Rate: 2.489558502409476e-09\nEpoch 12001, Training Loss: 0.29519003033638, Validation Loss: 0.2920034995181485\nEpoch 12001, Training Loss: 0.29518992602825167, Validation Loss: 0.2920034995181485\n\nLearning Rate: 2.368138341234385e-09\nEpoch 12101, Training Loss: 0.29518982470035554, Validation Loss: 0.29200327897834943\nEpoch 12101, Training Loss: 0.2951897144317627, Validation Loss: 0.29200327897834943\n\nLearning Rate: 2.2526400555747794e-09\nEpoch 12201, Training Loss: 0.2951896131038666, Validation Loss: 0.2920030703620927\nEpoch 12201, Training Loss: 0.2951894968748093, Validation Loss: 0.2920030703620927\n\nLearning Rate: 2.1427748251121743e-09\nEpoch 12301, Training Loss: 0.2951893866062164, Validation Loss: 0.29200279022418235\nEpoch 12301, Training Loss: 0.29518929719924925, Validation Loss: 0.29200279022418235\n\nLearning Rate: 2.0382679158046645e-09\nEpoch 12401, Training Loss: 0.29518919587135317, Validation Loss: 0.2920026054484769\nEpoch 12401, Training Loss: 0.2951891183853149, Validation Loss: 0.2920026054484769\n\nLearning Rate: 1.938857992874357e-09\nEpoch 12501, Training Loss: 0.29518902599811553, Validation Loss: 0.2920024087557625\nEpoch 12501, Training Loss: 0.2951889306306839, Validation Loss: 0.2920024087557625\n\nLearning Rate: 1.8442964673016205e-09\nEpoch 12601, Training Loss: 0.2951888293027878, Validation Loss: 0.2920021584169078\nEpoch 12601, Training Loss: 0.29518876373767855, Validation Loss: 0.2920021584169078\n\nLearning Rate: 1.754346874191966e-09\nEpoch 12701, Training Loss: 0.2951886624097824, Validation Loss: 0.29200192596009983\nEpoch 12701, Training Loss: 0.2951885640621185, Validation Loss: 0.29200192596009983\n\nLearning Rate: 1.6687842814610687e-09\nEpoch 12801, Training Loss: 0.2951885163784027, Validation Loss: 0.2920017054235675\nEpoch 12801, Training Loss: 0.2951884329319, Validation Loss: 0.2920017054235675\n\nLearning Rate: 1.5873947273592654e-09\nEpoch 12901, Training Loss: 0.2951883465051651, Validation Loss: 0.2920015564119556\nEpoch 12901, Training Loss: 0.29518826603889464, Validation Loss: 0.2920015564119556\n\nLearning Rate: 1.5099746854289754e-09\nEpoch 13001, Training Loss: 0.2951882123947144, Validation Loss: 0.29200137759802125\nEpoch 13001, Training Loss: 0.29518812596797944, Validation Loss: 0.29200137759802125\n\nLearning Rate: 1.4363305555570931e-09\nEpoch 13101, Training Loss: 0.29518804550170896, Validation Loss: 0.292001258385465\nEpoch 13101, Training Loss: 0.29518796503543854, Validation Loss: 0.292001258385465\n\nLearning Rate: 1.3662781798496493e-09\nEpoch 13201, Training Loss: 0.2951879113912582, Validation Loss: 0.29200109745684416\nEpoch 13201, Training Loss: 0.29518785178661344, Validation Loss: 0.29200109745684416\n\nLearning Rate: 1.2996423821181251e-09\nEpoch 13301, Training Loss: 0.29518780410289763, Validation Loss: 0.2920009603655078\nEpoch 13301, Training Loss: 0.29518772959709166, Validation Loss: 0.2920009603655078\n\nLearning Rate: 1.2362565298258273e-09\nEpoch 13401, Training Loss: 0.29518766701221466, Validation Loss: 0.2920008530797606\nEpoch 13401, Training Loss: 0.2951876163482666, Validation Loss: 0.2920008530797606\n\nLearning Rate: 1.1759621173989122e-09\nEpoch 13501, Training Loss: 0.29518757462501527, Validation Loss: 0.29200071002991984\nEpoch 13501, Training Loss: 0.2951874971389771, Validation Loss: 0.29200071002991984\n\nLearning Rate: 1.1186083698600678e-09\nEpoch 13601, Training Loss: 0.295187446475029, Validation Loss: 0.29200059081736357\nEpoch 13601, Training Loss: 0.2951874077320099, Validation Loss: 0.29200059081736357\n\nLearning Rate: 1.0640518657936792e-09\nEpoch 13701, Training Loss: 0.2951873689889908, Validation Loss: 0.2920004894868541\nEpoch 13701, Training Loss: 0.295187321305275, Validation Loss: 0.2920004894868541\n\nLearning Rate: 1.0121561786996492e-09\nEpoch 13801, Training Loss: 0.29518727362155917, Validation Loss: 0.29200037623280223\nEpoch 13801, Training Loss: 0.2951872229576111, Validation Loss: 0.29200037623280223\n\nLearning Rate: 9.627915358390244e-10\nEpoch 13901, Training Loss: 0.2951871782541275, Validation Loss: 0.2920002331829615\nEpoch 13901, Training Loss: 0.2951871246099472, Validation Loss: 0.2920002331829615\n\nLearning Rate: 9.15834493718325e-10\nEpoch 14001, Training Loss: 0.29518709480762484, Validation Loss: 0.29200010205012955\nEpoch 14001, Training Loss: 0.2951870381832123, Validation Loss: 0.29200010205012955\n\nLearning Rate: 8.711676294010725e-10\nEpoch 14101, Training Loss: 0.2951870024204254, Validation Loss: 0.2919999768823356\nEpoch 14101, Training Loss: 0.2951869606971741, Validation Loss: 0.2919999768823356\n\nLearning Rate: 8.286792468746023e-10\nEpoch 14201, Training Loss: 0.295186910033226, Validation Loss: 0.29199985767304604\nEpoch 14201, Training Loss: 0.29518687129020693, Validation Loss: 0.29199985767304604\n\nLearning Rate: 7.882630977378839e-10\nEpoch 14301, Training Loss: 0.29518683552742003, Validation Loss: 0.2919997801863545\nEpoch 14301, Training Loss: 0.29518681168556216, Validation Loss: 0.2919997801863545\n\nLearning Rate: 7.498181155118877e-10\nEpoch 14401, Training Loss: 0.29518677592277526, Validation Loss: 0.2919996550185605\nEpoch 14401, Training Loss: 0.2951867550611496, Validation Loss: 0.2919996550185605\n\nLearning Rate: 7.132481629081061e-10\nEpoch 14501, Training Loss: 0.2951867043972015, Validation Loss: 0.29199957753186895\nEpoch 14501, Training Loss: 0.2951866567134857, Validation Loss: 0.29199957753186895\n\nLearning Rate: 6.784617914232334e-10\nEpoch 14601, Training Loss: 0.2951866537332535, Validation Loss: 0.29199954772954656\nEpoch 14601, Training Loss: 0.2951866120100021, Validation Loss: 0.29199954772954656\n\nLearning Rate: 6.453720126588376e-10\nEpoch 14701, Training Loss: 0.29518658816814425, Validation Loss: 0.2919994762046262\nEpoch 14701, Training Loss: 0.29518657028675077, Validation Loss: 0.2919994762046262\n\nLearning Rate: 6.138960807941766e-10\nEpoch 14801, Training Loss: 0.2951865315437317, Validation Loss: 0.29199935103356545\nEpoch 14801, Training Loss: 0.29518650472164154, Validation Loss: 0.29199935103356545\n\nLearning Rate: 5.839552856681958e-10\nEpoch 14901, Training Loss: 0.2951864778995514, Validation Loss: 0.291999261629865\nEpoch 14901, Training Loss: 0.2951864361763, Validation Loss: 0.291999261629865\n\nLearning Rate: 5.554747559532864e-10\nEpoch 15001, Training Loss: 0.2951864182949066, Validation Loss: 0.29199914838234664\nEpoch 15001, Training Loss: 0.29518640637397764, Validation Loss: 0.29199914838234664\n\nLearning Rate: 5.283832719286026e-10\nEpoch 15101, Training Loss: 0.2951863706111908, Validation Loss: 0.2919990708989218\nEpoch 15101, Training Loss: 0.2951863706111908, Validation Loss: 0.2919990708989218\n\nLearning Rate: 5.026130873847568e-10\nEpoch 15201, Training Loss: 0.29518632888793944, Validation Loss: 0.2919990112910103\nEpoch 15201, Training Loss: 0.29518629908561705, Validation Loss: 0.2919990112910103\n\nLearning Rate: 4.780997602145365e-10\nEpoch 15301, Training Loss: 0.29518628120422363, Validation Loss: 0.2919989695684123\nEpoch 15301, Training Loss: 0.2951862573623657, Validation Loss: 0.2919989695684123\n\nLearning Rate: 4.5478199126601107e-10\nEpoch 15401, Training Loss: 0.2951862573623657, Validation Loss: 0.2919989397660899\nEpoch 15401, Training Loss: 0.2951862245798111, Validation Loss: 0.2919989397660899\n\nLearning Rate: 4.326014710550563e-10\nEpoch 15501, Training Loss: 0.29518618881702424, Validation Loss: 0.29199890996376754\nEpoch 15501, Training Loss: 0.29518618881702424, Validation Loss: 0.29199890996376754\n\nLearning Rate: 4.115027339539802e-10\nEpoch 15601, Training Loss: 0.2951861500740051, Validation Loss: 0.2919988622826651\nEpoch 15601, Training Loss: 0.29518614411354066, Validation Loss: 0.2919988622826651\n\nLearning Rate: 3.9143301949162666e-10\nEpoch 15701, Training Loss: 0.29518612325191496, Validation Loss: 0.2919988622826651\nEpoch 15701, Training Loss: 0.2951861023902893, Validation Loss: 0.2919988622826651\n\nLearning Rate: 3.723421404181176e-10\nEpoch 15801, Training Loss: 0.29518610537052153, Validation Loss: 0.2919988861232163\nEpoch 15801, Training Loss: 0.2951860785484314, Validation Loss: 0.2919988861232163\n\nLearning Rate: 3.5418235720431056e-10\nEpoch 15901, Training Loss: 0.2951860547065735, Validation Loss: 0.2919988384388471\nEpoch 15901, Training Loss: 0.2951860398054123, Validation Loss: 0.2919988384388471\n\nLearning Rate: 3.369082586621396e-10\nEpoch 16001, Training Loss: 0.29518601596355437, Validation Loss: 0.2919987728724312\nEpoch 16001, Training Loss: 0.29518601298332214, Validation Loss: 0.2919987728724312\n\nLearning Rate: 3.2047664838731196e-10\nEpoch 16101, Training Loss: 0.295185986161232, Validation Loss: 0.2919987073125487\nEpoch 16101, Training Loss: 0.2951859623193741, Validation Loss: 0.2919987073125487\n\nLearning Rate: 3.0484643674039564e-10\nEpoch 16201, Training Loss: 0.29518595039844514, Validation Loss: 0.29199867154518844\nEpoch 16201, Training Loss: 0.29518593549728395, Validation Loss: 0.29199867154518844\n\nLearning Rate: 2.8997853809617933e-10\nEpoch 16301, Training Loss: 0.29518592953681944, Validation Loss: 0.2919986179023148\nEpoch 16301, Training Loss: 0.29518591463565824, Validation Loss: 0.2919986179023148\n\nLearning Rate: 2.758357731043631e-10\nEpoch 16401, Training Loss: 0.295185911655426, Validation Loss: 0.2919986059853059\nEpoch 16401, Training Loss: 0.2951858967542648, Validation Loss: 0.2919986059853059\n\nLearning Rate: 2.623827757171666e-10\nEpoch 16501, Training Loss: 0.2951858639717102, Validation Loss: 0.2919985940650303\nEpoch 16501, Training Loss: 0.29518585801124575, Validation Loss: 0.2919985940650303\n\nLearning Rate: 2.4958590475136597e-10\nEpoch 16601, Training Loss: 0.2951858460903168, Validation Loss: 0.29199856426597465\nEpoch 16601, Training Loss: 0.2951858341693878, Validation Loss: 0.29199856426597465\n\nLearning Rate: 2.3741315976360535e-10\nEpoch 16701, Training Loss: 0.2951858311891556, Validation Loss: 0.29199848082077867\nEpoch 16701, Training Loss: 0.2951858162879944, Validation Loss: 0.29199848082077867\n\nLearning Rate: 2.2583410102861881e-10\nEpoch 16801, Training Loss: 0.2951858162879944, Validation Loss: 0.29199843909818063\nEpoch 16801, Training Loss: 0.2951858162879944, Validation Loss: 0.29199843909818063\n\nLearning Rate: 2.1481977342025457e-10\nEpoch 16901, Training Loss: 0.29518580436706543, Validation Loss: 0.29199834968794675\nEpoch 16901, Training Loss: 0.2951858103275299, Validation Loss: 0.29199834968794675\n\nLearning Rate: 2.043426340049569e-10\nEpoch 17001, Training Loss: 0.295185798406601, Validation Loss: 0.29199833180916673\nEpoch 17001, Training Loss: 0.29518578946590424, Validation Loss: 0.29199833180916673\n\nLearning Rate: 1.9437648316664114e-10\nEpoch 17101, Training Loss: 0.29518579244613646, Validation Loss: 0.29199826624275077\nEpoch 17101, Training Loss: 0.295185786485672, Validation Loss: 0.29199826624275077\n\nLearning Rate: 1.8489639909073016e-10\nEpoch 17201, Training Loss: 0.29518577456474304, Validation Loss: 0.2919982006796015\nEpoch 17201, Training Loss: 0.2951857656240463, Validation Loss: 0.2919982006796015\n\nLearning Rate: 1.7587867544352027e-10\nEpoch 17301, Training Loss: 0.2951857566833496, Validation Loss: 0.29199809935235876\nEpoch 17301, Training Loss: 0.2951857417821884, Validation Loss: 0.29199809935235876\n\nLearning Rate: 1.6730076209103412e-10\nEpoch 17401, Training Loss: 0.2951857507228851, Validation Loss: 0.2919980218656672\nEpoch 17401, Training Loss: 0.2951857417821884, Validation Loss: 0.2919980218656672\n\nLearning Rate: 1.591412087091198e-10\nEpoch 17501, Training Loss: 0.2951857417821884, Validation Loss: 0.2919980099453916\nEpoch 17501, Training Loss: 0.295185723900795, Validation Loss: 0.2919980099453916\n\nLearning Rate: 1.5137961114378494e-10\nEpoch 17601, Training Loss: 0.29518570601940153, Validation Loss: 0.29199806358826524\nEpoch 17601, Training Loss: 0.29518570601940153, Validation Loss: 0.29199806358826524\n\nLearning Rate: 1.4399656038763214e-10\nEpoch 17701, Training Loss: 0.2951857179403305, Validation Loss: 0.29199806954676966\nEpoch 17701, Training Loss: 0.29518571197986604, Validation Loss: 0.29199806954676966\n\nLearning Rate: 1.3697359404480337e-10\nEpoch 17801, Training Loss: 0.29518569707870485, Validation Loss: 0.29199804570621846\nEpoch 17801, Training Loss: 0.2951856940984726, Validation Loss: 0.29199804570621846\n\nLearning Rate: 1.3029315016306485e-10\nEpoch 17901, Training Loss: 0.2951857089996338, Validation Loss: 0.2919979920633448\nEpoch 17901, Training Loss: 0.2951857030391693, Validation Loss: 0.2919979920633448\n\nLearning Rate: 1.239385233175827e-10\nEpoch 18001, Training Loss: 0.2951856940984726, Validation Loss: 0.2919979562992513\nEpoch 18001, Training Loss: 0.29518569111824033, Validation Loss: 0.2919979562992513\n\nLearning Rate: 1.1789382283657008e-10\nEpoch 18101, Training Loss: 0.2951856881380081, Validation Loss: 0.2919979622610224\nEpoch 18101, Training Loss: 0.2951856732368469, Validation Loss: 0.2919979622610224\n\nLearning Rate: 1.1214393306434353e-10\nEpoch 18201, Training Loss: 0.29518568217754365, Validation Loss: 0.2919979622610224\nEpoch 18201, Training Loss: 0.2951856881380081, Validation Loss: 0.2919979622610224\n\nLearning Rate: 1.0667447556241993e-10\nEpoch 18301, Training Loss: 0.29518569111824033, Validation Loss: 0.2919979801430692\nEpoch 18301, Training Loss: 0.29518569111824033, Validation Loss: 0.2919979801430692\n\nLearning Rate: 1.0147177315413289e-10\nEpoch 18401, Training Loss: 0.2951856851577759, Validation Loss: 0.2919979920600781\nEpoch 18401, Training Loss: 0.2951856881380081, Validation Loss: 0.2919979920600781\n\nLearning Rate: 9.652281572285636e-11\nEpoch 18501, Training Loss: 0.2951856881380081, Validation Loss: 0.2919980218624005\nEpoch 18501, Training Loss: 0.2951856881380081, Validation Loss: 0.2919980218624005\n\nLearning Rate: 9.181522767830952e-11\nEpoch 18601, Training Loss: 0.29518568217754365, Validation Loss: 0.29199805762322734\nEpoch 18601, Training Loss: 0.2951856702566147, Validation Loss: 0.29199805762322734\n\nLearning Rate: 8.733723700958719e-11\nEpoch 18701, Training Loss: 0.29518568217754365, Validation Loss: 0.29199808146377854\nEpoch 18701, Training Loss: 0.29518567621707914, Validation Loss: 0.29199808146377854\n\nLearning Rate: 8.307764584752861e-11\nEpoch 18801, Training Loss: 0.29518569707870485, Validation Loss: 0.29199805166145615\nEpoch 18801, Training Loss: 0.2951856881380081, Validation Loss: 0.29199805166145615\n\nLearning Rate: 7.902580246281152e-11\nEpoch 18901, Training Loss: 0.29518568217754365, Validation Loss: 0.29199806954350294\nEpoch 18901, Training Loss: 0.2951856791973114, Validation Loss: 0.29199806954350294\n\nLearning Rate: 7.517157462974845e-11\nEpoch 19001, Training Loss: 0.2951856732368469, Validation Loss: 0.29199808146377854\nEpoch 19001, Training Loss: 0.29518566131591795, Validation Loss: 0.29199808146377854\n\nLearning Rate: 7.150532428917776e-11\nEpoch 19101, Training Loss: 0.2951856851577759, Validation Loss: 0.29199806358173175\nEpoch 19101, Training Loss: 0.2951856881380081, Validation Loss: 0.29199806358173175\n\nLearning Rate: 6.801788344709022e-11\nEpoch 19201, Training Loss: 0.2951856881380081, Validation Loss: 0.291998003977087\nEpoch 19201, Training Loss: 0.2951856791973114, Validation Loss: 0.291998003977087\n\nLearning Rate: 6.470053124872207e-11\nEpoch 19301, Training Loss: 0.2951856583356857, Validation Loss: 0.29199797417803136\nEpoch 19301, Training Loss: 0.2951856553554535, Validation Loss: 0.29199797417803136\n\nLearning Rate: 6.154497217078492e-11\nEpoch 19401, Training Loss: 0.29518564641475675, Validation Loss: 0.2919979503374801\nEpoch 19401, Training Loss: 0.29518565237522126, Validation Loss: 0.2919979503374801\n\nLearning Rate: 5.854331527729929e-11\nEpoch 19501, Training Loss: 0.29518566429615023, Validation Loss: 0.2919979503374801\nEpoch 19501, Training Loss: 0.29518566727638246, Validation Loss: 0.2919979503374801\n\nLearning Rate: 5.5688054487157525e-11\nEpoch 19601, Training Loss: 0.2951856791973114, Validation Loss: 0.2919979384172045\nEpoch 19601, Training Loss: 0.2951856881380081, Validation Loss: 0.2919979384172045\n\nLearning Rate: 5.2972049804072656e-11\nEpoch 19701, Training Loss: 0.29518566727638246, Validation Loss: 0.291997944375709\nEpoch 19701, Training Loss: 0.29518566727638246, Validation Loss: 0.291997944375709\n\nLearning Rate: 5.038850946197567e-11\nEpoch 19801, Training Loss: 0.2951856553554535, Validation Loss: 0.291997944375709\nEpoch 19801, Training Loss: 0.29518566131591795, Validation Loss: 0.291997944375709\n\nLearning Rate: 4.793097294121334e-11\nEpoch 19901, Training Loss: 0.2951856583356857, Validation Loss: 0.2919979682162602\nEpoch 19901, Training Loss: 0.2951856583356857, Validation Loss: 0.2919979682162602\n\nLearning Rate: 4.5593294813075984e-11\nEpoch 20001, Training Loss: 0.29518566727638246, Validation Loss: 0.2919979264969289\nEpoch 20001, Training Loss: 0.2951856553554535, Validation Loss: 0.2919979264969289\n\nLearning Rate: 4.336962937225616e-11\nEpoch 20101, Training Loss: 0.2951856583356857, Validation Loss: 0.29199783112819055\nEpoch 20101, Training Loss: 0.29518566131591795, Validation Loss: 0.29199783112819055\n\nLearning Rate: 4.125441601880946e-11\nEpoch 20201, Training Loss: 0.29518566429615023, Validation Loss: 0.29199779536409703\nEpoch 20201, Training Loss: 0.29518568217754365, Validation Loss: 0.29199779536409703\n\nLearning Rate: 3.9242365353062864e-11\nEpoch 20301, Training Loss: 0.29518566727638246, Validation Loss: 0.29199780132586817\nEpoch 20301, Training Loss: 0.29518566131591795, Validation Loss: 0.29199780132586817\n\nLearning Rate: 3.7328445948698947e-11\nEpoch 20401, Training Loss: 0.29518566429615023, Validation Loss: 0.29199778940559257\nEpoch 20401, Training Loss: 0.2951856702566147, Validation Loss: 0.29199778940559257\n\nLearning Rate: 3.550787177094009e-11\nEpoch 20501, Training Loss: 0.2951856583356857, Validation Loss: 0.2919977536447657\nEpoch 20501, Training Loss: 0.2951856583356857, Validation Loss: 0.2919977536447657\n\nLearning Rate: 3.377609020837014e-11\nEpoch 20601, Training Loss: 0.29518565237522126, Validation Loss: 0.2919977834438214\nEpoch 20601, Training Loss: 0.29518564939498904, Validation Loss: 0.2919977834438214\n\nLearning Rate: 3.2128770688465097e-11\nEpoch 20701, Training Loss: 0.2951856583356857, Validation Loss: 0.29199778940559257\nEpoch 20701, Training Loss: 0.2951856553554535, Validation Loss: 0.29199778940559257\n\nLearning Rate: 3.056179384836459e-11\nEpoch 20801, Training Loss: 0.2951856553554535, Validation Loss: 0.2919977596032702\nEpoch 20801, Training Loss: 0.2951856583356857, Validation Loss: 0.2919977596032702\n\nLearning Rate: 2.9071241233803865e-11\nEpoch 20901, Training Loss: 0.2951856553554535, Validation Loss: 0.29199776556504137\nEpoch 20901, Training Loss: 0.29518565237522126, Validation Loss: 0.29199776556504137\n\nLearning Rate: 2.765338550044708e-11\nEpoch 21001, Training Loss: 0.2951856583356857, Validation Loss: 0.29199777748531697\nEpoch 21001, Training Loss: 0.29518566131591795, Validation Loss: 0.29199777748531697\n\nLearning Rate: 2.6304681093118823e-11\nEpoch 21101, Training Loss: 0.29518566727638246, Validation Loss: 0.29199779536409703\nEpoch 21101, Training Loss: 0.29518566429615023, Validation Loss: 0.29199779536409703\n\nLearning Rate: 2.5021755379625982e-11\nEpoch 21201, Training Loss: 0.29518566131591795, Validation Loss: 0.29199776556504137\nEpoch 21201, Training Loss: 0.29518566429615023, Validation Loss: 0.29199776556504137\n\nLearning Rate: 2.380140021699877e-11\nEpoch 21301, Training Loss: 0.29518566727638246, Validation Loss: 0.2919977715235458\nEpoch 21301, Training Loss: 0.2951856553554535, Validation Loss: 0.2919977715235458\n\nLearning Rate: 2.2640563929061044e-11\nEpoch 21401, Training Loss: 0.2951856732368469, Validation Loss: 0.2919977715235458\nEpoch 21401, Training Loss: 0.2951856702566147, Validation Loss: 0.2919977715235458\n\nLearning Rate: 2.1536343675268676e-11\nEpoch 21501, Training Loss: 0.29518566131591795, Validation Loss: 0.2919977476829946\nEpoch 21501, Training Loss: 0.29518566429615023, Validation Loss: 0.2919977476829946\n\nLearning Rate: 2.0485978191733167e-11\nEpoch 21601, Training Loss: 0.2951856583356857, Validation Loss: 0.2919977417212234\nEpoch 21601, Training Loss: 0.29518566131591795, Validation Loss: 0.2919977417212234\n\nLearning Rate: 1.9486840886278312e-11\nEpoch 21701, Training Loss: 0.29518566429615023, Validation Loss: 0.2919977298009478\nEpoch 21701, Training Loss: 0.29518566429615023, Validation Loss: 0.2919977298009478\n\nLearning Rate: 1.8536433270263152e-11\nEpoch 21801, Training Loss: 0.2951856583356857, Validation Loss: 0.291997735762719\nEpoch 21801, Training Loss: 0.2951856583356857, Validation Loss: 0.291997735762719\n\nLearning Rate: 1.7632378710746528e-11\nEpoch 21901, Training Loss: 0.29518566727638246, Validation Loss: 0.2919977178806722\nEpoch 21901, Training Loss: 0.29518566429615023, Validation Loss: 0.2919977178806722\n\nLearning Rate: 1.6772416487369566e-11\nEpoch 22001, Training Loss: 0.29518566429615023, Validation Loss: 0.29199776556177465\nEpoch 22001, Training Loss: 0.29518566429615023, Validation Loss: 0.29199776556177465\n\nLearning Rate: 1.5954396139094496e-11\nEpoch 22101, Training Loss: 0.29518566727638246, Validation Loss: 0.29199773575945226\nEpoch 22101, Training Loss: 0.29518566429615023, Validation Loss: 0.29199773575945226\n\nLearning Rate: 1.517627208666302e-11\nEpoch 22201, Training Loss: 0.2951856583356857, Validation Loss: 0.2919977417212234\nEpoch 22201, Training Loss: 0.2951856583356857, Validation Loss: 0.2919977417212234\n\nLearning Rate: 1.4436098517326843e-11\nEpoch 22301, Training Loss: 0.2951856583356857, Validation Loss: 0.2919977417212234\nEpoch 22301, Training Loss: 0.2951856583356857, Validation Loss: 0.2919977417212234\n\nLearning Rate: 1.3732024519058932e-11\nEpoch 22401, Training Loss: 0.2951856583356857, Validation Loss: 0.2919977417212234\nEpoch 22401, Training Loss: 0.2951856583356857, Validation Loss: 0.2919977417212234\n\nLearning Rate: 1.306228945207789e-11\nEpoch 22501, Training Loss: 0.2951856583356857, Validation Loss: 0.2919977417212234\nEpoch 22501, Training Loss: 0.2951856583356857, Validation Loss: 0.2919977417212234\n\nLearning Rate: 1.242521854611124e-11\nEpoch 22601, Training Loss: 0.2951856583356857, Validation Loss: 0.2919977417212234\nEpoch 22601, Training Loss: 0.2951856583356857, Validation Loss: 0.2919977417212234\n\nLearning Rate: 1.1819218712387946e-11\nEpoch 22701, Training Loss: 0.2951856583356857, Validation Loss: 0.2919977417212234\nEpoch 22701, Training Loss: 0.2951856583356857, Validation Loss: 0.2919977417212234\n\nLearning Rate: 1.124277455988747e-11\nEpoch 22801, Training Loss: 0.2951856583356857, Validation Loss: 0.2919977178806722\nEpoch 22801, Training Loss: 0.2951856553554535, Validation Loss: 0.2919977178806722\n\nLearning Rate: 1.0694444605883352e-11\nEpoch 22901, Training Loss: 0.29518565237522126, Validation Loss: 0.2919977178806722\nEpoch 22901, Training Loss: 0.29518565237522126, Validation Loss: 0.2919977178806722\n\nLearning Rate: 1.0172857671305322e-11\nEpoch 23001, Training Loss: 0.29518565237522126, Validation Loss: 0.2919977178806722\nEpoch 23001, Training Loss: 0.29518565237522126, Validation Loss: 0.2919977178806722\n\nLearning Rate: 9.6767094519059e-12\nEpoch 23101, Training Loss: 0.29518565237522126, Validation Loss: 0.2919977178806722\nEpoch 23101, Training Loss: 0.29518565237522126, Validation Loss: 0.2919977178806722\n\nLearning Rate: 9.204759256657312e-12\nEpoch 23201, Training Loss: 0.29518565237522126, Validation Loss: 0.2919977178806722\nEpoch 23201, Training Loss: 0.29518565237522126, Validation Loss: 0.2919977178806722\n\nLearning Rate: 8.755826905222491e-12\nEpoch 23301, Training Loss: 0.29518565237522126, Validation Loss: 0.2919977178806722\nEpoch 23301, Training Loss: 0.29518565237522126, Validation Loss: 0.2919977178806722\n\nLearning Rate: 8.328789776741928e-12\nEpoch 23401, Training Loss: 0.29518565237522126, Validation Loss: 0.2919977178806722\nEpoch 23401, Training Loss: 0.29518565237522126, Validation Loss: 0.2919977178806722\n\nLearning Rate: 7.922580002556385e-12\nEpoch 23501, Training Loss: 0.29518565237522126, Validation Loss: 0.2919977178806722\nEpoch 23501, Training Loss: 0.29518565237522126, Validation Loss: 0.2919977178806722\n\nLearning Rate: 7.536181795845466e-12\nEpoch 23601, Training Loss: 0.29518565237522126, Validation Loss: 0.2919977178806722\nEpoch 23601, Training Loss: 0.29518565237522126, Validation Loss: 0.2919977178806722\n\nLearning Rate: 7.168628911504435e-12\nEpoch 23701, Training Loss: 0.29518565237522126, Validation Loss: 0.2919977178806722\nEpoch 23701, Training Loss: 0.29518565237522126, Validation Loss: 0.2919977178806722\n\nLearning Rate: 6.81900222990733e-12\nEpoch 23801, Training Loss: 0.29518565237522126, Validation Loss: 0.2919977178806722\nEpoch 23801, Training Loss: 0.29518565237522126, Validation Loss: 0.2919977178806722\n\nLearning Rate: 6.4864274585142015e-12\nEpoch 23901, Training Loss: 0.29518565237522126, Validation Loss: 0.2919977178806722\nEpoch 23901, Training Loss: 0.29518565237522126, Validation Loss: 0.2919977178806722\n\nLearning Rate: 6.170072945575029e-12\nEpoch 24001, Training Loss: 0.29518565237522126, Validation Loss: 0.2919977178806722\nEpoch 24001, Training Loss: 0.29518565237522126, Validation Loss: 0.2919977178806722\n\nLearning Rate: 5.869147600463149e-12\nEpoch 24101, Training Loss: 0.29518565237522126, Validation Loss: 0.2919977178806722\nEpoch 24101, Training Loss: 0.29518565237522126, Validation Loss: 0.2919977178806722\n\nLearning Rate: 5.582898915437703e-12\nEpoch 24201, Training Loss: 0.29518565237522126, Validation Loss: 0.2919977178806722\nEpoch 24201, Training Loss: 0.29518565237522126, Validation Loss: 0.2919977178806722\n\nLearning Rate: 5.310611083888208e-12\nEpoch 24301, Training Loss: 0.29518565237522126, Validation Loss: 0.2919977178806722\nEpoch 24301, Training Loss: 0.29518565237522126, Validation Loss: 0.2919977178806722\n\nLearning Rate: 5.051603210355669e-12\nEpoch 24401, Training Loss: 0.29518565237522126, Validation Loss: 0.2919977178806722\nEpoch 24401, Training Loss: 0.29518565237522126, Validation Loss: 0.2919977178806722\n\nLearning Rate: 4.80522760785412e-12\nEpoch 24501, Training Loss: 0.29518565237522126, Validation Loss: 0.2919977178806722\nEpoch 24501, Training Loss: 0.29518565237522126, Validation Loss: 0.2919977178806722\n\nLearning Rate: 4.570868178234788e-12\nEpoch 24601, Training Loss: 0.29518565237522126, Validation Loss: 0.2919977178806722\nEpoch 24601, Training Loss: 0.29518565237522126, Validation Loss: 0.2919977178806722\n\nLearning Rate: 4.347938871542771e-12\nEpoch 24701, Training Loss: 0.29518565237522126, Validation Loss: 0.2919977178806722\nEpoch 24701, Training Loss: 0.29518565237522126, Validation Loss: 0.2919977178806722\n\nLearning Rate: 4.1358822205135945e-12\nEpoch 24801, Training Loss: 0.29518565237522126, Validation Loss: 0.2919977178806722\nEpoch 24801, Training Loss: 0.29518565237522126, Validation Loss: 0.2919977178806722\n\nLearning Rate: 3.9341679465449676e-12\nEpoch 24901, Training Loss: 0.29518565237522126, Validation Loss: 0.2919977178806722\nEpoch 24901, Training Loss: 0.29518565237522126, Validation Loss: 0.2919977178806722\n\nLearning Rate: 3.742291633657745e-12\nEpoch 25001, Training Loss: 0.29518565237522126, Validation Loss: 0.2919977178806722\nEpoch 25001, Training Loss: 0.29518565237522126, Validation Loss: 0.2919977178806722\n\nLearning Rate: 3.5597734671301688e-12\nEpoch 25101, Training Loss: 0.29518565237522126, Validation Loss: 0.2919977178806722\nEpoch 25101, Training Loss: 0.29518565237522126, Validation Loss: 0.2919977178806722\n\nLearning Rate: 3.3861570336511275e-12\nEpoch 25201, Training Loss: 0.29518565237522126, Validation Loss: 0.2919977178806722\nEpoch 25201, Training Loss: 0.29518565237522126, Validation Loss: 0.2919977178806722\n\nLearning Rate: 3.2210081799920693e-12\nEpoch 25301, Training Loss: 0.29518565237522126, Validation Loss: 0.2919977178806722\nEpoch 25301, Training Loss: 0.29518565237522126, Validation Loss: 0.2919977178806722\n\nLearning Rate: 3.0639139273434942e-12\nEpoch 25401, Training Loss: 0.29518565237522126, Validation Loss: 0.2919977178806722\nEpoch 25401, Training Loss: 0.29518565237522126, Validation Loss: 0.2919977178806722\n\nLearning Rate: 2.914481438601174e-12\nEpoch 25501, Training Loss: 0.29518565237522126, Validation Loss: 0.2919977178806722\nEpoch 25501, Training Loss: 0.29518565237522126, Validation Loss: 0.2919977178806722\n\nLearning Rate: 2.7723370360196438e-12\nEpoch 25601, Training Loss: 0.29518565237522126, Validation Loss: 0.2919977178806722\nEpoch 25601, Training Loss: 0.29518565237522126, Validation Loss: 0.2919977178806722\n\nLearning Rate: 2.637125266776467e-12\nEpoch 25701, Training Loss: 0.29518565237522126, Validation Loss: 0.2919977178806722\nEpoch 25701, Training Loss: 0.29518565237522126, Validation Loss: 0.2919977178806722\n\nLearning Rate: 2.508508014110582e-12\nEpoch 25801, Training Loss: 0.29518565237522126, Validation Loss: 0.2919977178806722\nEpoch 25801, Training Loss: 0.29518565237522126, Validation Loss: 0.2919977178806722\n\nLearning Rate: 2.3861636518119946e-12\nEpoch 25901, Training Loss: 0.29518565237522126, Validation Loss: 0.2919977178806722\nEpoch 25901, Training Loss: 0.29518565237522126, Validation Loss: 0.2919977178806722\n\nLearning Rate: 2.269786239948507e-12\nEpoch 26001, Training Loss: 0.29518565237522126, Validation Loss: 0.2919977178806722\nEpoch 26001, Training Loss: 0.29518565237522126, Validation Loss: 0.2919977178806722\n\nLearning Rate: 2.1590847598182676e-12\nEpoch 26101, Training Loss: 0.29518565237522126, Validation Loss: 0.2919977178806722\nEpoch 26101, Training Loss: 0.29518565237522126, Validation Loss: 0.2919977178806722\n\nLearning Rate: 2.0537823862150395e-12\nEpoch 26201, Training Loss: 0.29518565237522126, Validation Loss: 0.2919977178806722\nEpoch 26201, Training Loss: 0.29518565237522126, Validation Loss: 0.2919977178806722\n\nLearning Rate: 1.953615795186372e-12\nEpoch 26301, Training Loss: 0.29518565237522126, Validation Loss: 0.2919977178806722\nEpoch 26301, Training Loss: 0.29518565237522126, Validation Loss: 0.2919977178806722\n\nLearning Rate: 1.8583345055536304e-12\nEpoch 26401, Training Loss: 0.29518565237522126, Validation Loss: 0.2919977178806722\nEpoch 26401, Training Loss: 0.29518565237522126, Validation Loss: 0.2919977178806722\n\nLearning Rate: 1.7677002525472546e-12\nEpoch 26501, Training Loss: 0.29518565237522126, Validation Loss: 0.2919977178806722\nEpoch 26501, Training Loss: 0.29518565237522126, Validation Loss: 0.2919977178806722\n\nLearning Rate: 1.681486391990933e-12\nEpoch 26601, Training Loss: 0.29518565237522126, Validation Loss: 0.2919977178806722\nEpoch 26601, Training Loss: 0.29518565237522126, Validation Loss: 0.2919977178806722\n\nLearning Rate: 1.5994773335447618e-12\nEpoch 26701, Training Loss: 0.29518565237522126, Validation Loss: 0.2919977178806722\nEpoch 26701, Training Loss: 0.29518565237522126, Validation Loss: 0.2919977178806722\n\nLearning Rate: 1.521468001590141e-12\nEpoch 26801, Training Loss: 0.29518565237522126, Validation Loss: 0.2919977178806722\nEpoch 26801, Training Loss: 0.29518565237522126, Validation Loss: 0.2919977178806722\n\nLearning Rate: 1.4472633224082607e-12\nEpoch 26901, Training Loss: 0.29518565237522126, Validation Loss: 0.2919977178806722\nEpoch 26901, Training Loss: 0.29518565237522126, Validation Loss: 0.2919977178806722\n\nLearning Rate: 1.3766777363697991e-12\nEpoch 27001, Training Loss: 0.29518565237522126, Validation Loss: 0.2919977178806722\nEpoch 27001, Training Loss: 0.29518565237522126, Validation Loss: 0.2919977178806722\n\nLearning Rate: 1.3095347339159903e-12\nEpoch 27101, Training Loss: 0.29518565237522126, Validation Loss: 0.2919977178806722\nEpoch 27101, Training Loss: 0.29518565237522126, Validation Loss: 0.2919977178806722\n\nLearning Rate: 1.2456664141707142e-12\nEpoch 27201, Training Loss: 0.29518565237522126, Validation Loss: 0.2919977178806722\nEpoch 27201, Training Loss: 0.29518565237522126, Validation Loss: 0.2919977178806722\n\nLearning Rate: 1.184913065079853e-12\nEpoch 27301, Training Loss: 0.29518565237522126, Validation Loss: 0.2919977178806722\nEpoch 27301, Training Loss: 0.29518565237522126, Validation Loss: 0.2919977178806722\n\nLearning Rate: 1.1271227640279907e-12\nEpoch 27401, Training Loss: 0.29518565237522126, Validation Loss: 0.2919977178806722\nEpoch 27401, Training Loss: 0.29518565237522126, Validation Loss: 0.2919977178806722\n\nLearning Rate: 1.0721509979337458e-12\nEpoch 27501, Training Loss: 0.29518565237522126, Validation Loss: 0.2919977178806722\nEpoch 27501, Training Loss: 0.29518565237522126, Validation Loss: 0.2919977178806722\n\nLearning Rate: 1.019860301873719e-12\nEpoch 27601, Training Loss: 0.29518565237522126, Validation Loss: 0.2919977178806722\nEpoch 27601, Training Loss: 0.29518565237522126, Validation Loss: 0.2919977178806722\n\nLearning Rate: 9.701199153313922e-13\nEpoch 27701, Training Loss: 0.29518565237522126, Validation Loss: 0.2919977178806722\nEpoch 27701, Training Loss: 0.29518565237522126, Validation Loss: 0.2919977178806722\n\nLearning Rate: 9.228054552113752e-13\nEpoch 27801, Training Loss: 0.29518565237522126, Validation Loss: 0.2919977178806722\nEpoch 27801, Training Loss: 0.29518565237522126, Validation Loss: 0.2919977178806722\n\nLearning Rate: 8.777986048013225e-13\nEpoch 27901, Training Loss: 0.29518565237522126, Validation Loss: 0.2919977178806722\nEpoch 27901, Training Loss: 0.29518565237522126, Validation Loss: 0.2919977178806722\n\nLearning Rate: 8.34986817903729e-13\nEpoch 28001, Training Loss: 0.29518565237522126, Validation Loss: 0.2919977178806722\nEpoch 28001, Training Loss: 0.29518565237522126, Validation Loss: 0.2919977178806722\n\nLearning Rate: 7.942630373977382e-13\nEpoch 28101, Training Loss: 0.29518565237522126, Validation Loss: 0.2919977178806722\nEpoch 28101, Training Loss: 0.29518565237522126, Validation Loss: 0.2919977178806722\n\nLearning Rate: 7.555254275271876e-13\nEpoch 28201, Training Loss: 0.29518565237522126, Validation Loss: 0.2919977178806722\nEpoch 28201, Training Loss: 0.29518565237522126, Validation Loss: 0.2919977178806722\n\nLearning Rate: 7.1867711924544e-13\nEpoch 28301, Training Loss: 0.29518565237522126, Validation Loss: 0.2919977178806722\nEpoch 28301, Training Loss: 0.29518565237522126, Validation Loss: 0.2919977178806722\n\nLearning Rate: 6.83625967980195e-13\nEpoch 28401, Training Loss: 0.29518565237522126, Validation Loss: 0.2919977178806722\nEpoch 28401, Training Loss: 0.29518565237522126, Validation Loss: 0.2919977178806722\n\nLearning Rate: 6.502843232125398e-13\nEpoch 28501, Training Loss: 0.29518565237522126, Validation Loss: 0.2919977178806722\nEpoch 28501, Training Loss: 0.29518565237522126, Validation Loss: 0.2919977178806722\n\nLearning Rate: 6.185688092940343e-13\nEpoch 28601, Training Loss: 0.29518565237522126, Validation Loss: 0.2919977178806722\nEpoch 28601, Training Loss: 0.29518565237522126, Validation Loss: 0.2919977178806722\n\nLearning Rate: 5.88400116953736e-13\nEpoch 28701, Training Loss: 0.29518565237522126, Validation Loss: 0.2919977178806722\nEpoch 28701, Training Loss: 0.29518565237522126, Validation Loss: 0.2919977178806722\n\nLearning Rate: 5.597028049737928e-13\nEpoch 28801, Training Loss: 0.29518565237522126, Validation Loss: 0.2919977178806722\nEpoch 28801, Training Loss: 0.29518565237522126, Validation Loss: 0.2919977178806722\n\nLearning Rate: 5.324051115376695e-13\nEpoch 28901, Training Loss: 0.29518565237522126, Validation Loss: 0.2919977178806722\nEpoch 28901, Training Loss: 0.29518565237522126, Validation Loss: 0.2919977178806722\n\nLearning Rate: 5.06438774779252e-13\nEpoch 29001, Training Loss: 0.29518565237522126, Validation Loss: 0.2919977178806722\nEpoch 29001, Training Loss: 0.29518565237522126, Validation Loss: 0.2919977178806722\n\nLearning Rate: 4.817388620840901e-13\nEpoch 29101, Training Loss: 0.29518565237522126, Validation Loss: 0.2919977178806722\nEpoch 29101, Training Loss: 0.29518565237522126, Validation Loss: 0.2919977178806722\n\nLearning Rate: 4.582436077159185e-13\nEpoch 29201, Training Loss: 0.29518565237522126, Validation Loss: 0.2919977178806722\nEpoch 29201, Training Loss: 0.29518565237522126, Validation Loss: 0.2919977178806722\n\nLearning Rate: 4.3589425836242e-13\nEpoch 29301, Training Loss: 0.29518565237522126, Validation Loss: 0.2919977178806722\nEpoch 29301, Training Loss: 0.29518565237522126, Validation Loss: 0.2919977178806722\n\nLearning Rate: 4.146349262139938e-13\nEpoch 29401, Training Loss: 0.29518565237522126, Validation Loss: 0.2919977178806722\nEpoch 29401, Training Loss: 0.29518565237522126, Validation Loss: 0.2919977178806722\n\nLearning Rate: 3.944124492081314e-13\nEpoch 29501, Training Loss: 0.29518565237522126, Validation Loss: 0.2919977178806722\nEpoch 29501, Training Loss: 0.29518565237522126, Validation Loss: 0.2919977178806722\n\nLearning Rate: 3.751762580899213e-13\nEpoch 29601, Training Loss: 0.29518565237522126, Validation Loss: 0.2919977178806722\nEpoch 29601, Training Loss: 0.29518565237522126, Validation Loss: 0.2919977178806722\n\nLearning Rate: 3.568782499562474e-13\nEpoch 29701, Training Loss: 0.29518565237522126, Validation Loss: 0.2919977178806722\nEpoch 29701, Training Loss: 0.29518565237522126, Validation Loss: 0.2919977178806722\n\nLearning Rate: 3.394726679674596e-13\nEpoch 29801, Training Loss: 0.29518565237522126, Validation Loss: 0.2919977178806722\nEpoch 29801, Training Loss: 0.29518565237522126, Validation Loss: 0.2919977178806722\n\nLearning Rate: 3.229159869257191e-13\nEpoch 29901, Training Loss: 0.29518565237522126, Validation Loss: 0.2919977178806722\nEpoch 29901, Training Loss: 0.29518565237522126, Validation Loss: 0.2919977178806722\n\nLearning Rate: 3.0716680443388893e-13\nEpoch 30001, Training Loss: 0.29518565237522126, Validation Loss: 0.2919977178806722\nEpoch 30001, Training Loss: 0.29518565237522126, Validation Loss: 0.2919977178806722\n\nLearning Rate: 2.921857373627983e-13\nEpoch 30101, Training Loss: 0.29518565237522126, Validation Loss: 0.2919977178806722\nEpoch 30101, Training Loss: 0.29518565237522126, Validation Loss: 0.2919977178806722\n\nLearning Rate: 2.779353233679805e-13\nEpoch 30201, Training Loss: 0.29518565237522126, Validation Loss: 0.2919977178806722\nEpoch 30201, Training Loss: 0.29518565237522126, Validation Loss: 0.2919977178806722\n\nLearning Rate: 2.6437992720961357e-13\nEpoch 30301, Training Loss: 0.29518565237522126, Validation Loss: 0.2919977178806722\nEpoch 30301, Training Loss: 0.29518565237522126, Validation Loss: 0.2919977178806722\n\nLearning Rate: 2.5148565164140274e-13\nEpoch 30401, Training Loss: 0.29518565237522126, Validation Loss: 0.2919977178806722\nEpoch 30401, Training Loss: 0.29518565237522126, Validation Loss: 0.2919977178806722\n\nLearning Rate: 2.3922025264556933e-13\nEpoch 30501, Training Loss: 0.29518565237522126, Validation Loss: 0.2919977178806722\nEpoch 30501, Training Loss: 0.29518565237522126, Validation Loss: 0.2919977178806722\n\nLearning Rate: 2.2755305880197853e-13\nEpoch 30601, Training Loss: 0.29518565237522126, Validation Loss: 0.2919977178806722\nEpoch 30601, Training Loss: 0.29518565237522126, Validation Loss: 0.2919977178806722\n\nLearning Rate: 2.1645489458977772e-13\nEpoch 30701, Training Loss: 0.29518565237522126, Validation Loss: 0.2919977178806722\nEpoch 30701, Training Loss: 0.29518565237522126, Validation Loss: 0.2919977178806722\n\nLearning Rate: 2.0589800742974855e-13\nEpoch 30801, Training Loss: 0.29518565237522126, Validation Loss: 0.2919977178806722\nEpoch 30801, Training Loss: 0.29518565237522126, Validation Loss: 0.2919977178806722\n\nLearning Rate: 1.9585599828493268e-13\nEpoch 30901, Training Loss: 0.29518565237522126, Validation Loss: 0.2919977178806722\nEpoch 30901, Training Loss: 0.29518565237522126, Validation Loss: 0.2919977178806722\n\nLearning Rate: 1.863037556459873e-13\nEpoch 31001, Training Loss: 0.29518565237522126, Validation Loss: 0.2919977178806722\nEpoch 31001, Training Loss: 0.29518565237522126, Validation Loss: 0.2919977178806722\n\nLearning Rate: 1.7721739273619142e-13\nEpoch 31101, Training Loss: 0.29518565237522126, Validation Loss: 0.2919977178806722\nEpoch 31101, Training Loss: 0.29518565237522126, Validation Loss: 0.2919977178806722\n\nLearning Rate: 1.6857418777907466e-13\nEpoch 31201, Training Loss: 0.29518565237522126, Validation Loss: 0.2919977178806722\nEpoch 31201, Training Loss: 0.29518565237522126, Validation Loss: 0.2919977178806722\n\nLearning Rate: 1.6035252717929953e-13\nEpoch 31301, Training Loss: 0.29518565237522126, Validation Loss: 0.2919977178806722\nEpoch 31301, Training Loss: 0.29518565237522126, Validation Loss: 0.2919977178806722\n\nLearning Rate: 1.5253185147471176e-13\nEpoch 31401, Training Loss: 0.29518565237522126, Validation Loss: 0.2919977178806722\nEpoch 31401, Training Loss: 0.29518565237522126, Validation Loss: 0.2919977178806722\n\nLearning Rate: 1.4509260392440524e-13\nEpoch 31501, Training Loss: 0.29518565237522126, Validation Loss: 0.2919977178806722\nEpoch 31501, Training Loss: 0.29518565237522126, Validation Loss: 0.2919977178806722\n\nLearning Rate: 1.3801618160423706e-13\nEpoch 31601, Training Loss: 0.29518565237522126, Validation Loss: 0.2919977178806722\nEpoch 31601, Training Loss: 0.29518565237522126, Validation Loss: 0.2919977178806722\n\nLearning Rate: 1.312848888875011e-13\nEpoch 31701, Training Loss: 0.29518565237522126, Validation Loss: 0.2919977178806722\nEpoch 31701, Training Loss: 0.29518565237522126, Validation Loss: 0.2919977178806722\n\nLearning Rate: 1.248818931944309e-13\nEpoch 31801, Training Loss: 0.29518565237522126, Validation Loss: 0.2919977178806722\nEpoch 31801, Training Loss: 0.29518565237522126, Validation Loss: 0.2919977178806722\n\nLearning Rate: 1.187911828998775e-13\nEpoch 31901, Training Loss: 0.29518565237522126, Validation Loss: 0.2919977178806722\nEpoch 31901, Training Loss: 0.29518565237522126, Validation Loss: 0.2919977178806722\n\nLearning Rate: 1.1299752729390435e-13\nEpoch 32001, Training Loss: 0.29518565237522126, Validation Loss: 0.2919977178806722\nEpoch 32001, Training Loss: 0.29518565237522126, Validation Loss: 0.2919977178806722\n\nLearning Rate: 1.0748643849517409e-13\nEpoch 32101, Training Loss: 0.29518565237522126, Validation Loss: 0.2919977178806722\nEpoch 32101, Training Loss: 0.29518565237522126, Validation Loss: 0.2919977178806722\n\nLearning Rate: 1.0224413522188716e-13\nEpoch 32201, Training Loss: 0.29518565237522126, Validation Loss: 0.2919977178806722\nEpoch 32201, Training Loss: 0.29518565237522126, Validation Loss: 0.2919977178806722\n\nLearning Rate: 9.725750832967549e-14\nEpoch 32301, Training Loss: 0.29518565237522126, Validation Loss: 0.2919977178806722\nEpoch 32301, Training Loss: 0.29518565237522126, Validation Loss: 0.2919977178806722\n\nLearning Rate: 9.251408803027392e-14\nEpoch 32401, Training Loss: 0.29518565237522126, Validation Loss: 0.2919977178806722\nEpoch 32401, Training Loss: 0.29518565237522126, Validation Loss: 0.2919977178806722\n\nLearning Rate: 8.80020127089948e-14\nEpoch 32501, Training Loss: 0.29518565237522126, Validation Loss: 0.2919977178806722\nEpoch 32501, Training Loss: 0.29518565237522126, Validation Loss: 0.2919977178806722\n\nLearning Rate: 8.370999926302957e-14\nEpoch 32601, Training Loss: 0.29518565237522126, Validation Loss: 0.2919977178806722\nEpoch 32601, Training Loss: 0.29518565237522126, Validation Loss: 0.2919977178806722\n\nLearning Rate: 7.962731488640351e-14\nEpoch 32701, Training Loss: 0.29518565237522126, Validation Loss: 0.2919977178806722\nEpoch 32701, Training Loss: 0.29518565237522126, Validation Loss: 0.2919977178806722\n\nLearning Rate: 7.574375023102808e-14\nEpoch 32801, Training Loss: 0.29518565237522126, Validation Loss: 0.2919977178806722\nEpoch 32801, Training Loss: 0.29518565237522126, Validation Loss: 0.2919977178806722\n\nLearning Rate: 7.204959387673625e-14\nEpoch 32901, Training Loss: 0.29518565237522126, Validation Loss: 0.2919977178806722\nEpoch 32901, Training Loss: 0.29518565237522126, Validation Loss: 0.2919977178806722\n\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","Cell \u001b[0;32mIn[52], line 22\u001b[0m\n\u001b[1;32m     19\u001b[0m scheduler \u001b[38;5;241m=\u001b[39m LambdaLR(optimizer, lr_lambda\u001b[38;5;241m=\u001b[39mcustom_lr_lambda)\n\u001b[1;32m     21\u001b[0m criterion \u001b[38;5;241m=\u001b[39m CustomLoss(nn\u001b[38;5;241m.\u001b[39mCrossEntropyLoss(), \u001b[38;5;241m0.0\u001b[39m, \u001b[38;5;241m0.0\u001b[39m, \u001b[38;5;241m0.0\u001b[39m, \u001b[38;5;241m0.0\u001b[39m, \u001b[38;5;241m0.0\u001b[39m)\n\u001b[0;32m---> 22\u001b[0m \u001b[43mevaluate_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcustom_train_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m100000\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscheduler\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_features\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mearly_stopping_patience\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10000\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     24\u001b[0m models\u001b[38;5;241m.\u001b[39mappend(model)\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExecution time: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m(time\u001b[38;5;241m.\u001b[39mtime()\u001b[38;5;250m \u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;250m \u001b[39mstart_time)\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.6f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m seconds\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n","Cell \u001b[0;32mIn[4], line 25\u001b[0m, in \u001b[0;36mevaluate_model\u001b[0;34m(model, custom_train_loader, criterion, optimizer, num_epochs, scheduler, batch_size, num_features, early_stopping_patience)\u001b[0m\n\u001b[1;32m     23\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m     24\u001b[0m scheduler\u001b[38;5;241m.\u001b[39mstep()\n\u001b[0;32m---> 25\u001b[0m running_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mlen\u001b[39m(labels)\n\u001b[1;32m     26\u001b[0m total_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem() \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mlen\u001b[39m(labels)\n\u001b[1;32m     27\u001b[0m num_items \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(labels)\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "],"ename":"KeyboardInterrupt","evalue":"","output_type":"error"}],"execution_count":52},{"cell_type":"code","source":"def custom_lr_lambda(step):\n    num_step_threshold = 100\n\n    if step < num_step_threshold:\n        return step / num_step_threshold\n    if step == num_step_threshold:\n        print(\"here\")\n    return 0.99995 ** (step - num_step_threshold)\n\nstart_time = time.time()\nmodel = TabularDenseNet(num_features, num_classes, num_control_points, 3, 10).to(device)\n\noptimizer = optim.SGD(model.parameters(), lr=0.001 * 10.0)\noptimizer = optim.Adam(model.parameters(), lr=0.001 * 10.0)\nscheduler = LambdaLR(optimizer, lr_lambda=custom_lr_lambda)\n\ncriterion = CustomLoss(nn.CrossEntropyLoss(), 0.0, 0.0, 0.0, 0.0, 0.0)\nevaluate_model(model, custom_train_loader, criterion, optimizer, 100000, scheduler, batch_size * 100, num_features, early_stopping_patience=10000)\n\nmodels.append(model)\nprint(f\"Execution time: {(time.time() - start_time):.6f} seconds\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-14T00:27:00.802694Z","iopub.execute_input":"2024-12-14T00:27:00.803443Z","iopub.status.idle":"2024-12-14T00:48:27.620740Z","shell.execute_reply.started":"2024-12-14T00:27:00.803407Z","shell.execute_reply":"2024-12-14T00:48:27.619517Z"}},"outputs":[{"name":"stdout","text":"torch.Size([72980, 21504])\nLearning Rate: 0.0001\ntorch.Size([18246, 21504])\nEpoch 1, Training Loss: 0.6931450963020325, Validation Loss: 0.6931456923484802\nEpoch 1, Training Loss: 0.6931450963020325, Validation Loss: 0.6931456923484802\n\nhere\nLearning Rate: 0.0099995\nEpoch 101, Training Loss: 0.3549061417579651, Validation Loss: 0.6106948256492615\nEpoch 101, Training Loss: 0.6079214811325073, Validation Loss: 0.6106948256492615\n\nLearning Rate: 0.00994962604194244\nEpoch 201, Training Loss: 0.31942218542099, Validation Loss: 0.2537877559661865\nEpoch 201, Training Loss: 0.24894890189170837, Validation Loss: 0.2537877559661865\n\nLearning Rate: 0.009900000837491795\nEpoch 301, Training Loss: 0.22309604287147522, Validation Loss: 0.23056474328041077\nEpoch 301, Training Loss: 0.22304952144622803, Validation Loss: 0.23056474328041077\n\nLearning Rate: 0.009850623145953332\nEpoch 401, Training Loss: 0.2246573567390442, Validation Loss: 0.23013480007648468\nEpoch 401, Training Loss: 0.22026780247688293, Validation Loss: 0.23013480007648468\n\nLearning Rate: 0.009801491732820465\nEpoch 501, Training Loss: 0.8137688040733337, Validation Loss: 0.9547179341316223\nEpoch 501, Training Loss: 0.9270901679992676, Validation Loss: 0.9547179341316223\n\nLearning Rate: 0.009752605369743893\nEpoch 601, Training Loss: 0.21505455672740936, Validation Loss: 0.23113764822483063\nEpoch 601, Training Loss: 0.214760884642601, Validation Loss: 0.23113764822483063\n\nLearning Rate: 0.009703962834500876\nEpoch 701, Training Loss: 0.21216636896133423, Validation Loss: 0.23428602516651154\nEpoch 701, Training Loss: 0.2165980339050293, Validation Loss: 0.23428602516651154\n\nLearning Rate: 0.009655562910964699\nEpoch 801, Training Loss: 0.24530304968357086, Validation Loss: 0.27369949221611023\nEpoch 801, Training Loss: 0.2536299228668213, Validation Loss: 0.27369949221611023\n\nLearning Rate: 0.009607404389074247\nEpoch 901, Training Loss: 0.23946532607078552, Validation Loss: 0.24644285440444946\nEpoch 901, Training Loss: 0.22533784806728363, Validation Loss: 0.24644285440444946\n\nLearning Rate: 0.009559486064803764\nEpoch 1001, Training Loss: 0.21318159997463226, Validation Loss: 0.2612128257751465\nEpoch 1001, Training Loss: 0.23892845213413239, Validation Loss: 0.2612128257751465\n\nLearning Rate: 0.009511806740132746\nEpoch 1101, Training Loss: 0.24325363337993622, Validation Loss: 0.33087480068206787\nEpoch 1101, Training Loss: 0.30648064613342285, Validation Loss: 0.33087480068206787\n\nLearning Rate: 0.00946436522301599\nEpoch 1201, Training Loss: 0.6857169270515442, Validation Loss: 0.3112427294254303\nEpoch 1201, Training Loss: 0.2820175290107727, Validation Loss: 0.3112427294254303\n\nLearning Rate: 0.009417160327353796\nEpoch 1301, Training Loss: 0.2993977963924408, Validation Loss: 0.2652095854282379\nEpoch 1301, Training Loss: 0.2316722273826599, Validation Loss: 0.2652095854282379\n\nLearning Rate: 0.0093701908729623\nEpoch 1401, Training Loss: 0.22519627213478088, Validation Loss: 0.3222818970680237\nEpoch 1401, Training Loss: 0.2916087210178375, Validation Loss: 0.3222818970680237\n\nLearning Rate: 0.009323455685543986\nEpoch 1501, Training Loss: 0.19971300661563873, Validation Loss: 0.23198191821575165\nEpoch 1501, Training Loss: 0.1996791660785675, Validation Loss: 0.23198191821575165\n\nLearning Rate: 0.009276953596658309\nEpoch 1601, Training Loss: 0.19730927050113678, Validation Loss: 0.2297050654888153\nEpoch 1601, Training Loss: 0.1972917765378952, Validation Loss: 0.2297050654888153\n\nLearning Rate: 0.009230683443692494\nEpoch 1701, Training Loss: 0.19582383334636688, Validation Loss: 0.22871175408363342\nEpoch 1701, Training Loss: 0.19581690430641174, Validation Loss: 0.22871175408363342\n\nLearning Rate: 0.009184644069832468\nEpoch 1801, Training Loss: 0.2212039828300476, Validation Loss: 0.2501077950000763\nEpoch 1801, Training Loss: 0.216769278049469, Validation Loss: 0.2501077950000763\n\nLearning Rate: 0.009138834324033935\nEpoch 1901, Training Loss: 0.1968384087085724, Validation Loss: 0.22945961356163025\nEpoch 1901, Training Loss: 0.19507089257240295, Validation Loss: 0.22945961356163025\n\nLearning Rate: 0.009093253060993598\nEpoch 2001, Training Loss: 0.19976463913917542, Validation Loss: 0.2302694320678711\nEpoch 2001, Training Loss: 0.19447867572307587, Validation Loss: 0.2302694320678711\n\nLearning Rate: 0.009047899141120528\nEpoch 2101, Training Loss: 0.20560136437416077, Validation Loss: 0.23231881856918335\nEpoch 2101, Training Loss: 0.19493581354618073, Validation Loss: 0.23231881856918335\n\nLearning Rate: 0.009002771430507671\nEpoch 2201, Training Loss: 0.22686274349689484, Validation Loss: 0.322574645280838\nEpoch 2201, Training Loss: 0.28446924686431885, Validation Loss: 0.322574645280838\n\nLearning Rate: 0.008957868800903497\nEpoch 2301, Training Loss: 0.2045382857322693, Validation Loss: 0.23306192457675934\nEpoch 2301, Training Loss: 0.194230318069458, Validation Loss: 0.23306192457675934\n\nLearning Rate: 0.0089131901296838\nEpoch 2401, Training Loss: 0.19679191708564758, Validation Loss: 0.23128089308738708\nEpoch 2401, Training Loss: 0.19049404561519623, Validation Loss: 0.23128089308738708\n\nLearning Rate: 0.008868734299823615\nEpoch 2501, Training Loss: 0.26636162400245667, Validation Loss: 0.265741229057312\nEpoch 2501, Training Loss: 0.22348462045192719, Validation Loss: 0.265741229057312\n\nLearning Rate: 0.008824500199869315\nEpoch 2601, Training Loss: 0.26046979427337646, Validation Loss: 0.24818463623523712\nEpoch 2601, Training Loss: 0.2053889036178589, Validation Loss: 0.24818463623523712\n\nLearning Rate: 0.008780486723910795\nEpoch 2701, Training Loss: 0.19069314002990723, Validation Loss: 0.24313995242118835\nEpoch 2701, Training Loss: 0.19969907402992249, Validation Loss: 0.24313995242118835\n\nLearning Rate: 0.008736692771553851\nEpoch 2801, Training Loss: 0.19550961256027222, Validation Loss: 0.26780328154563904\nEpoch 2801, Training Loss: 0.22319504618644714, Validation Loss: 0.26780328154563904\n\nLearning Rate: 0.00869311724789264\nEpoch 2901, Training Loss: 0.19074200093746185, Validation Loss: 0.24423782527446747\nEpoch 2901, Training Loss: 0.1984686255455017, Validation Loss: 0.24423782527446747\n\nLearning Rate: 0.008649759063482336\nEpoch 3001, Training Loss: 0.20511549711227417, Validation Loss: 0.2882298529148102\nEpoch 3001, Training Loss: 0.24144813418388367, Validation Loss: 0.2882298529148102\n\nLearning Rate: 0.008606617134311868\nEpoch 3101, Training Loss: 0.2603631913661957, Validation Loss: 0.24766239523887634\nEpoch 3101, Training Loss: 0.19978094100952148, Validation Loss: 0.24766239523887634\n\nLearning Rate: 0.008563690381776826\nEpoch 3201, Training Loss: 0.20059898495674133, Validation Loss: 0.2357996702194214\nEpoch 3201, Training Loss: 0.18718096613883972, Validation Loss: 0.2357996702194214\n\nLearning Rate: 0.008520977732652505\nEpoch 3301, Training Loss: 0.1878027766942978, Validation Loss: 0.2544196844100952\nEpoch 3301, Training Loss: 0.2050563246011734, Validation Loss: 0.2544196844100952\n\nLearning Rate: 0.008478478119067054\nEpoch 3401, Training Loss: 0.19633692502975464, Validation Loss: 0.2750479578971863\nEpoch 3401, Training Loss: 0.22468313574790955, Validation Loss: 0.2750479578971863\n\nLearning Rate: 0.008436190478474796\nEpoch 3501, Training Loss: 0.2400461882352829, Validation Loss: 0.24575038254261017\nEpoch 3501, Training Loss: 0.19439299404621124, Validation Loss: 0.24575038254261017\n\nLearning Rate: 0.008394113753629652\nEpoch 3601, Training Loss: 0.2608530819416046, Validation Loss: 0.26051458716392517\nEpoch 3601, Training Loss: 0.2079467475414276, Validation Loss: 0.26051458716392517\n\nLearning Rate: 0.008352246892558708\nEpoch 3701, Training Loss: 0.18863287568092346, Validation Loss: 0.24882808327674866\nEpoch 3701, Training Loss: 0.1952313929796219, Validation Loss: 0.24882808327674866\n\nLearning Rate: 0.008310588848535921\nEpoch 3801, Training Loss: 0.1805606633424759, Validation Loss: 0.23713240027427673\nEpoch 3801, Training Loss: 0.18395419418811798, Validation Loss: 0.23713240027427673\n\nLearning Rate: 0.008269138580055947\nEpoch 3901, Training Loss: 0.19297577440738678, Validation Loss: 0.23942691087722778\nEpoch 3901, Training Loss: 0.18552324175834656, Validation Loss: 0.23942691087722778\n\nLearning Rate: 0.0082278950508081\nEpoch 4001, Training Loss: 0.1825888603925705, Validation Loss: 0.25394079089164734\nEpoch 4001, Training Loss: 0.1986261010169983, Validation Loss: 0.25394079089164734\n\nLearning Rate: 0.008186857229650442\nEpoch 4101, Training Loss: 0.24595953524112701, Validation Loss: 0.25068455934524536\nEpoch 4101, Training Loss: 0.1943826824426651, Validation Loss: 0.25068455934524536\n\nLearning Rate: 0.008146024090584007\nEpoch 4201, Training Loss: 0.20937328040599823, Validation Loss: 0.26557639241218567\nEpoch 4201, Training Loss: 0.20779438316822052, Validation Loss: 0.26557639241218567\n\nLearning Rate: 0.008105394612727149\nEpoch 4301, Training Loss: 0.18335317075252533, Validation Loss: 0.23501935601234436\nEpoch 4301, Training Loss: 0.17820559442043304, Validation Loss: 0.23501935601234436\n\nLearning Rate: 0.008064967780290016\nEpoch 4401, Training Loss: 0.1854947805404663, Validation Loss: 0.26656636595726013\nEpoch 4401, Training Loss: 0.20819029211997986, Validation Loss: 0.26656636595726013\n\nLearning Rate: 0.008024742582549154\nEpoch 4501, Training Loss: 0.20904618501663208, Validation Loss: 0.2683522701263428\nEpoch 4501, Training Loss: 0.20795531570911407, Validation Loss: 0.2683522701263428\n\nLearning Rate: 0.00798471801382224\nEpoch 4601, Training Loss: 0.18355561792850494, Validation Loss: 0.2365463376045227\nEpoch 4601, Training Loss: 0.17755934596061707, Validation Loss: 0.2365463376045227\n\nLearning Rate: 0.00794489307344294\nEpoch 4701, Training Loss: 0.17801885306835175, Validation Loss: 0.2507287561893463\nEpoch 4701, Training Loss: 0.19031395018100739, Validation Loss: 0.2507287561893463\n\nLearning Rate: 0.007905266765735887\nEpoch 4801, Training Loss: 0.19811637699604034, Validation Loss: 0.2632923722267151\nEpoch 4801, Training Loss: 0.20081697404384613, Validation Loss: 0.2632923722267151\n\nLearning Rate: 0.007865838099991787\nEpoch 4901, Training Loss: 0.19655148684978485, Validation Loss: 0.2553834021091461\nEpoch 4901, Training Loss: 0.19385583698749542, Validation Loss: 0.2553834021091461\n\nLearning Rate: 0.007826606090442657\nEpoch 5001, Training Loss: 0.21148791909217834, Validation Loss: 0.2458895593881607\nEpoch 5001, Training Loss: 0.18340589106082916, Validation Loss: 0.2458895593881607\n\nLearning Rate: 0.007787569756237169\nEpoch 5101, Training Loss: 0.1745612621307373, Validation Loss: 0.23832960426807404\nEpoch 5101, Training Loss: 0.1754336804151535, Validation Loss: 0.23832960426807404\n\nLearning Rate: 0.007748728121416138\nEpoch 5201, Training Loss: 0.17831017076969147, Validation Loss: 0.25825634598731995\nEpoch 5201, Training Loss: 0.19452831149101257, Validation Loss: 0.25825634598731995\n\nLearning Rate: 0.007710080214888119\nEpoch 5301, Training Loss: 0.19064372777938843, Validation Loss: 0.25959229469299316\nEpoch 5301, Training Loss: 0.193548783659935, Validation Loss: 0.25959229469299316\n\nLearning Rate: 0.00767162507040512\nEpoch 5401, Training Loss: 0.19429992139339447, Validation Loss: 0.28000473976135254\nEpoch 5401, Training Loss: 0.21475601196289062, Validation Loss: 0.28000473976135254\n\nLearning Rate: 0.007633361726538456\nEpoch 5501, Training Loss: 0.21459005773067474, Validation Loss: 0.2603757083415985\nEpoch 5501, Training Loss: 0.19354388117790222, Validation Loss: 0.2603757083415985\n\nLearning Rate: 0.007595289226654707\nEpoch 5601, Training Loss: 0.1930311620235443, Validation Loss: 0.25826510787010193\nEpoch 5601, Training Loss: 0.1921166479587555, Validation Loss: 0.25826510787010193\n\nLearning Rate: 0.007557406618891798\nEpoch 5701, Training Loss: 0.1961938887834549, Validation Loss: 0.24458321928977966\nEpoch 5701, Training Loss: 0.1777607500553131, Validation Loss: 0.24458321928977966\n\nLearning Rate: 0.007519712956135207\nEpoch 5801, Training Loss: 0.17967860400676727, Validation Loss: 0.23842760920524597\nEpoch 5801, Training Loss: 0.17136210203170776, Validation Loss: 0.23842760920524597\n\nLearning Rate: 0.007482207295994283\nEpoch 5901, Training Loss: 0.17218710482120514, Validation Loss: 0.23900489509105682\nEpoch 5901, Training Loss: 0.17125433683395386, Validation Loss: 0.23900489509105682\n\nLearning Rate: 0.0074448887007786835\nEpoch 6001, Training Loss: 0.16998715698719025, Validation Loss: 0.24493464827537537\nEpoch 6001, Training Loss: 0.17565865814685822, Validation Loss: 0.24493464827537537\n\nLearning Rate: 0.007407756237474935\nEpoch 6101, Training Loss: 0.17246310412883759, Validation Loss: 0.25359827280044556\nEpoch 6101, Training Loss: 0.1844169646501541, Validation Loss: 0.25359827280044556\n\nLearning Rate: 0.007370808977723103\nEpoch 6201, Training Loss: 0.17747952044010162, Validation Loss: 0.25541067123413086\nEpoch 6201, Training Loss: 0.18399672210216522, Validation Loss: 0.25541067123413086\n\nLearning Rate: 0.007334045997793582\nEpoch 6301, Training Loss: 0.17970380187034607, Validation Loss: 0.269361287355423\nEpoch 6301, Training Loss: 0.19875319302082062, Validation Loss: 0.269361287355423\n\nLearning Rate: 0.007297466378564005\nEpoch 6401, Training Loss: 0.19157536327838898, Validation Loss: 0.26241472363471985\nEpoch 6401, Training Loss: 0.1895025223493576, Validation Loss: 0.26241472363471985\n\nLearning Rate: 0.0072610692054962565\nEpoch 6501, Training Loss: 0.18578656017780304, Validation Loss: 0.2767166197299957\nEpoch 6501, Training Loss: 0.2048305869102478, Validation Loss: 0.2767166197299957\n\nLearning Rate: 0.007224853568613618\nEpoch 6601, Training Loss: 0.199471578001976, Validation Loss: 0.26300352811813354\nEpoch 6601, Training Loss: 0.18919649720191956, Validation Loss: 0.26300352811813354\n\nLearning Rate: 0.007188818562478006\nEpoch 6701, Training Loss: 0.18825992941856384, Validation Loss: 0.27639955282211304\nEpoch 6701, Training Loss: 0.20336146652698517, Validation Loss: 0.27639955282211304\n\nLearning Rate: 0.007152963286167348\nEpoch 6801, Training Loss: 0.20260721445083618, Validation Loss: 0.26043346524238586\nEpoch 6801, Training Loss: 0.1859411895275116, Validation Loss: 0.26043346524238586\n\nLearning Rate: 0.007117286843253044\nEpoch 6901, Training Loss: 0.18754476308822632, Validation Loss: 0.27106523513793945\nEpoch 6901, Training Loss: 0.1970180869102478, Validation Loss: 0.27106523513793945\n\nLearning Rate: 0.007081788341777568\nEpoch 7001, Training Loss: 0.20174315571784973, Validation Loss: 0.2575066089630127\nEpoch 7001, Training Loss: 0.18229788541793823, Validation Loss: 0.2575066089630127\n\nLearning Rate: 0.00704646689423216\nEpoch 7101, Training Loss: 0.18572741746902466, Validation Loss: 0.26478880643844604\nEpoch 7101, Training Loss: 0.18979716300964355, Validation Loss: 0.26478880643844604\n\nLearning Rate: 0.007011321617534633\nEpoch 7201, Training Loss: 0.1992310881614685, Validation Loss: 0.25554001331329346\nEpoch 7201, Training Loss: 0.17949636280536652, Validation Loss: 0.25554001331329346\n\nLearning Rate: 0.0069763516330073085\nEpoch 7301, Training Loss: 0.1839902549982071, Validation Loss: 0.26099225878715515\nEpoch 7301, Training Loss: 0.18503069877624512, Validation Loss: 0.26099225878715515\n\nLearning Rate: 0.006941556066355037\nEpoch 7401, Training Loss: 0.19709014892578125, Validation Loss: 0.254815936088562\nEpoch 7401, Training Loss: 0.1778118759393692, Validation Loss: 0.254815936088562\n\nLearning Rate: 0.006906934047643343\nEpoch 7501, Training Loss: 0.18287332355976105, Validation Loss: 0.2605414092540741\nEpoch 7501, Training Loss: 0.18354958295822144, Validation Loss: 0.2605414092540741\n\nLearning Rate: 0.0068724847112766755\nEpoch 7601, Training Loss: 0.19606880843639374, Validation Loss: 0.2554430365562439\nEpoch 7601, Training Loss: 0.17735996842384338, Validation Loss: 0.2554430365562439\n\nLearning Rate: 0.006838207195976768\nEpoch 7701, Training Loss: 0.1823682337999344, Validation Loss: 0.26317867636680603\nEpoch 7701, Training Loss: 0.18509553372859955, Validation Loss: 0.26317867636680603\n\nLearning Rate: 0.0068041006447611034\nEpoch 7801, Training Loss: 0.1956452876329422, Validation Loss: 0.2572881281375885\nEpoch 7801, Training Loss: 0.17800498008728027, Validation Loss: 0.2572881281375885\n\nLearning Rate: 0.006770164204921488\nEpoch 7901, Training Loss: 0.18200841546058655, Validation Loss: 0.2680366337299347\nEpoch 7901, Training Loss: 0.18882504105567932, Validation Loss: 0.2680366337299347\n\nLearning Rate: 0.006736397028002737\nEpoch 8001, Training Loss: 0.19417275488376617, Validation Loss: 0.2599160075187683\nEpoch 8001, Training Loss: 0.17931602895259857, Validation Loss: 0.2599160075187683\n\nLearning Rate: 0.00670279826978146\nEpoch 8101, Training Loss: 0.1808905452489853, Validation Loss: 0.2726556658744812\nEpoch 8101, Training Loss: 0.19234761595726013, Validation Loss: 0.2726556658744812\n\nLearning Rate: 0.006669367090244947\nEpoch 8201, Training Loss: 0.1896161288022995, Validation Loss: 0.2622480094432831\nEpoch 8201, Training Loss: 0.1802813857793808, Validation Loss: 0.2622480094432831\n\nLearning Rate: 0.0066361026535701785\nEpoch 8301, Training Loss: 0.17757046222686768, Validation Loss: 0.273441880941391\nEpoch 8301, Training Loss: 0.19215954840183258, Validation Loss: 0.273441880941391\n\nLearning Rate: 0.006603004128102924\nEpoch 8401, Training Loss: 0.18139146268367767, Validation Loss: 0.2623697817325592\nEpoch 8401, Training Loss: 0.17917388677597046, Validation Loss: 0.2623697817325592\n\nLearning Rate: 0.006570070686336946\nEpoch 8501, Training Loss: 0.17111042141914368, Validation Loss: 0.2678457200527191\nEpoch 8501, Training Loss: 0.18573717772960663, Validation Loss: 0.2678457200527191\n\nLearning Rate: 0.006537301504893317\nEpoch 8601, Training Loss: 0.1693277209997177, Validation Loss: 0.2578800320625305\nEpoch 8601, Training Loss: 0.17391952872276306, Validation Loss: 0.2578800320625305\n\nLearning Rate: 0.00650469576449983\nEpoch 8701, Training Loss: 0.16344062983989716, Validation Loss: 0.2567489743232727\nEpoch 8701, Training Loss: 0.17391201853752136, Validation Loss: 0.2567489743232727\n\nLearning Rate: 0.0064722526499705205\nEpoch 8801, Training Loss: 0.16009223461151123, Validation Loss: 0.24953743815422058\nEpoch 8801, Training Loss: 0.16544771194458008, Validation Loss: 0.24953743815422058\n\nLearning Rate: 0.006439971350185275\nEpoch 8901, Training Loss: 0.15968351066112518, Validation Loss: 0.2447189837694168\nEpoch 8901, Training Loss: 0.16121666133403778, Validation Loss: 0.2447189837694168\n\nLearning Rate: 0.006407851058069567\nEpoch 9001, Training Loss: 0.16289354860782623, Validation Loss: 0.2430884689092636\nEpoch 9001, Training Loss: 0.15913279354572296, Validation Loss: 0.2430884689092636\n\nLearning Rate: 0.006375890970574267\nEpoch 9101, Training Loss: 0.1705874800682068, Validation Loss: 0.24543623626232147\nEpoch 9101, Training Loss: 0.16095870733261108, Validation Loss: 0.24543623626232147\n\nLearning Rate: 0.006344090288655573\nEpoch 9201, Training Loss: 0.17259936034679413, Validation Loss: 0.2525328993797302\nEpoch 9201, Training Loss: 0.16761857271194458, Validation Loss: 0.2525328993797302\n\nLearning Rate: 0.006312448217255025\nEpoch 9301, Training Loss: 0.18572281301021576, Validation Loss: 0.25767701864242554\nEpoch 9301, Training Loss: 0.1713649034500122, Validation Loss: 0.25767701864242554\n\nLearning Rate: 0.006280963965279637\nEpoch 9401, Training Loss: 0.1745053380727768, Validation Loss: 0.27111178636550903\nEpoch 9401, Training Loss: 0.18476954102516174, Validation Loss: 0.27111178636550903\n\nLearning Rate: 0.006249636745582113\nEpoch 9501, Training Loss: 0.17611195147037506, Validation Loss: 0.26234450936317444\nEpoch 9501, Training Loss: 0.17417943477630615, Validation Loss: 0.26234450936317444\n\nLearning Rate: 0.006218465774941166\nEpoch 9601, Training Loss: 0.16393764317035675, Validation Loss: 0.2627662122249603\nEpoch 9601, Training Loss: 0.1757478266954422, Validation Loss: 0.2627662122249603\n\nLearning Rate: 0.006187450274041943\nEpoch 9701, Training Loss: 0.15798069536685944, Validation Loss: 0.2516523599624634\nEpoch 9701, Training Loss: 0.1633862406015396, Validation Loss: 0.2516523599624634\n\nLearning Rate: 0.006156589467456534\nEpoch 9801, Training Loss: 0.15770651400089264, Validation Loss: 0.24488966166973114\nEpoch 9801, Training Loss: 0.1573919653892517, Validation Loss: 0.24488966166973114\n\nLearning Rate: 0.006125882583624589\nEpoch 9901, Training Loss: 0.1646030992269516, Validation Loss: 0.24578535556793213\nEpoch 9901, Training Loss: 0.15798625349998474, Validation Loss: 0.24578535556793213\n\nLearning Rate: 0.006095328854834027\nEpoch 10001, Training Loss: 0.1789461076259613, Validation Loss: 0.2544420063495636\nEpoch 10001, Training Loss: 0.16546322405338287, Validation Loss: 0.2544420063495636\n\nLearning Rate: 0.006064927517201843\nEpoch 10101, Training Loss: 0.17181548476219177, Validation Loss: 0.2692256569862366\nEpoch 10101, Training Loss: 0.17989392578601837, Validation Loss: 0.2692256569862366\n\nLearning Rate: 0.00603467781065501\nEpoch 10201, Training Loss: 0.17156939208507538, Validation Loss: 0.2620881199836731\nEpoch 10201, Training Loss: 0.1709347814321518, Validation Loss: 0.2620881199836731\n\nLearning Rate: 0.006004578978911474\nEpoch 10301, Training Loss: 0.15893158316612244, Validation Loss: 0.25807225704193115\nEpoch 10301, Training Loss: 0.1681576818227768, Validation Loss: 0.25807225704193115\n\nLearning Rate: 0.0059746302694612475\nEpoch 10401, Training Loss: 0.15536926686763763, Validation Loss: 0.24742349982261658\nEpoch 10401, Training Loss: 0.15686441957950592, Validation Loss: 0.24742349982261658\n\nLearning Rate: 0.005944830933547599\nEpoch 10501, Training Loss: 0.16464458405971527, Validation Loss: 0.24711473286151886\nEpoch 10501, Training Loss: 0.1565684676170349, Validation Loss: 0.24711473286151886\n\nLearning Rate: 0.005915180226148326\nEpoch 10601, Training Loss: 0.16927191615104675, Validation Loss: 0.26077327132225037\nEpoch 10601, Training Loss: 0.1696109175682068, Validation Loss: 0.26077327132225037\n\nLearning Rate: 0.005885677405957134\nEpoch 10701, Training Loss: 0.17418959736824036, Validation Loss: 0.26256340742111206\nEpoch 10701, Training Loss: 0.169534832239151, Validation Loss: 0.26256340742111206\n\nLearning Rate: 0.005856321735365101\nEpoch 10801, Training Loss: 0.15954457223415375, Validation Loss: 0.2615310549736023\nEpoch 10801, Training Loss: 0.16949322819709778, Validation Loss: 0.2615310549736023\n\nLearning Rate: 0.005827112480442238\nEpoch 10901, Training Loss: 0.1538253128528595, Validation Loss: 0.24879436194896698\nEpoch 10901, Training Loss: 0.15606459975242615, Validation Loss: 0.24879436194896698\n\nLearning Rate: 0.005798048910919136\nEpoch 11001, Training Loss: 0.1639818549156189, Validation Loss: 0.24832652509212494\nEpoch 11001, Training Loss: 0.15570464730262756, Validation Loss: 0.24832652509212494\n\nLearning Rate: 0.005769130300168714\nEpoch 11101, Training Loss: 0.16790863871574402, Validation Loss: 0.2643621563911438\nEpoch 11101, Training Loss: 0.17107261717319489, Validation Loss: 0.2643621563911438\n\nLearning Rate: 0.005740355925188043\nEpoch 11201, Training Loss: 0.16755203902721405, Validation Loss: 0.26224756240844727\nEpoch 11201, Training Loss: 0.16707666218280792, Validation Loss: 0.26224756240844727\n\nLearning Rate: 0.005711725066580285\nEpoch 11301, Training Loss: 0.1540554165840149, Validation Loss: 0.25413867831230164\nEpoch 11301, Training Loss: 0.1602565497159958, Validation Loss: 0.25413867831230164\n\nLearning Rate: 0.005683237008536689\nEpoch 11401, Training Loss: 0.15710066258907318, Validation Loss: 0.24675478041172028\nEpoch 11401, Training Loss: 0.15271809697151184, Validation Loss: 0.24675478041172028\n\nLearning Rate: 0.00565489103881871\nEpoch 11501, Training Loss: 0.17334821820259094, Validation Loss: 0.2576918303966522\nEpoch 11501, Training Loss: 0.1624002456665039, Validation Loss: 0.2576918303966522\n\nLearning Rate: 0.005626686448740195\nEpoch 11601, Training Loss: 0.16274382174015045, Validation Loss: 0.26788052916526794\nEpoch 11601, Training Loss: 0.17253966629505157, Validation Loss: 0.26788052916526794\n\nLearning Rate: 0.005598622533149665\nEpoch 11701, Training Loss: 0.15261182188987732, Validation Loss: 0.2534780502319336\nEpoch 11701, Training Loss: 0.15706467628479004, Validation Loss: 0.2534780502319336\n\nLearning Rate: 0.005570698590412687\nEpoch 11801, Training Loss: 0.15885421633720398, Validation Loss: 0.2482995241880417\nEpoch 11801, Training Loss: 0.15257997810840607, Validation Loss: 0.2482995241880417\n\nLearning Rate: 0.005542913922394331\nEpoch 11901, Training Loss: 0.16506586968898773, Validation Loss: 0.26539361476898193\nEpoch 11901, Training Loss: 0.1689688265323639, Validation Loss: 0.26539361476898193\n\nLearning Rate: 0.005515267834441719\nEpoch 12001, Training Loss: 0.15990643203258514, Validation Loss: 0.2606784999370575\nEpoch 12001, Training Loss: 0.16247844696044922, Validation Loss: 0.2606784999370575\n\nLearning Rate: 0.005487759635366649\nEpoch 12101, Training Loss: 0.15127791464328766, Validation Loss: 0.24862544238567352\nEpoch 12101, Training Loss: 0.15179035067558289, Validation Loss: 0.24862544238567352\n\nLearning Rate: 0.005460388637428328\nEpoch 12201, Training Loss: 0.16228286921977997, Validation Loss: 0.2564311921596527\nEpoch 12201, Training Loss: 0.15918385982513428, Validation Loss: 0.2564311921596527\n\nLearning Rate: 0.005433154156316166\nEpoch 12301, Training Loss: 0.16478754580020905, Validation Loss: 0.26275235414505005\nEpoch 12301, Training Loss: 0.16346192359924316, Validation Loss: 0.26275235414505005\n\nLearning Rate: 0.00540605551113267\nEpoch 12401, Training Loss: 0.1504414826631546, Validation Loss: 0.2523306906223297\nEpoch 12401, Training Loss: 0.15428759157657623, Validation Loss: 0.2523306906223297\n\nLearning Rate: 0.005379092024376426\nEpoch 12501, Training Loss: 0.15935489535331726, Validation Loss: 0.2525760531425476\nEpoch 12501, Training Loss: 0.154346764087677, Validation Loss: 0.2525760531425476\n\nLearning Rate: 0.005352263021925153\nEpoch 12601, Training Loss: 0.1654953956604004, Validation Loss: 0.2629362642765045\nEpoch 12601, Training Loss: 0.162647545337677, Validation Loss: 0.2629362642765045\n\nLearning Rate: 0.005325567833018856\nEpoch 12701, Training Loss: 0.15002457797527313, Validation Loss: 0.25354519486427307\nEpoch 12701, Training Loss: 0.15437158942222595, Validation Loss: 0.25354519486427307\n\nLearning Rate: 0.00529900579024305\nEpoch 12801, Training Loss: 0.15860208868980408, Validation Loss: 0.2531348764896393\nEpoch 12801, Training Loss: 0.15379579365253448, Validation Loss: 0.2531348764896393\n\nLearning Rate: 0.00527257622951208\nEpoch 12901, Training Loss: 0.16314995288848877, Validation Loss: 0.2629746198654175\nEpoch 12901, Training Loss: 0.1615438163280487, Validation Loss: 0.2629746198654175\n\nLearning Rate: 0.00524627849005251\nEpoch 13001, Training Loss: 0.14887481927871704, Validation Loss: 0.2515799403190613\nEpoch 13001, Training Loss: 0.151358500123024, Validation Loss: 0.2515799403190613\n\nLearning Rate: 0.00522011191438661\nEpoch 13101, Training Loss: 0.15979216992855072, Validation Loss: 0.2580408453941345\nEpoch 13101, Training Loss: 0.15746252238750458, Validation Loss: 0.2580408453941345\n\nLearning Rate: 0.005194075848315919\nEpoch 13201, Training Loss: 0.15732216835021973, Validation Loss: 0.26171180605888367\nEpoch 13201, Training Loss: 0.15914085507392883, Validation Loss: 0.26171180605888367\n\nLearning Rate: 0.00516816964090488\nEpoch 13301, Training Loss: 0.15043939650058746, Validation Loss: 0.24943505227565765\nEpoch 13301, Training Loss: 0.148183211684227, Validation Loss: 0.24943505227565765\n\nLearning Rate: 0.005142392644464574\nEpoch 13401, Training Loss: 0.15996846556663513, Validation Loss: 0.266299307346344\nEpoch 13401, Training Loss: 0.1643727719783783, Validation Loss: 0.266299307346344\n\nLearning Rate: 0.005116744214536525\nEpoch 13501, Training Loss: 0.14904528856277466, Validation Loss: 0.2561284601688385\nEpoch 13501, Training Loss: 0.15299059450626373, Validation Loss: 0.2561284601688385\n\nLearning Rate: 0.005091223709876587\nEpoch 13601, Training Loss: 0.16054381430149078, Validation Loss: 0.2549637258052826\nEpoch 13601, Training Loss: 0.1523294299840927, Validation Loss: 0.2549637258052826\n\nLearning Rate: 0.005065830492438911\nEpoch 13701, Training Loss: 0.15424923598766327, Validation Loss: 0.2646673917770386\nEpoch 13701, Training Loss: 0.16172286868095398, Validation Loss: 0.2646673917770386\n\nLearning Rate: 0.005040563927359997\nEpoch 13801, Training Loss: 0.15047885477542877, Validation Loss: 0.2502366900444031\nEpoch 13801, Training Loss: 0.14725609123706818, Validation Loss: 0.2502366900444031\n\nLearning Rate: 0.005015423382942816\nEpoch 13901, Training Loss: 0.1615915149450302, Validation Loss: 0.2632557451725006\nEpoch 13901, Training Loss: 0.1585003137588501, Validation Loss: 0.2632557451725006\n\nLearning Rate: 0.004990408230641022\nEpoch 14001, Training Loss: 0.14685682952404022, Validation Loss: 0.25183191895484924\nEpoch 14001, Training Loss: 0.14807628095149994, Validation Loss: 0.25183191895484924\n\nLearning Rate: 0.004965517845043233\nEpoch 14101, Training Loss: 0.1579168289899826, Validation Loss: 0.2650802433490753\nEpoch 14101, Training Loss: 0.16077494621276855, Validation Loss: 0.2650802433490753\n\nLearning Rate: 0.0049407516038574\nEpoch 14201, Training Loss: 0.1468132585287094, Validation Loss: 0.2555004954338074\nEpoch 14201, Training Loss: 0.1501092165708542, Validation Loss: 0.2555004954338074\n\nLearning Rate: 0.004916108887895241\nEpoch 14301, Training Loss: 0.16181519627571106, Validation Loss: 0.2596849799156189\nEpoch 14301, Training Loss: 0.15426887571811676, Validation Loss: 0.2596849799156189\n\nLearning Rate: 0.004891589081056771\nEpoch 14401, Training Loss: 0.14745768904685974, Validation Loss: 0.25752902030944824\nEpoch 14401, Training Loss: 0.15230637788772583, Validation Loss: 0.25752902030944824\n\nLearning Rate: 0.00486719157031489\nEpoch 14501, Training Loss: 0.15611222386360168, Validation Loss: 0.26025718450546265\nEpoch 14501, Training Loss: 0.15477623045444489, Validation Loss: 0.26025718450546265\n\nLearning Rate: 0.0048429157457000604\nEpoch 14601, Training Loss: 0.14719657599925995, Validation Loss: 0.25776708126068115\nEpoch 14601, Training Loss: 0.15078593790531158, Validation Loss: 0.25776708126068115\n\nLearning Rate: 0.004818761000285056\nEpoch 14701, Training Loss: 0.16022337973117828, Validation Loss: 0.25966930389404297\nEpoch 14701, Training Loss: 0.1529460996389389, Validation Loss: 0.25966930389404297\n\nLearning Rate: 0.004794726730169788\nEpoch 14801, Training Loss: 0.14589999616146088, Validation Loss: 0.2562788724899292\nEpoch 14801, Training Loss: 0.1497296690940857, Validation Loss: 0.2562788724899292\n\nLearning Rate: 0.004770812334466208\nEpoch 14901, Training Loss: 0.15561620891094208, Validation Loss: 0.26466673612594604\nEpoch 14901, Training Loss: 0.15770220756530762, Validation Loss: 0.26466673612594604\n\nLearning Rate: 0.004747017215283284\nEpoch 15001, Training Loss: 0.14486616849899292, Validation Loss: 0.2538405954837799\nEpoch 15001, Training Loss: 0.14616429805755615, Validation Loss: 0.2538405954837799\n\nLearning Rate: 0.0047233407777120505\nEpoch 15101, Training Loss: 0.1573718786239624, Validation Loss: 0.2637753188610077\nEpoch 15101, Training Loss: 0.15505817532539368, Validation Loss: 0.2637753188610077\n\nLearning Rate: 0.004699782429810739\nEpoch 15201, Training Loss: 0.14818009734153748, Validation Loss: 0.2525556981563568\nEpoch 15201, Training Loss: 0.14470158517360687, Validation Loss: 0.2525556981563568\n\nLearning Rate: 0.004676341582589975\nEpoch 15301, Training Loss: 0.1490049511194229, Validation Loss: 0.2631097733974457\nEpoch 15301, Training Loss: 0.1548248678445816, Validation Loss: 0.2631097733974457\n\nLearning Rate: 0.004653017649998053\nEpoch 15401, Training Loss: 0.15368223190307617, Validation Loss: 0.2607273757457733\nEpoch 15401, Training Loss: 0.1522785723209381, Validation Loss: 0.2607273757457733\n\nLearning Rate: 0.00462981004890629\nEpoch 15501, Training Loss: 0.14395278692245483, Validation Loss: 0.25437265634536743\nEpoch 15501, Training Loss: 0.14506159722805023, Validation Loss: 0.25437265634536743\n\nLearning Rate: 0.004606718199094438\nEpoch 15601, Training Loss: 0.15190085768699646, Validation Loss: 0.2633100748062134\nEpoch 15601, Training Loss: 0.15280601382255554, Validation Loss: 0.2633100748062134\n\nLearning Rate: 0.004583741523236182\nEpoch 15701, Training Loss: 0.15700788795948029, Validation Loss: 0.2609684467315674\nEpoch 15701, Training Loss: 0.15093575417995453, Validation Loss: 0.2609684467315674\n\nLearning Rate: 0.0045608794468847075\nEpoch 15801, Training Loss: 0.14454272389411926, Validation Loss: 0.25388044118881226\nEpoch 15801, Training Loss: 0.1440213918685913, Validation Loss: 0.25388044118881226\n\nLearning Rate: 0.004538131398458335\nEpoch 15901, Training Loss: 0.14381465315818787, Validation Loss: 0.2543056309223175\nEpoch 15901, Training Loss: 0.14392758905887604, Validation Loss: 0.2543056309223175\n\nLearning Rate: 0.004515496809226234\nEpoch 16001, Training Loss: 0.14946706593036652, Validation Loss: 0.2546023726463318\nEpoch 16001, Training Loss: 0.144183948636055, Validation Loss: 0.2546023726463318\n\nLearning Rate: 0.0044929751132941976\nEpoch 16101, Training Loss: 0.14656232297420502, Validation Loss: 0.2619454562664032\nEpoch 16101, Training Loss: 0.1510934680700302, Validation Loss: 0.2619454562664032\n\nLearning Rate: 0.0044705657475905035\nEpoch 16201, Training Loss: 0.1481514573097229, Validation Loss: 0.2575998604297638\nEpoch 16201, Training Loss: 0.14585119485855103, Validation Loss: 0.2575998604297638\n\nLearning Rate: 0.004448268151851827\nEpoch 16301, Training Loss: 0.14829100668430328, Validation Loss: 0.2642194628715515\nEpoch 16301, Training Loss: 0.15141192078590393, Validation Loss: 0.2642194628715515\n\nLearning Rate: 0.004426081768609242\nEpoch 16401, Training Loss: 0.15498676896095276, Validation Loss: 0.2635069191455841\nEpoch 16401, Training Loss: 0.1509452611207962, Validation Loss: 0.2635069191455841\n\nLearning Rate: 0.004404006043174276\nEpoch 16501, Training Loss: 0.1437540352344513, Validation Loss: 0.25601261854171753\nEpoch 16501, Training Loss: 0.14398550987243652, Validation Loss: 0.25601261854171753\n\nLearning Rate: 0.004382040423625048\nEpoch 16601, Training Loss: 0.14651653170585632, Validation Loss: 0.26067793369293213\nEpoch 16601, Training Loss: 0.14836056530475616, Validation Loss: 0.26067793369293213\n\nLearning Rate: 0.004360184360792465\nEpoch 16701, Training Loss: 0.15678593516349792, Validation Loss: 0.2640344202518463\nEpoch 16701, Training Loss: 0.1515902727842331, Validation Loss: 0.2640344202518463\n\nLearning Rate: 0.004338437308246499\nEpoch 16801, Training Loss: 0.1492946445941925, Validation Loss: 0.26473572850227356\nEpoch 16801, Training Loss: 0.15032245218753815, Validation Loss: 0.26473572850227356\n\nLearning Rate: 0.004316798722282517\nEpoch 16901, Training Loss: 0.14144282042980194, Validation Loss: 0.2549273371696472\nEpoch 16901, Training Loss: 0.1415666937828064, Validation Loss: 0.2549273371696472\n\nLearning Rate: 0.004295268061907695\nEpoch 17001, Training Loss: 0.14554956555366516, Validation Loss: 0.2602017819881439\nEpoch 17001, Training Loss: 0.14664700627326965, Validation Loss: 0.2602017819881439\n\nLearning Rate: 0.004273844788827485\nEpoch 17101, Training Loss: 0.14741763472557068, Validation Loss: 0.2641948461532593\nEpoch 17101, Training Loss: 0.15020431578159332, Validation Loss: 0.2641948461532593\n\nLearning Rate: 0.004252528367432165\nEpoch 17201, Training Loss: 0.15113860368728638, Validation Loss: 0.2745216190814972\nEpoch 17201, Training Loss: 0.16010721027851105, Validation Loss: 0.2745216190814972\n\nLearning Rate: 0.004231318264783443\nEpoch 17301, Training Loss: 0.1410970836877823, Validation Loss: 0.2563646733760834\nEpoch 17301, Training Loss: 0.14179880917072296, Validation Loss: 0.2563646733760834\n\nLearning Rate: 0.004210213950601135\nEpoch 17401, Training Loss: 0.15096348524093628, Validation Loss: 0.26447078585624695\nEpoch 17401, Training Loss: 0.14857225120067596, Validation Loss: 0.26447078585624695\n\nLearning Rate: 0.004189214897249904\nEpoch 17501, Training Loss: 0.1486635059118271, Validation Loss: 0.25810709595680237\nEpoch 17501, Training Loss: 0.14330871403217316, Validation Loss: 0.25810709595680237\n\nLearning Rate: 0.004168320579726074\nEpoch 17601, Training Loss: 0.14151336252689362, Validation Loss: 0.2574011981487274\nEpoch 17601, Training Loss: 0.14139185845851898, Validation Loss: 0.2574011981487274\n\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","Cell \u001b[0;32mIn[23], line 18\u001b[0m\n\u001b[1;32m     15\u001b[0m scheduler \u001b[38;5;241m=\u001b[39m LambdaLR(optimizer, lr_lambda\u001b[38;5;241m=\u001b[39mcustom_lr_lambda)\n\u001b[1;32m     17\u001b[0m criterion \u001b[38;5;241m=\u001b[39m CustomLoss(nn\u001b[38;5;241m.\u001b[39mCrossEntropyLoss(), \u001b[38;5;241m0.0\u001b[39m, \u001b[38;5;241m0.0\u001b[39m, \u001b[38;5;241m0.0\u001b[39m, \u001b[38;5;241m0.0\u001b[39m, \u001b[38;5;241m0.0\u001b[39m)\n\u001b[0;32m---> 18\u001b[0m \u001b[43mevaluate_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcustom_train_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m100000\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscheduler\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_features\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mearly_stopping_patience\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10000\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     20\u001b[0m models\u001b[38;5;241m.\u001b[39mappend(model)\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExecution time: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m(time\u001b[38;5;241m.\u001b[39mtime()\u001b[38;5;250m \u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;250m \u001b[39mstart_time)\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.6f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m seconds\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n","Cell \u001b[0;32mIn[8], line 25\u001b[0m, in \u001b[0;36mevaluate_model\u001b[0;34m(model, custom_train_loader, criterion, optimizer, num_epochs, scheduler, batch_size, num_features, early_stopping_patience)\u001b[0m\n\u001b[1;32m     23\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m     24\u001b[0m scheduler\u001b[38;5;241m.\u001b[39mstep()\n\u001b[0;32m---> 25\u001b[0m running_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mlen\u001b[39m(labels)\n\u001b[1;32m     26\u001b[0m total_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem() \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mlen\u001b[39m(labels)\n\u001b[1;32m     27\u001b[0m num_items \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(labels)\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "],"ename":"KeyboardInterrupt","evalue":"","output_type":"error"}],"execution_count":23},{"cell_type":"code","source":"for name, param in model.named_parameters():\n    print(name)\n    print(param)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-14T00:49:21.404573Z","iopub.execute_input":"2024-12-14T00:49:21.405236Z","iopub.status.idle":"2024-12-14T00:49:21.610053Z","shell.execute_reply.started":"2024-12-14T00:49:21.405199Z","shell.execute_reply":"2024-12-14T00:49:21.609201Z"}},"outputs":[{"name":"stdout","text":"layer_1.linear.weight\nParameter containing:\ntensor([[-1.7794e-04, -4.9612e-02, -1.1179e-01,  ...,  1.5865e-01,\n         -1.4340e-01, -2.1754e-01],\n        [ 9.0345e-02,  5.4603e-02,  2.3221e-01,  ...,  1.4463e-01,\n          1.3513e-01,  9.3167e-02],\n        [-9.0080e-02,  2.6050e-01,  9.0216e-02,  ...,  1.7373e-02,\n         -4.4720e-02, -5.2645e-03],\n        ...,\n        [-2.3088e-01, -1.1213e-01,  1.3565e-02,  ...,  2.6273e-01,\n         -2.5864e-01,  1.5783e-01],\n        [-2.4203e-01,  1.8848e-01,  2.5875e-01,  ...,  1.5293e-01,\n          1.9342e-01,  1.3151e-01],\n        [ 1.4840e-01, -5.3512e-02, -1.2958e-01,  ...,  1.1806e-01,\n         -3.0606e-02,  7.1894e-02]], device='cuda:0')\nlayer_1.linear.bias\nParameter containing:\ntensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n        0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0')\nlayer_2.linear.weight\nParameter containing:\ntensor([[ 0.0275,  0.0119, -0.0384,  ...,  0.0624, -0.0218, -0.0091],\n        [ 0.0393,  0.0565,  0.0366,  ..., -0.0507, -0.0503,  0.0253],\n        [-0.0132,  0.0236,  0.0025,  ..., -0.0559,  0.0123,  0.0309],\n        ...,\n        [-0.0490, -0.0307,  0.0470,  ...,  0.0336,  0.0661, -0.0049],\n        [ 0.0632,  0.0027,  0.0027,  ...,  0.0041,  0.0386, -0.0287],\n        [ 0.0611,  0.0326,  0.0537,  ..., -0.0293, -0.0342, -0.0490]],\n       device='cuda:0')\nlayer_2.linear.bias\nParameter containing:\ntensor([0., 0., 0.,  ..., 0., 0., 0.], device='cuda:0')\nfinal.linear.weight\nParameter containing:\ntensor([[-0.3952,  0.1830,  0.3032,  ..., -0.3714, -0.0672,  0.4404],\n        [ 0.3952, -0.1830, -0.3032,  ...,  0.3714,  0.0672, -0.4405]],\n       device='cuda:0', requires_grad=True)\nfinal.linear.bias\nParameter containing:\ntensor([ 0.2240, -0.2240], device='cuda:0', requires_grad=True)\n","output_type":"stream"}],"execution_count":24},{"cell_type":"code","source":"import numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy.stats import norm, laplace, kstest\n\n# Assuming `param` is the tensor containing the weights\nweights = model.final.linear.weight.detach().cpu().numpy().flatten()\n\n# Fit Normal Distribution\nmean_normal, std_normal = norm.fit(weights)\n\n# Fit Laplacian Distribution\nmean_laplace, scale_laplace = laplace.fit(weights)\n\n# Kolmogorov-Smirnov Test\nks_normal = kstest(weights, 'norm', args=(mean_normal, std_normal))\nks_laplace = kstest(weights, 'laplace', args=(mean_laplace, scale_laplace))\n\n# Print Test Results\nprint(\"Normal Distribution Fit:\")\nprint(f\"  Mean: {mean_normal}, Std Dev: {std_normal}\")\nprint(f\"  K-S Test Statistic: {ks_normal.statistic}, p-value: {ks_normal.pvalue}\")\n\nprint(\"\\nLaplacian Distribution Fit:\")\nprint(f\"  Mean: {mean_laplace}, Scale: {scale_laplace}\")\nprint(f\"  K-S Test Statistic: {ks_laplace.statistic}, p-value: {ks_laplace.pvalue}\")\n\n# Plot Distributions and Empirical Data\nx = np.linspace(min(weights), max(weights), 1000)\npdf_normal = norm.pdf(x, mean_normal, std_normal)\npdf_laplace = laplace.pdf(x, mean_laplace, scale_laplace)\n\nplt.hist(weights, bins=1000, density=True, alpha=0.6, color='gray', label='Empirical')\nplt.plot(x, pdf_normal, 'r-', label='Normal Fit')\nplt.plot(x, pdf_laplace, 'b-', label='Laplacian Fit')\nplt.title(\"Weight Distribution with Fitted Distributions\")\nplt.xlabel(\"Weight Value\")\nplt.ylabel(\"Density\")\nplt.legend()\nplt.show()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-14T00:50:09.739761Z","iopub.execute_input":"2024-12-14T00:50:09.740737Z","iopub.status.idle":"2024-12-14T00:50:11.538154Z","shell.execute_reply.started":"2024-12-14T00:50:09.740700Z","shell.execute_reply":"2024-12-14T00:50:11.537193Z"}},"outputs":[{"name":"stdout","text":"Normal Distribution Fit:\n  Mean: -7.671703315281775e-06, Std Dev: 0.7735763788223267\n  K-S Test Statistic: 0.21106331501309428, p-value: 0.0\n\nLaplacian Distribution Fit:\n  Mean: 0.0, Scale: 0.29194854554675875\n  K-S Test Statistic: 0.04948317944674219, p-value: 5.852132447621768e-92\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<Figure size 640x480 with 1 Axes>","image/png":"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"},"metadata":{}}],"execution_count":29},{"cell_type":"code","source":"def custom_lr_lambda(step):\n    num_step_threshold = 100\n\n    if step < num_step_threshold:\n        return step / num_step_threshold\n    if step == num_step_threshold:\n        print(\"here\")\n    return 0.99995 ** (step - num_step_threshold)\n\nstart_time = time.time()\nmodel = TabularDenseNet(num_features, num_classes, num_control_points, 3, 10).to(device)\n\noptimizer = optim.SGD(model.parameters(), lr=0.001 * 10.0)\noptimizer = optim.Adam(model.parameters(), lr=0.001 * 1.0)\nscheduler = LambdaLR(optimizer, lr_lambda=custom_lr_lambda)\n\ncriterion = CustomLoss(nn.CrossEntropyLoss(), 0.0, 0.0, 0.0, 0.0, 0.0)\nevaluate_model(model, custom_train_loader, criterion, optimizer, 10000, scheduler, batch_size * 100, num_features, early_stopping_patience=10000)\n\nmodels.append(model)\nprint(f\"Execution time: {(time.time() - start_time):.6f} seconds\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-13T23:37:16.347470Z","iopub.execute_input":"2024-12-13T23:37:16.348759Z","iopub.status.idle":"2024-12-14T00:02:50.061770Z","shell.execute_reply.started":"2024-12-13T23:37:16.348716Z","shell.execute_reply":"2024-12-14T00:02:50.059208Z"}},"outputs":[{"name":"stdout","text":"torch.Size([72980, 28672])\nLearning Rate: 1e-05\ntorch.Size([18246, 28672])\nEpoch 1, Training Loss: 0.6931473612785339, Validation Loss: 0.6931473612785339\nEpoch 1, Training Loss: 0.6931473612785339, Validation Loss: 0.6931473612785339\n\nLearning Rate: 2e-05\nEpoch 2, Training Loss: 0.6931473612785339, Validation Loss: 0.6783064007759094\nEpoch 2, Training Loss: 0.6782563328742981, Validation Loss: 0.6783064007759094\n\nLearning Rate: 3e-05\nEpoch 3, Training Loss: 0.6782563328742981, Validation Loss: 0.650874137878418\nEpoch 3, Training Loss: 0.650719165802002, Validation Loss: 0.650874137878418\n\nLearning Rate: 4e-05\nEpoch 4, Training Loss: 0.650719165802002, Validation Loss: 0.6153179407119751\nEpoch 4, Training Loss: 0.6149970293045044, Validation Loss: 0.6153179407119751\n\nLearning Rate: 5e-05\nEpoch 5, Training Loss: 0.6149970293045044, Validation Loss: 0.5778284072875977\nEpoch 5, Training Loss: 0.5772804021835327, Validation Loss: 0.5778284072875977\n\nLearning Rate: 6e-05\nEpoch 6, Training Loss: 0.5772804021835327, Validation Loss: 0.5449759364128113\nEpoch 6, Training Loss: 0.5441552996635437, Validation Loss: 0.5449759364128113\n\nLearning Rate: 7.000000000000001e-05\nEpoch 7, Training Loss: 0.5441552996635437, Validation Loss: 0.5217117071151733\nEpoch 7, Training Loss: 0.520607054233551, Validation Loss: 0.5217117071151733\n\nLearning Rate: 8e-05\nEpoch 8, Training Loss: 0.520607054233551, Validation Loss: 0.5097830295562744\nEpoch 8, Training Loss: 0.5084216594696045, Validation Loss: 0.5097830295562744\n\nLearning Rate: 8.999999999999999e-05\nEpoch 9, Training Loss: 0.5084216594696045, Validation Loss: 0.5075812935829163\nEpoch 9, Training Loss: 0.506022036075592, Validation Loss: 0.5075812935829163\n\nLearning Rate: 0.0001\nEpoch 10, Training Loss: 0.506022036075592, Validation Loss: 0.5113261938095093\nEpoch 10, Training Loss: 0.5096492171287537, Validation Loss: 0.5113261938095093\n\nLearning Rate: 0.00011\nEpoch 11, Training Loss: 0.5096492171287537, Validation Loss: 0.5166079998016357\nEpoch 11, Training Loss: 0.5149059295654297, Validation Loss: 0.5166079998016357\n\nLearning Rate: 0.00012\nEpoch 12, Training Loss: 0.5149059295654297, Validation Loss: 0.5194974541664124\nEpoch 12, Training Loss: 0.5178704857826233, Validation Loss: 0.5194974541664124\n\nLearning Rate: 0.00013000000000000002\nEpoch 13, Training Loss: 0.5178704857826233, Validation Loss: 0.5170722603797913\nEpoch 13, Training Loss: 0.5156241655349731, Validation Loss: 0.5170722603797913\n\nLearning Rate: 0.00014000000000000001\nEpoch 14, Training Loss: 0.5156241655349731, Validation Loss: 0.5075650215148926\nEpoch 14, Training Loss: 0.5063996315002441, Validation Loss: 0.5075650215148926\n\nLearning Rate: 0.00015\nEpoch 15, Training Loss: 0.5063996315002441, Validation Loss: 0.49038225412368774\nEpoch 15, Training Loss: 0.48960134387016296, Validation Loss: 0.49038225412368774\n\nLearning Rate: 0.00016\nEpoch 16, Training Loss: 0.48960134387016296, Validation Loss: 0.4661618769168854\nEpoch 16, Training Loss: 0.4658623933792114, Validation Loss: 0.4661618769168854\n\nLearning Rate: 0.00017\nEpoch 17, Training Loss: 0.4658623933792114, Validation Loss: 0.4369419813156128\nEpoch 17, Training Loss: 0.4372144043445587, Validation Loss: 0.4369419813156128\n\nLearning Rate: 0.00017999999999999998\nEpoch 18, Training Loss: 0.4372144043445587, Validation Loss: 0.40637436509132385\nEpoch 18, Training Loss: 0.40730300545692444, Validation Loss: 0.40637436509132385\n\nLearning Rate: 0.00019\nEpoch 19, Training Loss: 0.40730300545692444, Validation Loss: 0.3796054422855377\nEpoch 19, Training Loss: 0.3812730312347412, Validation Loss: 0.3796054422855377\n\nLearning Rate: 0.0002\nEpoch 20, Training Loss: 0.3812730312347412, Validation Loss: 0.36207515001296997\nEpoch 20, Training Loss: 0.36456459760665894, Validation Loss: 0.36207515001296997\n\nLearning Rate: 0.00021\nEpoch 21, Training Loss: 0.36456459760665894, Validation Loss: 0.35688525438308716\nEpoch 21, Training Loss: 0.3602432310581207, Validation Loss: 0.35688525438308716\n\nLearning Rate: 0.00022\nEpoch 22, Training Loss: 0.3602432310581207, Validation Loss: 0.36213093996047974\nEpoch 22, Training Loss: 0.36629748344421387, Validation Loss: 0.36213093996047974\n\nLearning Rate: 0.00023\nEpoch 23, Training Loss: 0.36629748344421387, Validation Loss: 0.3708812892436981\nEpoch 23, Training Loss: 0.37566256523132324, Validation Loss: 0.3708812892436981\n\nLearning Rate: 0.00024\nEpoch 24, Training Loss: 0.37566256523132324, Validation Loss: 0.3749467134475708\nEpoch 24, Training Loss: 0.38005542755126953, Validation Loss: 0.3749467134475708\n\nLearning Rate: 0.00025\nEpoch 25, Training Loss: 0.38005542755126953, Validation Loss: 0.3697971701622009\nEpoch 25, Training Loss: 0.37492236495018005, Validation Loss: 0.3697971701622009\n\nLearning Rate: 0.00026000000000000003\nEpoch 26, Training Loss: 0.37492236495018005, Validation Loss: 0.3567104935646057\nEpoch 26, Training Loss: 0.36158978939056396, Validation Loss: 0.3567104935646057\n\nLearning Rate: 0.00027\nEpoch 27, Training Loss: 0.36158978939056396, Validation Loss: 0.3411489725112915\nEpoch 27, Training Loss: 0.34562039375305176, Validation Loss: 0.3411489725112915\n\nLearning Rate: 0.00028000000000000003\nEpoch 28, Training Loss: 0.34562039375305176, Validation Loss: 0.3291611671447754\nEpoch 28, Training Loss: 0.3331754505634308, Validation Loss: 0.3291611671447754\n\nLearning Rate: 0.00029\nEpoch 29, Training Loss: 0.3331754505634308, Validation Loss: 0.3241594433784485\nEpoch 29, Training Loss: 0.3277580738067627, Validation Loss: 0.3241594433784485\n\nLearning Rate: 0.0003\nEpoch 30, Training Loss: 0.3277580738067627, Validation Loss: 0.3256649672985077\nEpoch 30, Training Loss: 0.32894375920295715, Validation Loss: 0.3256649672985077\n\nLearning Rate: 0.00031\nEpoch 31, Training Loss: 0.32894375920295715, Validation Loss: 0.33027079701423645\nEpoch 31, Training Loss: 0.3333432674407959, Validation Loss: 0.33027079701423645\n\nLearning Rate: 0.00032\nEpoch 32, Training Loss: 0.3333432674407959, Validation Loss: 0.33380821347236633\nEpoch 32, Training Loss: 0.33677804470062256, Validation Loss: 0.33380821347236633\n\nLearning Rate: 0.00033\nEpoch 33, Training Loss: 0.33677804470062256, Validation Loss: 0.3333683907985687\nEpoch 33, Training Loss: 0.3363158702850342, Validation Loss: 0.3333683907985687\n\nLearning Rate: 0.00034\nEpoch 34, Training Loss: 0.3363158702850342, Validation Loss: 0.32840466499328613\nEpoch 34, Training Loss: 0.33137786388397217, Validation Loss: 0.32840466499328613\n\nLearning Rate: 0.00035\nEpoch 35, Training Loss: 0.33137786388397217, Validation Loss: 0.32078179717063904\nEpoch 35, Training Loss: 0.32378748059272766, Validation Loss: 0.32078179717063904\n\nLearning Rate: 0.00035999999999999997\nEpoch 36, Training Loss: 0.32378748059272766, Validation Loss: 0.3138379156589508\nEpoch 36, Training Loss: 0.3168386220932007, Validation Loss: 0.3138379156589508\n\nLearning Rate: 0.00037\nEpoch 37, Training Loss: 0.3168386220932007, Validation Loss: 0.31059667468070984\nEpoch 37, Training Loss: 0.31352612376213074, Validation Loss: 0.31059667468070984\n\nLearning Rate: 0.00038\nEpoch 38, Training Loss: 0.31352612376213074, Validation Loss: 0.3118014335632324\nEpoch 38, Training Loss: 0.3145960569381714, Validation Loss: 0.3118014335632324\n\nLearning Rate: 0.00039000000000000005\nEpoch 39, Training Loss: 0.3145960569381714, Validation Loss: 0.3152276575565338\nEpoch 39, Training Loss: 0.31785258650779724, Validation Loss: 0.3152276575565338\n\nLearning Rate: 0.0004\nEpoch 40, Training Loss: 0.31785258650779724, Validation Loss: 0.31727463006973267\nEpoch 40, Training Loss: 0.31971773505210876, Validation Loss: 0.31727463006973267\n\nLearning Rate: 0.00041\nEpoch 41, Training Loss: 0.31971773505210876, Validation Loss: 0.31574803590774536\nEpoch 41, Training Loss: 0.3180117905139923, Validation Loss: 0.31574803590774536\n\nLearning Rate: 0.00042\nEpoch 42, Training Loss: 0.3180117905139923, Validation Loss: 0.31144416332244873\nEpoch 42, Training Loss: 0.31355947256088257, Validation Loss: 0.31144416332244873\n\nLearning Rate: 0.00043\nEpoch 43, Training Loss: 0.31355947256088257, Validation Loss: 0.30723482370376587\nEpoch 43, Training Loss: 0.3092636168003082, Validation Loss: 0.30723482370376587\n\nLearning Rate: 0.00044\nEpoch 44, Training Loss: 0.3092636168003082, Validation Loss: 0.3055626153945923\nEpoch 44, Training Loss: 0.3075673580169678, Validation Loss: 0.3055626153945923\n\nLearning Rate: 0.00045000000000000004\nEpoch 45, Training Loss: 0.3075673580169678, Validation Loss: 0.30652958154678345\nEpoch 45, Training Loss: 0.3085368275642395, Validation Loss: 0.30652958154678345\n\nLearning Rate: 0.00046\nEpoch 46, Training Loss: 0.3085368275642395, Validation Loss: 0.3080975413322449\nEpoch 46, Training Loss: 0.31009039282798767, Validation Loss: 0.3080975413322449\n\nLearning Rate: 0.00047\nEpoch 47, Training Loss: 0.31009039282798767, Validation Loss: 0.3079862594604492\nEpoch 47, Training Loss: 0.3099216818809509, Validation Loss: 0.3079862594604492\n\nLearning Rate: 0.00048\nEpoch 48, Training Loss: 0.3099216818809509, Validation Loss: 0.3055344820022583\nEpoch 48, Training Loss: 0.3073625862598419, Validation Loss: 0.3055344820022583\n\nLearning Rate: 0.00049\nEpoch 49, Training Loss: 0.3073625862598419, Validation Loss: 0.30209416151046753\nEpoch 49, Training Loss: 0.30377501249313354, Validation Loss: 0.30209416151046753\n\nLearning Rate: 0.0005\nEpoch 50, Training Loss: 0.30377501249313354, Validation Loss: 0.2997327148914337\nEpoch 50, Training Loss: 0.3012565076351166, Validation Loss: 0.2997327148914337\n\nLearning Rate: 0.00051\nEpoch 51, Training Loss: 0.3012565076351166, Validation Loss: 0.2992880642414093\nEpoch 51, Training Loss: 0.3006846308708191, Validation Loss: 0.2992880642414093\n\nLearning Rate: 0.0005200000000000001\nEpoch 52, Training Loss: 0.3006846308708191, Validation Loss: 0.2995661795139313\nEpoch 52, Training Loss: 0.30088961124420166, Validation Loss: 0.2995661795139313\n\nLearning Rate: 0.0005300000000000001\nEpoch 53, Training Loss: 0.30088961124420166, Validation Loss: 0.2986449897289276\nEpoch 53, Training Loss: 0.29995012283325195, Validation Loss: 0.2986449897289276\n\nLearning Rate: 0.00054\nEpoch 54, Training Loss: 0.29995012283325195, Validation Loss: 0.29598501324653625\nEpoch 54, Training Loss: 0.29731830954551697, Validation Loss: 0.29598501324653625\n\nLearning Rate: 0.00055\nEpoch 55, Training Loss: 0.29731830954551697, Validation Loss: 0.2929510772228241\nEpoch 55, Training Loss: 0.2943478524684906, Validation Loss: 0.2929510772228241\n\nLearning Rate: 0.0005600000000000001\nEpoch 56, Training Loss: 0.2943478524684906, Validation Loss: 0.29116126894950867\nEpoch 56, Training Loss: 0.2926333248615265, Validation Loss: 0.29116126894950867\n\nLearning Rate: 0.00057\nEpoch 57, Training Loss: 0.2926333248615265, Validation Loss: 0.2906825840473175\nEpoch 57, Training Loss: 0.2922112047672272, Validation Loss: 0.2906825840473175\n\nLearning Rate: 0.00058\nEpoch 58, Training Loss: 0.2922112047672272, Validation Loss: 0.29017770290374756\nEpoch 58, Training Loss: 0.2917287051677704, Validation Loss: 0.29017770290374756\n\nLearning Rate: 0.00059\nEpoch 59, Training Loss: 0.2917287051677704, Validation Loss: 0.28862690925598145\nEpoch 59, Training Loss: 0.2901722192764282, Validation Loss: 0.28862690925598145\n\nLearning Rate: 0.0006\nEpoch 60, Training Loss: 0.2901722192764282, Validation Loss: 0.28648510575294495\nEpoch 60, Training Loss: 0.28801199793815613, Validation Loss: 0.28648510575294495\n\nLearning Rate: 0.00061\nEpoch 61, Training Loss: 0.28801199793815613, Validation Loss: 0.2849949896335602\nEpoch 61, Training Loss: 0.28650563955307007, Validation Loss: 0.2849949896335602\n\nLearning Rate: 0.00062\nEpoch 62, Training Loss: 0.28650563955307007, Validation Loss: 0.2845490872859955\nEpoch 62, Training Loss: 0.28605371713638306, Validation Loss: 0.2845490872859955\n\nLearning Rate: 0.00063\nEpoch 63, Training Loss: 0.28605371713638306, Validation Loss: 0.2841993570327759\nEpoch 63, Training Loss: 0.2857036888599396, Validation Loss: 0.2841993570327759\n\nLearning Rate: 0.00064\nEpoch 64, Training Loss: 0.2857036888599396, Validation Loss: 0.2830337584018707\nEpoch 64, Training Loss: 0.28453055024147034, Validation Loss: 0.2830337584018707\n\nLearning Rate: 0.0006500000000000001\nEpoch 65, Training Loss: 0.28453055024147034, Validation Loss: 0.2814599275588989\nEpoch 65, Training Loss: 0.2829328775405884, Validation Loss: 0.2814599275588989\n\nLearning Rate: 0.00066\nEpoch 66, Training Loss: 0.2829328775405884, Validation Loss: 0.28049296140670776\nEpoch 66, Training Loss: 0.28192535042762756, Validation Loss: 0.28049296140670776\n\nLearning Rate: 0.00067\nEpoch 67, Training Loss: 0.28192535042762756, Validation Loss: 0.2802116572856903\nEpoch 67, Training Loss: 0.28159475326538086, Validation Loss: 0.2802116572856903\n\nLearning Rate: 0.00068\nEpoch 68, Training Loss: 0.28159475326538086, Validation Loss: 0.2797524333000183\nEpoch 68, Training Loss: 0.28109100461006165, Validation Loss: 0.2797524333000183\n\nLearning Rate: 0.00069\nEpoch 69, Training Loss: 0.28109100461006165, Validation Loss: 0.2786925435066223\nEpoch 69, Training Loss: 0.2800012230873108, Validation Loss: 0.2786925435066223\n\nLearning Rate: 0.0007\nEpoch 70, Training Loss: 0.2800012230873108, Validation Loss: 0.2776351869106293\nEpoch 70, Training Loss: 0.27892830967903137, Validation Loss: 0.2776351869106293\n\nLearning Rate: 0.00071\nEpoch 71, Training Loss: 0.27892830967903137, Validation Loss: 0.2771696150302887\nEpoch 71, Training Loss: 0.2784535586833954, Validation Loss: 0.2771696150302887\n\nLearning Rate: 0.0007199999999999999\nEpoch 72, Training Loss: 0.2784535586833954, Validation Loss: 0.27691489458084106\nEpoch 72, Training Loss: 0.27818629145622253, Validation Loss: 0.27691489458084106\n\nLearning Rate: 0.00073\nEpoch 73, Training Loss: 0.27818629145622253, Validation Loss: 0.2762396037578583\nEpoch 73, Training Loss: 0.27748924493789673, Validation Loss: 0.2762396037578583\n\nLearning Rate: 0.00074\nEpoch 74, Training Loss: 0.27748924493789673, Validation Loss: 0.2753588855266571\nEpoch 74, Training Loss: 0.2765815556049347, Validation Loss: 0.2753588855266571\n\nLearning Rate: 0.00075\nEpoch 75, Training Loss: 0.2765815556049347, Validation Loss: 0.27483734488487244\nEpoch 75, Training Loss: 0.2760395109653473, Validation Loss: 0.27483734488487244\n\nLearning Rate: 0.00076\nEpoch 76, Training Loss: 0.2760395109653473, Validation Loss: 0.27452531456947327\nEpoch 76, Training Loss: 0.27572524547576904, Validation Loss: 0.27452531456947327\n\nLearning Rate: 0.0007700000000000001\nEpoch 77, Training Loss: 0.27572524547576904, Validation Loss: 0.27389729022979736\nEpoch 77, Training Loss: 0.27511924505233765, Validation Loss: 0.27389729022979736\n\nLearning Rate: 0.0007800000000000001\nEpoch 78, Training Loss: 0.27511924505233765, Validation Loss: 0.2730233669281006\nEpoch 78, Training Loss: 0.2742886543273926, Validation Loss: 0.2730233669281006\n\nLearning Rate: 0.00079\nEpoch 79, Training Loss: 0.2742886543273926, Validation Loss: 0.2723677158355713\nEpoch 79, Training Loss: 0.27368736267089844, Validation Loss: 0.2723677158355713\n\nLearning Rate: 0.0008\nEpoch 80, Training Loss: 0.27368736267089844, Validation Loss: 0.27190136909484863\nEpoch 80, Training Loss: 0.27327460050582886, Validation Loss: 0.27190136909484863\n\nLearning Rate: 0.0008100000000000001\nEpoch 81, Training Loss: 0.27327460050582886, Validation Loss: 0.27124422788619995\nEpoch 81, Training Loss: 0.2726624310016632, Validation Loss: 0.27124422788619995\n\nLearning Rate: 0.00082\nEpoch 82, Training Loss: 0.2726624310016632, Validation Loss: 0.27045968174934387\nEpoch 82, Training Loss: 0.2719142735004425, Validation Loss: 0.27045968174934387\n\nLearning Rate: 0.00083\nEpoch 83, Training Loss: 0.2719142735004425, Validation Loss: 0.26987719535827637\nEpoch 83, Training Loss: 0.27136537432670593, Validation Loss: 0.26987719535827637\n\nLearning Rate: 0.00084\nEpoch 84, Training Loss: 0.27136537432670593, Validation Loss: 0.2694137692451477\nEpoch 84, Training Loss: 0.2709383964538574, Validation Loss: 0.2694137692451477\n\nLearning Rate: 0.00085\nEpoch 85, Training Loss: 0.2709383964538574, Validation Loss: 0.268797904253006\nEpoch 85, Training Loss: 0.27036312222480774, Validation Loss: 0.268797904253006\n\nLearning Rate: 0.00086\nEpoch 86, Training Loss: 0.27036312222480774, Validation Loss: 0.268128365278244\nEpoch 86, Training Loss: 0.2697339951992035, Validation Loss: 0.268128365278244\n\nLearning Rate: 0.00087\nEpoch 87, Training Loss: 0.2697339951992035, Validation Loss: 0.2676272690296173\nEpoch 87, Training Loss: 0.2692660689353943, Validation Loss: 0.2676272690296173\n\nLearning Rate: 0.00088\nEpoch 88, Training Loss: 0.2692660689353943, Validation Loss: 0.2671838402748108\nEpoch 88, Training Loss: 0.26884356141090393, Validation Loss: 0.2671838402748108\n\nLearning Rate: 0.0008900000000000001\nEpoch 89, Training Loss: 0.26884356141090393, Validation Loss: 0.2666339576244354\nEpoch 89, Training Loss: 0.26830214262008667, Validation Loss: 0.2666339576244354\n\nLearning Rate: 0.0009000000000000001\nEpoch 90, Training Loss: 0.26830214262008667, Validation Loss: 0.2661057412624359\nEpoch 90, Training Loss: 0.2677742838859558, Validation Loss: 0.2661057412624359\n\nLearning Rate: 0.00091\nEpoch 91, Training Loss: 0.2677742838859558, Validation Loss: 0.265702486038208\nEpoch 91, Training Loss: 0.2673690617084503, Validation Loss: 0.265702486038208\n\nLearning Rate: 0.00092\nEpoch 92, Training Loss: 0.2673690617084503, Validation Loss: 0.2652837038040161\nEpoch 92, Training Loss: 0.2669495940208435, Validation Loss: 0.2652837038040161\n\nLearning Rate: 0.00093\nEpoch 93, Training Loss: 0.2669495940208435, Validation Loss: 0.2647930681705475\nEpoch 93, Training Loss: 0.2664600908756256, Validation Loss: 0.2647930681705475\n\nLearning Rate: 0.00094\nEpoch 94, Training Loss: 0.2664600908756256, Validation Loss: 0.2643614411354065\nEpoch 94, Training Loss: 0.26603081822395325, Validation Loss: 0.2643614411354065\n\nLearning Rate: 0.00095\nEpoch 95, Training Loss: 0.26603081822395325, Validation Loss: 0.263994425535202\nEpoch 95, Training Loss: 0.26566678285598755, Validation Loss: 0.263994425535202\n\nLearning Rate: 0.00096\nEpoch 96, Training Loss: 0.26566678285598755, Validation Loss: 0.26357361674308777\nEpoch 96, Training Loss: 0.26525044441223145, Validation Loss: 0.26357361674308777\n\nLearning Rate: 0.0009699999999999999\nEpoch 97, Training Loss: 0.26525044441223145, Validation Loss: 0.2631305754184723\nEpoch 97, Training Loss: 0.264815092086792, Validation Loss: 0.2631305754184723\n\nLearning Rate: 0.00098\nEpoch 98, Training Loss: 0.264815092086792, Validation Loss: 0.2627462148666382\nEpoch 98, Training Loss: 0.26444312930107117, Validation Loss: 0.2627462148666382\n\nLearning Rate: 0.00099\nEpoch 99, Training Loss: 0.26444312930107117, Validation Loss: 0.26235532760620117\nEpoch 99, Training Loss: 0.2640700936317444, Validation Loss: 0.26235532760620117\n\nhere\nLearning Rate: 0.001\nEpoch 100, Training Loss: 0.2640700936317444, Validation Loss: 0.26191502809524536\nEpoch 100, Training Loss: 0.26365265250205994, Validation Loss: 0.26191502809524536\n\nLearning Rate: 0.00099995\nEpoch 101, Training Loss: 0.26365265250205994, Validation Loss: 0.2615010440349579\nEpoch 101, Training Loss: 0.2632652521133423, Validation Loss: 0.2615010440349579\n\nLearning Rate: 0.0009999000025\nEpoch 102, Training Loss: 0.2632652521133423, Validation Loss: 0.26112204790115356\nEpoch 102, Training Loss: 0.2629142999649048, Validation Loss: 0.26112204790115356\n\nLearning Rate: 0.0009998500074998751\nEpoch 103, Training Loss: 0.2629142999649048, Validation Loss: 0.26071739196777344\nEpoch 103, Training Loss: 0.26253756880760193, Validation Loss: 0.26071739196777344\n\nLearning Rate: 0.0009998000149995\nEpoch 104, Training Loss: 0.26253756880760193, Validation Loss: 0.2603188753128052\nEpoch 104, Training Loss: 0.26216593384742737, Validation Loss: 0.2603188753128052\n\nLearning Rate: 0.00099975002499875\nEpoch 105, Training Loss: 0.26216593384742737, Validation Loss: 0.2599619925022125\nEpoch 105, Training Loss: 0.26183459162712097, Validation Loss: 0.2599619925022125\n\nLearning Rate: 0.0009997000374975002\nEpoch 106, Training Loss: 0.26183459162712097, Validation Loss: 0.259602427482605\nEpoch 106, Training Loss: 0.2614993751049042, Validation Loss: 0.259602427482605\n\nLearning Rate: 0.0009996500524956252\nEpoch 107, Training Loss: 0.2614993751049042, Validation Loss: 0.259233683347702\nEpoch 107, Training Loss: 0.2611534595489502, Validation Loss: 0.259233683347702\n\nLearning Rate: 0.0009996000699930006\nEpoch 108, Training Loss: 0.2611534595489502, Validation Loss: 0.25889551639556885\nEpoch 108, Training Loss: 0.2608354985713959, Validation Loss: 0.25889551639556885\n\nLearning Rate: 0.0009995500899895008\nEpoch 109, Training Loss: 0.2608354985713959, Validation Loss: 0.2585727274417877\nEpoch 109, Training Loss: 0.2605293393135071, Validation Loss: 0.2585727274417877\n\nLearning Rate: 0.0009995001124850013\nEpoch 110, Training Loss: 0.2605293393135071, Validation Loss: 0.2582416832447052\nEpoch 110, Training Loss: 0.2602112591266632, Validation Loss: 0.2582416832447052\n\nLearning Rate: 0.000999450137479377\nEpoch 111, Training Loss: 0.2602112591266632, Validation Loss: 0.25792765617370605\nEpoch 111, Training Loss: 0.25990766286849976, Validation Loss: 0.25792765617370605\n\nLearning Rate: 0.0009994001649725032\nEpoch 112, Training Loss: 0.25990766286849976, Validation Loss: 0.2576335370540619\nEpoch 112, Training Loss: 0.2596225142478943, Validation Loss: 0.2576335370540619\n\nLearning Rate: 0.0009993501949642546\nEpoch 113, Training Loss: 0.2596225142478943, Validation Loss: 0.25733405351638794\nEpoch 113, Training Loss: 0.25933152437210083, Validation Loss: 0.25733405351638794\n\nLearning Rate: 0.0009993002274545062\nEpoch 114, Training Loss: 0.25933152437210083, Validation Loss: 0.2570389211177826\nEpoch 114, Training Loss: 0.2590445280075073, Validation Loss: 0.2570389211177826\n\nLearning Rate: 0.0009992502624431338\nEpoch 115, Training Loss: 0.2590445280075073, Validation Loss: 0.2567611634731293\nEpoch 115, Training Loss: 0.25877442955970764, Validation Loss: 0.2567611634731293\n\nLearning Rate: 0.0009992002999300114\nEpoch 116, Training Loss: 0.25877442955970764, Validation Loss: 0.2564834654331207\nEpoch 116, Training Loss: 0.2585039436817169, Validation Loss: 0.2564834654331207\n\nLearning Rate: 0.000999150339915015\nEpoch 117, Training Loss: 0.2585039436817169, Validation Loss: 0.25620394945144653\nEpoch 117, Training Loss: 0.25823143124580383, Validation Loss: 0.25620394945144653\n\nLearning Rate: 0.0009991003823980192\nEpoch 118, Training Loss: 0.25823143124580383, Validation Loss: 0.2559363543987274\nEpoch 118, Training Loss: 0.2579711079597473, Validation Loss: 0.2559363543987274\n\nLearning Rate: 0.0009990504273788994\nEpoch 119, Training Loss: 0.2579711079597473, Validation Loss: 0.25567254424095154\nEpoch 119, Training Loss: 0.25771474838256836, Validation Loss: 0.25567254424095154\n\nLearning Rate: 0.0009990004748575304\nEpoch 120, Training Loss: 0.25771474838256836, Validation Loss: 0.25540608167648315\nEpoch 120, Training Loss: 0.2574556767940521, Validation Loss: 0.25540608167648315\n\nLearning Rate: 0.0009989505248337875\nEpoch 121, Training Loss: 0.2574556767940521, Validation Loss: 0.255148321390152\nEpoch 121, Training Loss: 0.25720474123954773, Validation Loss: 0.255148321390152\n\nLearning Rate: 0.0009989005773075458\nEpoch 122, Training Loss: 0.25720474123954773, Validation Loss: 0.2548982501029968\nEpoch 122, Training Loss: 0.2569606602191925, Validation Loss: 0.2548982501029968\n\nLearning Rate: 0.0009988506322786805\nEpoch 123, Training Loss: 0.2569606602191925, Validation Loss: 0.25464820861816406\nEpoch 123, Training Loss: 0.256715327501297, Validation Loss: 0.25464820861816406\n\nLearning Rate: 0.0009988006897470665\nEpoch 124, Training Loss: 0.256715327501297, Validation Loss: 0.25440436601638794\nEpoch 124, Training Loss: 0.25647491216659546, Validation Loss: 0.25440436601638794\n\nLearning Rate: 0.0009987507497125792\nEpoch 125, Training Loss: 0.25647491216659546, Validation Loss: 0.25416913628578186\nEpoch 125, Training Loss: 0.25624173879623413, Validation Loss: 0.25416913628578186\n\nLearning Rate: 0.0009987008121750936\nEpoch 126, Training Loss: 0.25624173879623413, Validation Loss: 0.2539357542991638\nEpoch 126, Training Loss: 0.25600895285606384, Validation Loss: 0.2539357542991638\n\nLearning Rate: 0.0009986508771344849\nEpoch 127, Training Loss: 0.25600895285606384, Validation Loss: 0.25370660424232483\nEpoch 127, Training Loss: 0.25577881932258606, Validation Loss: 0.25370660424232483\n\nLearning Rate: 0.000998600944590628\nEpoch 128, Training Loss: 0.25577881932258606, Validation Loss: 0.25348523259162903\nEpoch 128, Training Loss: 0.2555552124977112, Validation Loss: 0.25348523259162903\n\nLearning Rate: 0.0009985510145433987\nEpoch 129, Training Loss: 0.2555552124977112, Validation Loss: 0.2532665431499481\nEpoch 129, Training Loss: 0.25533339381217957, Validation Loss: 0.2532665431499481\n\nLearning Rate: 0.0009985010869926715\nEpoch 130, Training Loss: 0.25533339381217957, Validation Loss: 0.25305017828941345\nEpoch 130, Training Loss: 0.25511330366134644, Validation Loss: 0.25305017828941345\n\nLearning Rate: 0.0009984511619383217\nEpoch 131, Training Loss: 0.25511330366134644, Validation Loss: 0.25283971428871155\nEpoch 131, Training Loss: 0.25489869713783264, Validation Loss: 0.25283971428871155\n\nLearning Rate: 0.000998401239380225\nEpoch 132, Training Loss: 0.25489869713783264, Validation Loss: 0.2526322305202484\nEpoch 132, Training Loss: 0.2546866834163666, Validation Loss: 0.2526322305202484\n\nLearning Rate: 0.0009983513193182558\nEpoch 133, Training Loss: 0.2546866834163666, Validation Loss: 0.25242629647254944\nEpoch 133, Training Loss: 0.25447607040405273, Validation Loss: 0.25242629647254944\n\nLearning Rate: 0.00099830140175229\nEpoch 134, Training Loss: 0.25447607040405273, Validation Loss: 0.2522243559360504\nEpoch 134, Training Loss: 0.2542695999145508, Validation Loss: 0.2522243559360504\n\nLearning Rate: 0.0009982514866822023\nEpoch 135, Training Loss: 0.2542695999145508, Validation Loss: 0.25202497839927673\nEpoch 135, Training Loss: 0.25406613945961, Validation Loss: 0.25202497839927673\n\nLearning Rate: 0.0009982015741078682\nEpoch 136, Training Loss: 0.25406613945961, Validation Loss: 0.25182628631591797\nEpoch 136, Training Loss: 0.25386378169059753, Validation Loss: 0.25182628631591797\n\nLearning Rate: 0.000998151664029163\nEpoch 137, Training Loss: 0.25386378169059753, Validation Loss: 0.25163039565086365\nEpoch 137, Training Loss: 0.25366461277008057, Validation Loss: 0.25163039565086365\n\nLearning Rate: 0.0009981017564459614\nEpoch 138, Training Loss: 0.25366461277008057, Validation Loss: 0.2514375150203705\nEpoch 138, Training Loss: 0.25346848368644714, Validation Loss: 0.2514375150203705\n\nLearning Rate: 0.0009980518513581392\nEpoch 139, Training Loss: 0.25346848368644714, Validation Loss: 0.2512461841106415\nEpoch 139, Training Loss: 0.25327378511428833, Validation Loss: 0.2512461841106415\n\nLearning Rate: 0.0009980019487655712\nEpoch 140, Training Loss: 0.25327378511428833, Validation Loss: 0.25105783343315125\nEpoch 140, Training Loss: 0.25308170914649963, Validation Loss: 0.25105783343315125\n\nLearning Rate: 0.000997952048668133\nEpoch 141, Training Loss: 0.25308170914649963, Validation Loss: 0.2508726418018341\nEpoch 141, Training Loss: 0.2528926432132721, Validation Loss: 0.2508726418018341\n\nLearning Rate: 0.0009979021510656996\nEpoch 142, Training Loss: 0.2528926432132721, Validation Loss: 0.2506892681121826\nEpoch 142, Training Loss: 0.2527051866054535, Validation Loss: 0.2506892681121826\n\nLearning Rate: 0.0009978522559581462\nEpoch 143, Training Loss: 0.2527051866054535, Validation Loss: 0.250508576631546\nEpoch 143, Training Loss: 0.25251996517181396, Validation Loss: 0.250508576631546\n\nLearning Rate: 0.0009978023633453483\nEpoch 144, Training Loss: 0.25251996517181396, Validation Loss: 0.25033122301101685\nEpoch 144, Training Loss: 0.2523374855518341, Validation Loss: 0.25033122301101685\n\nLearning Rate: 0.0009977524732271812\nEpoch 145, Training Loss: 0.2523374855518341, Validation Loss: 0.25015637278556824\nEpoch 145, Training Loss: 0.25215673446655273, Validation Loss: 0.25015637278556824\n\nLearning Rate: 0.0009977025856035197\nEpoch 146, Training Loss: 0.25215673446655273, Validation Loss: 0.24998416006565094\nEpoch 146, Training Loss: 0.2519780099391937, Validation Loss: 0.24998416006565094\n\nLearning Rate: 0.0009976527004742397\nEpoch 147, Training Loss: 0.2519780099391937, Validation Loss: 0.2498149275779724\nEpoch 147, Training Loss: 0.2518017590045929, Validation Loss: 0.2498149275779724\n\nLearning Rate: 0.0009976028178392158\nEpoch 148, Training Loss: 0.2518017590045929, Validation Loss: 0.24964769184589386\nEpoch 148, Training Loss: 0.2516273260116577, Validation Loss: 0.24964769184589386\n\nLearning Rate: 0.0009975529376983238\nEpoch 149, Training Loss: 0.2516273260116577, Validation Loss: 0.24948252737522125\nEpoch 149, Training Loss: 0.2514546811580658, Validation Loss: 0.24948252737522125\n\nLearning Rate: 0.000997503060051439\nEpoch 150, Training Loss: 0.2514546811580658, Validation Loss: 0.24931979179382324\nEpoch 150, Training Loss: 0.2512843608856201, Validation Loss: 0.24931979179382324\n\nLearning Rate: 0.0009974531848984363\nEpoch 151, Training Loss: 0.2512843608856201, Validation Loss: 0.249159038066864\nEpoch 151, Training Loss: 0.2511157989501953, Validation Loss: 0.249159038066864\n\nLearning Rate: 0.0009974033122391915\nEpoch 152, Training Loss: 0.2511157989501953, Validation Loss: 0.2490001767873764\nEpoch 152, Training Loss: 0.2509489059448242, Validation Loss: 0.2490001767873764\n\nLearning Rate: 0.0009973534420735797\nEpoch 153, Training Loss: 0.2509489059448242, Validation Loss: 0.24884335696697235\nEpoch 153, Training Loss: 0.25078409910202026, Validation Loss: 0.24884335696697235\n\nLearning Rate: 0.000997303574401476\nEpoch 154, Training Loss: 0.25078409910202026, Validation Loss: 0.24868817627429962\nEpoch 154, Training Loss: 0.2506209909915924, Validation Loss: 0.24868817627429962\n\nLearning Rate: 0.0009972537092227559\nEpoch 155, Training Loss: 0.2506209909915924, Validation Loss: 0.24853451550006866\nEpoch 155, Training Loss: 0.2504595220088959, Validation Loss: 0.24853451550006866\n\nLearning Rate: 0.0009972038465372946\nEpoch 156, Training Loss: 0.2504595220088959, Validation Loss: 0.24838274717330933\nEpoch 156, Training Loss: 0.25029993057250977, Validation Loss: 0.24838274717330933\n\nLearning Rate: 0.0009971539863449677\nEpoch 157, Training Loss: 0.25029993057250977, Validation Loss: 0.2482328712940216\nEpoch 157, Training Loss: 0.250141978263855, Validation Loss: 0.2482328712940216\n\nLearning Rate: 0.0009971041286456505\nEpoch 158, Training Loss: 0.250141978263855, Validation Loss: 0.24808476865291595\nEpoch 158, Training Loss: 0.24998560547828674, Validation Loss: 0.24808476865291595\n\nLearning Rate: 0.0009970542734392184\nEpoch 159, Training Loss: 0.24998560547828674, Validation Loss: 0.24793857336044312\nEpoch 159, Training Loss: 0.2498309463262558, Validation Loss: 0.24793857336044312\n\nLearning Rate: 0.0009970044207255463\nEpoch 160, Training Loss: 0.2498309463262558, Validation Loss: 0.24779416620731354\nEpoch 160, Training Loss: 0.24967794120311737, Validation Loss: 0.24779416620731354\n\nLearning Rate: 0.0009969545705045102\nEpoch 161, Training Loss: 0.24967794120311737, Validation Loss: 0.2476513683795929\nEpoch 161, Training Loss: 0.24952644109725952, Validation Loss: 0.2476513683795929\n\nLearning Rate: 0.000996904722775985\nEpoch 162, Training Loss: 0.24952644109725952, Validation Loss: 0.2475103735923767\nEpoch 162, Training Loss: 0.24937653541564941, Validation Loss: 0.2475103735923767\n\nLearning Rate: 0.000996854877539846\nEpoch 163, Training Loss: 0.24937653541564941, Validation Loss: 0.24737118184566498\nEpoch 163, Training Loss: 0.24922822415828705, Validation Loss: 0.24737118184566498\n\nLearning Rate: 0.000996805034795969\nEpoch 164, Training Loss: 0.24922822415828705, Validation Loss: 0.24723368883132935\nEpoch 164, Training Loss: 0.2490812987089157, Validation Loss: 0.24723368883132935\n\nLearning Rate: 0.0009967551945442292\nEpoch 165, Training Loss: 0.2490812987089157, Validation Loss: 0.24709787964820862\nEpoch 165, Training Loss: 0.24893595278263092, Validation Loss: 0.24709787964820862\n\nLearning Rate: 0.0009967053567845022\nEpoch 166, Training Loss: 0.24893595278263092, Validation Loss: 0.24696356058120728\nEpoch 166, Training Loss: 0.24879199266433716, Validation Loss: 0.24696356058120728\n\nLearning Rate: 0.000996655521516663\nEpoch 167, Training Loss: 0.24879199266433716, Validation Loss: 0.24683061242103577\nEpoch 167, Training Loss: 0.2486494928598404, Validation Loss: 0.24683061242103577\n\nLearning Rate: 0.000996605688740587\nEpoch 168, Training Loss: 0.2486494928598404, Validation Loss: 0.24669907987117767\nEpoch 168, Training Loss: 0.24850842356681824, Validation Loss: 0.24669907987117767\n\nLearning Rate: 0.00099655585845615\nEpoch 169, Training Loss: 0.24850842356681824, Validation Loss: 0.24656905233860016\nEpoch 169, Training Loss: 0.2483687400817871, Validation Loss: 0.24656905233860016\n\nLearning Rate: 0.0009965060306632273\nEpoch 170, Training Loss: 0.2483687400817871, Validation Loss: 0.24644030630588531\nEpoch 170, Training Loss: 0.24823038280010223, Validation Loss: 0.24644030630588531\n\nLearning Rate: 0.000996456205361694\nEpoch 171, Training Loss: 0.24823038280010223, Validation Loss: 0.2463129758834839\nEpoch 171, Training Loss: 0.248093381524086, Validation Loss: 0.2463129758834839\n\nLearning Rate: 0.0009964063825514261\nEpoch 172, Training Loss: 0.248093381524086, Validation Loss: 0.24618691205978394\nEpoch 172, Training Loss: 0.2479577362537384, Validation Loss: 0.24618691205978394\n\nLearning Rate: 0.0009963565622322984\nEpoch 173, Training Loss: 0.2479577362537384, Validation Loss: 0.24606208503246307\nEpoch 173, Training Loss: 0.2478232979774475, Validation Loss: 0.24606208503246307\n\nLearning Rate: 0.0009963067444041867\nEpoch 174, Training Loss: 0.2478232979774475, Validation Loss: 0.24593861401081085\nEpoch 174, Training Loss: 0.24769021570682526, Validation Loss: 0.24593861401081085\n\nLearning Rate: 0.0009962569290669666\nEpoch 175, Training Loss: 0.24769021570682526, Validation Loss: 0.24581652879714966\nEpoch 175, Training Loss: 0.2475583553314209, Validation Loss: 0.24581652879714966\n\nLearning Rate: 0.0009962071162205133\nEpoch 176, Training Loss: 0.2475583553314209, Validation Loss: 0.2456957846879959\nEpoch 176, Training Loss: 0.24742773175239563, Validation Loss: 0.2456957846879959\n\nLearning Rate: 0.0009961573058647022\nEpoch 177, Training Loss: 0.24742773175239563, Validation Loss: 0.245576411485672\nEpoch 177, Training Loss: 0.24729833006858826, Validation Loss: 0.245576411485672\n\nLearning Rate: 0.000996107497999409\nEpoch 178, Training Loss: 0.24729833006858826, Validation Loss: 0.24545830488204956\nEpoch 178, Training Loss: 0.24717013537883759, Validation Loss: 0.24545830488204956\n\nLearning Rate: 0.000996057692624509\nEpoch 179, Training Loss: 0.24717013537883759, Validation Loss: 0.2453414648771286\nEpoch 179, Training Loss: 0.24704311788082123, Validation Loss: 0.2453414648771286\n\nLearning Rate: 0.0009960078897398778\nEpoch 180, Training Loss: 0.24704311788082123, Validation Loss: 0.24522589147090912\nEpoch 180, Training Loss: 0.2469172328710556, Validation Loss: 0.24522589147090912\n\nLearning Rate: 0.0009959580893453908\nEpoch 181, Training Loss: 0.2469172328710556, Validation Loss: 0.24511151015758514\nEpoch 181, Training Loss: 0.2467925250530243, Validation Loss: 0.24511151015758514\n\nLearning Rate: 0.0009959082914409235\nEpoch 182, Training Loss: 0.2467925250530243, Validation Loss: 0.24499842524528503\nEpoch 182, Training Loss: 0.24666893482208252, Validation Loss: 0.24499842524528503\n\nLearning Rate: 0.0009958584960263516\nEpoch 183, Training Loss: 0.24666893482208252, Validation Loss: 0.24488648772239685\nEpoch 183, Training Loss: 0.24654649198055267, Validation Loss: 0.24488648772239685\n\nLearning Rate: 0.0009958087031015502\nEpoch 184, Training Loss: 0.24654649198055267, Validation Loss: 0.24477560818195343\nEpoch 184, Training Loss: 0.2464250922203064, Validation Loss: 0.24477560818195343\n\nLearning Rate: 0.0009957589126663952\nEpoch 185, Training Loss: 0.2464250922203064, Validation Loss: 0.24466575682163239\nEpoch 185, Training Loss: 0.24630475044250488, Validation Loss: 0.24466575682163239\n\nLearning Rate: 0.0009957091247207617\nEpoch 186, Training Loss: 0.24630475044250488, Validation Loss: 0.2445569932460785\nEpoch 186, Training Loss: 0.2461855113506317, Validation Loss: 0.2445569932460785\n\nLearning Rate: 0.0009956593392645258\nEpoch 187, Training Loss: 0.2461855113506317, Validation Loss: 0.24444928765296936\nEpoch 187, Training Loss: 0.2460673302412033, Validation Loss: 0.24444928765296936\n\nLearning Rate: 0.0009956095562975626\nEpoch 188, Training Loss: 0.2460673302412033, Validation Loss: 0.2443425953388214\nEpoch 188, Training Loss: 0.2459501177072525, Validation Loss: 0.2443425953388214\n\nLearning Rate: 0.0009955597758197477\nEpoch 189, Training Loss: 0.2459501177072525, Validation Loss: 0.24423696100711823\nEpoch 189, Training Loss: 0.24583393335342407, Validation Loss: 0.24423696100711823\n\nLearning Rate: 0.0009955099978309566\nEpoch 190, Training Loss: 0.24583393335342407, Validation Loss: 0.24413232505321503\nEpoch 190, Training Loss: 0.24571874737739563, Validation Loss: 0.24413232505321503\n\nLearning Rate: 0.0009954602223310651\nEpoch 191, Training Loss: 0.24571874737739563, Validation Loss: 0.24402868747711182\nEpoch 191, Training Loss: 0.24560458958148956, Validation Loss: 0.24402868747711182\n\nLearning Rate: 0.0009954104493199486\nEpoch 192, Training Loss: 0.24560458958148956, Validation Loss: 0.24392607808113098\nEpoch 192, Training Loss: 0.24549132585525513, Validation Loss: 0.24392607808113098\n\nLearning Rate: 0.0009953606787974827\nEpoch 193, Training Loss: 0.24549132585525513, Validation Loss: 0.2438245266675949\nEpoch 193, Training Loss: 0.24537906050682068, Validation Loss: 0.2438245266675949\n\nLearning Rate: 0.0009953109107635428\nEpoch 194, Training Loss: 0.24537906050682068, Validation Loss: 0.24372395873069763\nEpoch 194, Training Loss: 0.24526771903038025, Validation Loss: 0.24372395873069763\n\nLearning Rate: 0.0009952611452180046\nEpoch 195, Training Loss: 0.24526771903038025, Validation Loss: 0.24362443387508392\nEpoch 195, Training Loss: 0.24515727162361145, Validation Loss: 0.24362443387508392\n\nLearning Rate: 0.0009952113821607438\nEpoch 196, Training Loss: 0.24515727162361145, Validation Loss: 0.24352587759494781\nEpoch 196, Training Loss: 0.24504779279232025, Validation Loss: 0.24352587759494781\n\nLearning Rate: 0.0009951616215916356\nEpoch 197, Training Loss: 0.24504779279232025, Validation Loss: 0.2434282749891281\nEpoch 197, Training Loss: 0.2449391931295395, Validation Loss: 0.2434282749891281\n\nLearning Rate: 0.000995111863510556\nEpoch 198, Training Loss: 0.2449391931295395, Validation Loss: 0.243331640958786\nEpoch 198, Training Loss: 0.24483150243759155, Validation Loss: 0.243331640958786\n\nLearning Rate: 0.0009950621079173807\nEpoch 199, Training Loss: 0.24483150243759155, Validation Loss: 0.24323591589927673\nEpoch 199, Training Loss: 0.24472463130950928, Validation Loss: 0.24323591589927673\n\nLearning Rate: 0.0009950123548119847\nEpoch 200, Training Loss: 0.24472463130950928, Validation Loss: 0.24314109981060028\nEpoch 200, Training Loss: 0.24461862444877625, Validation Loss: 0.24314109981060028\n\nLearning Rate: 0.0009949626041942442\nEpoch 201, Training Loss: 0.24461862444877625, Validation Loss: 0.24304714798927307\nEpoch 201, Training Loss: 0.24451351165771484, Validation Loss: 0.24304714798927307\n\nLearning Rate: 0.0009949128560640345\nEpoch 202, Training Loss: 0.24451351165771484, Validation Loss: 0.2429540753364563\nEpoch 202, Training Loss: 0.2444092184305191, Validation Loss: 0.2429540753364563\n\nLearning Rate: 0.0009948631104212313\nEpoch 203, Training Loss: 0.2444092184305191, Validation Loss: 0.2428617924451828\nEpoch 203, Training Loss: 0.24430570006370544, Validation Loss: 0.2428617924451828\n\nLearning Rate: 0.0009948133672657102\nEpoch 204, Training Loss: 0.24430570006370544, Validation Loss: 0.24277035892009735\nEpoch 204, Training Loss: 0.24420303106307983, Validation Loss: 0.24277035892009735\n\nLearning Rate: 0.0009947636265973468\nEpoch 205, Training Loss: 0.24420303106307983, Validation Loss: 0.24267971515655518\nEpoch 205, Training Loss: 0.24410118162631989, Validation Loss: 0.24267971515655518\n\nLearning Rate: 0.0009947138884160171\nEpoch 206, Training Loss: 0.24410118162631989, Validation Loss: 0.24258990585803986\nEpoch 206, Training Loss: 0.24400009214878082, Validation Loss: 0.24258990585803986\n\nLearning Rate: 0.0009946641527215964\nEpoch 207, Training Loss: 0.24400009214878082, Validation Loss: 0.242500901222229\nEpoch 207, Training Loss: 0.24389982223510742, Validation Loss: 0.242500901222229\n\nLearning Rate: 0.0009946144195139602\nEpoch 208, Training Loss: 0.24389982223510742, Validation Loss: 0.24241264164447784\nEpoch 208, Training Loss: 0.24380025267601013, Validation Loss: 0.24241264164447784\n\nLearning Rate: 0.0009945646887929845\nEpoch 209, Training Loss: 0.24380025267601013, Validation Loss: 0.24232521653175354\nEpoch 209, Training Loss: 0.24370147287845612, Validation Loss: 0.24232521653175354\n\nLearning Rate: 0.0009945149605585448\nEpoch 210, Training Loss: 0.24370147287845612, Validation Loss: 0.24223853647708893\nEpoch 210, Training Loss: 0.2436034232378006, Validation Loss: 0.24223853647708893\n\nLearning Rate: 0.0009944652348105169\nEpoch 211, Training Loss: 0.2436034232378006, Validation Loss: 0.24215266108512878\nEpoch 211, Training Loss: 0.24350613355636597, Validation Loss: 0.24215266108512878\n\nLearning Rate: 0.0009944155115487764\nEpoch 212, Training Loss: 0.24350613355636597, Validation Loss: 0.24206756055355072\nEpoch 212, Training Loss: 0.24340957403182983, Validation Loss: 0.24206756055355072\n\nLearning Rate: 0.000994365790773199\nEpoch 213, Training Loss: 0.24340957403182983, Validation Loss: 0.24198320508003235\nEpoch 213, Training Loss: 0.2433137446641922, Validation Loss: 0.24198320508003235\n\nLearning Rate: 0.0009943160724836602\nEpoch 214, Training Loss: 0.2433137446641922, Validation Loss: 0.24189960956573486\nEpoch 214, Training Loss: 0.2432185709476471, Validation Loss: 0.24189960956573486\n\nLearning Rate: 0.0009942663566800361\nEpoch 215, Training Loss: 0.2432185709476471, Validation Loss: 0.2418166995048523\nEpoch 215, Training Loss: 0.2431241273880005, Validation Loss: 0.2418166995048523\n\nLearning Rate: 0.000994216643362202\nEpoch 216, Training Loss: 0.2431241273880005, Validation Loss: 0.2417345643043518\nEpoch 216, Training Loss: 0.24303032457828522, Validation Loss: 0.2417345643043518\n\nLearning Rate: 0.000994166932530034\nEpoch 217, Training Loss: 0.24303032457828522, Validation Loss: 0.24165311455726624\nEpoch 217, Training Loss: 0.24293731153011322, Validation Loss: 0.24165311455726624\n\nLearning Rate: 0.0009941172241834076\nEpoch 218, Training Loss: 0.24293731153011322, Validation Loss: 0.24157236516475677\nEpoch 218, Training Loss: 0.2428448349237442, Validation Loss: 0.24157236516475677\n\nLearning Rate: 0.0009940675183221983\nEpoch 219, Training Loss: 0.2428448349237442, Validation Loss: 0.24149225652217865\nEpoch 219, Training Loss: 0.24275310337543488, Validation Loss: 0.24149225652217865\n\nLearning Rate: 0.0009940178149462824\nEpoch 220, Training Loss: 0.24275310337543488, Validation Loss: 0.24141284823417664\nEpoch 220, Training Loss: 0.2426619976758957, Validation Loss: 0.24141284823417664\n\nLearning Rate: 0.0009939681140555348\nEpoch 221, Training Loss: 0.2426619976758957, Validation Loss: 0.24133406579494476\nEpoch 221, Training Loss: 0.24257154762744904, Validation Loss: 0.24133406579494476\n\nLearning Rate: 0.0009939184156498321\nEpoch 222, Training Loss: 0.24257154762744904, Validation Loss: 0.241255983710289\nEpoch 222, Training Loss: 0.24248169362545013, Validation Loss: 0.241255983710289\n\nLearning Rate: 0.0009938687197290498\nEpoch 223, Training Loss: 0.24248169362545013, Validation Loss: 0.24117855727672577\nEpoch 223, Training Loss: 0.24239252507686615, Validation Loss: 0.24117855727672577\n\nLearning Rate: 0.0009938190262930632\nEpoch 224, Training Loss: 0.24239252507686615, Validation Loss: 0.2411017268896103\nEpoch 224, Training Loss: 0.24230393767356873, Validation Loss: 0.2411017268896103\n\nLearning Rate: 0.0009937693353417485\nEpoch 225, Training Loss: 0.24230393767356873, Validation Loss: 0.24102550745010376\nEpoch 225, Training Loss: 0.24221594631671906, Validation Loss: 0.24102550745010376\n\nLearning Rate: 0.0009937196468749814\nEpoch 226, Training Loss: 0.24221594631671906, Validation Loss: 0.24094998836517334\nEpoch 226, Training Loss: 0.24212856590747833, Validation Loss: 0.24094998836517334\n\nLearning Rate: 0.0009936699608926378\nEpoch 227, Training Loss: 0.24212856590747833, Validation Loss: 0.24087508022785187\nEpoch 227, Training Loss: 0.24204178154468536, Validation Loss: 0.24087508022785187\n\nLearning Rate: 0.0009936202773945932\nEpoch 228, Training Loss: 0.24204178154468536, Validation Loss: 0.24080079793930054\nEpoch 228, Training Loss: 0.24195557832717896, Validation Loss: 0.24080079793930054\n\nLearning Rate: 0.0009935705963807235\nEpoch 229, Training Loss: 0.24195557832717896, Validation Loss: 0.24072709679603577\nEpoch 229, Training Loss: 0.2418699562549591, Validation Loss: 0.24072709679603577\n\nLearning Rate: 0.0009935209178509043\nEpoch 230, Training Loss: 0.2418699562549591, Validation Loss: 0.24065403640270233\nEpoch 230, Training Loss: 0.24178491532802582, Validation Loss: 0.24065403640270233\n\nLearning Rate: 0.0009934712418050119\nEpoch 231, Training Loss: 0.24178491532802582, Validation Loss: 0.24058154225349426\nEpoch 231, Training Loss: 0.2417004257440567, Validation Loss: 0.24058154225349426\n\nLearning Rate: 0.0009934215682429217\nEpoch 232, Training Loss: 0.2417004257440567, Validation Loss: 0.24050964415073395\nEpoch 232, Training Loss: 0.24161654710769653, Validation Loss: 0.24050964415073395\n\nLearning Rate: 0.0009933718971645094\nEpoch 233, Training Loss: 0.24161654710769653, Validation Loss: 0.2404383271932602\nEpoch 233, Training Loss: 0.24153313040733337, Validation Loss: 0.2404383271932602\n\nLearning Rate: 0.0009933222285696513\nEpoch 234, Training Loss: 0.24153313040733337, Validation Loss: 0.24036753177642822\nEpoch 234, Training Loss: 0.24145032465457916, Validation Loss: 0.24036753177642822\n\nLearning Rate: 0.0009932725624582227\nEpoch 235, Training Loss: 0.24145032465457916, Validation Loss: 0.2402973175048828\nEpoch 235, Training Loss: 0.24136804044246674, Validation Loss: 0.2402973175048828\n\nLearning Rate: 0.0009932228988301\nEpoch 236, Training Loss: 0.24136804044246674, Validation Loss: 0.24022765457630157\nEpoch 236, Training Loss: 0.2412862479686737, Validation Loss: 0.24022765457630157\n\nLearning Rate: 0.0009931732376851584\nEpoch 237, Training Loss: 0.2412862479686737, Validation Loss: 0.2401585578918457\nEpoch 237, Training Loss: 0.24120499193668365, Validation Loss: 0.2401585578918457\n\nLearning Rate: 0.000993123579023274\nEpoch 238, Training Loss: 0.24120499193668365, Validation Loss: 0.24008995294570923\nEpoch 238, Training Loss: 0.24112428724765778, Validation Loss: 0.24008995294570923\n\nLearning Rate: 0.000993073922844323\nEpoch 239, Training Loss: 0.24112428724765778, Validation Loss: 0.24002189934253693\nEpoch 239, Training Loss: 0.2410440742969513, Validation Loss: 0.24002189934253693\n\nLearning Rate: 0.0009930242691481806\nEpoch 240, Training Loss: 0.2410440742969513, Validation Loss: 0.2399543672800064\nEpoch 240, Training Loss: 0.2409643679857254, Validation Loss: 0.2399543672800064\n\nLearning Rate: 0.0009929746179347233\nEpoch 241, Training Loss: 0.2409643679857254, Validation Loss: 0.23988734185695648\nEpoch 241, Training Loss: 0.2408851534128189, Validation Loss: 0.23988734185695648\n\nLearning Rate: 0.0009929249692038266\nEpoch 242, Training Loss: 0.2408851534128189, Validation Loss: 0.23982083797454834\nEpoch 242, Training Loss: 0.240806445479393, Validation Loss: 0.23982083797454834\n\nLearning Rate: 0.0009928753229553665\nEpoch 243, Training Loss: 0.240806445479393, Validation Loss: 0.2397548109292984\nEpoch 243, Training Loss: 0.2407282441854477, Validation Loss: 0.2397548109292984\n\nLearning Rate: 0.0009928256791892187\nEpoch 244, Training Loss: 0.2407282441854477, Validation Loss: 0.23968933522701263\nEpoch 244, Training Loss: 0.24065044522285461, Validation Loss: 0.23968933522701263\n\nLearning Rate: 0.0009927760379052591\nEpoch 245, Training Loss: 0.24065044522285461, Validation Loss: 0.23962433636188507\nEpoch 245, Training Loss: 0.24057316780090332, Validation Loss: 0.23962433636188507\n\nLearning Rate: 0.0009927263991033638\nEpoch 246, Training Loss: 0.24057316780090332, Validation Loss: 0.23955978453159332\nEpoch 246, Training Loss: 0.24049629271030426, Validation Loss: 0.23955978453159332\n\nLearning Rate: 0.0009926767627834088\nEpoch 247, Training Loss: 0.24049629271030426, Validation Loss: 0.23949573934078217\nEpoch 247, Training Loss: 0.24041999876499176, Validation Loss: 0.23949573934078217\n\nLearning Rate: 0.0009926271289452697\nEpoch 248, Training Loss: 0.24041999876499176, Validation Loss: 0.2394321858882904\nEpoch 248, Training Loss: 0.2403441071510315, Validation Loss: 0.2394321858882904\n\nLearning Rate: 0.0009925774975888223\nEpoch 249, Training Loss: 0.2403441071510315, Validation Loss: 0.23936910927295685\nEpoch 249, Training Loss: 0.24026866257190704, Validation Loss: 0.23936910927295685\n\nLearning Rate: 0.000992527868713943\nEpoch 250, Training Loss: 0.24026866257190704, Validation Loss: 0.2393064647912979\nEpoch 250, Training Loss: 0.2401936948299408, Validation Loss: 0.2393064647912979\n\nLearning Rate: 0.0009924782423205072\nEpoch 251, Training Loss: 0.2401936948299408, Validation Loss: 0.2392442673444748\nEpoch 251, Training Loss: 0.2401191145181656, Validation Loss: 0.2392442673444748\n\nLearning Rate: 0.0009924286184083912\nEpoch 252, Training Loss: 0.2401191145181656, Validation Loss: 0.23918259143829346\nEpoch 252, Training Loss: 0.24004502594470978, Validation Loss: 0.23918259143829346\n\nLearning Rate: 0.0009923789969774708\nEpoch 253, Training Loss: 0.24004502594470978, Validation Loss: 0.23912127315998077\nEpoch 253, Training Loss: 0.2399713546037674, Validation Loss: 0.23912127315998077\n\nLearning Rate: 0.000992329378027622\nEpoch 254, Training Loss: 0.2399713546037674, Validation Loss: 0.2390604466199875\nEpoch 254, Training Loss: 0.23989805579185486, Validation Loss: 0.2390604466199875\n\nLearning Rate: 0.0009922797615587206\nEpoch 255, Training Loss: 0.23989805579185486, Validation Loss: 0.2390000820159912\nEpoch 255, Training Loss: 0.2398252636194229, Validation Loss: 0.2390000820159912\n\nLearning Rate: 0.0009922301475706425\nEpoch 256, Training Loss: 0.2398252636194229, Validation Loss: 0.2389400750398636\nEpoch 256, Training Loss: 0.2397528439760208, Validation Loss: 0.2389400750398636\n\nLearning Rate: 0.0009921805360632641\nEpoch 257, Training Loss: 0.2397528439760208, Validation Loss: 0.23888052999973297\nEpoch 257, Training Loss: 0.23968079686164856, Validation Loss: 0.23888052999973297\n\nLearning Rate: 0.000992130927036461\nEpoch 258, Training Loss: 0.23968079686164856, Validation Loss: 0.23882141709327698\nEpoch 258, Training Loss: 0.23960919678211212, Validation Loss: 0.23882141709327698\n\nLearning Rate: 0.0009920813204901092\nEpoch 259, Training Loss: 0.23960919678211212, Validation Loss: 0.238762766122818\nEpoch 259, Training Loss: 0.2395380139350891, Validation Loss: 0.238762766122818\n\nLearning Rate: 0.0009920317164240845\nEpoch 260, Training Loss: 0.2395380139350891, Validation Loss: 0.23870447278022766\nEpoch 260, Training Loss: 0.23946717381477356, Validation Loss: 0.23870447278022766\n\nLearning Rate: 0.0009919821148382634\nEpoch 261, Training Loss: 0.23946717381477356, Validation Loss: 0.23864664137363434\nEpoch 261, Training Loss: 0.23939679563045502, Validation Loss: 0.23864664137363434\n\nLearning Rate: 0.0009919325157325214\nEpoch 262, Training Loss: 0.23939679563045502, Validation Loss: 0.23858916759490967\nEpoch 262, Training Loss: 0.23932674527168274, Validation Loss: 0.23858916759490967\n\nLearning Rate: 0.0009918829191067349\nEpoch 263, Training Loss: 0.23932674527168274, Validation Loss: 0.23853208124637604\nEpoch 263, Training Loss: 0.2392571121454239, Validation Loss: 0.23853208124637604\n\nLearning Rate: 0.0009918333249607795\nEpoch 264, Training Loss: 0.2392571121454239, Validation Loss: 0.23847542703151703\nEpoch 264, Training Loss: 0.23918788135051727, Validation Loss: 0.23847542703151703\n\nLearning Rate: 0.0009917837332945316\nEpoch 265, Training Loss: 0.23918788135051727, Validation Loss: 0.23841914534568787\nEpoch 265, Training Loss: 0.2391190230846405, Validation Loss: 0.23841914534568787\n\nLearning Rate: 0.0009917341441078667\nEpoch 266, Training Loss: 0.2391190230846405, Validation Loss: 0.23836326599121094\nEpoch 266, Training Loss: 0.2390505075454712, Validation Loss: 0.23836326599121094\n\nLearning Rate: 0.0009916845574006615\nEpoch 267, Training Loss: 0.2390505075454712, Validation Loss: 0.23830777406692505\nEpoch 267, Training Loss: 0.23898237943649292, Validation Loss: 0.23830777406692505\n\nLearning Rate: 0.0009916349731727914\nEpoch 268, Training Loss: 0.23898237943649292, Validation Loss: 0.23825260996818542\nEpoch 268, Training Loss: 0.2389145791530609, Validation Loss: 0.23825260996818542\n\nLearning Rate: 0.0009915853914241328\nEpoch 269, Training Loss: 0.2389145791530609, Validation Loss: 0.23819786310195923\nEpoch 269, Training Loss: 0.23884716629981995, Validation Loss: 0.23819786310195923\n\nLearning Rate: 0.0009915358121545615\nEpoch 270, Training Loss: 0.23884716629981995, Validation Loss: 0.2381434589624405\nEpoch 270, Training Loss: 0.23878009617328644, Validation Loss: 0.2381434589624405\n\nLearning Rate: 0.0009914862353639538\nEpoch 271, Training Loss: 0.23878009617328644, Validation Loss: 0.238089457154274\nEpoch 271, Training Loss: 0.23871338367462158, Validation Loss: 0.238089457154274\n\nLearning Rate: 0.0009914366610521857\nEpoch 272, Training Loss: 0.23871338367462158, Validation Loss: 0.23803576827049255\nEpoch 272, Training Loss: 0.23864704370498657, Validation Loss: 0.23803576827049255\n\nLearning Rate: 0.000991387089219133\nEpoch 273, Training Loss: 0.23864704370498657, Validation Loss: 0.23798249661922455\nEpoch 273, Training Loss: 0.23858101665973663, Validation Loss: 0.23798249661922455\n\nLearning Rate: 0.000991337519864672\nEpoch 274, Training Loss: 0.23858101665973663, Validation Loss: 0.23792952299118042\nEpoch 274, Training Loss: 0.23851534724235535, Validation Loss: 0.23792952299118042\n\nLearning Rate: 0.0009912879529886788\nEpoch 275, Training Loss: 0.23851534724235535, Validation Loss: 0.23787695169448853\nEpoch 275, Training Loss: 0.23844999074935913, Validation Loss: 0.23787695169448853\n\nLearning Rate: 0.0009912383885910293\nEpoch 276, Training Loss: 0.23844999074935913, Validation Loss: 0.2378246784210205\nEpoch 276, Training Loss: 0.23838500678539276, Validation Loss: 0.2378246784210205\n\nLearning Rate: 0.0009911888266715997\nEpoch 277, Training Loss: 0.23838500678539276, Validation Loss: 0.23777276277542114\nEpoch 277, Training Loss: 0.23832032084465027, Validation Loss: 0.23777276277542114\n\nLearning Rate: 0.0009911392672302662\nEpoch 278, Training Loss: 0.23832032084465027, Validation Loss: 0.23772118985652924\nEpoch 278, Training Loss: 0.23825596272945404, Validation Loss: 0.23772118985652924\n\nLearning Rate: 0.0009910897102669047\nEpoch 279, Training Loss: 0.23825596272945404, Validation Loss: 0.23766997456550598\nEpoch 279, Training Loss: 0.23819194734096527, Validation Loss: 0.23766997456550598\n\nLearning Rate: 0.0009910401557813914\nEpoch 280, Training Loss: 0.23819194734096527, Validation Loss: 0.2376190423965454\nEpoch 280, Training Loss: 0.23812824487686157, Validation Loss: 0.2376190423965454\n\nLearning Rate: 0.0009909906037736025\nEpoch 281, Training Loss: 0.23812824487686157, Validation Loss: 0.2375684678554535\nEpoch 281, Training Loss: 0.23806482553482056, Validation Loss: 0.2375684678554535\n\nLearning Rate: 0.0009909410542434138\nEpoch 282, Training Loss: 0.23806482553482056, Validation Loss: 0.23751823604106903\nEpoch 282, Training Loss: 0.238001748919487, Validation Loss: 0.23751823604106903\n\nLearning Rate: 0.0009908915071907016\nEpoch 283, Training Loss: 0.238001748919487, Validation Loss: 0.23746827244758606\nEpoch 283, Training Loss: 0.23793895542621613, Validation Loss: 0.23746827244758606\n\nLearning Rate: 0.000990841962615342\nEpoch 284, Training Loss: 0.23793895542621613, Validation Loss: 0.23741866648197174\nEpoch 284, Training Loss: 0.23787648975849152, Validation Loss: 0.23741866648197174\n\nLearning Rate: 0.0009907924205172112\nEpoch 285, Training Loss: 0.23787648975849152, Validation Loss: 0.2373693734407425\nEpoch 285, Training Loss: 0.23781435191631317, Validation Loss: 0.2373693734407425\n\nLearning Rate: 0.0009907428808961854\nEpoch 286, Training Loss: 0.23781435191631317, Validation Loss: 0.23732036352157593\nEpoch 286, Training Loss: 0.2377524971961975, Validation Loss: 0.23732036352157593\n\nLearning Rate: 0.0009906933437521406\nEpoch 287, Training Loss: 0.2377524971961975, Validation Loss: 0.23727166652679443\nEpoch 287, Training Loss: 0.23769092559814453, Validation Loss: 0.23727166652679443\n\nLearning Rate: 0.000990643809084953\nEpoch 288, Training Loss: 0.23769092559814453, Validation Loss: 0.2372233122587204\nEpoch 288, Training Loss: 0.23762966692447662, Validation Loss: 0.2372233122587204\n\nLearning Rate: 0.0009905942768944987\nEpoch 289, Training Loss: 0.23762966692447662, Validation Loss: 0.23717524111270905\nEpoch 289, Training Loss: 0.2375687062740326, Validation Loss: 0.23717524111270905\n\nLearning Rate: 0.000990544747180654\nEpoch 290, Training Loss: 0.2375687062740326, Validation Loss: 0.237127423286438\nEpoch 290, Training Loss: 0.23750796914100647, Validation Loss: 0.237127423286438\n\nLearning Rate: 0.000990495219943295\nEpoch 291, Training Loss: 0.23750796914100647, Validation Loss: 0.2370799481868744\nEpoch 291, Training Loss: 0.2374476045370102, Validation Loss: 0.2370799481868744\n\nLearning Rate: 0.000990445695182298\nEpoch 292, Training Loss: 0.2374476045370102, Validation Loss: 0.23703277111053467\nEpoch 292, Training Loss: 0.2373874932527542, Validation Loss: 0.23703277111053467\n\nLearning Rate: 0.0009903961728975387\nEpoch 293, Training Loss: 0.2373874932527542, Validation Loss: 0.23698586225509644\nEpoch 293, Training Loss: 0.23732765018939972, Validation Loss: 0.23698586225509644\n\nLearning Rate: 0.0009903466530888938\nEpoch 294, Training Loss: 0.23732765018939972, Validation Loss: 0.2369392365217209\nEpoch 294, Training Loss: 0.23726807534694672, Validation Loss: 0.2369392365217209\n\nLearning Rate: 0.0009902971357562394\nEpoch 295, Training Loss: 0.23726807534694672, Validation Loss: 0.2368929088115692\nEpoch 295, Training Loss: 0.23720873892307281, Validation Loss: 0.2368929088115692\n\nLearning Rate: 0.0009902476208994517\nEpoch 296, Training Loss: 0.23720873892307281, Validation Loss: 0.23684683442115784\nEpoch 296, Training Loss: 0.23714976012706757, Validation Loss: 0.23684683442115784\n\nLearning Rate: 0.0009901981085184066\nEpoch 297, Training Loss: 0.23714976012706757, Validation Loss: 0.23680107295513153\nEpoch 297, Training Loss: 0.23709096014499664, Validation Loss: 0.23680107295513153\n\nLearning Rate: 0.0009901485986129807\nEpoch 298, Training Loss: 0.23709096014499664, Validation Loss: 0.2367555797100067\nEpoch 298, Training Loss: 0.23703248798847198, Validation Loss: 0.2367555797100067\n\nLearning Rate: 0.0009900990911830502\nEpoch 299, Training Loss: 0.23703248798847198, Validation Loss: 0.236710324883461\nEpoch 299, Training Loss: 0.23697428405284882, Validation Loss: 0.236710324883461\n\nLearning Rate: 0.0009900495862284909\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","Cell \u001b[0;32mIn[50], line 18\u001b[0m\n\u001b[1;32m     15\u001b[0m scheduler \u001b[38;5;241m=\u001b[39m LambdaLR(optimizer, lr_lambda\u001b[38;5;241m=\u001b[39mcustom_lr_lambda)\n\u001b[1;32m     17\u001b[0m criterion \u001b[38;5;241m=\u001b[39m CustomLoss(nn\u001b[38;5;241m.\u001b[39mCrossEntropyLoss(), \u001b[38;5;241m0.0\u001b[39m, \u001b[38;5;241m0.0\u001b[39m, \u001b[38;5;241m0.0\u001b[39m, \u001b[38;5;241m0.0\u001b[39m, \u001b[38;5;241m0.0\u001b[39m)\n\u001b[0;32m---> 18\u001b[0m \u001b[43mevaluate_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcustom_train_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m10000\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscheduler\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_features\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mearly_stopping_patience\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10000\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     20\u001b[0m models\u001b[38;5;241m.\u001b[39mappend(model)\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExecution time: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m(time\u001b[38;5;241m.\u001b[39mtime()\u001b[38;5;250m \u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;250m \u001b[39mstart_time)\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.6f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m seconds\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n","Cell \u001b[0;32mIn[10], line 44\u001b[0m, in \u001b[0;36mevaluate_model\u001b[0;34m(model, custom_train_loader, criterion, optimizer, num_epochs, scheduler, batch_size, num_features, early_stopping_patience)\u001b[0m\n\u001b[1;32m     41\u001b[0m     inputs \u001b[38;5;241m=\u001b[39m custom_train_loader\u001b[38;5;241m.\u001b[39mtrain_data_tensor[start_idx:end_idx]\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, num_features)\n\u001b[1;32m     42\u001b[0m     labels \u001b[38;5;241m=\u001b[39m custom_train_loader\u001b[38;5;241m.\u001b[39mtrain_labels_tensor[start_idx:end_idx]\n\u001b[0;32m---> 44\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     45\u001b[0m     train_reg_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m criterion\u001b[38;5;241m.\u001b[39mregular_loss(outputs, labels)\u001b[38;5;241m.\u001b[39mitem() \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mlen\u001b[39m(labels)\n\u001b[1;32m     47\u001b[0m val_iter \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n","Cell \u001b[0;32mIn[43], line 25\u001b[0m, in \u001b[0;36mTabularDenseNet.forward\u001b[0;34m(self, x, val_iter)\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x, val_iter\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m     24\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstored_x \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m val_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m---> 25\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfinal\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstored_x\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     26\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstored_x_val \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m val_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     27\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfinal(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstored_x_val)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n","Cell \u001b[0;32mIn[17], line 22\u001b[0m, in \u001b[0;36mCustomLinear.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[0;32m---> 22\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/linear.py:117\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    116\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 117\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "],"ename":"KeyboardInterrupt","evalue":"","output_type":"error"}],"execution_count":50},{"cell_type":"code","source":"torch.set_printoptions(threshold=float('inf'), linewidth=200)\n\n\nfor name, param in model.named_parameters():\n    print(name)\n    print(param)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-14T00:03:36.151623Z","iopub.execute_input":"2024-12-14T00:03:36.152121Z"}},"outputs":[{"name":"stdout","text":"layer_1.linear.weight\nParameter containing:\ntensor([[-1.7794e-04, -4.9612e-02, -1.1179e-01,  1.3254e-01,  8.0957e-02,  1.6317e-01, -9.8446e-02,  2.6680e-01,  8.2090e-02,  1.5345e-01, -2.6573e-01,  1.5865e-01, -1.4340e-01, -2.1754e-01],\n        [ 9.0345e-02,  5.4603e-02,  2.3221e-01,  3.5820e-04, -2.0248e-01,  1.3884e-01,  1.8395e-01, -1.1233e-02,  2.0405e-02, -1.7900e-01,  1.9903e-01,  1.4463e-01,  1.3513e-01,  9.3167e-02],\n        [-9.0080e-02,  2.6050e-01,  9.0216e-02, -8.2031e-02,  2.6675e-02, -1.3426e-01,  2.4115e-01,  8.7179e-02,  2.2191e-02,  5.0701e-02, -2.5366e-02,  1.7373e-02, -4.4720e-02, -5.2645e-03],\n        [-6.8847e-02,  2.5165e-01,  2.0753e-01, -1.5881e-01, -1.1557e-01, -1.2935e-01, -2.4445e-01,  3.8486e-02, -2.5448e-01,  1.0821e-01,  3.3905e-02,  2.6198e-01, -1.0196e-01, -1.9220e-01],\n        [-6.1130e-02, -2.4087e-01, -1.6049e-01,  1.2265e-01,  1.3486e-01, -2.3504e-01,  2.1390e-01, -2.4547e-01, -2.2743e-01,  2.5949e-02, -1.6097e-01, -2.1915e-01, -1.1450e-01, -2.5113e-01],\n        [ 1.5213e-01,  1.7273e-01,  2.1285e-01,  1.0259e-01,  2.1743e-01,  6.5236e-02,  2.5727e-01, -9.2829e-02,  7.3302e-02, -2.3745e-02, -2.2071e-01,  9.5411e-02, -8.5706e-02,  1.4637e-01],\n        [ 1.4494e-01,  2.6285e-01, -1.8282e-01,  2.0297e-01, -2.4981e-01,  1.6722e-01,  1.2366e-01, -2.0725e-01,  7.5791e-02, -7.1899e-02,  1.7788e-02,  5.5097e-02,  3.0370e-02,  2.6097e-01],\n        [-1.3570e-01, -3.2204e-02, -2.7568e-02,  1.9464e-01, -2.2215e-01,  7.9243e-02, -1.3195e-01, -2.5526e-01, -4.3206e-02, -2.6722e-01,  2.0885e-01,  1.2351e-02,  9.5198e-02,  1.3234e-01],\n        [ 1.9372e-01,  1.4688e-01,  2.4780e-01,  2.6131e-01,  1.1152e-01,  1.6533e-01,  5.2628e-03,  1.6908e-01,  2.5204e-01,  8.6568e-02,  2.0614e-01,  1.1209e-01,  1.3052e-01, -2.1727e-01],\n        [-9.1002e-02,  2.0496e-02, -2.4437e-01,  4.9930e-03,  8.5018e-02, -2.6590e-01,  5.5472e-02,  2.6628e-01,  7.2115e-03, -1.3555e-01, -1.7946e-01, -1.6304e-01,  3.0281e-02, -1.0186e-01],\n        [-1.1517e-01, -2.0135e-01, -1.4600e-01,  1.4310e-01,  1.3841e-01,  1.5946e-01,  1.3087e-02, -2.5046e-01, -2.2116e-01, -1.9892e-01, -2.0549e-01, -6.8567e-03,  2.4838e-01, -1.5646e-01],\n        [ 1.3378e-01, -1.6334e-01, -8.8601e-02, -9.9864e-02, -1.5084e-01,  6.5460e-02, -1.1125e-01, -2.7043e-02, -7.0426e-02, -1.4016e-01,  1.6272e-02, -2.2950e-01, -4.1829e-02, -1.8100e-01],\n        [-1.5980e-01, -2.6588e-01, -1.5960e-01,  1.1754e-01, -2.5721e-01,  1.8311e-01,  8.2818e-02, -2.3417e-01,  4.5279e-02, -2.0369e-01,  1.9405e-02,  1.7181e-02, -1.2711e-01,  2.1798e-01],\n        [ 1.4164e-01, -1.4821e-01, -1.6444e-02, -6.8874e-02, -1.5609e-01,  1.3828e-01, -2.3779e-01,  2.1171e-01, -1.9499e-01, -8.6767e-03,  1.1695e-01, -4.7747e-02,  3.8192e-02,  1.6435e-01],\n        [ 1.1900e-01, -9.3737e-02,  5.7011e-02, -2.0376e-01,  1.0868e-01,  1.4517e-01,  3.7261e-02,  2.4019e-01,  1.5714e-01, -6.9875e-02,  1.3752e-01,  1.7778e-01, -1.9012e-01,  2.1745e-01],\n        [ 2.3422e-01,  3.7430e-02, -1.6248e-01,  3.5251e-02, -1.2989e-01,  2.3888e-01,  1.7295e-01,  1.1380e-01,  8.8558e-02, -2.4712e-01, -2.3628e-01, -5.7013e-02, -5.5303e-02,  1.1956e-01],\n        [ 6.0591e-02, -1.3598e-01,  4.2221e-02,  3.8585e-02, -1.5528e-01, -1.6415e-01,  1.5515e-01, -2.6335e-01,  4.6593e-02, -2.2710e-01, -1.0843e-01, -9.1649e-02, -2.2259e-01,  1.2209e-01],\n        [-1.9507e-01, -2.4341e-01, -4.7263e-02,  5.8652e-03, -9.0645e-02,  1.6118e-01, -2.5711e-01, -1.6084e-01, -1.3773e-01, -9.6201e-02,  1.2411e-01, -5.9798e-03, -2.1437e-01,  1.8818e-01],\n        [ 8.6014e-02, -2.4492e-02,  3.1855e-02, -1.7306e-01, -1.9992e-02,  1.3344e-01, -7.6820e-02, -1.9128e-02,  9.3041e-02,  1.2076e-01,  1.3267e-01,  2.1774e-01, -2.6595e-01,  3.6487e-02],\n        [-5.1764e-02,  5.9481e-02, -4.7864e-02,  4.8972e-02, -1.7872e-01,  2.0789e-01, -2.4636e-01,  2.3613e-01,  1.4668e-01,  1.2950e-01, -1.1331e-01,  2.0465e-01, -1.4505e-01,  1.8223e-01],\n        [-1.9737e-01, -1.6873e-03,  2.0275e-01, -1.9853e-01,  1.2516e-01,  2.0028e-02,  2.2709e-01,  4.1749e-02, -4.0749e-03,  2.0806e-01,  8.9510e-02,  2.5728e-01,  1.9003e-01, -2.3311e-01],\n        [ 2.2092e-01,  1.5152e-01, -4.6472e-02,  2.6147e-01, -1.6276e-01,  1.9955e-01,  2.2975e-01, -1.9006e-01,  1.2663e-01, -6.7950e-02, -1.1714e-01, -2.3040e-01, -1.5482e-01, -7.7331e-02],\n        [ 2.4842e-01,  8.5429e-02,  2.0287e-01,  1.0264e-01, -9.5322e-02,  1.8231e-01, -8.3594e-02,  2.6645e-01, -5.5581e-02, -5.7896e-02,  1.3684e-01,  1.4350e-01,  1.5253e-01, -2.3714e-01],\n        [-2.3077e-01,  2.4018e-01, -1.9016e-01,  6.2820e-02, -4.2439e-02, -5.2214e-02,  1.2176e-01,  1.0170e-01,  4.2001e-02,  5.3272e-02, -1.1767e-01,  1.7239e-02, -2.0659e-01, -7.3845e-03],\n        [ 1.8163e-01, -2.3841e-02, -7.5730e-02, -1.7212e-02,  9.2210e-02,  2.2427e-02, -2.6223e-01,  2.1775e-01, -1.3555e-01, -8.8863e-02,  1.8619e-01,  1.3399e-01, -1.8940e-01,  5.2724e-02],\n        [ 1.2296e-01,  1.8979e-01,  2.3106e-01, -2.3857e-02,  2.4448e-01,  7.8063e-02,  1.4642e-01, -1.0345e-04,  1.3211e-02, -1.2767e-01,  4.4182e-02, -2.2091e-01, -2.0113e-03,  1.9705e-01],\n        [-2.0069e-01,  1.3147e-02,  8.9108e-02,  1.2890e-01, -1.0639e-01,  1.5515e-01,  9.3160e-02,  1.2796e-01,  1.6358e-03, -1.3871e-01,  7.2584e-02, -2.1145e-01,  2.6718e-01,  2.5319e-01],\n        [-5.6815e-02, -9.0156e-02,  2.6073e-01,  6.3107e-02, -5.4110e-02,  2.9563e-03, -8.7938e-02,  1.0264e-01, -1.7956e-02, -1.2355e-02,  1.5372e-01, -3.1631e-02,  2.3050e-01, -1.7785e-01],\n        [ 8.5554e-02, -1.8518e-01,  2.4964e-01,  2.6589e-01, -2.2305e-01, -5.5395e-02, -2.1480e-01, -1.8866e-01,  7.8592e-02, -1.0286e-01,  1.7089e-01, -1.0287e-01,  2.1897e-01, -1.4561e-01],\n        [ 2.6422e-01, -1.2249e-01,  1.4469e-01, -8.6253e-02,  1.8166e-01,  1.9422e-01,  2.0644e-01,  3.5220e-02, -1.4337e-01,  9.1409e-02,  2.1879e-01, -2.0734e-01, -7.4923e-02,  5.4427e-02],\n        [-2.2480e-01, -2.1319e-01,  1.3822e-02, -1.2706e-01, -1.9162e-01, -1.5051e-01, -2.5387e-01,  2.6089e-01,  1.6184e-01,  2.0202e-01,  2.3217e-01,  1.2251e-01, -6.5952e-03,  1.4748e-01],\n        [ 2.1179e-01,  7.1911e-02, -1.8837e-01,  2.3735e-02,  1.7401e-01, -1.1455e-01, -1.3190e-01,  9.1042e-02,  1.0690e-01, -1.5573e-01, -9.3350e-02,  6.0107e-02,  2.1147e-01,  1.6330e-01],\n        [ 1.0127e-01,  2.2763e-01, -1.7806e-01, -2.5349e-01,  1.5365e-01, -1.5700e-01,  3.5461e-02, -8.9732e-02, -2.8807e-02,  2.3213e-01,  2.8288e-02,  1.8000e-01,  2.4164e-01,  1.8377e-02],\n        [-1.6563e-01,  2.5808e-01, -6.9702e-02, -9.6984e-03,  1.9485e-01,  2.4533e-01, -1.4061e-03, -1.6938e-01, -6.5509e-02, -9.7068e-02, -1.2071e-01, -1.1495e-01, -1.1713e-01,  2.0223e-01],\n        [ 4.7241e-02, -9.6841e-02, -2.2182e-01,  2.1473e-01, -1.5026e-01, -8.2975e-02, -8.2873e-02,  2.9569e-02,  3.7788e-02,  6.5127e-02,  7.4823e-02, -2.4791e-01, -5.8277e-02, -9.7328e-02],\n        [-1.6849e-01, -1.1246e-01, -9.6059e-02,  2.1603e-01,  5.0270e-02,  4.6448e-02,  1.5834e-01, -3.2276e-02,  7.3123e-03,  8.6200e-02, -1.7946e-01,  9.0161e-03,  1.1162e-01, -1.9996e-01],\n        [-6.7285e-02,  9.2261e-02,  2.3170e-01, -2.0721e-01, -1.8446e-01, -2.0669e-01, -1.8848e-01, -4.3511e-02, -2.3574e-01,  5.4140e-02, -7.1463e-02,  1.2095e-01, -5.2690e-02, -7.1507e-02],\n        [ 2.5192e-01,  8.8462e-02, -5.8204e-02, -2.4731e-01, -4.3096e-02,  1.5176e-01,  1.3402e-01,  2.1285e-01, -9.5000e-02, -2.3773e-01, -2.4317e-01, -3.9815e-04, -1.6728e-01,  1.2259e-01],\n        [-1.2379e-01,  8.0594e-02, -5.9206e-03,  4.1846e-02, -1.6954e-01,  1.6569e-02,  6.3537e-02,  2.2292e-01,  3.4649e-02,  2.2444e-01, -1.5205e-01, -1.4319e-01, -9.2350e-03, -2.0246e-01],\n        [-2.0629e-01,  2.4377e-01,  2.4718e-01, -5.7282e-02,  1.5257e-01, -2.0009e-02, -1.5417e-02,  2.0191e-01,  2.1235e-01,  1.2716e-01, -2.1833e-02, -1.0515e-01, -2.6045e-01,  2.4470e-01],\n        [ 1.2329e-01,  2.2405e-03,  1.7581e-01,  2.5632e-01, -9.4742e-02, -1.9146e-01,  1.4283e-01, -2.6554e-01,  6.0452e-02,  9.3317e-02,  3.6116e-02,  1.0916e-01, -4.0371e-02,  1.5852e-01],\n        [-1.1528e-01, -1.8449e-01, -1.7605e-01,  1.0346e-01,  2.5407e-01, -2.4198e-01,  1.2433e-01,  1.2759e-01,  1.5210e-01,  2.9054e-02,  2.6152e-01, -4.5449e-02, -2.6706e-01,  8.4134e-03],\n        [ 1.2547e-01, -1.7889e-02,  2.3347e-01, -1.3177e-01,  1.3182e-01,  2.6318e-01,  9.1597e-02, -2.0656e-01, -2.1107e-01, -2.0485e-01, -9.8731e-02, -3.0667e-03, -4.6091e-02,  5.7164e-02],\n        [-1.5120e-01,  2.3674e-01,  5.6032e-02, -2.8975e-02, -8.7836e-02, -2.1390e-01, -1.9479e-01, -8.2008e-02,  2.1731e-01,  2.0110e-01,  1.3000e-01,  1.4834e-01,  2.1072e-01,  1.5221e-01],\n        [-1.5281e-01, -8.5102e-02,  1.9792e-01,  1.3006e-01, -6.0478e-02,  1.6233e-01, -2.4879e-01,  9.7443e-02,  1.2040e-01,  2.0515e-01, -1.9014e-01,  1.9728e-01,  9.7406e-02,  4.8787e-02],\n        [-2.6577e-01,  2.8247e-02,  4.4055e-03, -1.5818e-01,  2.0998e-01,  2.6674e-01, -9.0425e-02,  2.6198e-01, -1.4796e-01, -2.3957e-01, -1.8912e-01,  1.9061e-01, -1.0116e-01, -2.5978e-01],\n        [ 2.1125e-01,  4.9468e-02,  1.4955e-01,  1.6037e-01,  2.4941e-01, -9.1886e-03, -2.4713e-01, -1.2685e-01,  1.9135e-01, -1.8768e-01,  5.7444e-02,  6.8876e-02, -5.0633e-02,  1.2441e-01],\n        [-1.0825e-01,  7.3797e-04, -1.9675e-01, -8.6837e-02,  2.5229e-01,  1.0331e-01,  7.2540e-02, -1.9642e-01, -6.1316e-02,  1.8420e-01,  1.8626e-01, -1.8514e-01,  2.4290e-02,  2.2392e-01],\n        [-1.4132e-01, -1.0837e-02, -1.8366e-01,  2.1762e-01,  9.5530e-02,  2.2949e-01,  1.8614e-01, -2.3758e-01, -1.8859e-01, -7.9825e-02, -9.6684e-02,  1.1407e-01, -1.2253e-01, -1.4988e-01],\n        [-5.3899e-02,  3.5367e-02, -1.7158e-01, -2.6446e-01, -2.2499e-01,  7.0274e-02,  1.2423e-01, -2.5703e-01, -4.3858e-02,  4.1395e-02, -2.2028e-01,  1.8510e-03,  4.6042e-02, -1.7002e-01],\n        [-2.5221e-01, -1.8072e-01, -7.6362e-02, -5.3905e-03,  7.3870e-02,  8.1852e-02,  2.5212e-01, -5.2116e-02,  1.1826e-01,  1.2833e-01,  1.9080e-01,  2.0556e-01,  2.8693e-02, -4.1534e-02],\n        [-2.5543e-01,  5.0751e-02,  1.3959e-01, -2.3125e-01,  1.6479e-01, -2.1107e-01,  2.6433e-01, -4.4802e-02,  3.0393e-02, -1.8822e-01, -1.0532e-01, -3.9124e-02,  6.0363e-02,  1.0914e-01],\n        [ 4.0399e-02, -2.2804e-01, -1.5507e-01, -6.3733e-02, -1.3957e-02, -2.1957e-01,  9.7687e-02,  7.1033e-02,  1.1880e-01, -1.5542e-01, -2.2804e-03, -5.5977e-03,  1.1784e-01, -1.1953e-01],\n        [-2.0365e-01,  2.2808e-01,  2.2141e-01, -1.5387e-01,  1.7138e-01,  6.6955e-02, -6.4397e-02,  2.1434e-01, -1.6878e-01, -2.3314e-01,  1.8825e-01, -1.9270e-01, -2.3884e-01, -1.1480e-01],\n        [-1.5244e-01,  1.6850e-01,  2.0067e-01,  1.2965e-01,  2.2931e-01, -2.2102e-01, -2.1549e-01,  1.9180e-02, -2.0392e-01,  2.0205e-01, -2.2705e-01,  1.1516e-01,  2.3789e-01,  4.6369e-02],\n        [-2.2073e-01, -2.2671e-01,  1.1170e-01, -1.5035e-01,  2.4196e-01,  2.0785e-01,  4.0087e-02,  8.5027e-02, -1.6454e-01,  2.4285e-01,  1.8690e-01, -8.1161e-02, -4.8741e-02, -1.3957e-01],\n        [ 1.4789e-01,  2.2583e-01,  2.1790e-01, -1.1565e-01, -1.0384e-02,  1.1536e-02,  2.2926e-02, -1.4516e-01, -4.2234e-02,  3.4939e-02, -7.2584e-02, -1.9489e-01, -2.6429e-01, -2.2889e-01],\n        [ 7.7422e-02, -1.1037e-01,  1.6410e-01,  7.6738e-02, -1.3532e-01, -1.2820e-01,  1.6523e-01,  1.3539e-01,  5.4193e-02,  2.1027e-01, -1.3079e-01,  1.5300e-01,  1.8044e-01, -1.7514e-01],\n        [-1.6253e-01, -1.0459e-01,  1.6960e-02, -1.0324e-01,  1.3059e-01, -2.5782e-01, -1.6456e-01, -2.3481e-01, -2.1834e-01,  1.7133e-01,  1.6973e-01,  2.0217e-01, -2.3580e-01,  2.5222e-01],\n        [ 1.3597e-01, -1.3563e-01,  5.4603e-02, -2.5813e-01,  2.4049e-01,  2.3582e-01, -1.1993e-01, -3.1827e-02,  1.1558e-01,  1.4538e-01, -1.0167e-03,  1.5226e-02,  1.7457e-01,  1.0381e-01],\n        [ 7.3989e-02, -6.7518e-02,  1.3165e-01, -1.2723e-01, -9.7818e-02, -2.4331e-01, -1.2386e-01,  1.6286e-01,  1.4744e-01, -1.5412e-01, -8.6550e-02,  7.2739e-03,  1.2139e-01,  1.8602e-01],\n        [-2.0200e-01,  2.2005e-01,  1.8998e-01, -1.1205e-01, -1.9551e-01, -2.5103e-01, -1.8948e-02,  1.8820e-01, -2.2567e-01,  9.2672e-02, -1.9762e-01, -8.3234e-02, -1.6455e-01,  1.0147e-01],\n        [ 1.2880e-01,  2.3836e-01, -1.5322e-01, -5.1793e-02,  7.3704e-02,  2.5408e-01,  1.9725e-01,  6.2209e-02,  2.0731e-01, -7.2186e-03,  1.4229e-01, -1.0129e-01,  8.8344e-02,  9.6173e-02],\n        [ 9.5266e-02,  2.3176e-01,  1.7139e-01, -2.3381e-01, -1.1561e-01,  1.8504e-02,  2.3834e-01,  1.8025e-01,  2.4142e-01, -9.4714e-02, -1.6510e-01, -2.5334e-01,  1.6810e-01,  5.2600e-02],\n        [-1.8332e-02,  1.4513e-02,  1.1438e-01,  2.5127e-01, -1.5954e-01,  2.4119e-01, -9.8762e-02,  9.1040e-02,  9.9155e-02, -7.3672e-02,  1.8419e-01, -2.3651e-01,  2.4803e-01, -1.0107e-01],\n        [-1.6836e-01, -1.4109e-01,  2.4101e-01,  7.1024e-02,  1.8272e-01,  1.5520e-01, -1.5781e-02,  2.3059e-01, -1.8329e-01,  2.6672e-01, -2.0331e-01, -1.9002e-01,  2.3082e-01, -1.0693e-01],\n        [ 2.0110e-01,  1.3922e-01,  2.3167e-01,  1.4367e-01, -1.1887e-01, -1.1439e-01, -1.5891e-01, -8.4532e-02,  8.4922e-02,  5.7892e-02,  4.6993e-03,  1.4445e-01, -2.3782e-01,  1.3249e-01],\n        [ 1.4923e-01, -2.4152e-01, -4.7496e-02,  1.8659e-01,  4.4841e-02,  6.3068e-02, -2.2264e-01,  5.1523e-02,  2.0876e-01, -2.3417e-01, -1.4946e-01,  1.9997e-01, -2.3142e-01,  1.8280e-01],\n        [ 6.6277e-02, -1.7078e-01, -1.8907e-01,  2.4819e-01, -1.2578e-01, -2.3460e-01, -2.0501e-02,  8.1082e-02, -1.0888e-03, -2.1005e-01,  2.4634e-01,  1.5110e-01,  1.5852e-01, -1.0575e-01],\n        [-1.4378e-01, -3.1807e-02,  1.0020e-01, -2.2688e-01, -1.5133e-01, -2.4363e-01,  1.2657e-01,  3.7730e-02, -1.8640e-02, -1.8549e-01,  3.9151e-02,  1.5349e-01, -2.6077e-01, -5.2922e-02],\n        [ 1.3482e-01, -1.3418e-01, -2.1579e-01, -1.6419e-01,  6.4122e-02, -9.6653e-02,  1.1619e-02,  1.0581e-04, -1.5681e-01,  1.0565e-01, -3.3077e-02,  2.2682e-01,  8.0206e-02,  2.5076e-02],\n        [ 4.3315e-02, -2.8035e-02, -1.4770e-01,  1.9700e-01, -1.0881e-01,  3.6931e-02, -1.0379e-01, -1.2108e-01,  5.5506e-02,  2.1486e-01, -5.2925e-02, -1.9749e-01,  2.4176e-01,  4.6043e-02],\n        [-2.2259e-01,  1.4795e-01,  1.7914e-02, -2.4441e-01, -2.2308e-01, -1.3283e-02,  2.4804e-01,  2.1687e-01, -2.6041e-01,  1.6333e-02, -4.0560e-02, -1.3300e-01,  2.1853e-01,  9.0287e-03],\n        [ 9.3616e-02, -7.7488e-02, -6.1543e-02, -5.4852e-02, -1.2203e-01, -2.6556e-01, -3.0645e-02, -5.6875e-02,  2.1809e-01,  7.6447e-02,  1.8340e-01,  3.3797e-02,  1.2421e-01,  8.5581e-02],\n        [-2.5213e-01, -5.6580e-02, -9.2573e-02,  6.8573e-02,  1.8785e-01, -1.9138e-01, -1.3559e-01,  2.0426e-01,  4.1653e-02,  7.7388e-02,  3.2418e-02, -2.4291e-01, -1.9989e-01,  1.8617e-01],\n        [ 9.8373e-02,  2.2479e-01, -1.8205e-01,  2.1964e-01,  1.9801e-01, -2.3640e-02,  8.2999e-02, -1.4471e-02,  2.6022e-01,  2.5190e-01,  9.9439e-02,  1.5995e-01,  2.0405e-01,  2.4167e-01],\n        [-9.2651e-02,  1.1153e-01,  1.6902e-01,  2.4034e-01,  2.0184e-01,  2.6468e-01,  1.2827e-01, -1.2753e-01, -7.2769e-02,  2.0356e-01, -1.3515e-02,  1.6639e-01,  2.4481e-01,  1.0443e-01],\n        [ 2.3708e-01, -2.0646e-02,  8.4852e-02, -2.2065e-01, -3.1731e-02, -1.6581e-01, -3.1938e-02,  1.9320e-02, -1.1346e-01, -1.4200e-01, -2.1533e-01, -1.5304e-01, -1.2995e-01,  1.6261e-01],\n        [ 2.5763e-01, -4.5892e-02,  6.2582e-02,  2.5479e-01, -2.6154e-01,  1.7833e-01,  8.1192e-02, -1.6244e-01,  2.6025e-01, -6.2098e-02,  1.0364e-01,  1.0171e-01,  1.0438e-01,  2.4587e-01],\n        [ 4.3538e-02,  2.2961e-01,  4.2219e-02,  1.4505e-01, -1.0765e-01,  9.9605e-03, -2.3201e-01, -2.4727e-01,  1.6416e-01,  2.4482e-01, -2.2632e-01, -7.2883e-02,  3.7169e-02,  1.8099e-01],\n        [-7.8660e-02,  3.5543e-02, -1.6281e-01, -1.7041e-01, -1.8405e-01, -1.3662e-01, -7.0005e-02, -8.6209e-02, -4.5114e-02, -1.6235e-01, -2.6643e-01, -6.1523e-02,  2.5408e-01,  8.5993e-02],\n        [ 6.6088e-02,  3.9137e-02, -1.8674e-01, -3.3144e-02,  2.1759e-01, -1.6236e-01,  1.8465e-01, -9.2880e-02, -1.1209e-01,  3.3233e-02,  2.2789e-01, -1.2777e-01,  1.9017e-01, -2.0251e-01],\n        [-1.0920e-01, -1.1003e-01,  6.2983e-02,  9.8404e-02, -3.0981e-02,  2.6367e-01,  1.4142e-02,  2.3236e-01,  2.7800e-02,  1.1088e-01,  1.1311e-01,  7.7666e-02, -2.2141e-01, -4.6071e-02],\n        [-1.9636e-01, -1.7911e-01, -1.5381e-01, -1.6969e-01, -9.9204e-02,  2.2136e-01,  6.1268e-02,  2.0543e-01, -4.8921e-02, -1.7057e-01, -3.5254e-02, -1.1781e-01,  4.0587e-02,  1.0786e-01],\n        [-1.2945e-01,  1.1495e-02,  1.1486e-01, -1.7241e-02,  3.5347e-02,  9.0602e-03,  1.9041e-03,  1.7679e-01,  7.0929e-03, -1.1402e-01, -8.4253e-02, -2.3838e-01,  1.5374e-01, -2.1484e-01],\n        [-6.8985e-02,  3.2884e-02, -1.7300e-01, -2.5833e-01, -1.7147e-01, -9.4013e-02,  1.9196e-01, -2.0967e-01, -1.6047e-01, -2.3930e-01,  2.3118e-01,  7.8805e-02,  1.7041e-01,  2.8884e-02],\n        [ 2.6073e-01, -8.3568e-02, -1.5786e-01,  1.1145e-01,  1.8331e-02,  4.5954e-02, -1.8662e-01, -1.9863e-01, -1.4147e-01, -6.8136e-02,  4.3499e-02, -1.5101e-01, -8.2166e-02, -2.1365e-01],\n        [ 9.3726e-02,  2.3777e-01, -2.0353e-01,  2.0677e-01, -1.4858e-01, -7.4781e-02, -3.7838e-03,  8.1627e-02, -2.2219e-01,  2.0863e-01,  7.9942e-02, -2.3745e-01, -2.4106e-01,  1.1695e-01],\n        [-2.2527e-01, -9.0298e-03,  2.1583e-01,  1.9401e-01,  2.6639e-01,  1.3995e-01, -1.9830e-01,  2.5373e-01, -2.2649e-01, -2.0991e-01,  1.9184e-01, -1.7531e-01, -2.4687e-01, -6.4135e-02],\n        [-1.5524e-01,  8.8013e-02, -1.6437e-01,  1.8761e-01,  2.0782e-01,  1.1875e-01,  1.8054e-01,  2.2103e-01,  9.5759e-02,  6.4104e-02, -2.6216e-01, -2.3714e-01,  8.2705e-02, -1.2670e-01],\n        [ 1.2328e-01,  1.5034e-01,  1.6725e-02,  1.2278e-01,  1.2695e-01, -1.6272e-01,  1.1344e-01, -3.2273e-02, -1.5730e-01, -2.6210e-01,  7.6045e-02, -1.2299e-01,  7.1237e-02,  1.7302e-01],\n        [-4.9966e-02, -1.4356e-01, -4.5008e-02, -1.4700e-01, -3.6072e-02, -6.6216e-02, -2.5630e-01,  1.4379e-01,  1.5181e-01, -2.1102e-01,  2.2653e-01, -2.3538e-01, -1.5134e-01,  7.9503e-02],\n        [ 4.5820e-02,  1.6341e-01, -7.1493e-02,  1.9964e-01, -2.2270e-01,  1.4593e-01,  9.8422e-02,  9.1684e-02, -1.5748e-01,  7.5557e-02, -1.3513e-01, -1.3364e-01,  1.3793e-01,  5.9683e-02],\n        [ 2.1915e-01,  2.5785e-01,  2.5976e-01, -2.5065e-01,  1.0973e-01,  1.2635e-02, -1.4786e-01,  2.0237e-01, -2.0892e-01, -2.3681e-02, -9.8614e-03, -9.6038e-03, -2.6223e-01,  4.7309e-02],\n        [-7.7714e-02, -1.6531e-01, -9.4863e-02,  1.7910e-01,  2.5392e-01, -5.6623e-02, -2.6327e-02,  1.4745e-01, -1.2281e-02,  2.6537e-02, -2.4366e-01,  9.8358e-02, -2.3443e-01,  1.3673e-01],\n        [ 2.1470e-01,  1.3083e-01,  1.9395e-01, -6.5517e-02,  4.8911e-02, -1.0209e-01, -1.0428e-01,  1.2015e-01, -2.5866e-01,  4.0350e-02, -1.2250e-02, -1.1575e-01, -4.1697e-03,  2.2268e-01],\n        [ 1.6980e-01,  1.1950e-01,  6.9053e-03,  4.2073e-02, -1.4627e-01,  2.3538e-01, -1.6354e-01,  1.4249e-02,  1.3435e-01,  2.5591e-01, -9.1112e-02,  9.9031e-02,  2.0324e-01,  2.2166e-01],\n        [ 2.0773e-01, -9.8207e-02,  2.3326e-02, -1.6895e-02,  5.4850e-02, -9.3698e-02, -1.8104e-01,  2.2859e-01, -2.5117e-01, -2.6283e-01, -1.7545e-01,  1.7646e-02,  2.6495e-01, -1.3656e-01],\n        [-8.0802e-02,  1.7778e-01,  1.8406e-02, -5.5325e-02,  9.9039e-02, -4.4008e-02,  2.6102e-01,  1.0727e-01, -9.9271e-02, -1.5436e-01,  2.8551e-02, -3.5321e-02, -1.7616e-01,  1.7521e-01],\n        [-3.6605e-02, -8.9851e-02,  2.2586e-02, -2.4099e-01,  2.3720e-01,  1.7958e-01,  9.8505e-02,  1.9192e-01, -8.3330e-02,  2.6918e-02, -1.6610e-01,  1.6917e-01,  2.5712e-01, -1.7329e-01],\n        [-9.9991e-02,  1.2386e-01,  1.8075e-01, -2.0282e-01,  2.6836e-02, -1.6560e-01,  1.0491e-01, -1.7187e-01,  1.6586e-01,  8.3701e-02,  1.0811e-01,  6.0077e-02,  2.5532e-01,  1.9389e-01],\n        [ 1.2490e-01, -1.7166e-01, -2.6540e-01,  1.7410e-01,  1.2067e-02, -2.0458e-01, -2.2341e-01, -2.0748e-01, -9.2766e-02,  7.3843e-02, -1.6071e-01,  2.4846e-01, -1.1812e-01,  2.0700e-01],\n        [ 1.2439e-01,  8.0748e-02, -7.0736e-02,  5.7616e-02,  2.4917e-01, -2.2991e-01, -2.3544e-01,  2.0957e-01, -1.1600e-01, -1.5753e-01, -1.7131e-01, -9.7918e-02, -8.9126e-02, -2.0577e-02],\n        [-1.6090e-01,  1.2482e-01, -7.4709e-02, -6.8187e-02, -1.1550e-01, -2.0801e-01, -2.5839e-01, -1.6312e-01,  1.5595e-01,  1.6662e-01,  6.4520e-02,  2.1512e-01,  2.6444e-01,  8.7125e-02],\n        [ 2.5232e-01,  7.0392e-02, -1.7755e-01,  2.0237e-01, -9.5906e-02, -9.7787e-02, -4.2496e-02,  1.0735e-01,  1.7355e-01,  2.5662e-01, -9.6079e-02,  1.8418e-01,  2.1731e-01,  6.3789e-02],\n        [-1.2476e-01, -1.6407e-01,  8.5631e-02, -3.5958e-02, -1.2393e-01,  1.6099e-01,  8.2625e-02,  4.1251e-02, -1.3846e-01,  1.6128e-01,  1.0959e-01,  2.5277e-01,  2.2771e-01, -1.4351e-02],\n        [ 1.6092e-01,  1.9575e-01,  8.2598e-03, -2.1980e-01,  1.8975e-01,  1.9326e-01, -7.9473e-02,  2.0247e-01,  1.7035e-01,  8.3676e-02, -2.1603e-01,  6.8142e-02, -1.1417e-01,  1.8677e-01],\n        [-2.5490e-01,  3.1454e-02,  1.7151e-01,  2.6225e-01, -8.7817e-02, -2.1175e-01,  3.0751e-03,  2.4222e-01,  1.1281e-01,  1.1644e-02,  2.0369e-01,  2.0954e-01, -2.4971e-01,  1.8760e-01],\n        [ 5.7083e-02, -1.4005e-01,  6.2041e-02,  2.8645e-03,  1.1010e-01, -2.4014e-01, -2.3037e-01,  1.6254e-01, -7.2298e-02,  1.1868e-01, -3.7048e-02, -1.1893e-02, -2.2339e-01, -7.7683e-02],\n        [-2.5733e-01, -2.2856e-01, -1.8025e-02,  1.0155e-01, -2.3532e-01,  1.6510e-01, -5.6551e-02,  1.1578e-01, -1.6031e-01, -2.3865e-01, -2.9167e-02, -1.0083e-01,  1.2706e-01, -1.0382e-01],\n        [-1.1708e-01, -2.3943e-02,  9.5579e-02, -1.0616e-01, -2.3085e-01, -2.5048e-01,  1.5223e-01, -2.3497e-01,  1.9935e-01, -1.8613e-01,  2.2081e-01,  7.6671e-02,  1.4006e-01, -2.0531e-01],\n        [-2.3935e-01,  2.0522e-01, -1.5333e-01,  1.8278e-01,  2.5412e-01, -1.1012e-01, -2.4578e-01,  5.9554e-02, -2.1999e-01, -1.4934e-01,  5.0265e-02,  1.6776e-01, -1.5057e-01,  2.2659e-01],\n        [-2.3806e-01,  2.4704e-01,  3.6304e-02,  1.9238e-01, -5.4103e-02, -9.6412e-02,  2.5565e-01,  1.2134e-01, -1.7507e-01, -9.6683e-02, -1.8307e-01,  1.5991e-01,  1.2303e-01,  7.2401e-02],\n        [-5.0591e-03,  1.2836e-02, -1.0718e-01, -7.0687e-02,  3.8623e-02, -2.3265e-01,  1.8963e-01,  4.3237e-02, -2.4065e-01,  4.0657e-02, -5.0259e-02,  2.6345e-02,  5.8574e-02, -1.4838e-01],\n        [-2.5990e-01,  1.6031e-01,  8.2572e-02, -1.0251e-01, -2.1715e-01, -1.6293e-01,  7.8239e-02,  1.5828e-01,  2.0705e-01,  1.9804e-01, -1.6900e-01, -1.3691e-01, -9.7360e-03,  6.4716e-02],\n        [-1.9017e-01, -1.5251e-01, -2.5292e-01,  1.9252e-01,  1.5219e-01, -8.2876e-02,  1.4570e-02, -1.6778e-02,  5.9209e-03,  1.6910e-01, -1.3055e-01,  1.9079e-01, -6.8748e-02,  1.2715e-01],\n        [ 1.1643e-01,  1.7142e-01,  1.8203e-01, -4.4890e-02, -4.8582e-02, -5.4956e-02,  2.8308e-04, -3.4025e-02, -5.3801e-02,  2.4540e-01,  1.2112e-01,  6.8195e-02, -2.0080e-01,  1.3833e-01],\n        [-1.8205e-01, -1.1608e-01, -1.3378e-01, -4.2054e-02,  1.2001e-01, -1.9625e-02, -2.2071e-01,  8.4481e-02,  1.8690e-01,  6.6764e-03,  1.4233e-01,  2.0651e-01, -1.3544e-01, -2.2960e-01],\n        [ 1.4554e-01, -1.2544e-01,  8.6841e-02, -2.2517e-01,  2.0788e-01,  2.6418e-01, -5.5052e-02,  1.3021e-01, -2.3913e-01,  6.4051e-02, -1.9984e-01,  2.2443e-01,  1.3850e-01,  1.4969e-01],\n        [-1.2653e-01, -1.7378e-01,  5.8760e-02,  1.4716e-01,  8.5899e-02, -4.3093e-02,  1.4933e-01,  1.6178e-01, -6.6393e-02,  9.9665e-02,  1.0264e-01, -9.2481e-03, -1.1351e-01, -1.7220e-01],\n        [ 2.2148e-03, -1.8448e-01,  1.3297e-01, -1.6691e-01,  1.0274e-01,  1.6362e-01, -3.8689e-02,  2.4883e-01, -2.4703e-01, -9.8976e-02,  2.1334e-01,  1.9903e-01, -1.1505e-01, -3.3850e-03],\n        [ 1.2505e-01,  2.2876e-01,  1.5423e-01, -2.3083e-01, -1.6239e-01,  1.4214e-01, -1.5912e-01, -1.5378e-01, -1.8183e-01, -1.2863e-01,  1.4512e-01, -2.6614e-01,  3.5664e-02,  3.5795e-02],\n        [-2.4325e-01, -2.2132e-01,  8.6574e-02, -2.6004e-01, -2.5286e-01, -1.2077e-01,  3.8886e-02,  3.5104e-03, -3.1436e-02,  2.0718e-02, -6.2737e-02, -2.3691e-01,  1.5035e-01, -2.1153e-01],\n        [ 1.9316e-01,  6.4618e-03,  9.2036e-02, -1.5245e-01, -2.0184e-01, -1.4345e-02, -6.7581e-02, -2.6428e-01,  1.6025e-02, -1.5791e-01,  1.1744e-01,  4.2422e-02, -8.5906e-03,  1.5570e-01],\n        [-1.5083e-01, -6.5894e-02, -1.0203e-01, -4.8392e-02, -2.2458e-01,  2.2333e-01, -9.6362e-02, -2.0226e-01,  2.0126e-01, -2.6218e-01, -1.6506e-01, -6.3591e-02, -2.3071e-01,  2.2646e-01],\n        [ 2.1619e-01,  2.3849e-01, -1.6765e-01,  2.4662e-01,  1.1641e-01, -3.8602e-04,  2.5243e-01,  2.0028e-01, -2.5774e-02, -1.7699e-01,  5.1721e-02, -2.5321e-01, -3.6998e-03, -2.1870e-01],\n        [-1.0115e-01, -1.0763e-01, -1.5446e-01,  1.4737e-01,  1.7130e-01,  1.8083e-01,  2.4029e-01, -1.5850e-01,  1.8591e-01, -1.4847e-01, -2.6672e-01,  7.4064e-02,  1.2702e-01, -2.0940e-01],\n        [-1.7803e-01,  1.0357e-01,  1.8097e-01, -7.9640e-02, -1.1769e-02, -2.1288e-01,  1.5368e-01,  1.8998e-01,  1.7665e-01, -7.3000e-03,  1.4728e-01, -5.1180e-02, -8.3919e-03,  4.5723e-02],\n        [ 8.5996e-02,  3.6531e-02, -7.8314e-02, -1.5255e-01,  1.8355e-01, -1.8254e-01,  1.3534e-03, -6.8376e-02,  2.4943e-01,  1.5430e-01,  9.2846e-02,  5.5654e-02, -2.2138e-01, -1.1826e-01],\n        [ 2.0214e-01,  6.8963e-03,  3.5788e-02,  1.6950e-01, -1.3125e-01, -7.8159e-02, -4.0694e-02, -3.7727e-02,  1.1494e-01, -7.1458e-02,  9.1569e-02,  1.1415e-01, -2.2065e-01, -1.0479e-01],\n        [ 8.0765e-02,  9.8971e-02, -7.7858e-02, -1.8480e-01,  1.4387e-02,  1.2981e-01, -1.7985e-01,  2.0433e-01, -2.1921e-02,  2.2167e-01,  1.7901e-01, -2.2064e-01, -1.6147e-03, -1.3382e-01],\n        [-2.6667e-01,  3.0411e-02, -2.5197e-01, -1.4964e-01,  1.5130e-01,  2.0434e-01, -8.6676e-02, -2.6698e-01, -3.0561e-02,  1.7478e-01,  1.4162e-01, -2.6675e-01, -2.5925e-01, -1.9707e-01],\n        [-3.8046e-02,  4.0233e-02, -1.4016e-01,  2.1112e-01,  5.9912e-02,  5.7907e-02,  1.0616e-01, -6.4591e-02,  5.3759e-02, -1.2010e-01, -1.8732e-01, -7.8167e-02,  1.1371e-01, -2.2234e-01],\n        [ 9.4129e-02, -2.5319e-01, -1.8049e-01, -9.8920e-03, -1.9441e-01, -2.2035e-02, -1.9158e-01,  1.5340e-01,  1.0430e-01,  1.2749e-01, -1.0854e-02,  1.8376e-01, -1.7069e-01,  2.2143e-01],\n        [ 3.5438e-02,  1.4823e-01,  1.1946e-01, -1.9506e-01,  2.2512e-03, -2.4775e-01,  4.6928e-02, -1.5208e-01, -7.6633e-02, -2.2526e-01, -1.1583e-02,  1.4903e-01,  4.0120e-02,  7.0941e-02],\n        [-1.9393e-01,  2.2694e-01, -3.5081e-02,  1.9700e-01, -2.6893e-02, -1.3769e-01,  2.3839e-02,  2.8991e-03, -2.1644e-01, -1.3972e-01, -6.4450e-03,  1.2288e-02, -1.0721e-01, -1.0198e-01],\n        [-8.5001e-02,  1.1849e-01, -9.3567e-02, -2.3468e-01, -2.5535e-01,  2.2027e-01, -1.3586e-01,  1.5898e-02, -4.8446e-03, -1.7316e-01, -1.3246e-01,  1.1030e-01, -2.1826e-01,  1.1286e-01],\n        [ 1.1436e-01, -3.2469e-02,  1.2345e-01, -2.2217e-02, -8.3853e-03, -2.2087e-01,  5.9670e-02, -2.0661e-01,  1.8002e-01, -2.1724e-01, -3.9846e-02, -2.1460e-01, -2.1540e-01, -4.5716e-02],\n        [ 6.7653e-02, -1.4810e-01,  2.6226e-01,  1.7323e-01, -2.2757e-01, -2.0920e-01,  6.6963e-02, -9.9064e-02,  1.3300e-01, -2.3071e-01, -1.2956e-01,  3.9533e-02, -1.5269e-01,  9.5542e-02],\n        [ 1.1254e-01,  2.2022e-01, -2.6110e-01, -1.9494e-01,  1.6689e-01, -2.0249e-02,  2.3087e-01, -2.0551e-01, -1.6641e-01, -2.9369e-02, -1.7769e-01,  8.5174e-02, -4.9752e-02, -8.6060e-02],\n        [-2.5459e-01, -2.2735e-01,  2.3081e-01,  2.9921e-02,  2.6292e-02,  5.3678e-02,  1.7441e-01,  8.0726e-02, -1.7252e-01,  2.4565e-01,  4.4124e-02, -6.9742e-02, -2.6412e-01, -1.1615e-01],\n        [-1.2120e-01, -3.1982e-02, -2.4122e-01,  2.4239e-01, -4.9900e-02,  4.6452e-04,  9.7747e-02, -2.0608e-01,  1.8124e-01, -3.4573e-02,  2.7261e-02, -9.8888e-02,  1.9442e-01, -1.5021e-01],\n        [ 4.9986e-02, -8.7596e-02, -1.1694e-02, -1.6155e-01, -2.1120e-01, -1.5506e-01, -5.0430e-03, -1.6825e-01, -2.0252e-01, -1.6410e-01,  2.2979e-01,  1.8317e-01,  7.6958e-02, -1.4844e-02],\n        [-7.3961e-02,  2.4105e-01, -5.1465e-02, -1.5508e-01,  1.6890e-01, -8.5756e-02,  1.2184e-02, -2.4517e-01,  1.1992e-01,  1.9874e-01, -1.8077e-01, -2.1658e-01, -2.6637e-01,  1.0277e-01],\n        [ 3.0346e-02, -9.0794e-02, -7.3783e-02, -2.0725e-01, -9.1094e-02, -5.7415e-02, -2.2924e-01, -5.8879e-02, -2.1899e-02,  8.7610e-02,  2.0670e-01,  1.8049e-01,  6.2953e-02,  5.8438e-02],\n        [-7.8731e-02,  9.7462e-02, -2.6422e-02,  9.1338e-03, -2.6597e-01,  1.7675e-01, -2.6664e-01,  2.9194e-02, -1.9677e-01,  2.4663e-01,  1.1889e-01, -1.1484e-01, -1.9594e-04,  1.2196e-01],\n        [-1.5428e-01,  1.8488e-01,  2.6550e-01, -1.0035e-01,  1.6580e-01,  1.9209e-01, -2.2191e-01, -1.1382e-01, -2.5788e-01, -1.8010e-01, -2.4122e-01, -1.4410e-02,  1.8063e-01,  9.2380e-02],\n        [-1.2080e-02,  2.4640e-01, -2.1933e-01, -2.0158e-01,  2.1098e-01, -1.8018e-01,  2.5454e-01, -9.0637e-02, -2.3439e-01, -1.9830e-01,  1.4953e-01, -2.3722e-02, -1.2195e-01, -7.8812e-03],\n        [ 1.2951e-02, -1.7433e-02,  1.4920e-01, -5.5113e-02, -1.0826e-01,  1.8304e-01,  2.0130e-01, -5.4700e-02, -1.3172e-01,  8.2418e-02,  1.3486e-01, -2.2423e-02, -1.1853e-01, -1.7242e-01],\n        [-2.6678e-01, -5.2846e-02, -1.0432e-01, -2.7930e-02, -1.9079e-01, -1.2041e-01,  2.0177e-01, -2.4871e-01, -1.3818e-01,  1.9680e-01,  9.7138e-02,  2.3811e-01,  1.8869e-02,  1.6395e-01],\n        [ 1.7841e-01,  2.1420e-01, -1.3070e-02, -1.9555e-01, -2.0570e-01, -2.1100e-01,  1.5521e-01, -2.6205e-01,  2.2950e-02,  7.3021e-02,  1.4579e-01,  5.1389e-02,  1.4587e-01,  8.1997e-02],\n        [ 2.4899e-01,  9.2100e-02, -5.3020e-02, -9.9386e-02, -2.3008e-01, -5.0359e-02, -2.9581e-03,  1.3147e-01, -1.1903e-01, -2.2660e-01, -1.1933e-01, -1.6471e-01,  1.6319e-01, -2.4221e-01],\n        [-2.6193e-01, -2.2017e-01,  1.4327e-01, -2.5403e-01,  2.4696e-02, -1.7349e-02,  2.1582e-01, -2.0869e-01,  2.4171e-01, -7.5542e-02,  1.3366e-01, -9.6330e-02,  6.9602e-02,  2.0793e-01],\n        [-1.4526e-01, -2.2064e-01,  2.6171e-03,  1.3303e-01,  8.4223e-02, -8.8670e-02, -2.5718e-01, -2.3792e-01,  1.6623e-01,  1.8365e-01, -1.3540e-01,  2.3952e-01, -2.4270e-01,  2.3953e-01],\n        [-1.4289e-01, -1.0766e-01, -2.3702e-01,  1.9672e-01, -2.4834e-01,  6.0909e-02, -1.3139e-01, -1.7512e-01,  1.6432e-01,  1.7368e-01, -7.2110e-02,  2.3673e-01, -2.2713e-01, -4.6223e-02],\n        [-4.1826e-02,  4.1729e-02,  7.4320e-02,  2.3564e-01,  2.1961e-01, -3.3874e-02, -1.8662e-01,  2.4986e-01, -7.2545e-02,  2.2460e-01, -2.4065e-01,  2.1776e-01,  1.7011e-01, -2.4518e-02],\n        [-1.8920e-01, -1.8708e-01, -1.0637e-02,  1.6035e-01, -7.1424e-02, -1.9994e-01, -1.0559e-01, -2.2185e-01, -2.1843e-01,  1.4044e-01, -4.1915e-03, -7.2758e-02, -1.7897e-01,  5.4451e-02],\n        [ 1.2087e-01, -1.0212e-01,  1.7960e-01, -1.6608e-01, -4.2767e-02, -1.0252e-01, -2.5856e-01, -2.6723e-01,  2.6193e-01, -7.9694e-02, -4.0292e-02,  1.7726e-01, -2.5405e-01,  8.6352e-02],\n        [ 1.0644e-01, -2.5647e-01,  2.6669e-01,  1.0015e-01, -6.5344e-02,  2.1917e-01, -2.4653e-01,  1.9647e-01,  4.8001e-03, -1.7160e-02, -2.4532e-01,  1.7769e-02, -1.2583e-01,  2.4846e-01],\n        [-2.5096e-01, -7.1934e-02,  9.7365e-03,  8.2324e-02,  2.0492e-01,  1.3070e-01,  7.4230e-02, -1.3933e-01, -2.2220e-01, -2.1757e-01,  3.4602e-02,  1.8353e-01,  1.0508e-01,  4.5202e-02],\n        [ 8.2653e-02,  2.4507e-01,  4.0865e-03,  4.1193e-02, -1.1860e-01,  6.6037e-02, -9.3044e-03, -1.7069e-01,  1.7112e-01,  9.8260e-02,  2.5236e-01, -1.3820e-01,  9.0818e-02, -2.5155e-01],\n        [ 1.2371e-01,  1.7455e-01, -1.6103e-01,  1.0837e-01, -1.0185e-01,  2.2688e-01, -1.4884e-01, -5.1275e-02,  2.1949e-01, -4.0880e-02, -1.9682e-01,  1.5887e-01, -1.1542e-01, -1.0795e-01],\n        [-1.7054e-01,  2.4771e-01, -2.0089e-01,  2.5693e-01, -2.5635e-02, -2.5202e-01,  2.2179e-02, -4.2134e-02, -1.9736e-01, -2.6727e-02, -1.0543e-02, -6.9434e-02, -1.8997e-01,  1.3191e-01],\n        [-2.2699e-01,  6.3592e-02, -2.0135e-01, -7.9483e-02, -2.3999e-01,  2.0797e-02,  1.7602e-01, -1.1223e-01,  9.0920e-02, -1.5052e-01, -1.9823e-02,  1.0839e-02, -1.7577e-01,  2.4776e-01],\n        [-2.1314e-01, -6.7321e-02, -1.9136e-01,  2.0750e-01,  1.3465e-01,  2.2148e-01,  2.0954e-01,  2.0546e-01,  9.4967e-02,  2.2753e-01,  3.8536e-02,  1.5227e-01, -8.8098e-02, -2.0714e-01],\n        [-2.4041e-01,  1.6833e-01,  2.3629e-01, -1.6631e-01,  3.4491e-02,  5.5762e-02, -5.9816e-02, -1.2516e-01, -8.6618e-02,  1.4244e-01,  7.0718e-02, -1.8032e-01,  1.0969e-01,  2.6200e-01],\n        [ 9.0270e-02,  1.5668e-02, -2.0997e-01,  8.0462e-02,  1.5008e-01, -1.2128e-01, -1.0249e-01, -2.5700e-02, -2.1371e-02,  1.3606e-01,  1.8633e-01, -6.3838e-02, -1.5164e-01, -2.6256e-01],\n        [ 1.3079e-01, -1.9991e-01,  1.9182e-01, -1.9511e-01, -6.5763e-02,  2.1345e-01,  2.3640e-01, -1.9791e-01, -2.6555e-01,  2.0192e-01, -7.1257e-02,  1.5680e-01,  2.3411e-01, -1.5071e-01],\n        [ 1.7464e-01,  1.8605e-01, -8.4548e-02,  1.7551e-01, -1.9187e-02, -1.3610e-01,  7.5431e-02,  9.2678e-02, -2.2869e-01,  2.4040e-01, -9.8648e-02,  1.9516e-01, -1.7831e-01,  1.8973e-01],\n        [ 2.4570e-01,  2.4251e-01,  9.6367e-02, -5.5780e-02,  2.2117e-01,  2.2176e-01,  8.1911e-02,  2.0663e-01,  2.5728e-01,  1.9896e-01, -7.0773e-02, -1.2482e-01, -1.9297e-01,  2.0380e-01],\n        [ 2.2215e-01, -1.2102e-01,  1.2761e-01,  5.4408e-02,  9.8209e-02, -1.4692e-01,  2.2227e-01,  9.0686e-02, -1.8659e-01, -1.1405e-01,  2.3591e-01, -5.9888e-02,  1.5876e-01, -1.6775e-01],\n        [-1.2178e-01,  1.2355e-01,  2.6632e-01, -1.1558e-01,  1.4186e-01,  5.2821e-02, -2.6284e-01, -1.5902e-01, -2.4201e-01,  2.6262e-01,  2.5555e-01,  2.4664e-01, -2.6701e-02, -9.7634e-02],\n        [-2.3248e-01, -1.2193e-01, -2.0727e-01, -1.0604e-01,  1.9569e-01, -1.9639e-01,  2.2273e-01, -2.3386e-01,  1.8055e-01, -4.4576e-02,  2.5294e-01, -2.6131e-01, -1.6304e-01,  1.5279e-01],\n        [-2.4360e-01,  3.4547e-02, -2.1620e-01, -3.3298e-02,  2.8586e-02,  1.7274e-01,  4.9828e-02,  1.4835e-01, -2.1532e-01, -2.6368e-01, -1.2916e-02,  1.4910e-01, -8.5909e-02, -1.5691e-01],\n        [-3.2459e-02, -2.5920e-01, -5.1125e-02, -1.7462e-01,  1.8266e-01,  1.6987e-01, -2.0003e-01, -8.2871e-02,  8.5053e-02, -4.4430e-02,  9.2004e-02, -8.8842e-02, -2.1958e-01, -2.4646e-01],\n        [ 1.9425e-01, -2.5650e-01, -2.1675e-01, -1.3363e-01,  6.2196e-02,  2.5852e-01, -1.1327e-01,  1.7981e-01, -2.4772e-01, -1.0722e-01, -2.2332e-01, -9.0392e-02,  7.1090e-02,  9.8755e-02],\n        [-2.3761e-01,  2.2013e-01, -1.9750e-01,  1.8132e-01,  1.1645e-02, -1.7934e-01, -1.9550e-01, -1.1774e-01,  2.2959e-01, -1.1947e-02, -1.3223e-01,  2.2887e-01, -5.0697e-03,  1.1579e-01],\n        [ 2.3392e-01,  2.2278e-01,  1.2324e-02, -3.7920e-02, -4.4726e-02, -1.7847e-01,  2.3398e-01,  1.7221e-01, -2.5920e-01, -1.1428e-01, -1.6421e-01, -2.3444e-01, -2.4737e-01, -1.1805e-01],\n        [-4.8813e-02,  6.7577e-02, -1.9244e-02,  6.9409e-02,  1.7657e-01, -2.3216e-01, -1.2256e-01, -1.6378e-01, -3.9477e-02,  2.6008e-02,  3.2177e-02,  8.7258e-02, -2.2206e-01,  6.4691e-02],\n        [ 1.0773e-02,  2.5304e-01,  7.9885e-02,  2.6726e-01,  2.2624e-02, -6.1423e-02, -6.7176e-02, -1.5460e-01,  1.6142e-01,  1.7238e-01,  1.7145e-01,  2.2351e-01,  1.3677e-01, -8.5875e-02],\n        [-1.9165e-01,  1.9194e-01,  6.4842e-04, -2.0301e-02,  1.5104e-01,  2.6395e-01,  7.4469e-02,  2.5224e-02, -7.0488e-02,  1.7219e-01, -9.2360e-03,  2.3141e-01, -2.3100e-01, -7.3906e-03],\n        [ 2.5352e-01,  1.0138e-01,  1.3291e-01, -2.0716e-01, -2.5201e-01,  2.0957e-01,  2.1209e-02, -1.2027e-01, -1.9744e-01,  2.0173e-01,  5.2320e-02, -1.8013e-01,  2.6431e-01, -5.0746e-02],\n        [ 2.2657e-01,  1.4152e-01, -2.4140e-01, -1.5898e-01, -1.9693e-01,  2.4309e-01,  2.4060e-01, -7.0506e-02,  7.1361e-02,  1.6880e-01,  2.2221e-01, -3.0960e-02, -2.6149e-01,  2.2307e-01],\n        [-1.1014e-01,  1.5713e-01,  1.2187e-01,  5.4497e-02,  2.4503e-01, -2.2664e-01, -2.3661e-01, -8.9381e-02, -7.4309e-02, -6.6004e-02,  2.1274e-01,  1.7476e-03,  6.0554e-02, -2.6087e-01],\n        [ 2.4127e-01,  6.8014e-02,  2.3675e-01, -2.1122e-01, -1.9329e-01,  1.0118e-02, -3.9544e-02,  1.6189e-02,  8.6272e-02, -2.4951e-01,  6.3279e-02, -4.3335e-02, -1.6166e-01,  2.5433e-01],\n        [ 3.5687e-02,  1.8166e-01, -1.6699e-01, -1.4068e-01,  1.9117e-01,  1.8893e-01,  1.2996e-01,  2.0235e-01,  2.1378e-01, -1.8714e-01,  8.6686e-02,  1.0916e-02,  1.5991e-02,  4.9747e-02],\n        [ 1.6306e-01, -2.2045e-02, -5.0212e-03, -2.0644e-01, -1.7007e-01, -2.2888e-01, -5.7742e-03, -1.9237e-01,  7.0041e-04, -1.1966e-01, -8.6844e-02,  2.4475e-01, -2.2763e-01, -1.1089e-01],\n        [ 1.9840e-02, -7.9777e-02,  9.8939e-02,  2.5256e-01,  1.3541e-01,  1.3368e-01, -2.3599e-02, -4.7279e-02,  7.3170e-02,  1.4197e-01,  1.7061e-01,  9.9650e-02,  1.4086e-01,  5.9469e-02],\n        [-4.7042e-03, -5.2126e-02, -2.0500e-01, -2.6374e-01, -2.5598e-01,  2.0740e-01,  6.6917e-02,  2.4251e-01, -2.0071e-01,  2.2447e-01, -2.6052e-01, -4.8678e-02,  1.1953e-01, -2.4711e-01],\n        [ 1.7728e-01, -2.4462e-01,  1.7581e-01,  2.2027e-01,  2.0371e-01,  5.9278e-03,  1.5547e-01,  1.1928e-01,  2.1243e-01, -2.5233e-01, -2.5529e-01, -1.7753e-01,  2.3623e-01,  8.8208e-02],\n        [-2.1107e-02, -6.1282e-02,  1.7314e-01,  1.8640e-01, -2.0219e-01, -4.0583e-02,  1.7417e-01, -1.8455e-01,  1.7959e-01,  4.3420e-02, -4.0551e-02, -1.8013e-01,  1.2988e-01, -1.9971e-01],\n        [ 2.5264e-01,  1.6548e-01, -4.6374e-02, -8.4594e-03, -2.4031e-01,  2.4120e-01,  6.8346e-02, -8.5147e-02, -2.6694e-01, -7.4741e-02, -3.7346e-02,  1.3433e-01, -1.5304e-01, -1.5946e-01],\n        [ 1.4721e-01, -9.4755e-02,  2.3225e-01, -1.5181e-01,  1.0063e-01, -7.7388e-02,  1.3931e-01,  2.0336e-01, -3.9858e-02,  4.7311e-02, -1.5181e-01,  1.1124e-01, -2.0794e-01,  1.0406e-01],\n        [ 2.4718e-01,  2.4564e-01,  2.7584e-02, -1.1965e-02,  1.6771e-01,  6.2103e-02, -5.7288e-03, -2.5633e-01,  3.9211e-02, -3.4988e-02, -8.9977e-02, -8.0057e-02, -1.5328e-01,  1.6316e-01],\n        [-2.2274e-02, -8.6239e-02, -2.0514e-01, -2.4046e-01,  1.7991e-01,  3.8371e-02,  2.5208e-01, -1.4037e-01, -4.6023e-02,  2.2178e-01,  1.6486e-01, -2.2260e-01,  6.8499e-02,  8.0094e-02],\n        [-9.3225e-02,  1.6031e-01, -2.3540e-01, -2.0559e-01,  6.5842e-02, -1.2166e-01,  1.8637e-01,  6.1065e-02,  9.1258e-02,  1.6607e-01, -6.7916e-02, -1.9797e-01, -1.1989e-01,  8.6908e-02],\n        [ 5.6423e-02, -8.7047e-02, -1.1889e-01, -1.4218e-01, -2.0919e-01,  3.1157e-02,  2.7907e-02,  1.5818e-01, -7.7329e-02,  1.3461e-01, -1.3070e-02,  8.4345e-02,  1.1326e-01, -1.8346e-01],\n        [ 8.5036e-02,  4.1233e-02,  8.4064e-02,  3.1021e-03,  2.2212e-01,  1.6668e-01,  1.8922e-01, -1.3973e-01,  1.7685e-01,  1.4863e-01,  1.2197e-01, -2.6459e-01, -2.2691e-01,  7.6959e-02],\n        [-1.0899e-01, -5.0049e-02, -5.5980e-02, -1.3549e-01, -1.2612e-01,  2.6373e-01, -7.4057e-03,  2.0033e-01,  2.4088e-01,  1.4138e-01, -2.0027e-01, -9.3277e-02, -2.3622e-01,  1.5413e-01],\n        [ 1.2742e-01, -1.3360e-01,  2.3574e-01,  9.3176e-02,  1.0287e-01, -8.4043e-02,  7.5274e-02, -2.1746e-01, -9.8188e-02,  2.4622e-01,  1.6646e-01,  1.8223e-01,  7.0121e-02, -3.9801e-02],\n        [-1.1193e-01, -1.2187e-01, -3.2288e-02, -1.4700e-01,  1.7037e-01,  3.1190e-02,  1.8274e-01,  9.6674e-03, -2.1037e-01,  1.5675e-01,  2.4792e-01, -3.2011e-02,  7.4416e-02,  1.1197e-01],\n        [-5.8922e-02, -1.9480e-01,  2.6623e-01,  1.9401e-01,  1.4714e-01,  2.3273e-01, -1.1081e-01, -1.7786e-01,  2.0764e-01, -1.0088e-01, -5.8863e-03,  1.0196e-01,  1.3247e-01, -2.0787e-02],\n        [-1.6023e-01, -1.3249e-01,  2.5376e-01, -1.9787e-01, -7.0826e-02,  3.4756e-02,  5.1573e-02,  7.6004e-02,  1.5054e-01, -6.7549e-02,  7.5817e-02, -6.4151e-02,  1.4849e-01, -3.1858e-03],\n        [-2.1735e-01,  8.9232e-02, -2.1971e-02, -1.7336e-01, -2.2374e-01,  4.0616e-03,  2.6564e-01, -4.7419e-02, -1.0400e-01, -5.7138e-02,  6.3478e-02, -1.5319e-01,  1.6414e-01,  7.4186e-02],\n        [-5.1043e-02,  2.5629e-01,  1.8710e-01, -2.2123e-01,  6.8622e-02,  9.9517e-02,  2.0524e-01, -1.6153e-01,  1.9648e-01, -1.7437e-01, -6.1118e-02, -1.7513e-01, -7.3822e-02, -1.1657e-01],\n        [-1.0466e-01, -1.8450e-01,  2.2478e-01,  2.1051e-01, -1.6957e-01,  1.4785e-01,  2.5893e-02, -5.3583e-03,  1.9370e-01,  4.7476e-02,  1.3240e-01, -2.5289e-01, -1.5561e-02,  1.4187e-02],\n        [-2.4717e-01, -2.3685e-01,  2.3213e-01,  1.0144e-01, -2.1614e-02, -8.1948e-02, -1.4642e-01,  2.1309e-01, -1.4591e-01,  2.1529e-01, -5.2101e-02,  5.7655e-02, -1.4005e-02, -1.5102e-01],\n        [ 1.9305e-01,  1.5660e-01, -2.4553e-01, -1.3208e-01, -1.3253e-01, -2.6668e-01, -1.8097e-01, -9.7344e-02, -2.4714e-01,  1.2572e-01,  1.2485e-01,  2.2793e-01, -1.5308e-01, -5.2314e-02],\n        [-2.2167e-01, -1.3430e-01,  2.1230e-01,  6.8691e-02,  2.2162e-01, -6.4487e-02, -4.8396e-02, -9.4965e-02, -2.6242e-01,  1.8667e-01, -1.5995e-01,  1.5415e-01,  1.2261e-02, -6.1433e-02],\n        [-1.1659e-01, -2.0786e-01, -1.4714e-01,  7.8965e-02, -4.5178e-03, -7.1955e-02,  2.3901e-01, -1.5997e-02,  1.7855e-03, -1.8349e-01,  1.1266e-01, -3.0658e-03,  1.5456e-01, -1.5665e-01],\n        [ 2.1681e-01,  1.3502e-01,  4.4635e-02, -1.2508e-02, -7.3263e-03,  4.5977e-02, -6.8887e-02, -1.3760e-01, -2.0216e-01, -2.0985e-01,  3.1022e-02, -1.2375e-01, -1.9086e-01,  9.3137e-02],\n        [ 8.9301e-02,  2.7273e-02, -1.8855e-01,  7.5746e-02, -1.4111e-01, -2.2329e-01,  2.3137e-02,  1.1711e-02, -1.3180e-01, -1.2512e-01, -1.8407e-01,  8.3856e-05,  2.0563e-01, -4.8305e-02],\n        [ 1.2890e-01,  3.1788e-02,  7.8678e-02, -2.5334e-01,  3.5006e-02, -2.0255e-01,  2.2924e-01,  1.0269e-01, -2.1891e-02,  1.7571e-01, -5.8136e-02,  2.1536e-01,  1.9480e-01, -1.5555e-01],\n        [-1.1852e-01, -3.8338e-02,  2.3949e-01, -1.4299e-01, -7.7466e-02,  1.5745e-01, -7.9462e-02,  2.3409e-01,  2.2674e-01,  1.8391e-01, -3.0134e-02,  2.6338e-01, -1.4946e-01,  7.3491e-02],\n        [-1.2392e-01, -2.3561e-01, -7.4644e-02,  1.7196e-01, -2.2431e-01,  2.4978e-02, -1.1091e-02, -2.0092e-01,  1.0980e-01, -2.3620e-01,  2.0747e-01,  9.6616e-02,  3.2556e-02, -8.6389e-02],\n        [-7.5830e-02,  5.4001e-02,  6.0449e-02,  1.0682e-02,  3.0782e-02, -2.0367e-02,  7.2927e-02, -5.8791e-02, -1.9531e-01, -1.1407e-01, -2.2087e-01, -2.3861e-01,  2.5785e-01,  2.3288e-01],\n        [-2.1769e-01, -1.9467e-01, -2.2085e-01,  2.0819e-01,  2.4724e-01, -1.2069e-01,  2.7966e-02, -1.2793e-01,  1.9167e-01, -1.0623e-01, -1.0552e-01,  1.5205e-01, -1.8923e-01,  1.8443e-01],\n        [ 2.6435e-01, -1.6115e-01, -5.8694e-02, -1.3616e-01, -2.0267e-01,  1.3460e-01, -1.4524e-01, -1.0685e-01,  1.3072e-02,  1.1966e-01,  5.6996e-02, -2.2577e-01, -1.5246e-01,  1.6553e-02],\n        [-4.5582e-02, -1.3384e-01, -5.1853e-02, -2.3395e-02,  2.0564e-01,  9.6476e-02,  1.7790e-01, -1.4999e-01,  2.4095e-01, -3.9416e-02, -2.2585e-01, -1.4065e-01, -1.1385e-01,  5.9013e-02],\n        [-1.4008e-01, -6.0948e-02, -1.0797e-01,  1.6566e-01,  1.9343e-01, -6.5636e-02,  1.1144e-01, -1.7074e-01, -1.9731e-01, -1.6284e-01,  9.9342e-02,  2.2014e-01,  9.0554e-02, -7.9612e-02],\n        [ 6.8599e-02, -2.9517e-02, -1.2543e-02,  6.1727e-02,  7.9328e-02,  4.1486e-02, -2.4220e-01, -2.3449e-01, -1.4548e-01, -2.0165e-01, -8.7532e-02,  2.4224e-02, -1.0442e-01,  1.5918e-01],\n        [-2.3088e-01, -1.1213e-01,  1.3565e-02, -3.8905e-02,  2.2392e-02,  8.5995e-02, -2.0885e-01, -2.1593e-01,  1.5766e-02, -6.9567e-02, -2.0205e-01,  2.6273e-01, -2.5864e-01,  1.5783e-01],\n        [-2.4203e-01,  1.8848e-01,  2.5875e-01,  3.8359e-02, -2.1822e-01,  2.6079e-01,  6.3373e-02, -1.7246e-01, -2.4120e-01, -1.2443e-01,  1.6969e-01,  1.5293e-01,  1.9342e-01,  1.3151e-01],\n        [ 1.4840e-01, -5.3512e-02, -1.2958e-01,  1.4956e-01,  2.4675e-01, -2.6382e-01,  1.5141e-01, -1.9683e-01, -2.3851e-01, -2.5529e-01,  1.7889e-01,  1.1806e-01, -3.0606e-02,  7.1894e-02],\n        [-9.2931e-02,  4.9404e-02, -1.4271e-01, -1.2860e-01,  1.8730e-01, -2.4896e-01,  2.0343e-01,  6.6804e-02,  1.1117e-01, -1.7245e-01, -2.0091e-01, -5.1258e-02, -1.5958e-01,  6.6883e-02],\n        [-3.6342e-03, -1.1888e-01, -2.3000e-01, -8.1049e-02, -7.4994e-02,  1.2702e-01, -7.5010e-02, -8.6664e-02,  2.3672e-01, -1.4803e-01,  2.5846e-01, -6.6433e-02, -1.9753e-01,  4.6753e-02],\n        [-4.2523e-02, -2.3938e-01, -1.8399e-01, -1.9031e-01, -6.4995e-02,  1.4967e-01,  2.7693e-02,  1.1299e-01,  6.5770e-03,  1.7275e-02,  1.8161e-01, -1.8341e-01,  1.4960e-01, -5.5291e-02],\n        [-1.4280e-01, -1.5165e-02, -1.7213e-02,  1.2556e-01,  2.5991e-01,  1.6824e-01,  1.8597e-01, -5.9003e-02, -2.9322e-02,  1.2907e-02,  1.3162e-02,  4.6169e-02,  1.1250e-01,  2.5982e-01],\n        [ 1.6373e-01,  2.0915e-01, -2.2758e-01, -1.5937e-02, -1.7300e-01,  2.1919e-01,  1.3322e-01,  1.8472e-01,  1.6783e-02, -1.5914e-02,  1.4442e-01, -2.2462e-02, -1.9046e-01,  8.3786e-02],\n        [ 6.1689e-02, -1.8415e-01,  7.6051e-02, -9.7774e-02,  1.7931e-01, -2.1380e-01, -2.0890e-01,  2.4853e-01, -2.3248e-01, -1.1049e-01, -1.8035e-01,  7.9348e-02,  1.1449e-01, -2.5852e-01],\n        [-1.2657e-01,  1.4370e-01,  2.7482e-02, -8.2529e-02, -2.0105e-01, -2.2155e-02,  2.5973e-01, -9.8722e-02, -7.4790e-02, -1.1373e-01,  8.6973e-02,  4.8636e-02, -2.5568e-01,  1.8988e-01],\n        [ 2.4622e-01,  2.3436e-01,  1.6793e-01,  1.0146e-01,  1.2978e-01, -2.1691e-01, -1.9426e-01,  1.4545e-01, -1.5585e-01, -2.6642e-02, -1.4367e-02,  1.3757e-01,  9.5463e-02,  7.1993e-02],\n        [ 2.4203e-01,  1.3930e-01, -2.0924e-01,  2.2749e-01, -8.3612e-03,  1.6951e-01, -1.6377e-01, -4.2346e-02,  1.1636e-01,  1.8325e-01, -8.2442e-03, -2.6544e-02, -1.4090e-01,  1.4535e-01],\n        [-1.3071e-01, -5.4280e-02,  3.7424e-02,  7.2476e-02, -2.0308e-01,  1.4232e-01, -1.8316e-01, -5.7820e-02,  2.2732e-01,  2.2849e-01, -8.4157e-02, -1.1668e-01,  1.0013e-01, -2.6272e-02],\n        [ 2.2691e-01,  1.5536e-01, -2.8465e-02, -2.1954e-01,  4.2399e-02, -2.8224e-02,  1.8930e-01,  9.7971e-02, -2.2026e-01, -2.6559e-01,  1.6834e-01, -1.2599e-01, -6.9512e-02, -1.2455e-01],\n        [-1.9015e-01,  6.6941e-02, -2.4686e-01,  1.2988e-02, -1.1713e-01, -1.1958e-01,  8.5153e-02, -2.2490e-01,  1.1439e-01, -4.8339e-02,  1.2771e-01, -9.2241e-02,  2.2227e-01,  2.5271e-01],\n        [ 7.3398e-02,  2.3461e-01,  2.4883e-01,  1.6483e-01,  5.3549e-02, -2.8330e-02,  1.4723e-01,  6.1556e-02,  1.8497e-01, -7.8922e-02,  2.5406e-01, -4.8237e-02, -1.6001e-01, -2.4609e-01],\n        [ 1.8716e-01, -1.1580e-01, -1.2047e-01,  4.9080e-02,  3.1525e-02, -1.7154e-02, -1.1323e-01,  1.9874e-01, -2.4203e-01, -1.0215e-01,  1.4901e-01,  1.5127e-01, -2.3528e-01,  1.9222e-01],\n        [-7.4011e-02, -2.6061e-01, -7.0866e-03,  2.2233e-01,  2.6385e-01, -5.5165e-02, -1.8070e-01, -8.0779e-02,  2.2816e-01, -2.0118e-01, -2.5873e-01, -2.5334e-01,  2.5069e-01,  7.1616e-02],\n        [ 4.1554e-02,  2.4082e-01,  5.9066e-02, -4.3367e-03, -1.4714e-01,  6.4686e-02, -5.5288e-02, -2.1474e-01,  2.1138e-01, -6.8100e-02,  1.3322e-01,  1.4630e-01, -1.9433e-01, -2.3570e-01],\n        [ 1.0990e-01,  4.7566e-02, -1.5365e-01,  1.2958e-01,  5.8202e-02, -1.5336e-01,  2.3651e-01, -3.7196e-02,  2.4862e-01,  1.3282e-01, -1.3809e-01, -1.1831e-01, -9.5302e-02, -7.8520e-02],\n        [-4.7647e-02,  7.6545e-02, -2.4954e-01, -1.2168e-01,  1.5986e-01,  1.3980e-01, -7.9201e-02,  9.1009e-02, -1.9708e-01,  2.4765e-01,  1.4095e-01, -4.4434e-02,  2.3198e-02,  9.4411e-02],\n        [ 2.4968e-01, -2.0422e-01,  1.1926e-01, -2.5018e-01,  2.6581e-01, -2.5359e-01, -5.7855e-02,  1.2701e-01, -1.8999e-01,  1.7625e-02,  4.0771e-02, -1.6877e-01,  6.2684e-02, -1.6601e-02],\n        [ 2.3074e-01, -9.9069e-02, -2.4355e-01,  1.2013e-01, -3.3255e-02, -1.9065e-01, -9.5171e-02, -1.2313e-01,  3.8655e-02, -1.8345e-01,  8.2686e-02, -1.1239e-01,  1.3616e-01, -7.4157e-02],\n        [ 1.7622e-01, -3.1889e-02, -1.4642e-01,  1.6093e-02, -1.9461e-01, -2.3436e-01, -1.2888e-01,  7.2294e-02, -8.7189e-02,  9.6257e-02, -3.2830e-02,  2.0093e-01,  8.6918e-02, -2.3800e-01],\n        [ 2.2813e-01,  2.6071e-01,  1.4278e-01,  4.4236e-02, -1.0117e-02, -1.4910e-01,  3.9745e-04,  1.9567e-01,  1.4874e-01, -2.5188e-01,  1.2474e-01, -7.4637e-02, -1.6746e-01, -1.5933e-01],\n        [ 7.0991e-02,  7.6508e-02, -1.0986e-01, -2.3724e-01,  2.3066e-01, -1.3108e-01, -2.5856e-01, -6.0556e-02, -2.3238e-01, -1.3691e-01,  1.1292e-01, -3.6803e-02,  9.9152e-02,  9.5002e-03],\n        [ 1.5215e-01,  5.2507e-03, -2.3008e-01,  1.4053e-01, -1.5939e-01, -2.4822e-01, -2.5844e-01,  1.7849e-01,  1.4513e-02,  2.0850e-02, -1.1481e-01,  1.1127e-01,  2.1664e-01,  2.6752e-02],\n        [-7.5163e-03, -1.0458e-01, -2.5053e-01, -2.3096e-01, -7.3485e-02,  1.9507e-01,  9.6929e-02,  3.3864e-02, -8.6509e-03,  1.4994e-01, -5.1554e-02,  6.3963e-02,  1.6381e-02, -1.2258e-01],\n        [-1.3234e-01,  2.0109e-01, -8.4703e-02, -6.6472e-02, -1.8785e-01, -6.5490e-03, -1.6908e-01, -2.6266e-01, -2.2613e-01,  2.1024e-01, -2.0476e-01, -2.0466e-01, -7.4219e-02,  2.9670e-02],\n        [-1.8723e-01, -2.1120e-01, -2.4596e-01, -2.5523e-01, -1.7665e-01,  4.3822e-02,  2.2140e-01,  2.0921e-01,  2.0731e-01,  8.7267e-02, -2.4573e-01,  1.3511e-01, -1.8690e-01,  9.0674e-02],\n        [-1.9842e-01, -6.1489e-02,  9.4008e-02,  2.6431e-01, -1.2080e-01, -7.9937e-03,  9.4896e-02,  6.4246e-02,  2.3511e-01, -4.4704e-02, -7.4007e-02,  2.3827e-01, -9.5341e-02,  2.0896e-01],\n        [ 3.5318e-02,  4.3722e-02, -2.3936e-03, -2.1595e-01, -1.0368e-01,  2.0932e-01,  2.1419e-01,  2.5102e-01, -2.3532e-01, -1.0330e-01, -5.5252e-02,  1.4972e-01,  1.6707e-01,  1.8409e-01],\n        [-1.8579e-02,  1.5475e-01, -5.0108e-02,  1.9243e-01,  2.0630e-01,  1.4730e-01,  2.3943e-02,  1.7410e-02,  1.8937e-01, -2.4700e-01,  2.2763e-01,  1.3558e-01,  1.5818e-01, -3.8498e-03],\n        [-6.3484e-02,  1.1845e-01,  6.6987e-02, -1.0904e-01,  2.2651e-01, -2.3530e-01, -1.6054e-01, -1.8717e-01,  1.6088e-01,  1.8146e-01, -2.3876e-01, -1.1436e-01, -1.9916e-02, -1.0679e-01],\n        [-8.4777e-02, -6.9255e-02,  6.0744e-02,  1.3853e-01,  6.4961e-02, -5.4557e-02,  4.3424e-02,  1.3190e-01,  1.2284e-01,  2.3765e-01, -1.6042e-01,  2.4962e-01, -8.7173e-02, -3.6235e-02],\n        [ 1.5727e-01,  2.2599e-01,  1.4633e-01,  2.2579e-01,  1.4049e-01,  2.5047e-01, -2.5560e-01,  2.6243e-01, -1.1231e-01, -8.4626e-02, -7.3174e-02, -2.0635e-01,  2.5321e-01,  1.6194e-01],\n        [ 1.1777e-01, -7.1488e-02,  2.3560e-01, -1.4298e-01,  1.1113e-01, -2.6442e-01,  1.4153e-01, -1.0792e-01,  4.8118e-02, -1.0869e-01,  2.3117e-01,  1.9292e-01, -4.1583e-02,  4.1203e-02],\n        [-1.9945e-01, -1.0219e-02, -1.2874e-01, -1.2287e-01, -2.4337e-01, -7.4963e-02,  4.3414e-02, -3.9199e-02,  4.0003e-02, -1.3580e-01, -6.4209e-02, -2.0268e-01, -1.4834e-01, -1.1504e-01],\n        [ 1.8574e-01,  1.2051e-01, -1.2990e-01, -2.3421e-01,  3.9131e-02,  7.1062e-02, -2.3453e-01, -2.0047e-01, -2.4566e-01,  1.7141e-02,  2.2286e-01,  2.6077e-01, -8.3391e-02, -1.2293e-01],\n        [-1.5021e-01, -1.4042e-01, -2.4296e-01, -2.0247e-01, -1.2701e-02, -2.8961e-02,  3.2666e-02,  1.9673e-01, -1.6579e-01, -5.0577e-02, -1.9324e-01,  2.5563e-01, -8.1269e-02,  2.5794e-02],\n        [ 1.3063e-01,  2.5552e-01,  2.2298e-01, -3.9157e-02, -1.9263e-01,  3.5086e-02, -1.3620e-01,  2.3202e-01, -3.5596e-02,  1.8565e-01,  1.5401e-01, -1.2670e-01,  2.4527e-01,  7.3167e-02],\n        [-2.0001e-01, -2.0377e-01,  2.2540e-01, -5.3584e-02, -1.0351e-01, -1.6212e-01,  1.7903e-01, -6.3018e-02, -1.3569e-01, -8.5005e-02, -7.3268e-02, -1.2688e-01,  6.5167e-02, -1.9367e-01],\n        [-1.2251e-01,  2.3051e-01, -1.8807e-02,  1.9249e-01, -2.4790e-01, -2.4989e-03,  2.6105e-01,  2.0965e-01, -1.5303e-01, -9.4634e-02, -2.2484e-01, -6.6778e-02,  2.5098e-01, -1.0232e-01],\n        [ 7.2473e-03,  2.0945e-01,  2.0346e-01, -1.3150e-02, -7.4597e-04,  2.1330e-01,  2.1114e-01, -1.1722e-01, -1.0338e-01, -1.9544e-01,  1.5140e-01, -2.0582e-01,  2.2852e-01,  2.0613e-01],\n        [ 1.8938e-01,  2.6577e-01,  4.2217e-02, -3.8190e-02, -1.2100e-01, -2.3604e-01,  1.6086e-02, -2.0653e-01,  4.6538e-02, -2.0637e-01,  2.5423e-01,  7.8756e-02, -1.3921e-01, -4.1563e-02],\n        [-6.9994e-03,  1.4631e-01,  1.3239e-03, -1.8698e-01, -2.3970e-01,  6.0259e-02, -1.7089e-01, -9.8192e-02, -1.0318e-01,  1.1418e-01,  1.2785e-02,  1.4957e-01,  1.2449e-01, -4.5798e-02],\n        [-1.0443e-01, -2.2221e-02, -1.5409e-01, -6.0864e-02,  1.8190e-01,  2.0598e-01,  1.2107e-01, -1.7605e-01,  1.1385e-01,  5.2456e-02, -5.7826e-02, -2.3309e-04, -1.2125e-01, -2.2816e-01],\n        [ 6.9245e-02, -1.3374e-01, -1.9962e-01, -1.3025e-01, -1.4992e-01, -1.5825e-01, -1.9828e-01, -7.6319e-02,  1.2375e-01,  2.1710e-01, -1.6112e-02, -1.6145e-01,  5.7608e-02,  2.4267e-01],\n        [ 2.3023e-01, -1.6306e-01,  1.7837e-01, -1.1181e-01,  1.4998e-01, -9.5679e-02,  1.5946e-01, -1.5355e-01, -1.7931e-01,  6.6538e-02,  1.7945e-01,  4.4226e-02,  1.7428e-02, -1.3702e-01],\n        [-7.2007e-02, -6.4663e-02, -1.3125e-01,  1.0855e-01,  4.0699e-02,  4.9082e-02,  1.1887e-01,  2.6860e-02, -1.8597e-01,  8.3919e-02, -1.7469e-01, -9.1124e-02, -2.3602e-01, -4.9485e-02],\n        [-2.1013e-01,  2.2203e-01,  9.1177e-02,  1.1996e-01,  2.4536e-01, -1.9189e-01,  1.1691e-01,  2.6401e-01,  2.4283e-01, -6.9615e-02,  6.6897e-02, -2.0273e-01, -2.0126e-01,  1.0138e-01],\n        [-5.2776e-02,  9.4355e-02,  9.9565e-03,  3.5904e-02, -1.2135e-01, -1.1042e-01,  4.2863e-02,  1.6086e-01, -2.3643e-01, -2.6102e-01,  2.1193e-02,  2.0925e-01,  1.4643e-01, -6.4496e-02],\n        [ 6.5777e-03,  4.2595e-02,  9.7365e-02,  1.0336e-01,  9.1699e-02,  2.2691e-01,  1.9578e-01,  1.9214e-01, -1.1765e-01,  3.6476e-02,  8.3065e-02,  6.0054e-02, -1.4436e-01,  2.4946e-01],\n        [ 1.9996e-02,  5.7439e-03, -1.4725e-01,  2.4337e-02, -5.0627e-02, -1.6713e-01,  1.5477e-02, -1.4666e-01, -8.7980e-02, -1.0875e-01,  2.0917e-01,  1.6303e-02,  3.9693e-02,  7.8771e-04],\n        [-1.0577e-01,  3.8314e-02, -2.3826e-01, -1.6226e-01, -1.0160e-01,  1.3829e-01,  3.8294e-02, -7.1228e-02, -1.7692e-01, -2.0232e-02,  1.2171e-01, -8.3409e-02,  2.6472e-03,  1.9892e-01],\n        [-1.7705e-01,  1.3151e-02, -1.2702e-01, -1.9249e-01,  6.1552e-02, -8.3877e-02, -2.5893e-01,  2.3352e-01, -2.3784e-01,  1.4118e-01, -1.4737e-02, -4.3925e-02, -7.8738e-02, -8.3289e-02],\n        [-3.9807e-02,  1.4461e-01, -3.1960e-02,  2.1157e-01, -1.1600e-01,  4.3143e-02,  2.4766e-01, -2.1710e-01,  9.0603e-02,  1.1067e-01,  1.9939e-01,  1.2424e-01, -1.8108e-01,  1.6260e-01],\n        [-2.2550e-01,  9.6627e-02,  1.4593e-02,  2.4163e-01,  1.2218e-01,  1.2149e-01, -2.3872e-01,  1.5798e-01, -1.4628e-01,  2.2435e-01,  1.6379e-01, -1.4734e-03, -1.5590e-01, -5.9760e-02],\n        [ 6.4785e-02, -9.1137e-02, -2.0369e-01, -1.9059e-01,  1.2830e-01,  1.3269e-01,  1.4201e-01, -1.3390e-01, -7.1134e-02, -2.6031e-01,  1.0791e-01,  5.2839e-02, -2.6286e-01,  1.4609e-01],\n        [ 7.5976e-02, -6.1524e-02,  1.8912e-01,  1.2202e-01,  2.5301e-01, -1.2069e-01, -1.7094e-01,  1.4570e-01,  1.4938e-01,  7.5593e-02, -2.0579e-01, -2.3912e-01,  1.6821e-01,  1.3688e-01],\n        [ 2.3721e-01,  1.6067e-01, -6.3022e-04, -4.4390e-02, -2.2718e-02, -1.4717e-01,  4.9494e-02,  1.2460e-01, -2.3110e-01,  1.7601e-01,  1.2235e-01, -2.2959e-01,  2.4595e-01, -1.8485e-01],\n        [ 2.5507e-01, -2.3064e-01, -1.5905e-01,  1.0677e-01,  1.8347e-01,  1.3671e-01, -8.3509e-02,  1.6818e-01,  1.4647e-02,  8.9079e-02, -5.1329e-02, -9.0536e-02, -5.3381e-03,  4.3440e-02],\n        [-2.0950e-01,  1.5023e-01, -1.9563e-01, -2.6420e-01, -1.6363e-01,  1.0680e-01, -1.5378e-02, -1.4684e-01,  1.6676e-01,  1.7964e-01,  8.8137e-02, -1.8767e-01, -1.6086e-01,  7.2721e-02],\n        [ 1.3580e-01, -2.2859e-02,  1.1944e-02, -1.2498e-01, -2.3883e-01, -8.4924e-02, -6.9839e-02,  1.7264e-01, -2.0857e-01, -1.6442e-02,  1.6885e-01, -1.9184e-01,  1.5577e-01,  7.2382e-03],\n        [-1.2448e-01, -8.1762e-03,  1.2390e-01,  2.0491e-02,  1.3176e-01,  1.7531e-01,  8.1843e-02, -8.0753e-02,  4.5661e-02,  2.1315e-02,  3.1749e-02,  9.6452e-02,  2.5909e-01, -9.9386e-02],\n        [-2.3308e-01,  4.5936e-02,  1.5450e-01,  2.2651e-01, -8.8565e-02, -2.6573e-01,  1.8734e-01, -2.1272e-01, -1.2807e-01,  8.9025e-02, -2.3579e-01,  6.8341e-02, -3.8138e-02,  2.0649e-01],\n        [ 3.8578e-02, -1.3222e-01,  8.8188e-02, -3.7147e-02, -1.5204e-01, -2.0576e-01, -4.3762e-02,  2.4243e-01,  7.1138e-03,  5.7459e-02,  1.4815e-01, -2.2340e-01,  4.9107e-02,  1.2374e-01],\n        [-1.0563e-01, -1.0693e-02,  9.0080e-02,  1.2588e-01, -1.4146e-01,  1.1751e-01, -1.7320e-01, -7.0330e-02, -1.1270e-01, -9.4970e-02, -9.0582e-02,  1.7076e-01, -2.4564e-01, -1.2905e-01],\n        [ 1.9264e-01,  9.3016e-02, -1.2250e-01,  2.4911e-02,  1.5797e-03,  2.1693e-01,  1.1922e-02,  1.0305e-01,  3.8691e-02, -2.5755e-01,  7.1416e-02,  1.7016e-01,  1.7094e-01, -1.6340e-01],\n        [-1.5279e-01,  8.3943e-02, -2.3559e-01, -2.0300e-01, -5.1840e-02, -5.4837e-02,  1.4755e-01, -2.6304e-01,  2.1755e-01, -1.8096e-01,  1.2167e-01,  2.4853e-02, -1.3131e-01, -2.1128e-01],\n        [ 1.6228e-01,  7.4240e-02, -1.2144e-01, -1.2375e-02, -1.0920e-01, -3.9727e-02, -2.5559e-01, -2.2331e-01, -4.5390e-02,  3.0779e-02, -2.2411e-02, -2.2450e-01,  1.3113e-01, -1.1552e-01],\n        [ 2.0371e-01, -3.4799e-03, -2.4227e-01, -8.1184e-02,  1.1675e-01, -2.5917e-01,  1.3953e-01, -2.6238e-01,  1.2479e-01,  1.7966e-01,  5.4619e-02,  2.0330e-01,  1.5335e-01,  9.8197e-02],\n        [ 1.9710e-01,  3.1046e-02,  1.2289e-01, -1.1759e-01, -2.1113e-01, -1.9702e-01,  1.2093e-01, -1.6128e-01,  1.0892e-01,  1.6576e-01, -1.5035e-01, -2.0919e-01,  6.1572e-02,  1.2779e-01],\n        [ 2.5023e-01,  6.3988e-03, -1.3308e-01, -1.0299e-01, -2.5600e-01,  2.0440e-01, -9.5215e-02, -7.3191e-02,  2.4780e-01, -1.2990e-01,  2.1546e-01, -1.0146e-01,  2.5639e-01, -2.4033e-01],\n        [-1.3248e-01, -1.7196e-01, -1.9961e-01,  1.7453e-01, -2.0525e-01, -7.5348e-02,  1.1522e-01,  2.3977e-01,  8.5570e-02,  2.1266e-01,  1.8744e-01, -2.3044e-01, -1.6734e-01,  2.8281e-02],\n        [-2.0666e-01, -2.2474e-01, -1.4097e-01,  1.0460e-01,  1.1270e-01,  6.8164e-03,  7.8519e-02, -2.0193e-01, -6.8585e-04,  1.1141e-01, -2.4910e-01,  2.2728e-01, -1.2093e-01,  3.0879e-02],\n        [-2.4441e-01, -5.9238e-03,  2.2969e-01, -1.6760e-01, -1.0945e-01, -7.0920e-02, -3.8153e-02, -1.8882e-01, -4.8192e-02,  1.3328e-01, -6.2716e-02, -2.3412e-01, -7.6011e-02, -2.2622e-01],\n        [-2.2862e-03, -1.7374e-01, -2.2061e-01, -1.2598e-01, -7.1027e-02, -2.4300e-01, -1.5966e-01,  8.3631e-02,  2.6708e-01,  1.0739e-01, -2.6276e-01,  1.5994e-01, -3.4898e-02, -1.2729e-01],\n        [ 1.0795e-01,  1.2042e-01,  2.3821e-02,  2.6122e-01,  1.8888e-01, -1.7046e-01, -2.2491e-01, -2.4204e-01, -1.7057e-01,  1.0444e-01, -2.4277e-01,  2.5732e-01,  1.7554e-01,  1.2278e-01],\n        [-9.6079e-03, -9.2674e-02, -1.1972e-01, -1.1167e-01, -2.4090e-01, -1.3514e-01, -2.1668e-01,  1.0417e-01,  1.2832e-01, -7.5477e-02, -2.9889e-02,  8.8611e-03,  4.5976e-02,  1.2048e-01],\n        [ 2.2247e-01,  1.2275e-01, -1.2408e-01, -1.7744e-01, -1.2925e-01,  2.3561e-01,  2.5412e-01, -2.9550e-02, -2.6409e-01,  2.6336e-01,  4.2229e-02,  1.7468e-01,  1.5176e-01, -2.2167e-01],\n        [ 1.0219e-01, -6.2100e-02, -1.4073e-01,  1.3899e-01,  1.9666e-03, -1.5421e-01, -2.4871e-01, -1.9173e-01,  4.2965e-02,  6.6910e-02,  1.6282e-01,  3.0435e-02, -1.9746e-01, -1.9630e-01],\n        [ 2.0543e-01, -6.8825e-02, -3.4497e-02, -6.9275e-02, -2.6091e-01, -1.2940e-01, -9.1812e-02, -2.1075e-01, -1.9415e-01,  2.4607e-01,  1.4488e-01,  1.5396e-02, -1.5373e-01,  1.7130e-01],\n        [ 1.7952e-01, -1.0761e-01, -1.2557e-01, -2.0501e-01,  2.6664e-01, -3.3632e-03, -1.3708e-01, -1.7486e-01,  9.4556e-03,  1.9999e-01, -1.5540e-01, -3.6654e-02, -1.9894e-01, -2.0212e-02],\n        [-1.6242e-01,  6.1995e-02, -7.4084e-02, -1.9971e-01, -6.8682e-02,  7.7349e-02, -2.2755e-01,  1.2280e-01,  6.8776e-02, -5.2450e-02,  9.5825e-02,  1.7581e-01,  2.0088e-01,  3.1043e-02],\n        [ 1.4666e-01,  1.8769e-01,  1.9385e-01, -1.1955e-01, -1.7375e-01,  1.0796e-02, -2.6549e-01, -2.0245e-01,  1.5400e-01,  1.3227e-01,  6.1051e-02,  1.2371e-03,  1.0003e-01, -4.5400e-02],\n        [ 1.9884e-01,  1.6226e-01,  7.0984e-02,  8.1227e-02,  2.5262e-01,  1.3845e-01, -2.0987e-01, -1.2825e-01, -1.2929e-01,  1.1677e-01, -1.9689e-01, -2.4409e-01,  9.1868e-02, -1.3023e-01],\n        [ 2.1603e-01, -2.2594e-01, -2.6954e-02,  1.4617e-01, -1.3876e-01, -7.1836e-02, -7.8787e-02, -8.3511e-02, -2.4120e-01, -3.6273e-02,  6.3343e-02, -1.7394e-01,  1.4076e-01,  1.7417e-01],\n        [ 1.8491e-01,  2.8242e-02,  2.5899e-01, -9.2788e-02,  2.3401e-01, -1.7580e-01,  6.0131e-02,  1.9628e-01,  2.4850e-01, -5.5481e-03, -8.5897e-02, -9.1912e-02, -1.7118e-01, -4.6194e-02],\n        [ 4.7772e-02,  2.6508e-01,  8.2142e-02, -1.8528e-01,  1.2873e-01, -3.0241e-02,  6.8875e-02,  2.2756e-01,  1.7226e-01,  1.9724e-01, -2.6250e-01,  2.1840e-01,  1.2002e-01, -8.6343e-02],\n        [ 1.2465e-01,  2.4131e-01, -1.5570e-01, -2.1381e-01, -2.1867e-02, -1.0111e-01,  1.6568e-01,  2.5593e-02,  1.4445e-01,  4.3323e-02, -4.0248e-03, -1.5725e-01,  1.8089e-01, -2.0931e-01],\n        [-1.1668e-01, -1.0806e-01, -1.0757e-01,  1.0905e-01,  1.6603e-01,  2.1582e-01, -1.3685e-01,  2.5742e-01, -2.3116e-01,  5.5366e-02, -1.5175e-01,  2.4403e-01,  9.8986e-02, -1.3964e-01],\n        [ 1.2039e-01, -4.9258e-02, -4.6121e-02, -1.9578e-01,  3.8057e-02, -1.7910e-02, -3.8382e-02, -1.0839e-01,  1.8149e-01,  2.0677e-01, -2.0098e-01,  1.1967e-01,  6.0326e-02,  8.6307e-02],\n        [-7.2584e-02, -4.7317e-02, -1.6750e-01,  2.0738e-01, -5.4741e-02, -8.5918e-02, -9.2571e-02,  2.3680e-01,  1.6240e-01,  5.8804e-02, -2.3096e-01, -2.5568e-02, -1.3497e-01,  1.0545e-01],\n        [-6.7763e-02,  1.6689e-01,  2.5465e-01, -5.0836e-02, -5.4863e-02,  2.4506e-01,  2.0301e-01, -2.1226e-01,  2.0897e-01,  6.4043e-02, -1.5333e-01, -2.0651e-01, -1.0902e-01,  2.2293e-01],\n        [-1.6584e-01, -1.6429e-01,  1.9455e-01, -1.3158e-01,  1.9215e-01,  1.7213e-01,  1.6985e-01, -3.3825e-02, -2.7936e-02,  1.5241e-01,  1.6011e-01,  1.6984e-01,  1.3082e-01,  1.0094e-01],\n        [-1.3236e-01,  1.7401e-01, -2.5560e-01,  1.1195e-01,  2.6039e-01,  1.0787e-01, -7.9341e-02,  3.2469e-02,  1.9007e-01,  1.8206e-01,  1.5087e-01,  2.2458e-01, -1.5793e-01, -2.2769e-01],\n        [-1.8827e-01,  2.2518e-01, -6.6371e-02, -2.2354e-01,  1.1770e-01,  1.3583e-01, -1.7404e-01, -1.4652e-01, -1.0531e-01, -5.1078e-02, -4.8799e-02,  2.9010e-02,  3.0704e-02, -2.6815e-02],\n        [ 8.1847e-02,  5.5278e-02,  1.3892e-01, -1.8284e-01, -2.0892e-01,  2.3369e-01,  2.4992e-02, -5.6507e-02,  1.0455e-01, -5.6112e-02,  1.7350e-01, -1.3406e-01,  1.8973e-01, -6.3403e-02],\n        [-1.1813e-01, -2.4083e-01, -1.2543e-01,  1.7954e-01,  1.7486e-01,  1.9590e-01, -2.2716e-01, -6.0273e-02, -1.9156e-01,  3.2977e-02,  2.5636e-01, -1.4708e-01,  2.0168e-01, -1.1142e-01],\n        [ 1.2416e-01, -2.4803e-01,  1.8231e-01, -5.3862e-02, -1.3906e-01,  2.5305e-02, -2.5450e-02, -1.4054e-01, -2.4530e-01, -2.1860e-01, -1.8341e-01,  4.7644e-02,  3.2081e-03,  4.0263e-02],\n        [-1.7695e-02, -1.1350e-01, -1.3456e-01, -1.4105e-01,  3.4822e-02, -1.2009e-01, -1.6525e-01, -5.3784e-02,  1.9204e-01, -1.6864e-02, -1.5957e-01, -1.2264e-01, -1.2954e-01,  6.6631e-02],\n        [-2.2794e-01,  1.3694e-01,  6.5968e-02, -1.8780e-01,  1.9390e-01, -1.3159e-01,  6.1797e-02,  6.9437e-02,  5.3063e-03,  1.1901e-01, -7.1585e-03, -1.9073e-01, -6.7870e-02, -2.2267e-01],\n        [ 4.9362e-02,  2.2311e-01,  1.4550e-01,  1.5661e-01, -8.8782e-02, -4.1312e-02,  1.1107e-01,  3.5698e-02, -9.0618e-02,  2.1582e-02, -1.6853e-01,  2.2667e-01,  1.4157e-02, -1.1168e-01],\n        [-8.9342e-02, -1.8679e-02, -7.1360e-02,  2.5265e-01, -2.3730e-02, -6.8134e-02,  1.7619e-01,  2.5404e-01,  1.1951e-01,  1.7243e-02, -1.8986e-01,  2.4338e-01,  1.4619e-01,  7.6277e-02],\n        [-2.6083e-01, -2.7445e-03,  1.5834e-01, -5.3213e-02, -6.1440e-03,  5.7130e-02, -1.9877e-01,  4.5972e-02, -1.6608e-01, -1.2236e-01,  1.6971e-01, -2.7228e-02,  1.9776e-01,  1.8884e-01],\n        [-2.6490e-01, -7.8194e-02, -1.0118e-01,  1.2010e-01,  2.9830e-02,  8.0616e-02,  1.9931e-01,  5.9369e-02, -8.7289e-02,  2.6143e-01,  1.7109e-01, -9.3361e-02, -2.6691e-01,  1.2716e-01],\n        [ 2.2030e-02,  5.9327e-02,  1.4068e-01, -2.6086e-01, -2.3787e-02,  1.2645e-01,  1.9124e-01, -1.3291e-01, -2.2763e-01,  3.4755e-02, -2.6184e-01,  1.5423e-01,  6.6759e-02,  2.2009e-01],\n        [-8.5223e-03,  2.3116e-01, -2.6242e-01, -1.2513e-01,  1.6103e-01,  5.9609e-02,  1.6916e-01, -1.0412e-01, -1.6832e-01, -5.0763e-02,  1.9344e-02, -7.9187e-02,  2.3323e-01,  1.9894e-01],\n        [ 1.6263e-02, -1.2010e-01,  1.6650e-01,  2.5802e-01,  3.7646e-02, -3.0660e-02,  2.3666e-01, -1.4550e-01,  2.4348e-01, -9.8490e-02, -2.5486e-01,  1.1287e-01, -1.4766e-01,  1.7771e-01],\n        [-9.2567e-03, -8.8531e-03, -1.2575e-02,  1.0631e-01,  6.6465e-02, -3.9495e-02,  7.3386e-02, -1.1981e-03,  1.6855e-01, -1.9895e-01,  2.1603e-02, -1.8023e-01, -1.0558e-01, -3.9665e-02],\n        [-2.3430e-02,  9.4824e-02, -2.1095e-01,  1.8616e-01, -4.8726e-02,  4.7642e-02,  1.5115e-01,  7.9209e-02,  2.5024e-01,  1.4076e-01,  2.1847e-02, -8.1738e-02,  2.3697e-02,  2.3819e-01],\n        [ 2.5633e-02,  1.1893e-01,  2.6547e-01, -3.6613e-02, -4.0141e-02, -4.2258e-02,  2.2881e-01, -1.2028e-01,  1.4904e-01,  2.6323e-01, -1.4704e-01, -2.1745e-01,  2.5587e-01, -1.7614e-01],\n        [ 2.8867e-02, -6.4791e-02, -2.6679e-01, -2.2921e-01, -7.4239e-02,  2.5328e-01,  2.5173e-01,  1.4430e-01, -2.5261e-01,  4.7156e-02, -2.5967e-01, -8.0600e-02,  2.6269e-01, -3.0192e-03],\n        [-1.2725e-01, -1.2478e-01,  1.1390e-01, -2.5424e-01, -2.5885e-01,  8.0597e-02,  1.4553e-01,  2.7734e-02, -2.5027e-01,  1.9723e-01, -2.1761e-01, -1.3642e-03,  3.8124e-02,  1.1260e-01],\n        [ 6.0573e-02, -8.8607e-02,  2.4864e-01, -1.1790e-01, -3.7451e-02, -1.4962e-01, -3.1283e-02, -1.2437e-01,  3.6715e-02,  2.6572e-01,  1.8333e-02,  1.2395e-01, -6.6201e-02,  1.1758e-01],\n        [-2.0954e-01,  1.9372e-01,  1.8853e-01, -1.5551e-01,  1.1189e-01, -1.2388e-01, -1.4015e-01,  7.4573e-02,  1.9055e-01, -1.6099e-01,  9.9349e-03,  1.3122e-01, -4.1953e-02,  4.1355e-02],\n        [-9.9368e-02, -2.1720e-01, -1.8385e-01,  2.1885e-01, -1.5775e-01,  6.4967e-02,  1.9631e-01,  7.1414e-02, -1.3883e-01, -2.5909e-01,  1.2357e-02, -6.8301e-02,  1.6003e-01,  2.0192e-02],\n        [-8.6032e-03,  1.9260e-02,  2.4041e-01, -2.5516e-01,  1.7511e-01, -4.2583e-03, -1.8171e-01, -1.0716e-01, -1.2556e-01, -2.3472e-01,  2.1121e-01, -6.8059e-03,  1.9128e-01, -2.1731e-01],\n        [ 2.1725e-01, -1.6305e-01,  1.6448e-01, -2.5590e-01, -8.2588e-02, -2.0961e-01, -1.9885e-01, -1.7348e-01,  1.6091e-01, -1.2000e-02,  7.4304e-02, -2.1887e-01, -1.3401e-01, -1.0635e-01],\n        [ 2.3581e-01,  2.5465e-01,  1.5560e-01,  8.5114e-02,  2.0283e-01,  9.4250e-02,  1.7522e-01, -1.7029e-01,  8.2383e-03,  1.1539e-01,  2.4997e-01,  6.8901e-02, -1.9810e-01,  6.9012e-02],\n        [-2.4711e-01, -1.5571e-02,  2.5359e-01, -2.1752e-01, -1.6208e-01, -1.2608e-01, -2.3721e-01,  1.0268e-01,  2.2991e-01, -1.6486e-01, -1.8286e-01,  1.6754e-01, -1.5640e-01, -1.8247e-01],\n        [-1.8398e-01, -2.5132e-01, -2.5471e-01,  2.1744e-01,  4.2196e-03, -2.3893e-01,  1.1568e-01, -1.5888e-01,  1.8231e-01,  1.5036e-01,  1.2234e-01,  1.1888e-01, -2.3069e-01,  2.2583e-01],\n        [-2.5723e-02,  1.8289e-01,  8.7658e-02,  1.7609e-01,  8.1270e-02, -9.9940e-02,  1.1096e-01, -1.7591e-01, -3.3562e-02, -1.3950e-01,  9.8726e-02, -1.0882e-01,  1.9625e-01, -2.4152e-01],\n        [ 2.0045e-01, -4.8366e-02,  7.1117e-02, -8.0124e-02,  2.6701e-01,  1.8793e-01,  4.8526e-02, -2.1202e-01,  2.7502e-02, -6.5537e-02,  1.4715e-01,  3.5134e-02,  8.7970e-02,  2.9261e-02],\n        [-1.3322e-01, -8.3001e-02,  1.9770e-01,  4.1905e-02, -9.4605e-04,  5.1791e-03, -7.1671e-02, -2.4205e-01,  1.4963e-01,  2.9714e-02, -2.0285e-01,  5.8316e-02, -1.9949e-01, -6.8703e-02],\n        [-1.2798e-01,  2.2238e-01,  1.5773e-01,  2.6191e-01,  1.6490e-01,  1.8418e-01, -1.9943e-01,  1.5606e-01,  1.2601e-01,  1.5783e-01, -2.2283e-01,  4.2033e-02, -2.6475e-01, -1.7350e-01],\n        [-5.6004e-02, -8.9877e-02, -1.1491e-01,  2.1726e-01,  1.3258e-02,  2.0777e-01,  1.8880e-01,  6.5535e-03, -4.1537e-02,  2.6490e-01, -1.1457e-01, -2.2962e-02,  2.5035e-01,  1.5633e-01],\n        [ 2.4149e-01,  4.8311e-02,  2.3844e-01, -1.3014e-01, -1.3305e-01,  2.4200e-01,  3.4086e-02,  1.8218e-01,  1.8252e-01,  2.3617e-01, -2.6337e-01, -2.6717e-01, -1.2022e-01, -2.3367e-01],\n        [ 5.1123e-02, -4.5740e-02, -1.9233e-01, -1.2182e-01, -3.4493e-02, -2.2431e-01,  1.0472e-01, -2.0063e-01, -2.5121e-01,  6.8512e-02,  2.6348e-01, -1.0851e-01,  7.8647e-02, -1.4444e-01],\n        [-1.1207e-01, -9.4249e-02, -1.9725e-02, -8.6318e-02,  5.1633e-02, -1.5829e-01,  2.4184e-02, -1.7884e-01, -4.8204e-02, -2.4781e-01,  1.4595e-01,  8.9887e-02, -9.0346e-02, -1.4979e-01],\n        [-2.3730e-01,  2.3692e-01,  1.5001e-01,  1.8446e-01,  1.4060e-01, -5.9591e-02, -1.2405e-02,  1.0685e-01,  9.6793e-02,  1.0533e-01, -7.9438e-02,  2.2113e-01,  1.0261e-02, -5.3673e-02],\n        [ 1.3869e-01,  7.5913e-02, -7.7104e-02,  9.3547e-02,  1.7361e-01,  1.8923e-01, -2.6080e-01, -1.9909e-01,  2.0245e-01, -6.5663e-02,  1.9007e-01,  1.5870e-02, -1.7752e-01, -7.7258e-02],\n        [ 2.2364e-01,  6.0256e-03, -2.0469e-01,  1.7083e-01,  1.3248e-01,  1.9538e-01,  1.4586e-01, -1.8542e-01, -1.3097e-01, -5.9110e-02,  2.3473e-01,  2.2412e-01, -4.3696e-02, -2.3856e-01],\n        [ 1.1372e-01, -1.3562e-02,  2.0872e-01,  1.8749e-01,  1.3205e-01,  3.9389e-02,  4.9651e-03, -7.4889e-02,  4.7142e-02,  2.5955e-01, -6.0829e-02,  2.3751e-01,  2.0336e-02, -2.0541e-01],\n        [-1.7550e-01,  2.0316e-01, -1.5749e-01,  2.0713e-01, -1.8638e-01,  1.0169e-01,  9.9275e-02, -1.6217e-03,  1.2482e-01,  2.0864e-01, -8.0437e-02, -2.1752e-01, -3.1017e-02,  5.8207e-02],\n        [ 1.6181e-01,  1.3720e-01, -7.1764e-03,  1.2365e-01, -1.5700e-01,  1.8249e-01,  8.9363e-02, -1.5672e-01,  2.1857e-01, -7.8252e-02,  4.8771e-02,  8.4476e-02, -1.3021e-01, -1.1175e-01],\n        [-9.4432e-02, -4.8445e-02, -2.7826e-02, -1.5620e-01, -1.6091e-01, -1.9846e-01,  1.1239e-01, -1.0975e-01,  2.2569e-01,  2.2120e-01, -9.0909e-02, -2.0489e-01, -1.0269e-01,  1.7963e-01],\n        [ 2.1010e-01,  1.6384e-01,  2.5712e-01, -4.1977e-02, -2.3043e-01, -1.5246e-01,  4.7657e-02,  1.3598e-01, -1.9852e-01, -1.7613e-01,  1.5461e-01,  2.1435e-01,  1.1772e-01,  3.8270e-02],\n        [ 1.0114e-01, -7.3514e-03, -1.0477e-01, -1.0967e-01, -2.4946e-01,  1.6152e-01, -2.4764e-01,  1.2254e-01, -8.7808e-02, -1.2591e-01, -9.6234e-02, -2.6710e-01,  2.4250e-01,  9.2574e-02],\n        [ 3.5900e-02,  2.5721e-01, -1.6312e-01, -2.8656e-02, -1.2263e-01,  2.0775e-01, -2.5587e-01, -1.6206e-01, -2.5195e-01, -6.4087e-02,  2.5918e-01,  3.9172e-02, -1.9054e-01,  2.3490e-01],\n        [ 2.0656e-01,  2.1255e-01, -2.4760e-01,  2.6046e-01, -2.5895e-01,  9.4398e-02,  1.4427e-01, -2.6372e-01, -1.4486e-01,  4.3179e-02, -4.1264e-03,  9.4975e-02,  1.5516e-01, -4.5808e-02],\n        [ 3.0802e-02,  2.0278e-02, -2.6126e-01, -1.1443e-01, -1.2140e-01, -2.6554e-01, -7.0128e-02,  1.0265e-01, -1.7817e-01,  2.8785e-02, -5.0033e-02, -1.7222e-01, -1.9391e-01,  1.9114e-01],\n        [ 4.9478e-02, -2.6299e-01,  5.2574e-02,  1.2977e-01, -2.1594e-01,  1.5472e-01, -1.9066e-01, -1.6626e-01,  1.9276e-01, -1.9680e-02,  5.9039e-02,  2.1475e-01,  1.8338e-01, -8.1122e-02],\n        [-1.1387e-01,  1.0204e-01, -9.2799e-03,  1.8018e-01, -7.1998e-02, -2.4989e-01,  1.4133e-01,  6.5925e-02,  1.1346e-01,  1.6993e-01,  1.5714e-01,  1.0452e-01, -1.9489e-01,  1.5155e-01],\n        [ 2.1405e-01, -5.7374e-02, -1.9422e-01, -4.5684e-03, -1.4109e-02, -2.0938e-01, -2.5774e-01, -1.7403e-01, -1.3150e-01,  2.5288e-01, -1.8992e-01, -1.3765e-01,  2.7272e-02, -2.2141e-01],\n        [ 2.6103e-01, -2.4747e-01,  6.1987e-02, -1.9015e-01, -1.9056e-01,  3.2993e-02, -3.6120e-02, -2.2339e-01,  1.8101e-01,  2.3518e-01, -6.1757e-02, -1.3289e-02, -1.6272e-01, -7.7940e-02],\n        [ 7.8634e-03, -1.3550e-01, -2.4372e-01, -1.0304e-01, -2.3789e-01,  5.9518e-02,  1.8496e-01,  1.4535e-01,  2.4698e-01,  9.3401e-02, -7.1184e-03,  7.6368e-02,  2.1548e-01, -3.0600e-02],\n        [-1.8598e-01,  1.5975e-01,  2.5359e-02,  1.0062e-01, -2.4923e-01, -2.1389e-01, -2.3318e-02, -1.9665e-01,  2.7119e-02,  1.2534e-02,  1.6085e-01, -1.9429e-01, -2.2489e-01,  6.8179e-02],\n        [ 6.8799e-02, -2.2668e-01, -9.8223e-02,  1.4967e-01, -2.6284e-01, -1.5201e-03, -2.4525e-01,  1.7807e-01,  3.3767e-02, -2.3928e-01, -2.3629e-01, -1.3840e-01, -1.6919e-01,  1.9543e-03],\n        [-8.8195e-02,  2.5092e-01, -6.7542e-02,  1.6722e-01, -2.0869e-01,  7.2401e-02,  1.2843e-01,  1.6275e-01,  5.5105e-02, -2.1041e-01,  2.3216e-01,  2.0421e-01, -1.3389e-01, -1.2958e-02],\n        [ 1.2941e-01, -1.7332e-01, -2.0368e-01, -2.0578e-01, -2.2907e-02,  4.5565e-02, -5.2272e-02,  7.5727e-02,  3.4529e-02, -6.2617e-02, -5.0594e-05,  5.2566e-03, -5.9258e-02, -1.1111e-02],\n        [-6.4784e-02, -1.2799e-01, -6.2677e-02, -5.2599e-02, -6.1475e-02,  6.0912e-02, -7.9250e-02,  8.4311e-02,  2.5658e-01, -7.4611e-02,  1.1351e-01,  6.1858e-02,  1.1325e-01,  8.3823e-02],\n        [-6.7893e-02,  1.1957e-01,  2.3009e-01,  2.5907e-01,  4.3408e-02,  1.3539e-01,  1.0291e-01,  1.8862e-01, -2.0696e-01,  3.6698e-03, -1.3812e-01, -2.4447e-01,  2.1344e-01, -1.7188e-01],\n        [ 2.6373e-01,  1.2282e-01, -1.7263e-01,  2.0313e-01,  1.8916e-01, -1.4432e-01, -1.9372e-01,  2.6199e-01, -1.5609e-01,  5.5870e-02,  3.3032e-04, -2.1603e-01,  1.9403e-01,  2.5295e-01],\n        [-1.0494e-01,  1.8023e-01, -2.4083e-01,  9.2280e-02, -3.9935e-02, -1.4493e-02,  1.5390e-01, -1.0160e-01,  7.7514e-02, -1.6431e-01,  1.2511e-01,  1.9352e-02,  1.0481e-01,  1.1586e-01],\n        [ 1.2113e-01,  1.7679e-01, -1.4451e-01, -1.7818e-01, -3.5616e-02, -1.8446e-01,  1.4977e-01,  2.5207e-01, -1.2083e-01,  1.8732e-01,  9.2575e-02,  2.1607e-01, -6.6249e-02, -7.2676e-02],\n        [-2.0248e-01, -2.3163e-01,  2.2865e-01, -3.9329e-02,  4.8458e-02, -1.3502e-01, -9.5225e-02, -1.6156e-01, -9.1042e-02,  1.9380e-01,  1.3336e-01,  5.1169e-02,  6.2106e-02, -1.4018e-01],\n        [-9.4785e-02, -5.1971e-02, -4.4245e-02, -1.2912e-01, -4.7103e-02, -2.7343e-02,  2.9560e-02,  2.6576e-02, -1.4225e-02,  1.6176e-01,  8.0580e-02, -2.4538e-01,  1.5240e-01, -1.2843e-01],\n        [ 1.7853e-01, -2.2650e-01,  1.7989e-01, -2.3387e-01,  1.1265e-01, -1.8330e-01, -9.0722e-02,  1.4487e-02, -1.6292e-01,  1.1792e-01, -2.2374e-01, -1.9180e-01, -2.0145e-01, -2.4213e-01],\n        [ 2.5883e-01, -2.3216e-01, -1.8160e-01, -4.8513e-02, -1.0996e-01, -2.6644e-01, -5.8717e-02, -7.5379e-02,  2.5910e-01, -2.4297e-01,  1.7299e-02, -1.9676e-01,  1.1397e-01, -1.3738e-01],\n        [-2.1912e-01, -7.4302e-02, -2.0776e-01,  2.3739e-01,  1.9349e-01, -2.1065e-01,  4.5899e-02,  1.1682e-02, -2.6938e-02, -1.2094e-01, -8.7853e-02,  1.1749e-01, -2.7221e-02, -1.8406e-01],\n        [-6.2694e-02, -2.3321e-01,  1.2418e-01, -7.6957e-02, -7.9063e-02,  2.1458e-01,  1.4951e-01,  7.1500e-02,  3.5579e-02, -4.5353e-02, -9.5293e-02, -1.3774e-01,  9.3415e-02, -4.8349e-02],\n        [-1.9132e-01,  4.3562e-03,  4.1869e-02,  1.2995e-01,  1.6541e-01, -1.2110e-01,  2.5791e-01, -1.9409e-01,  4.3889e-02,  2.6137e-01, -2.2956e-01, -1.9734e-01,  1.0347e-01, -1.3854e-01],\n        [-1.0649e-01,  8.3181e-02,  2.0844e-01,  1.7092e-01, -6.5313e-02,  1.1184e-01, -2.0751e-01, -1.3797e-01,  5.2051e-02,  1.2750e-01, -2.0964e-01, -2.1243e-01, -2.8707e-02, -2.0813e-01],\n        [-1.0896e-02,  4.1002e-03,  2.4324e-01, -2.0357e-01,  1.5801e-02,  9.2722e-02,  2.4701e-01, -1.3317e-01, -1.4876e-01,  1.4263e-01,  4.2820e-02, -1.3936e-01,  1.0393e-01,  1.9346e-01],\n        [ 2.3305e-01, -1.3574e-01,  1.4864e-02, -2.6492e-01,  1.3603e-01, -1.6245e-01, -8.8594e-02,  3.6460e-02, -7.8517e-02, -1.4224e-01,  2.0201e-01, -2.6116e-01,  7.7688e-02, -2.6363e-01],\n        [-4.2191e-02, -1.5067e-01, -2.2647e-01, -9.9444e-02,  1.8968e-01, -8.0800e-02,  1.6140e-02,  3.2650e-02, -1.6400e-01,  1.6964e-01, -1.1798e-01,  2.6121e-01, -1.7259e-01, -5.1049e-02],\n        [ 2.0243e-01, -1.0470e-01,  7.2752e-02, -7.4141e-03,  1.2441e-02,  1.7552e-01, -1.8928e-01,  3.5938e-05, -1.8962e-01, -1.3030e-01, -1.2229e-01,  1.9579e-02, -1.3488e-01,  1.9411e-01],\n        [ 1.7873e-02,  8.8730e-03, -1.5573e-02, -1.0789e-02,  1.6647e-01, -2.0731e-01,  1.4295e-01,  1.3283e-01, -3.4413e-02, -4.2826e-02,  8.5530e-02, -1.4679e-01,  7.7350e-04,  1.7226e-01],\n        [-1.4962e-01,  1.1444e-02, -2.5873e-01, -2.1551e-01,  7.9405e-02, -1.0582e-01, -1.7875e-01,  1.6693e-01, -2.2480e-01, -2.1625e-01, -6.8856e-02, -2.7049e-02,  2.5499e-02,  5.4826e-02],\n        [-1.3385e-01, -1.3593e-01, -1.4769e-01, -2.2735e-01, -1.1377e-01,  1.8205e-01, -1.6863e-01,  3.1769e-03,  2.0915e-01,  1.0348e-02,  2.5101e-01, -2.4974e-01, -4.5076e-03,  8.5004e-02],\n        [ 2.4527e-01, -2.7221e-02, -2.0772e-01, -3.0225e-02, -1.3540e-02, -1.6005e-01,  1.3923e-02, -1.0481e-01, -2.2089e-01, -1.2491e-01, -5.7885e-02, -1.4956e-01,  1.3951e-01, -1.5282e-01],\n        [ 8.5806e-02, -2.0464e-01,  1.7409e-02, -2.5290e-01,  8.7704e-02, -1.1767e-01, -1.0932e-01, -7.3918e-03, -2.0229e-01,  1.2621e-01,  5.3696e-02,  1.3915e-02,  7.9593e-02,  5.4942e-02],\n        [-1.0530e-01, -1.1604e-01, -1.1391e-01,  5.4891e-02, -1.8571e-01, -2.7439e-03,  1.7475e-01, -6.5901e-03,  2.2151e-01,  1.5122e-01, -2.4390e-01, -2.1756e-01,  2.5245e-01,  1.2238e-01],\n        [-2.5413e-01,  4.5695e-02,  1.1730e-01,  1.0368e-01,  7.6938e-02,  5.9869e-02, -1.7449e-01,  1.1577e-01,  5.5210e-02, -2.2879e-01, -1.4848e-01,  1.8062e-01, -9.0813e-03,  2.5438e-01],\n        [ 1.4235e-01, -1.4395e-01,  1.9191e-02, -2.3977e-01, -1.5462e-01, -2.5653e-01, -2.1441e-01, -8.4353e-03, -1.7916e-01, -2.4149e-01, -2.0559e-01, -2.2604e-01,  8.5189e-02, -1.0456e-01],\n        [ 7.2386e-02, -8.7239e-02, -1.0556e-01, -1.8756e-02, -4.6498e-02,  1.5686e-01, -1.4667e-01,  1.1415e-01, -1.9025e-01, -7.5433e-02, -1.1980e-01,  3.4963e-02,  1.2667e-01,  1.1269e-01],\n        [-3.8712e-02,  1.6895e-01, -4.5869e-02, -1.5201e-01,  1.9253e-01,  1.3350e-01,  3.6692e-02,  4.4051e-02,  1.8596e-01, -2.1502e-02,  7.7561e-02,  1.3885e-02, -2.2919e-01, -1.9067e-01],\n        [-2.6420e-01, -3.4381e-02, -2.1756e-01, -2.3733e-01, -1.8872e-01, -2.2757e-01,  6.0340e-02, -4.7886e-02,  9.3484e-02,  9.8798e-02,  5.5311e-03, -1.2091e-01, -1.4043e-01,  2.3438e-01],\n        [-2.5897e-01, -1.1201e-01,  2.2250e-01,  5.0867e-02,  2.2923e-01, -2.4351e-01, -1.2314e-01,  7.6335e-02, -2.6135e-01,  2.6624e-01,  2.7562e-02,  5.0610e-03, -7.4978e-02,  1.5407e-01],\n        [ 2.0094e-02, -2.0361e-01,  1.2688e-01,  2.4682e-01,  2.3396e-01,  1.1667e-01, -2.3750e-01,  1.3424e-01,  7.9934e-02, -2.5810e-01, -9.4786e-02, -2.5097e-01, -1.1973e-01, -8.9856e-02],\n        [-1.7593e-01,  9.6982e-02, -9.0635e-02, -2.2018e-01, -2.1549e-01, -9.6886e-02,  8.6210e-02, -4.5653e-02,  1.7961e-02,  1.3316e-01,  2.3459e-01, -1.5092e-01,  2.6527e-01,  2.4520e-01],\n        [ 1.8212e-01,  1.7119e-01,  1.2807e-01, -2.4363e-01,  1.7352e-01,  1.6505e-03,  6.4835e-02, -1.4128e-01, -2.1506e-01, -1.9981e-01, -6.4917e-02,  2.2109e-01, -1.4094e-01, -4.5402e-02],\n        [ 2.2713e-02, -1.7767e-01, -1.6922e-01,  7.2196e-02,  2.0452e-01, -2.5735e-01, -6.3826e-02, -6.5849e-02, -4.1899e-02,  2.3178e-01,  3.0011e-02, -1.0278e-01, -2.4391e-02, -2.8114e-02],\n        [-2.5591e-01, -2.7342e-02,  5.4775e-02,  2.7171e-02,  4.4414e-02, -2.0457e-01, -5.1273e-02,  9.7795e-03, -1.2952e-01, -1.0552e-01,  1.2361e-01,  1.8182e-01, -1.7779e-01, -2.6786e-02],\n        [-1.2860e-01, -2.6591e-01, -1.9432e-01,  1.3014e-01, -1.4515e-01,  1.4181e-01, -1.3723e-01, -1.4262e-01,  1.4997e-03,  1.5110e-01,  2.0271e-01,  2.3422e-01, -7.1336e-02,  1.4528e-01],\n        [-2.6158e-01, -7.5140e-03,  2.0815e-02,  2.0352e-02,  7.8277e-02,  1.4611e-01,  1.8480e-01, -7.9655e-03, -1.4842e-01, -1.5518e-01, -4.3385e-02,  1.5939e-01,  1.0159e-01, -2.0666e-01],\n        [ 1.4135e-01,  1.3276e-01, -2.3040e-01,  3.2275e-02, -1.7683e-01, -2.2418e-01,  4.5962e-02, -1.0017e-01,  2.2512e-01, -1.3416e-01,  6.2623e-02, -5.9625e-02, -1.7201e-01, -7.7108e-02],\n        [ 8.5397e-02,  8.7442e-02, -4.4810e-02, -2.5201e-01, -4.3670e-02,  1.0842e-01, -1.4815e-01,  1.1315e-01,  2.4464e-01, -1.9493e-01,  1.8840e-01, -1.9910e-01,  3.8581e-02, -1.3859e-02],\n        [ 2.0648e-01,  6.5782e-02, -6.1725e-02, -1.1746e-02,  4.6561e-02, -9.6173e-02,  3.4167e-02, -1.6131e-01, -4.6255e-02, -5.5903e-03,  8.8282e-02, -1.6413e-01,  2.1276e-01,  7.4213e-02],\n        [-9.1838e-02, -1.6330e-01, -8.4103e-02, -1.0641e-01,  9.5662e-02,  8.0056e-03,  4.9944e-02, -1.9100e-01,  4.9943e-02,  1.8569e-01,  2.3084e-01, -1.4804e-01,  4.4250e-02,  5.3126e-02],\n        [ 2.1441e-01, -5.6490e-02,  2.4712e-01, -5.4557e-02, -2.4987e-01,  2.4111e-01, -2.6631e-02, -1.2208e-01, -1.9446e-01,  2.5439e-01, -9.5335e-02, -1.5346e-01,  2.5671e-01, -2.8282e-02],\n        [ 9.4202e-02,  1.1981e-03,  1.6836e-01, -1.8811e-01,  1.5810e-01,  2.5427e-01, -1.6423e-01,  1.8992e-01, -8.2998e-02,  1.6813e-01,  2.4069e-01, -1.5918e-01, -1.6970e-01, -1.9455e-01],\n        [ 1.7050e-01, -1.9148e-01, -1.5869e-02,  2.4590e-01, -6.2872e-02,  1.7931e-01, -2.5754e-01,  2.2973e-01, -1.4454e-01,  1.2528e-01, -1.8897e-01, -2.6695e-01, -5.4966e-03, -2.4723e-02],\n        [ 3.3835e-05,  1.5363e-01,  2.6365e-01,  1.5934e-01, -1.5212e-01, -6.2915e-02,  2.2877e-01,  8.7895e-02, -3.2380e-02,  1.6844e-01, -1.1154e-02,  4.1585e-02, -9.6475e-02, -7.9960e-02],\n        [ 2.6003e-01, -9.0302e-02,  1.7442e-01, -1.6720e-02,  2.2097e-01, -9.1257e-02, -1.1361e-01,  2.4861e-01,  4.2837e-02, -2.0566e-01,  1.7475e-01,  1.6418e-01,  2.1231e-01, -5.9638e-02],\n        [-2.1494e-01, -5.5400e-02, -1.2554e-01, -2.2958e-01,  2.2306e-01,  1.4567e-02,  1.7329e-01, -1.1517e-01, -3.9328e-02,  2.4631e-01, -4.3128e-02,  1.2592e-01,  1.2996e-01,  1.2670e-01],\n        [-2.6513e-01, -2.6189e-01,  1.1274e-01,  1.6439e-01,  2.4863e-01,  6.1575e-02, -2.1902e-01,  1.1234e-01,  1.4204e-01,  4.6374e-02,  7.1815e-02,  9.7212e-02, -1.2183e-01, -1.9004e-01],\n        [-1.8223e-02, -2.1746e-01,  1.2919e-01,  4.8803e-02, -1.8956e-01, -8.6885e-02,  8.9477e-02, -2.1771e-01,  1.9799e-02,  1.8950e-01, -2.1930e-01, -1.2098e-01, -2.6037e-01,  1.6761e-01],\n        [ 8.4936e-02,  6.9494e-02,  9.4688e-02, -7.5183e-02, -2.0238e-01,  2.3183e-01,  1.8695e-01, -1.1976e-02, -9.9970e-02,  2.3344e-01,  7.8916e-02,  6.5503e-02, -2.1343e-01,  2.8036e-03],\n        [ 2.2814e-01, -1.7362e-02,  2.3189e-01,  5.1238e-03, -1.2614e-01, -2.5203e-01, -2.9811e-04,  1.1972e-01,  2.4133e-01, -1.2131e-01, -2.2170e-02, -2.2384e-03,  2.7812e-02, -2.2405e-01],\n        [-4.4900e-02, -4.8427e-02,  6.6246e-02,  4.2627e-02, -9.7782e-02, -1.0939e-01, -7.5799e-02, -1.6815e-01,  2.5410e-03,  1.0030e-01, -2.0212e-01, -2.4085e-01, -1.2890e-01,  1.2831e-01],\n        [-2.2342e-01, -1.9841e-01, -2.0799e-01, -1.6382e-01, -2.9067e-02, -2.0616e-01,  2.6462e-01, -1.5765e-01,  2.4381e-01,  2.2775e-01, -1.6586e-01, -2.6119e-01,  1.6516e-01,  2.2963e-01],\n        [-1.2503e-01,  1.2240e-01, -9.1209e-02, -7.1448e-02,  1.3538e-01,  2.1442e-01, -1.8249e-03,  1.1992e-01,  2.6631e-01, -2.0651e-02, -1.8518e-02,  2.1072e-01, -1.6736e-01,  1.2383e-01],\n        [ 9.4993e-02,  1.1887e-01,  1.7292e-01,  1.4729e-01, -3.6183e-02, -1.5412e-01,  8.5386e-02, -1.9894e-01, -2.8210e-02, -7.8224e-02,  2.0533e-01,  1.6200e-01, -2.7384e-02, -2.0593e-01],\n        [-2.1813e-01,  2.1002e-01, -9.9682e-02,  2.6268e-01, -2.3516e-01,  2.2771e-01,  1.9718e-01, -2.0177e-01, -1.3415e-01,  4.6733e-02, -2.1783e-01,  1.1568e-01,  8.5288e-02,  8.8998e-02],\n        [ 2.2472e-01, -4.1140e-02, -2.7796e-02, -1.5328e-01,  1.7105e-01,  2.0252e-01,  2.3408e-01,  2.8078e-02, -2.6911e-02,  1.2360e-01,  1.8381e-01,  2.4384e-01,  2.1980e-01, -1.6895e-01],\n        [-6.0116e-03,  1.5220e-01, -3.8431e-02,  2.5571e-01,  2.6338e-01,  9.4715e-02, -1.6345e-01, -9.1915e-02,  1.0999e-01, -1.3117e-01, -2.1868e-01, -4.4319e-02, -2.1128e-01, -2.3865e-01],\n        [ 1.7636e-01, -1.2308e-01, -1.5322e-01, -1.8557e-01,  2.1639e-01, -5.3297e-02, -1.4140e-01,  1.0066e-02, -1.7516e-01,  2.0013e-02, -1.9287e-01, -1.3606e-02,  1.6024e-01,  1.9544e-01],\n        [ 2.0903e-01,  2.5448e-01, -1.5645e-01, -1.5243e-01,  1.3577e-01,  7.8034e-02,  1.5277e-01,  1.5673e-01,  6.4626e-02,  2.5870e-01, -1.9244e-01, -1.2906e-01,  2.6236e-01, -3.8229e-02],\n        [ 2.2451e-01, -1.0978e-01,  1.0731e-02, -7.8913e-02,  2.1493e-01,  1.1775e-01,  3.6172e-02,  6.7190e-02,  5.8177e-02, -2.6200e-01, -1.7726e-01, -9.7839e-02,  5.7948e-02,  2.3013e-01],\n        [-1.4781e-01,  5.8077e-02,  6.1946e-02, -6.4472e-02, -1.7893e-02, -2.1129e-01,  2.1829e-01, -5.0102e-02, -8.2931e-02,  2.9113e-02, -1.4318e-01, -1.6893e-01, -7.6255e-02,  1.1692e-01],\n        [-1.5687e-01,  2.5635e-02, -2.4112e-02,  1.6039e-01, -1.5648e-02, -3.6083e-02,  1.9783e-01, -2.4831e-01, -3.8159e-02,  1.3878e-01, -2.4525e-01,  1.9746e-01,  2.6179e-01,  5.5230e-02],\n        [-1.2515e-01,  1.0383e-01, -8.6664e-02, -5.5457e-02, -1.6560e-01, -1.8196e-01,  1.3869e-01,  2.0868e-01,  6.3243e-02,  2.6715e-01, -5.3351e-02, -2.3443e-01, -1.5406e-01,  1.8662e-01],\n        [ 3.8517e-02,  1.1659e-01, -1.8134e-01, -2.3303e-01, -1.9122e-01, -1.5582e-01,  2.4451e-01,  2.1435e-01, -9.4354e-02,  1.5174e-01,  2.2253e-01,  1.7885e-01,  1.0154e-01,  1.4615e-02],\n        [-9.3878e-02,  1.7640e-02,  7.9153e-02,  2.4497e-01,  2.5312e-01,  4.2056e-02,  5.9604e-02,  1.2307e-01,  3.2563e-02,  1.3264e-02,  1.8944e-01, -3.4998e-03,  1.7298e-01,  2.1151e-01],\n        [ 4.3437e-02, -5.1305e-02,  1.1582e-01, -1.5723e-01, -8.6001e-02, -1.7102e-01,  1.2671e-01, -7.3802e-02,  4.6060e-02, -2.8900e-02, -4.0020e-02,  6.2869e-02,  2.3810e-01, -8.5673e-02],\n        [ 1.9508e-01, -1.8636e-01,  2.3402e-01, -2.1609e-01, -1.6710e-01, -1.9890e-01,  1.1061e-01, -2.0455e-01, -1.0900e-01, -1.6590e-01, -3.5130e-03,  1.9434e-01,  2.3532e-01,  2.5990e-02],\n        [ 1.9807e-01, -2.3358e-01,  7.8835e-02,  1.3705e-01, -1.9975e-01, -8.5301e-02,  1.2474e-01, -1.3780e-01, -1.7810e-02,  1.7357e-02,  1.9977e-01,  2.4519e-01,  7.7080e-02, -3.9843e-02],\n        [-2.0031e-01,  1.6678e-02,  7.2462e-02, -1.3516e-01,  3.9315e-02,  1.6153e-01, -2.0375e-01,  2.4311e-01,  1.1289e-01, -1.6333e-02, -6.7419e-02,  8.7133e-02, -2.0221e-01,  2.2413e-02],\n        [-1.1874e-01, -1.7993e-01,  1.1857e-01,  1.5773e-01,  2.1905e-01,  8.8504e-02, -1.0810e-01, -1.9715e-01,  2.4694e-01, -1.2207e-01,  2.6165e-01,  8.4592e-02, -2.4416e-02, -1.9128e-01],\n        [-3.6568e-02,  1.9366e-01, -1.2494e-01,  1.6267e-01, -1.2396e-01, -2.3858e-01,  1.8250e-01,  9.7814e-02, -1.0874e-01,  1.5627e-01,  1.9217e-01,  1.0839e-01,  2.1462e-01,  2.1677e-01],\n        [-2.3182e-01, -2.6069e-01,  1.7017e-01, -2.4660e-01, -5.1621e-02, -1.2347e-01, -8.6298e-02, -8.1633e-02, -2.5392e-01,  1.0216e-01,  1.4778e-01,  1.3122e-01,  2.0474e-01, -2.1457e-01],\n        [ 1.6575e-01, -1.4188e-01,  8.0335e-02, -6.1208e-03, -1.6316e-02, -1.3214e-01, -1.5850e-01,  8.9198e-03,  9.4094e-02,  1.6664e-01, -1.1775e-01, -2.2588e-03, -2.3066e-01,  1.4682e-01],\n        [-1.6246e-01,  1.0503e-01, -1.8908e-02,  2.0198e-02,  4.4605e-02,  2.6124e-01, -2.4511e-01,  1.8614e-01, -1.2639e-01,  1.8261e-01,  2.0582e-01, -3.1861e-02,  2.1921e-01,  2.2651e-01],\n        [ 9.8418e-03,  2.4929e-01, -7.9285e-02, -2.4392e-01, -2.1752e-01,  8.8464e-02,  1.7162e-01,  2.2011e-01,  1.4851e-01,  1.8259e-01, -3.6356e-02,  7.9220e-04,  2.2210e-01, -6.4795e-02],\n        [ 1.1118e-01, -7.3743e-02, -1.3244e-01,  1.3729e-01, -2.6625e-01,  1.2924e-01, -9.9639e-04, -1.9657e-02, -1.6231e-01,  2.5584e-01,  4.5057e-02,  3.3548e-02,  2.2135e-01, -1.5681e-02],\n        [ 1.6092e-01,  2.4334e-01, -1.1292e-01,  2.5382e-03,  2.6163e-01, -1.5948e-01,  1.7282e-01, -1.6476e-01, -2.0959e-01, -2.6653e-01,  4.0752e-02,  2.1060e-02,  1.1387e-02,  2.7622e-02],\n        [-2.6415e-01, -5.7424e-02, -1.5672e-02, -1.3751e-01,  7.6499e-02,  1.9959e-01,  1.5352e-01, -2.5547e-03,  2.4805e-01, -2.3832e-01,  1.6834e-01,  1.2120e-01,  8.5916e-02,  1.8465e-01],\n        [ 2.6248e-01,  2.5162e-01, -2.5345e-01,  2.1803e-01, -1.9063e-01, -2.8274e-02,  6.8019e-02, -5.5834e-02, -1.6089e-02,  6.8460e-02, -4.8079e-02, -1.1979e-01,  6.1826e-02,  7.2583e-02],\n        [-7.3296e-02, -2.3676e-01,  1.4836e-02, -1.6828e-01,  2.3595e-01,  3.2573e-02,  9.7760e-02,  1.7120e-01,  7.1921e-02,  2.3668e-01, -1.1340e-01, -1.1305e-01, -2.2613e-01,  2.0675e-01],\n        [ 1.1955e-01, -1.4864e-02,  5.7793e-02,  1.2573e-01, -2.3699e-01,  2.9016e-03, -2.5424e-01,  1.8418e-01, -5.8270e-02,  7.0287e-02, -1.4925e-01,  7.4118e-02, -2.8208e-02, -1.2840e-01],\n        [-1.5770e-01,  2.0471e-01, -1.6165e-01, -1.9657e-01, -1.7380e-01,  5.5587e-02, -1.9263e-01, -1.1361e-01,  1.3447e-01,  2.0354e-01,  9.8420e-02,  4.4670e-02, -2.4110e-01,  4.9903e-02],\n        [ 7.4712e-02, -3.4029e-02, -1.2276e-01,  1.4464e-01, -3.4990e-02, -2.2208e-01,  2.0923e-01,  2.2730e-01,  2.3046e-01,  5.6103e-02,  4.2138e-02, -8.5181e-02,  1.5755e-01, -3.8371e-02],\n        [ 1.3906e-01,  1.0146e-01,  1.3739e-01, -1.9625e-01,  1.4413e-01,  5.3734e-02,  2.6626e-01, -4.7776e-02,  1.6039e-01, -1.4390e-01, -1.0393e-01, -8.6747e-02,  5.4108e-02, -3.4172e-02],\n        [-1.9723e-01, -1.3699e-01, -2.4883e-01,  1.4305e-01,  7.7671e-02,  1.6680e-01,  4.0152e-02,  1.4240e-01,  1.7905e-01,  1.4207e-01, -2.3017e-01, -2.2792e-01,  1.7528e-01, -1.1617e-01],\n        [ 1.1828e-01, -1.2251e-01,  1.5876e-01,  1.7641e-01,  1.8164e-01, -6.5344e-02, -1.9678e-01,  2.1926e-01,  2.0755e-01, -2.6216e-01,  6.3812e-02, -2.2129e-01, -1.8751e-01,  3.6552e-02],\n        [ 1.6380e-01, -1.3089e-03, -7.7312e-03,  1.6239e-01, -2.2459e-02, -8.5291e-02,  1.8921e-01,  2.3972e-01,  1.8795e-01,  1.7558e-01, -4.0291e-02, -1.7802e-01,  1.8769e-01, -2.5795e-01],\n        [-2.9319e-02, -1.9955e-01,  5.4712e-03,  2.1168e-01, -1.3673e-01, -2.2782e-01, -2.3453e-01, -1.2996e-01,  1.6947e-01,  1.3089e-01, -8.1427e-02,  1.3670e-01, -1.8319e-03, -1.4611e-01],\n        [ 1.8863e-01, -2.9539e-02,  9.9655e-02,  3.4929e-03,  1.7468e-01, -1.2293e-01,  3.5251e-03,  2.0936e-01, -9.5199e-02, -2.6553e-01, -8.1421e-03, -1.6229e-01, -2.6220e-01, -1.0608e-01],\n        [ 2.4262e-01, -7.5548e-02, -2.6020e-01,  1.7935e-01,  3.7875e-02,  1.6060e-01, -1.1468e-02,  1.6631e-01,  5.5046e-02, -2.5456e-02,  1.9150e-01, -2.4131e-01,  1.3447e-01,  1.9370e-01],\n        [-1.9273e-01,  2.3468e-01, -2.1658e-01, -7.4964e-02,  1.9786e-01, -1.7454e-01,  2.5343e-01, -2.0997e-02,  5.8079e-02,  1.4105e-01,  8.1892e-03, -1.4223e-01,  1.7594e-01,  1.7169e-02],\n        [ 2.0148e-01,  2.4157e-01,  1.0690e-01, -1.9269e-02,  1.1194e-01, -1.8222e-02, -9.3956e-02,  2.3956e-01, -8.3230e-02,  1.5761e-01,  2.6537e-01, -1.8200e-01, -2.1312e-01, -1.8273e-01],\n        [ 1.9459e-01,  2.4785e-01, -1.5259e-01,  1.0364e-01, -6.5913e-03, -1.2715e-01, -1.2837e-01, -6.8524e-02,  1.9887e-01, -2.3131e-01, -1.1942e-01,  2.4996e-01, -5.6328e-02, -1.3594e-01],\n        [ 2.6273e-01,  1.4682e-01,  1.5517e-01, -2.5049e-01, -1.4823e-01,  1.3377e-02, -9.6149e-02,  1.9073e-01, -2.0140e-01, -3.0897e-02, -8.5599e-02,  1.8752e-03, -4.1726e-03, -7.4421e-02],\n        [ 1.8909e-01, -3.5360e-02, -1.6665e-01, -1.1897e-01,  9.7915e-02,  2.6185e-01,  1.6358e-01, -1.7781e-02,  1.3524e-01,  2.4761e-01, -8.5557e-02, -1.6407e-01,  1.9227e-01, -1.6132e-01],\n        [-1.5395e-01,  1.0612e-01,  2.1831e-01, -2.0691e-01,  1.3514e-03, -1.1469e-02, -9.7675e-02, -2.5234e-01,  2.5795e-02, -1.3792e-01, -1.4543e-01, -9.9660e-02, -6.5783e-02, -3.6368e-02],\n        [-1.6561e-01, -2.1769e-01,  3.6090e-02, -2.3670e-01,  1.5463e-01, -2.2166e-01,  1.5277e-01,  1.1090e-01,  2.2213e-01,  7.7812e-02,  8.8300e-02, -3.5554e-02,  3.1166e-02,  1.4994e-01],\n        [ 2.0271e-01,  1.3164e-01,  2.1068e-01,  3.0095e-02,  1.0884e-01, -1.1065e-01, -8.2289e-02,  1.0031e-01,  7.3327e-02, -1.3281e-01,  1.3520e-01,  1.4672e-01, -1.7798e-01, -1.1017e-01],\n        [ 8.4106e-02,  1.0385e-01,  2.4057e-01, -6.8909e-02,  8.6936e-02, -6.9274e-02, -2.2544e-01, -2.4583e-01, -1.6119e-01, -2.1299e-01,  1.2332e-01,  7.4995e-02,  2.4797e-01,  1.4129e-01],\n        [ 2.0734e-02, -1.7407e-01, -2.4018e-02,  8.2839e-02, -1.7437e-01, -2.4540e-01,  3.7871e-02,  1.6280e-01, -1.4866e-01,  5.5947e-02, -1.5375e-01, -9.8945e-02,  4.2100e-02,  1.3421e-02],\n        [ 8.8618e-02, -8.9112e-02,  4.8555e-02, -2.2419e-01, -1.0210e-02,  1.0046e-01, -1.2242e-01,  1.0003e-01, -7.1864e-02,  1.6867e-01, -1.8578e-01,  6.6465e-02, -1.6227e-01, -2.2231e-01],\n        [ 1.9381e-01,  2.1011e-01, -6.5878e-02,  1.0648e-01,  1.0623e-01,  2.0763e-01, -1.1048e-01, -6.8479e-02, -1.0240e-01,  2.5106e-01,  8.8688e-02, -1.8535e-01,  1.1114e-01, -1.4879e-01],\n        [ 2.0874e-01, -8.9157e-02, -6.1993e-02,  2.1536e-01, -1.6378e-01,  4.1095e-02, -2.8491e-02, -1.6217e-01, -9.8469e-02,  1.5312e-01,  2.5424e-01,  1.8014e-01, -1.9674e-01,  1.6801e-01],\n        [ 1.9643e-01, -1.9890e-02, -3.6471e-03,  1.2613e-01, -8.4491e-02, -1.8328e-01, -1.4381e-01,  3.4578e-02, -2.1903e-01, -2.5633e-01,  1.0674e-01, -9.4265e-02, -2.1091e-01,  1.7323e-01],\n        [ 4.9705e-02,  2.0090e-01,  1.6093e-01,  1.0714e-01,  1.4095e-01,  6.2420e-02,  2.4053e-01, -1.7979e-01, -2.4774e-01, -6.3238e-02, -2.5251e-01, -2.3895e-01,  2.2768e-01, -1.5876e-01],\n        [-7.9647e-02, -2.5163e-01, -5.2030e-02,  2.7563e-03,  1.0654e-01, -2.2387e-01,  2.3787e-01, -8.7100e-03,  1.3089e-01, -2.3844e-01, -2.4394e-01, -2.4821e-01, -1.1970e-01, -1.1886e-01],\n        [-1.3910e-01, -6.8687e-02,  5.1779e-02, -1.5223e-01,  3.1945e-02, -2.3665e-01, -1.3744e-01,  1.1736e-01,  1.9841e-01,  4.8476e-02, -1.4506e-01,  2.0127e-01,  2.2787e-01,  1.5572e-01],\n        [ 8.0029e-02, -7.7797e-02,  1.3856e-01, -4.0931e-02, -1.1576e-01, -1.1610e-01, -1.5212e-01,  3.2054e-02, -1.6355e-01, -3.1900e-02,  2.5171e-01, -2.1579e-01,  1.5758e-01, -1.3755e-01],\n        [-4.4845e-02,  1.8200e-01,  1.7786e-01, -1.7380e-01, -1.1746e-01, -8.0282e-02,  8.4204e-02, -1.1619e-01,  2.6268e-01, -2.2959e-01,  2.2177e-01, -2.3552e-01,  2.2335e-01, -2.3077e-01],\n        [-1.5115e-01, -1.3870e-01, -2.0966e-01, -1.1403e-02,  2.3213e-01, -6.4669e-04, -2.0576e-01,  8.8532e-02,  2.6241e-01,  1.1214e-01,  1.8734e-01, -1.4630e-01,  5.8765e-02, -2.1547e-02],\n        [-2.2493e-02,  2.1292e-01,  1.9444e-01,  3.3511e-02, -1.0833e-01, -1.5112e-01, -1.1969e-02, -1.9141e-01, -7.2623e-02, -8.9415e-02,  1.2149e-01, -5.7071e-02, -1.0248e-01, -9.7814e-02],\n        [ 2.4105e-01,  3.1108e-02,  1.5774e-01, -1.2804e-02,  7.7602e-02, -2.2643e-01, -9.0323e-02, -1.1390e-01,  1.8947e-01,  1.5665e-01,  2.2210e-01, -8.8595e-02, -9.7913e-02, -7.6290e-02],\n        [ 2.4119e-01, -2.6601e-01, -2.3784e-01,  1.6387e-01,  5.1190e-02, -8.1466e-02, -2.0223e-01, -2.5272e-01,  2.0916e-02,  4.1335e-02, -1.2403e-01,  2.4385e-01,  5.3387e-02, -1.7162e-01],\n        [ 4.0228e-02,  1.4166e-01,  9.7972e-02,  2.2916e-01, -1.3841e-01, -1.3903e-02, -2.0640e-01,  6.4814e-02,  2.2651e-01,  7.7328e-02,  3.4441e-02,  6.4837e-02,  2.0604e-01, -2.5214e-01],\n        [-1.1234e-01, -9.8518e-02,  2.6248e-01, -1.2677e-01, -2.5986e-02,  2.4024e-01, -1.4036e-01, -1.0984e-01, -1.6523e-01,  1.1473e-01, -6.5729e-02, -8.5795e-02,  1.9595e-01, -7.6769e-02],\n        [-9.4901e-02,  9.2012e-02,  1.3926e-02,  1.4024e-01,  1.0419e-01, -2.2258e-01,  1.7624e-01,  1.3277e-01, -6.1164e-02,  1.3239e-01,  2.0210e-01, -2.5379e-01,  6.4841e-02,  1.7695e-01],\n        [-1.3849e-01,  9.5040e-02,  8.4948e-02, -2.2408e-01,  1.2634e-01, -2.4183e-01,  8.7151e-02,  1.4160e-01, -1.5776e-01, -1.1333e-02, -1.5282e-03, -1.4678e-01, -2.5407e-02, -2.0949e-01],\n        [-1.6905e-02,  1.3110e-01,  8.4088e-02,  1.2614e-02, -1.0448e-01, -1.6079e-01,  1.6679e-01,  8.0562e-02, -1.5272e-01,  1.4624e-02, -1.3968e-01,  2.0996e-01, -6.2612e-02,  4.4029e-02],\n        [-9.4680e-02, -2.3179e-01,  4.6537e-02,  1.5954e-01, -1.1033e-01, -8.7124e-02, -2.4915e-02, -1.5820e-01,  8.8819e-02, -2.6045e-01,  5.7567e-02,  4.2105e-02,  4.7032e-04, -2.5765e-02],\n        [-4.4012e-02, -2.2406e-01,  2.5809e-01,  5.6066e-02, -1.9628e-01, -7.7842e-02, -3.6162e-02, -2.3962e-02,  8.9968e-03,  2.7022e-02,  8.5467e-02,  1.9132e-01, -4.7002e-02,  5.6685e-02],\n        [-1.5437e-01,  3.3531e-03,  1.1711e-01, -5.1965e-02,  1.5069e-01,  9.7400e-02, -1.5077e-01,  7.9576e-02, -1.4988e-01, -3.2831e-02,  7.8104e-02,  2.5611e-01,  1.6371e-01,  1.5522e-01],\n        [-1.4269e-01, -2.1406e-01,  1.1353e-01,  7.1460e-02, -2.3151e-01, -1.8639e-01,  2.4412e-01,  2.3092e-01, -6.8468e-02, -2.3359e-01, -2.5727e-01,  1.3769e-01,  2.5894e-02, -1.1559e-01],\n        [-7.6834e-02, -2.0337e-02, -2.3613e-01, -2.3650e-01, -9.1775e-02, -1.2837e-02, -1.7798e-01, -1.9606e-01,  2.5771e-01,  6.0556e-02, -1.6192e-01,  1.0482e-01, -2.4851e-01, -1.6751e-01],\n        [ 2.2110e-01, -1.7747e-01,  8.1393e-02, -1.1157e-01,  8.6949e-02,  1.2646e-02, -1.9990e-01,  4.3692e-02, -1.5991e-02, -4.3408e-03, -1.1009e-02, -1.9018e-01, -2.4495e-01,  4.9474e-02],\n        [-8.8514e-03, -8.0584e-02, -1.8323e-01,  1.7860e-01,  2.6649e-02, -1.2224e-01,  1.4729e-01,  1.1699e-02,  2.0232e-01, -2.1158e-01,  1.5124e-01, -1.3995e-01, -2.4515e-01, -2.2924e-01],\n        [ 1.0208e-01, -3.1073e-02,  1.9126e-01, -2.0712e-01,  6.8471e-02, -8.1830e-02, -7.3612e-02,  1.4505e-01,  4.1602e-02,  2.1164e-01, -7.9627e-02,  1.1085e-01, -1.4353e-01,  9.2618e-02],\n        [-1.2898e-02, -4.8637e-02, -2.6191e-01,  2.3170e-01, -1.9206e-01,  1.2207e-01, -1.9124e-01,  9.8750e-02,  1.0605e-01,  6.4581e-02, -8.7444e-02, -1.2176e-01, -2.5893e-02,  1.5125e-02],\n        [-2.4689e-01, -5.0195e-02, -2.1564e-01, -1.4981e-01, -1.0445e-01,  1.8065e-02, -1.9113e-01, -8.4544e-02,  9.2105e-02, -1.6482e-02, -6.9330e-02, -2.1056e-01,  4.0313e-02,  1.0548e-01],\n        [-6.5003e-02,  4.5601e-02,  9.4389e-02,  1.0766e-01, -1.1993e-01,  1.6996e-01,  1.7283e-01,  1.5832e-02, -1.7032e-01, -2.1410e-01, -2.5874e-01, -2.2043e-01,  2.6624e-01, -2.3947e-01],\n        [-1.1170e-01,  1.8216e-01,  9.8176e-02, -6.6586e-02,  9.0314e-02,  1.0931e-01,  2.5139e-01, -1.7345e-01, -1.9329e-01, -1.5826e-01,  1.7305e-01, -1.9011e-01,  2.3803e-01, -2.5041e-01],\n        [ 1.6028e-01,  3.1098e-03, -3.1656e-02,  6.3053e-02, -5.4538e-02,  9.0911e-02, -1.8404e-01, -2.2396e-01,  1.2316e-01,  1.5622e-01, -2.4435e-02, -1.9297e-01, -1.7812e-01,  4.3350e-02],\n        [-1.9821e-01, -1.1568e-01, -1.1626e-01, -1.3802e-01,  1.7757e-01, -4.8265e-02,  1.7482e-01, -2.6121e-01, -2.4573e-01,  2.4259e-01,  4.5106e-02,  1.9240e-02,  1.4855e-01,  2.2711e-01],\n        [-1.0944e-01,  1.1007e-02,  5.9531e-02, -1.1786e-01, -1.6319e-01, -1.5275e-01,  2.4996e-01,  2.5217e-01,  2.4338e-01, -2.1604e-01, -4.3160e-02, -1.0202e-01, -1.6046e-01,  2.4246e-01],\n        [ 1.0511e-01, -2.5567e-01,  2.2622e-01, -3.2145e-02, -6.4890e-02,  1.6059e-01,  1.5019e-01,  1.9012e-01,  1.8180e-01, -5.7477e-02,  2.6347e-01, -2.5164e-01,  2.0944e-01,  2.5105e-01],\n        [ 1.3971e-01, -6.0745e-02, -9.7197e-02,  1.2260e-01, -1.6018e-01,  8.4555e-02,  4.0264e-03,  1.1069e-01, -6.7816e-02,  1.9523e-01, -2.1993e-01, -1.7072e-01, -2.0039e-01,  1.5992e-01],\n        [-1.5390e-01, -1.8678e-01, -2.0912e-01, -1.1698e-01, -2.1960e-01,  1.7861e-01, -5.2159e-02,  2.2914e-02,  2.3586e-01, -1.5666e-01, -1.2829e-01, -5.7564e-03, -8.1956e-02,  2.8470e-02],\n        [ 4.6216e-03, -2.2256e-02,  2.5134e-01, -2.2667e-01,  8.0027e-02,  7.3213e-02,  9.3621e-02,  1.3314e-01, -9.1914e-02,  6.7377e-02, -1.3946e-02, -1.8771e-01,  1.0241e-01,  1.5474e-01],\n        [-4.8686e-03, -1.8366e-01,  9.1853e-02, -1.4629e-01,  2.9125e-02,  1.3633e-01,  5.5322e-03,  2.3751e-01, -1.4132e-01,  1.9344e-01, -1.9286e-01,  2.6317e-01,  2.5880e-01, -2.3200e-01],\n        [-1.3150e-01, -8.9901e-02, -2.4508e-01,  1.5516e-01, -1.1347e-01,  1.9770e-01, -2.2632e-01, -1.3093e-01,  2.4706e-01,  5.0696e-02,  2.5714e-01, -8.6221e-02,  6.3454e-02,  1.7956e-02],\n        [-1.6166e-01, -1.8863e-01,  1.9156e-03,  2.6398e-01,  1.5795e-01,  1.0522e-01, -2.6410e-01,  2.4697e-02,  2.3930e-01, -6.5501e-03,  2.3116e-01, -1.9688e-01,  8.3201e-02,  2.5945e-01],\n        [ 1.6291e-01,  2.2587e-01,  2.3405e-01, -2.5559e-01, -1.3027e-01, -1.1247e-01,  2.2426e-01,  1.3161e-01,  1.8702e-01, -1.2461e-01,  2.4184e-03, -2.6473e-01, -8.7993e-02, -3.6308e-02],\n        [-1.2672e-01, -2.4598e-01,  1.9625e-01, -4.9986e-03, -2.6014e-01, -1.6105e-01,  1.2700e-01, -7.1711e-02,  5.5877e-02, -1.0914e-01, -2.5068e-01, -4.0274e-02, -2.3260e-01, -1.2904e-01],\n        [ 6.0113e-02, -1.3541e-01, -1.8456e-01, -2.7917e-02, -6.4354e-02, -4.0981e-03,  1.1561e-01, -2.0686e-01,  8.2328e-03, -2.5502e-01,  8.5672e-02,  1.1997e-02,  9.7733e-02, -1.3855e-02],\n        [ 7.7558e-02,  1.1415e-01, -1.0728e-01,  1.1115e-01, -3.0133e-03,  1.9244e-02,  5.7619e-02, -2.3302e-01,  7.3458e-02,  9.7686e-02,  2.2256e-01,  7.3187e-02,  6.1863e-02, -6.9878e-02],\n        [-2.2920e-01, -2.1594e-02, -6.1492e-02,  1.5903e-01,  1.8347e-01,  1.9144e-01, -1.3464e-01,  3.4958e-02,  9.0396e-02, -2.6433e-02, -3.7178e-02, -1.8902e-01,  2.4369e-01,  1.9352e-01],\n        [ 2.1863e-03, -1.9898e-02, -1.3563e-01,  6.6383e-02, -5.9798e-02,  4.7869e-02, -3.2279e-02,  1.7493e-02, -1.0777e-01,  5.9675e-02,  8.0164e-02, -1.1261e-01, -2.1778e-01, -7.4431e-03],\n        [ 1.0480e-01, -5.3993e-03,  3.8446e-02,  1.8945e-01,  7.3932e-02,  2.1596e-01,  1.1833e-02,  7.6293e-02,  1.7673e-02, -8.9618e-02,  6.3203e-02, -6.3373e-02,  1.5229e-01,  2.0834e-01],\n        [ 2.6395e-01, -4.0766e-02, -1.4493e-01, -1.5607e-02,  8.5981e-02,  2.2038e-01, -1.9251e-01,  1.2595e-01, -2.2762e-01,  2.0717e-01,  2.4809e-01,  2.1646e-01,  7.5139e-02, -2.5126e-01],\n        [-2.2865e-01, -1.5301e-01, -3.1012e-02, -2.6820e-02,  1.2400e-01,  4.4812e-02,  3.0840e-02, -9.7363e-02,  3.3950e-02,  2.5504e-01,  1.8277e-01, -2.2076e-01, -1.6415e-01,  2.3558e-01],\n        [-1.8931e-01,  2.0594e-01, -9.5946e-02, -4.8020e-02,  2.1019e-01,  5.6582e-02, -9.2251e-02, -1.4694e-01,  2.4890e-01, -2.1544e-02,  5.4737e-02, -1.8681e-01, -3.9312e-02, -2.0154e-01],\n        [ 2.0402e-01,  3.2382e-02,  1.3933e-01, -5.2115e-02,  2.1517e-01, -2.3799e-01,  3.7794e-02, -1.8162e-01, -6.5278e-02,  7.5980e-02, -4.2036e-02,  1.5273e-01, -1.1560e-01, -8.4223e-02],\n        [-8.0220e-02, -9.9801e-02, -6.3290e-02,  2.9747e-02,  1.0109e-01, -2.2815e-01,  2.1363e-01, -7.1465e-03,  1.3143e-01, -2.0801e-01,  1.0514e-01,  1.5989e-01,  3.0332e-02, -2.5546e-01],\n        [-2.5304e-01,  1.9591e-01,  1.7846e-01, -6.9479e-02,  1.1687e-01,  2.6595e-01, -2.3434e-02,  2.1350e-01,  2.2802e-01,  1.5600e-01,  2.4092e-01, -9.8660e-02, -9.5259e-02, -8.6352e-02],\n        [ 1.7311e-01, -5.7901e-02,  3.9793e-03,  1.1241e-01,  1.8486e-01, -2.5419e-01,  7.5832e-02,  2.7559e-02,  1.8288e-01, -1.6111e-01,  2.0856e-01,  1.7886e-02,  1.4766e-01,  1.5474e-01],\n        [-2.8854e-02, -2.0815e-01, -4.3318e-02,  2.2598e-03, -9.4929e-02,  1.1941e-02,  9.9959e-03,  9.8528e-02, -2.6701e-01,  2.1994e-01,  1.2842e-01, -8.8525e-02,  1.4468e-01,  1.2815e-01],\n        [ 9.1308e-02, -2.6380e-01,  9.2166e-02, -1.6202e-01,  2.0871e-01,  2.2463e-01,  2.3556e-01,  9.2667e-02, -1.3357e-01,  2.0544e-01,  2.0930e-01, -3.9072e-03, -1.4804e-01,  2.2980e-03],\n        [-6.8587e-02,  6.8318e-02, -1.2324e-01, -1.4682e-02,  2.2345e-01, -1.1376e-01, -2.1886e-01,  2.5542e-01,  2.5284e-01,  7.4683e-02, -2.6575e-01,  3.2738e-02,  4.8573e-02,  8.3248e-02],\n        [ 1.8218e-01, -1.3052e-01,  2.1687e-01,  1.9108e-01, -2.0943e-01, -1.6892e-01, -1.6211e-01,  3.6261e-03, -1.1783e-02, -1.2659e-01,  1.7461e-01, -1.8631e-01,  1.2168e-01,  1.1020e-01],\n        [ 2.0759e-01,  3.7743e-02,  6.4909e-02, -1.4150e-01,  1.2410e-01,  2.0877e-01, -1.5581e-01, -7.5751e-02,  2.0067e-01,  1.2133e-01,  2.6458e-01, -3.7036e-02,  8.3678e-02, -1.8848e-01],\n        [-1.1108e-01,  9.2034e-03, -2.1876e-02,  1.9690e-01,  1.7947e-01, -2.6591e-01,  1.7536e-01,  2.8867e-03,  2.3111e-01,  1.2965e-01,  5.2521e-02,  6.4732e-02, -1.8867e-01,  1.6003e-01],\n        [-4.9562e-02, -1.9198e-01, -7.4273e-02, -2.5205e-01, -6.6502e-02,  1.0193e-01, -1.7573e-01, -4.2505e-02,  2.1976e-01,  1.7726e-01, -1.7351e-01,  2.4147e-01,  3.4402e-03, -3.2654e-02],\n        [-2.5572e-01,  2.3640e-01, -1.1637e-01,  2.0457e-01,  2.1464e-01,  9.7249e-02,  2.2466e-03,  8.6275e-02, -2.9951e-02, -1.8583e-01, -1.1527e-01,  2.2783e-01,  2.1982e-01, -2.1416e-01],\n        [ 2.0030e-01, -2.4168e-01, -5.7060e-02, -1.2342e-01, -7.1873e-02,  2.8095e-02,  1.3997e-01,  1.3416e-01,  6.7844e-02,  1.3080e-01,  8.4166e-02,  1.2052e-01,  9.0942e-02, -1.4932e-01],\n        [ 9.0034e-02, -2.0361e-01,  2.6608e-01,  2.1038e-01,  3.1937e-02,  1.8123e-02,  1.1403e-01, -2.5175e-01, -1.8357e-01, -2.2589e-01, -1.0755e-01, -9.0481e-02,  1.7384e-01, -2.6476e-01],\n        [-2.3504e-01, -1.6786e-01, -9.3519e-02,  1.6868e-01,  2.0012e-01, -1.3896e-01,  1.8681e-01,  3.4984e-02,  2.6594e-01, -4.8332e-02, -2.3529e-04, -2.5979e-01,  5.1841e-02,  2.2183e-01],\n        [-1.7319e-01,  2.4701e-01, -2.2577e-01, -2.6445e-01,  5.6688e-02, -2.4374e-01, -1.1981e-01,  2.3065e-01,  2.2198e-03, -1.4340e-01,  9.6663e-02, -2.1702e-01, -2.5833e-01,  2.4533e-02],\n        [ 1.0588e-01,  1.6063e-01, -2.4500e-01, -2.6209e-01,  5.2656e-02,  5.6438e-02,  8.2733e-02,  5.8402e-02, -4.8094e-02, -1.2446e-01,  3.9170e-02, -2.3860e-01,  1.8881e-01,  8.5998e-02],\n        [ 5.9226e-02, -1.4174e-01, -3.7730e-02,  2.3575e-01, -1.0270e-01, -5.7931e-02, -9.1577e-02, -1.1009e-01,  1.7255e-01, -1.2510e-02,  2.3737e-02, -1.8262e-01,  8.1099e-02,  1.6102e-01],\n        [-1.8815e-01,  1.4034e-01,  1.1310e-01, -1.9456e-01, -6.4832e-02,  6.7984e-02,  4.4483e-02, -2.7626e-02,  1.5983e-01, -7.4210e-02, -1.0164e-02, -1.1431e-01,  1.5063e-01, -1.0907e-01],\n        [-9.7081e-02, -1.9657e-01, -5.5252e-03,  1.5728e-01, -1.9039e-02, -1.1248e-01,  2.5693e-01, -6.9797e-03, -8.2456e-02,  2.3326e-01,  1.3583e-01,  2.5937e-01, -1.5947e-01, -2.6649e-01],\n        [ 5.0316e-03,  1.8170e-01,  2.4396e-02,  2.3938e-01,  3.8607e-02, -4.2706e-02, -3.2769e-02, -2.2261e-01,  1.0550e-01,  1.6523e-01, -1.4212e-01,  2.2529e-01, -8.6795e-02, -2.1963e-01],\n        [ 2.4420e-01, -1.7551e-01,  4.7451e-02,  8.0799e-02,  2.5700e-01,  2.6517e-02,  1.9373e-01,  1.5066e-01, -1.7189e-01, -1.4517e-01, -1.1836e-01, -2.3219e-01,  1.9946e-01,  1.8676e-01],\n        [ 7.3455e-02, -1.4823e-01,  1.7425e-01,  2.3069e-01, -1.4042e-01, -1.3368e-01, -2.0862e-01, -5.8111e-02,  1.9561e-01, -8.5659e-02,  5.4310e-02, -2.0314e-01, -1.3582e-01,  2.6643e-01],\n        [-1.3259e-01, -1.2993e-01, -2.4455e-01, -3.2538e-02, -2.3606e-02, -5.2647e-02, -1.2242e-02,  1.6624e-01, -3.8172e-03,  2.2144e-01,  2.5090e-01, -2.3790e-01,  1.3931e-01,  1.6442e-01],\n        [-3.4889e-02,  9.3128e-02, -1.0183e-02,  2.3261e-01, -1.1507e-01, -1.9767e-01, -1.6675e-01,  1.9946e-01,  3.0684e-02, -2.3236e-01,  2.2253e-01,  2.5071e-01,  2.6531e-01, -1.6852e-01],\n        [-1.5168e-01,  1.7197e-02,  2.3489e-01,  6.9552e-02,  8.6589e-02,  1.0283e-01, -2.0710e-01, -1.0050e-01,  4.3034e-02,  2.4128e-01,  1.0719e-01,  6.8409e-02, -2.3758e-01, -7.4219e-02],\n        [ 2.0240e-01,  2.1736e-01, -2.1223e-01,  2.3109e-01,  8.5517e-02, -2.6714e-01,  1.3104e-02,  6.1787e-02,  9.8158e-03,  5.3243e-02,  6.8413e-02, -6.1679e-02,  2.6631e-01,  1.4668e-01],\n        [ 2.6039e-01,  9.1112e-02,  1.4326e-01, -1.0220e-01,  1.0453e-01,  1.1276e-01, -1.3066e-01, -1.5669e-01, -1.2445e-01, -1.1067e-01, -2.5271e-01, -2.3492e-01, -3.2827e-02,  1.3548e-01],\n        [ 1.7659e-01,  2.0363e-01, -8.5739e-02, -3.6047e-03,  2.1527e-01, -2.2558e-01, -8.3441e-02, -4.2636e-02, -2.4883e-01,  8.9080e-02, -1.0225e-01,  2.2402e-01, -1.7117e-01, -1.5149e-01],\n        [-1.1016e-01, -8.6520e-02,  1.8226e-01,  8.6512e-02,  4.5130e-02,  4.2764e-02, -5.1174e-02, -7.4066e-02, -2.2812e-01, -4.5265e-02, -8.8873e-02,  1.6175e-01, -2.0487e-01, -9.5239e-03],\n        [-1.5262e-01,  9.8992e-02,  1.7291e-01, -2.5134e-01, -4.7288e-03,  1.4578e-01,  1.5640e-01, -1.9567e-02, -1.2286e-02,  4.6131e-02, -7.0311e-02, -2.6028e-01, -2.6621e-01, -1.6499e-01],\n        [-3.3568e-02,  4.8991e-03, -3.5300e-02, -9.5831e-02, -2.2983e-01,  2.4444e-01,  1.2809e-01,  1.3375e-01, -1.4917e-01, -2.2852e-01,  1.1013e-01,  8.0323e-02, -2.3535e-03, -2.1108e-01],\n        [ 4.8738e-02, -1.7427e-02, -8.8009e-02, -1.8512e-01, -2.5653e-01,  4.0621e-02,  8.4411e-02,  1.0293e-01, -1.1801e-01, -2.1329e-01, -1.5996e-02, -1.5627e-01, -1.5361e-01, -3.3787e-03],\n        [-1.1921e-01,  1.5810e-01,  2.4170e-01,  2.0614e-01, -4.3269e-02,  2.1257e-01, -2.2075e-01,  2.4446e-01, -2.2765e-01,  1.4725e-01, -2.6058e-01, -4.8782e-02,  8.9816e-03, -7.1269e-02],\n        [-1.3046e-01,  5.5841e-03, -3.0144e-02,  1.5024e-01,  9.5880e-02,  6.6988e-02, -1.2809e-01,  1.9432e-01,  2.4548e-01, -2.4894e-01,  1.9480e-01, -2.4021e-01, -5.3224e-02, -4.9676e-02],\n        [-4.8754e-02, -3.2253e-02,  1.2425e-02, -3.1228e-02,  1.8867e-01,  2.3027e-01, -1.4307e-01,  1.2729e-01, -1.8181e-01,  1.3957e-02, -2.5995e-01,  1.5075e-01,  1.3977e-01, -3.6597e-02],\n        [-1.6328e-01, -6.3711e-02, -1.9297e-01,  2.5776e-01, -1.0008e-01, -2.0564e-01, -5.0870e-02,  6.5458e-02,  1.9968e-01,  2.2261e-01, -1.3665e-01,  2.3494e-01, -1.5433e-01,  4.2792e-02],\n        [ 1.5596e-01,  5.6544e-02, -1.9736e-01,  3.9742e-02,  1.8874e-01,  1.8170e-01, -7.8954e-02, -8.4452e-02, -1.0182e-01,  1.0449e-01,  1.5731e-01,  8.0669e-02,  2.0693e-01,  1.8195e-01],\n        [ 7.1004e-02, -8.3202e-02, -2.3123e-02, -6.9545e-02, -9.0046e-03,  3.4779e-02,  3.4449e-02, -2.5280e-01,  1.6913e-01,  2.4235e-01, -2.1919e-01, -2.1525e-02,  2.4840e-01, -1.2982e-01],\n        [-1.2185e-01,  1.7890e-01, -1.0507e-01,  2.3177e-01, -5.9560e-02,  2.3650e-01,  1.1776e-01,  1.8949e-01, -7.8832e-02,  2.3060e-01,  3.5961e-02,  2.0313e-01, -9.9706e-02,  6.6423e-02],\n        [-1.7438e-01,  3.2256e-02,  8.6694e-02,  2.0275e-01, -2.3994e-01,  8.6500e-02, -1.3195e-01, -2.2738e-01,  1.9469e-01,  2.0851e-01,  2.4913e-01,  1.5718e-01, -1.5131e-01, -6.6080e-02],\n        [ 2.3976e-01, -6.3416e-02,  1.4797e-01, -1.1619e-01,  1.7241e-01, -2.0335e-01, -1.1656e-01,  1.0522e-01, -1.4724e-01,  2.4818e-01,  1.0339e-01,  1.0484e-01,  2.4859e-01,  8.0344e-02],\n        [-4.4686e-02,  2.4542e-01, -2.6665e-01, -9.3403e-03, -1.2293e-01, -5.1649e-02, -2.4993e-01,  5.9803e-02,  1.3039e-01, -2.5949e-01,  1.1704e-01,  1.3636e-01,  2.3360e-01,  1.1828e-03],\n        [-2.5031e-01, -4.3968e-02,  2.4153e-01,  3.1790e-02,  5.4476e-02, -1.9114e-01,  1.8528e-01, -2.4258e-01, -2.6402e-02, -7.9941e-03,  2.6724e-01,  2.4806e-01, -1.5962e-01, -1.1571e-01],\n        [-1.1066e-01,  8.7543e-02, -2.5187e-01,  2.1625e-01, -1.4459e-01,  1.8642e-01,  4.2502e-02, -1.4900e-01, -2.0903e-01, -1.0039e-01,  6.0402e-02, -2.1626e-01,  1.8164e-01, -1.9668e-01],\n        [ 2.3063e-01,  7.9860e-02,  1.2808e-01, -1.4509e-01,  8.7043e-02, -8.0524e-02,  1.1156e-01, -1.8461e-01, -3.2319e-02,  2.2524e-01,  6.6485e-02, -2.3123e-01,  1.8392e-03,  1.8472e-02],\n        [ 2.2120e-01,  4.4181e-02,  1.6151e-01, -2.0798e-01,  1.9442e-01,  2.0630e-01,  2.4849e-01, -2.5331e-01,  1.1919e-01, -2.5907e-01,  1.9513e-01, -2.3211e-01, -2.2558e-01,  1.4480e-01],\n        [-2.4423e-01, -1.4352e-02,  1.8725e-03,  2.3524e-01, -5.8126e-02,  2.3981e-01, -1.7320e-01,  1.6177e-01,  1.7481e-01, -1.4539e-02,  3.0759e-02, -1.4113e-01, -1.0146e-01, -9.1281e-02],\n        [ 2.1913e-01, -2.2136e-01,  1.5500e-01,  1.6117e-01,  1.1757e-01,  8.6553e-03, -2.1348e-01, -2.0474e-02,  1.1057e-01,  2.3127e-01, -1.8289e-01, -3.8706e-02, -2.1361e-01,  4.2223e-02],\n        [-2.6502e-01,  1.2370e-01,  2.4644e-01, -2.3797e-01,  1.0277e-01, -4.5217e-02, -1.0300e-02,  6.8775e-02,  2.5439e-01, -1.1787e-01, -9.1551e-02,  1.0283e-01, -1.2675e-01, -4.9226e-03],\n        [-4.8601e-03,  1.6250e-01,  1.9239e-01,  1.4436e-01,  2.2937e-01,  2.6290e-01, -2.5211e-01, -9.4483e-03, -2.6544e-01, -1.5898e-01, -1.4836e-01, -1.8226e-01, -1.7381e-01,  1.5280e-02],\n        [ 8.7645e-03, -4.3840e-02,  2.2668e-01, -5.1809e-02,  1.7217e-01, -3.3813e-02,  1.1864e-02, -1.5683e-01,  1.9923e-01,  1.8113e-01, -1.9674e-03, -7.4054e-02,  1.1160e-01,  4.3380e-02],\n        [ 2.2153e-01, -2.3590e-01,  1.6494e-01,  1.5904e-01,  1.1685e-01,  2.9750e-03,  2.1362e-01,  1.2514e-01,  7.4991e-02,  1.1370e-01, -2.0711e-01, -2.6147e-01,  1.3674e-01,  4.7460e-02],\n        [-1.5219e-01, -1.6071e-01,  1.1176e-01,  1.0562e-01,  2.6513e-03, -7.9790e-04,  1.5187e-01, -2.0005e-01, -5.7614e-02,  3.4474e-02, -1.7467e-01, -2.4543e-01, -1.6987e-01, -2.5118e-01],\n        [-1.4924e-01, -2.4287e-01,  5.6541e-02,  9.9913e-02,  1.2238e-01, -2.2014e-01,  5.0119e-02,  1.0519e-01,  2.3576e-01, -2.0683e-01,  1.7322e-01, -5.4741e-03,  2.0171e-01,  5.0577e-02],\n        [-2.5761e-03, -9.4653e-02, -1.5717e-01, -1.1626e-01,  1.4367e-01, -1.2748e-01, -2.1853e-01, -2.4919e-01, -2.2145e-01,  1.1678e-01,  1.0910e-01, -1.3854e-01,  1.0061e-01, -1.3811e-01],\n        [-1.5263e-01,  3.1029e-02,  2.0633e-01, -4.9879e-02,  1.3152e-01,  2.1621e-02, -1.2807e-01,  2.1999e-02,  9.0982e-02, -2.3186e-01,  1.5455e-02, -1.0593e-01,  1.0760e-01,  2.4703e-02],\n        [-1.4504e-01, -1.9100e-02, -7.8302e-02,  9.3102e-02, -3.1144e-02,  2.6582e-01,  2.3756e-01, -2.5200e-01, -1.8479e-01, -1.0977e-01, -7.8942e-02, -2.2132e-01, -1.4392e-02,  8.9490e-03],\n        [-2.5375e-01,  4.1290e-02, -2.6608e-01, -1.7218e-01,  2.1451e-01,  1.1606e-03,  2.6216e-01, -8.3864e-03, -9.5369e-02, -1.7270e-01,  1.2665e-01,  3.1195e-02,  1.3462e-01,  2.0824e-01],\n        [-1.3224e-01, -5.6938e-02,  1.6855e-01, -2.0786e-01, -1.7353e-01, -5.7474e-02, -5.9006e-03,  7.7158e-02, -8.2461e-02,  1.6838e-01, -3.9446e-02, -6.1903e-02,  1.8469e-01,  1.8068e-01],\n        [ 6.8780e-02,  1.2336e-01,  1.1258e-01, -2.1024e-01,  2.6595e-02, -2.1958e-01, -9.2196e-02, -2.9309e-02, -2.3238e-01,  1.8959e-01,  1.2751e-01, -1.3470e-01,  2.5809e-01, -1.2689e-01],\n        [ 1.3842e-01, -2.2152e-01,  1.4747e-01,  2.2530e-01,  2.3991e-01, -4.8655e-02,  2.8824e-02, -3.0926e-02, -2.0340e-01, -1.0316e-01, -1.7178e-02, -2.3873e-01,  2.5259e-01,  5.9123e-02],\n        [-2.0114e-02, -2.3994e-01,  2.1506e-01,  1.6229e-01, -2.4260e-01,  1.5073e-02, -9.8192e-02,  5.8541e-02, -1.0091e-01,  3.7482e-02,  1.1108e-02,  2.2827e-01,  5.8330e-02, -1.4665e-01],\n        [ 2.0361e-01, -7.2752e-02, -2.2809e-01, -1.1663e-01, -4.4820e-02, -2.6715e-01,  9.3954e-02, -1.8107e-01,  2.3300e-01,  5.6205e-02, -1.4788e-01, -2.5467e-01, -1.6072e-01, -2.5152e-03],\n        [-4.9818e-02, -2.0764e-01,  7.9348e-02,  3.5772e-02, -5.5209e-02, -1.2032e-01, -5.3326e-02, -7.4120e-02, -5.1709e-02, -8.1600e-03,  1.7223e-01,  2.6958e-02,  2.2350e-01, -1.4975e-01],\n        [-1.7680e-01,  2.3624e-01, -1.6742e-01,  1.7266e-01,  1.8053e-01,  5.9193e-02,  2.2897e-01,  2.1221e-01, -1.5742e-01,  2.1852e-02, -5.4975e-02, -1.9727e-01,  1.9263e-01,  2.4609e-01],\n        [ 2.0590e-01, -2.4214e-01,  9.7760e-02, -4.1899e-02,  1.1502e-01,  2.3139e-01,  7.6401e-02, -2.6586e-01, -9.8386e-02, -4.4195e-02, -6.4805e-03, -1.1822e-01, -2.4947e-01,  2.6170e-01],\n        [-9.6720e-02, -1.8321e-01, -1.8102e-01,  6.1055e-03,  1.8520e-01,  1.0751e-01, -1.9404e-01,  6.9105e-02,  1.6068e-01, -2.1027e-01,  1.7613e-01, -2.0133e-01,  2.6383e-02,  8.4558e-03],\n        [ 5.7538e-02,  1.7940e-01, -2.0255e-02,  1.6548e-01,  2.0074e-01, -1.4758e-01,  1.7213e-01,  1.9369e-01,  2.0607e-01,  9.3952e-03,  2.6012e-02,  3.7710e-02,  2.4973e-01, -2.3862e-01],\n        [ 1.2436e-01, -9.7163e-02,  9.3530e-02,  6.0416e-02, -4.2214e-03, -2.6523e-02, -1.7347e-01,  2.6317e-01,  2.5872e-01, -1.6266e-01,  5.0985e-02, -2.2605e-01, -1.4865e-01,  1.8810e-01],\n        [ 2.2175e-01,  1.6305e-01,  2.2926e-01, -4.9028e-02,  5.1325e-02,  1.0027e-01, -1.9339e-01, -2.3270e-01, -1.3996e-02, -7.1867e-02, -1.9710e-01, -5.5470e-02,  1.0441e-01, -2.2296e-01],\n        [ 6.7762e-03,  4.5129e-02,  6.1188e-02,  1.9969e-01,  1.1007e-01, -1.2258e-01, -1.1117e-01, -2.1393e-01,  1.7390e-01,  2.2593e-01, -2.5656e-01, -2.1973e-01, -3.0834e-03, -3.4143e-02],\n        [ 2.5134e-01, -2.4743e-01,  1.6111e-01, -2.5688e-01,  3.5612e-02,  1.6721e-01, -1.1953e-01,  4.9236e-02,  1.8801e-01, -4.9043e-02,  1.1881e-01, -1.6358e-01, -2.2917e-01,  1.8844e-01],\n        [-2.5512e-01, -1.0903e-01,  5.0471e-02,  7.5323e-02,  1.6074e-01,  1.4210e-01, -2.0940e-01, -1.6105e-01,  1.7710e-01, -1.5215e-01,  6.4772e-02, -1.6425e-01, -2.4393e-01,  2.3466e-01],\n        [-2.1336e-01, -9.7288e-02,  3.5683e-02,  1.1664e-01, -1.7097e-01, -8.7099e-02, -2.4678e-01,  6.4244e-02,  2.3106e-02, -1.6533e-01,  1.9280e-01,  4.5749e-02, -2.5941e-01, -5.9408e-02],\n        [-2.2520e-02, -2.4215e-01, -8.5085e-04,  2.4627e-01,  1.4089e-01,  1.1826e-01, -7.5575e-02, -5.9278e-02, -1.8262e-01, -1.9635e-01, -1.9251e-01,  7.5797e-02,  1.3001e-01,  9.2868e-02],\n        [-3.3757e-02,  1.8814e-01, -1.0511e-01, -1.2211e-01, -1.6652e-01,  3.6622e-02, -2.3781e-01, -4.1789e-02, -1.9527e-01, -2.0956e-01,  1.7898e-01,  1.3447e-01,  1.7752e-01,  1.8919e-01],\n        [ 1.8683e-01,  4.1007e-02, -2.4399e-01,  1.2415e-01, -1.6847e-01, -6.4976e-02, -6.2993e-03, -1.9252e-01,  2.5629e-01,  2.0508e-01, -7.5857e-02, -7.2394e-02, -2.3109e-01,  7.6826e-02],\n        [-5.5115e-02, -5.4374e-03,  2.0538e-02,  1.4306e-01,  2.0615e-01,  2.1292e-02,  1.3298e-01, -6.4232e-02,  9.8196e-02,  1.0870e-01, -1.1584e-01, -1.0115e-01,  1.9744e-01, -2.2805e-01],\n        [-1.8013e-01, -5.1658e-02,  1.0592e-01, -2.1666e-01, -1.9932e-01, -4.4734e-02,  1.9894e-01, -1.0046e-01, -1.9984e-02,  7.7710e-02,  2.1224e-01, -9.0697e-02,  2.2386e-01,  2.6034e-01],\n        [ 1.1189e-01,  1.6847e-01,  2.4406e-01,  1.1415e-01,  2.3909e-01,  1.9784e-01, -8.4127e-02,  6.8948e-02,  2.5102e-01,  2.3610e-01, -4.1003e-02,  6.9754e-02,  2.5276e-01, -1.8997e-01],\n        [ 2.1187e-01,  1.1469e-02,  5.2332e-02,  1.0334e-01,  2.3044e-01, -1.2799e-01,  1.6361e-01,  2.3275e-01,  2.0618e-01,  1.4814e-01,  1.9653e-01,  1.5584e-01, -1.5449e-01,  1.3414e-01],\n        [-1.9299e-01,  1.5646e-01, -2.4733e-01,  3.8988e-02,  1.0989e-01,  2.9011e-02, -1.6439e-01, -6.2548e-02,  2.3581e-02, -7.5018e-02,  2.5667e-01,  9.6276e-02, -9.9025e-02,  5.1115e-02],\n        [-6.1946e-02,  1.8937e-01,  3.7195e-02,  1.5817e-01, -1.0095e-01,  2.5202e-01,  2.3933e-01,  1.3071e-01, -1.2335e-01, -2.5135e-01, -1.9295e-01, -8.4147e-02,  1.5088e-01,  2.6170e-01],\n        [-3.5252e-02,  2.4850e-01, -1.6979e-01, -1.6202e-01,  1.1835e-01,  6.6630e-02,  2.0848e-01, -2.3356e-01,  2.1753e-01, -2.5967e-01,  1.3289e-01, -2.5879e-01,  5.0704e-02, -5.0087e-02],\n        [-6.0143e-02,  6.9720e-02, -2.2334e-01,  1.3245e-01,  2.1438e-01,  1.9323e-01,  2.5851e-01,  1.6335e-01, -2.4383e-01,  6.0424e-02, -1.5762e-01,  2.0877e-01, -3.8124e-02, -1.8383e-01],\n        [-9.7684e-02,  2.6513e-01, -2.3029e-01, -1.2905e-01, -1.6364e-01,  1.2235e-01,  2.0090e-01, -1.1118e-01,  1.2507e-01,  8.8906e-02, -7.6199e-02,  1.3044e-01,  2.5633e-01,  5.6869e-02],\n        [ 4.4326e-02, -2.2586e-01, -1.0072e-01, -1.2801e-01, -1.1051e-01,  6.0311e-02,  2.3660e-02, -2.5931e-01, -1.4172e-01,  1.0559e-01,  2.2610e-01,  3.7882e-02,  1.7242e-01, -9.9269e-02],\n        [ 4.9080e-02, -6.7425e-02, -2.3096e-01,  1.8219e-01, -1.6275e-01,  1.7672e-01, -2.1284e-01, -4.5454e-02,  1.0227e-01,  2.1568e-01, -2.4347e-01, -1.3357e-01,  2.2805e-01,  2.1984e-01],\n        [ 2.6507e-02,  1.1541e-01,  2.2783e-01,  1.9746e-01, -5.9540e-02,  2.5260e-01, -1.8175e-01, -1.2185e-01,  8.6412e-02,  1.3548e-01,  2.1991e-01,  9.1132e-02,  5.9423e-02,  6.2190e-03],\n        [-1.8279e-01, -2.2706e-02,  3.2139e-02,  1.3852e-01,  2.3911e-01,  2.6022e-01,  8.1419e-02,  2.1302e-01,  2.0341e-01, -5.3421e-02, -6.8973e-02, -1.4385e-01, -1.3441e-01,  8.8615e-02],\n        [ 2.3999e-01, -1.4851e-01, -1.1856e-02, -2.2780e-01,  1.3561e-02, -2.4412e-01,  2.6597e-01, -1.7792e-01, -2.0277e-01, -2.1075e-01, -2.3059e-01,  1.2694e-01,  1.6147e-01,  3.2036e-02],\n        [ 2.2535e-02, -2.2900e-01,  2.4513e-01,  1.3567e-01,  1.3053e-01, -1.4111e-01,  7.6029e-02,  7.5021e-02,  1.4515e-01, -8.2258e-02,  5.5362e-02,  2.6633e-01,  2.1951e-01, -1.0921e-01],\n        [ 4.7228e-02, -1.1385e-01, -8.0971e-02, -4.5779e-02,  2.7056e-03,  1.6010e-02,  1.3312e-01,  5.4780e-02, -2.5860e-03, -1.2185e-01,  2.3455e-01,  2.2052e-01,  2.5134e-02,  2.5185e-01],\n        [-1.7185e-02, -2.3565e-01,  2.0841e-01,  1.5232e-01, -1.5845e-01, -9.3655e-02, -1.7212e-01, -3.2996e-03, -3.0670e-02, -1.0161e-01, -1.7353e-01,  1.1275e-01, -1.0769e-01, -9.5826e-02],\n        [ 2.5683e-01,  1.1824e-01, -1.0307e-01, -1.9720e-01,  6.3084e-02, -6.3927e-02,  1.9842e-01,  5.5227e-02,  1.6948e-01, -6.5287e-02,  1.9003e-01,  5.3691e-02,  2.6645e-01,  2.3284e-02],\n        [-1.0115e-03, -1.4283e-01, -4.1024e-02,  1.4507e-01,  7.4412e-03,  2.5997e-01, -2.6614e-01, -8.9842e-03,  1.3545e-03, -1.4655e-01, -1.5686e-01,  8.7644e-02,  7.0295e-02, -2.0469e-01],\n        [-2.0966e-01,  2.2624e-01, -1.5886e-01, -1.3336e-01, -7.4852e-02, -1.4825e-01, -2.5976e-01, -1.4398e-01, -1.3278e-01, -2.3917e-01,  1.3898e-02, -5.9465e-02,  1.1499e-01,  1.0289e-01],\n        [ 8.0163e-02, -2.1377e-01,  1.1883e-01, -5.4670e-02, -1.2187e-01,  1.9519e-01,  2.3269e-01, -2.0092e-01,  2.3303e-01,  2.4988e-02, -9.4626e-02, -4.0802e-02, -1.0897e-01,  6.3692e-02],\n        [ 7.0485e-02,  1.9816e-01, -1.7641e-04, -1.3442e-01, -4.0750e-02, -1.9814e-01,  1.3421e-01, -1.1384e-01,  2.8923e-02, -3.7134e-02,  1.7280e-01, -1.8456e-02,  9.2717e-02, -1.3158e-01],\n        [-1.7898e-01,  1.7489e-01, -1.2908e-01, -2.3576e-01,  1.4638e-01,  2.1024e-01,  2.5560e-01,  1.5399e-01,  2.3307e-02,  2.3443e-01,  2.3477e-01, -6.5682e-02,  8.5289e-02, -1.3489e-02],\n        [-2.5083e-01,  2.1373e-01,  1.2591e-01,  1.6822e-01,  1.7413e-01,  2.1015e-01, -2.8491e-02, -2.6542e-01,  2.5185e-01,  1.3783e-01, -1.9766e-02,  1.5125e-01,  4.1210e-02, -1.5429e-01],\n        [-2.4732e-01,  1.4386e-01,  2.2169e-01,  2.0090e-01, -8.2016e-02, -1.6493e-01,  6.1203e-02, -2.5957e-01,  3.1852e-02,  1.3890e-01,  1.9252e-01, -9.5289e-02,  1.0841e-02, -1.8540e-01],\n        [-1.5528e-01,  4.1953e-02, -6.8634e-02, -4.5341e-02,  1.4401e-01, -2.0760e-01,  1.7792e-01,  2.3785e-01,  1.3063e-01,  2.2449e-01, -2.0443e-01,  1.9864e-01,  1.0937e-01,  9.7493e-02],\n        [ 1.9973e-01,  2.4689e-01, -2.4295e-01, -2.0877e-01,  1.2984e-01, -1.9368e-01,  2.0977e-01, -6.6689e-02,  6.1614e-02, -2.0371e-01,  1.6970e-01,  7.6741e-02,  1.4012e-01, -1.1591e-01],\n        [ 1.8879e-01,  5.7322e-02,  8.1856e-03,  9.1444e-02, -2.0712e-01, -1.4416e-01,  1.7846e-01,  1.3324e-01,  8.7634e-02, -2.6050e-01, -2.4025e-01, -7.6348e-03,  5.8821e-02, -8.6576e-02],\n        [ 2.6476e-01,  3.6788e-02, -1.7446e-01,  2.3795e-02,  1.2792e-01, -1.7271e-01, -2.4090e-01,  9.5267e-02,  1.2353e-01, -9.0005e-02, -2.2877e-01,  1.7731e-01, -9.1289e-03, -2.1109e-01],\n        [ 6.6245e-03, -2.1393e-01, -6.6858e-02,  1.0502e-02,  4.7523e-02,  1.2014e-01,  1.0129e-01,  1.5016e-01,  3.2972e-02, -1.3915e-01,  7.0775e-02,  1.7605e-01, -8.1475e-02,  1.9822e-01],\n        [ 3.3809e-02,  4.6600e-02,  8.0828e-02, -1.2784e-01,  5.8742e-02,  2.4197e-01,  3.8654e-02, -7.6101e-02,  1.5584e-01,  4.0473e-02,  1.9469e-01,  2.0723e-02, -6.4936e-02, -1.2992e-01],\n        [-3.0812e-02,  9.0122e-03, -7.4938e-02, -1.1426e-02,  6.7505e-02, -5.7553e-02, -1.2206e-01, -2.7625e-02, -6.7806e-02,  1.6964e-01,  2.3975e-01, -1.8090e-01,  4.2261e-02, -2.1989e-01],\n        [-1.5000e-01,  4.0102e-02,  1.2780e-01, -1.6669e-01, -5.7406e-03, -8.3044e-02,  1.6896e-02, -1.2667e-01, -1.4766e-01,  1.0148e-01,  2.1476e-01,  1.6843e-01, -1.9027e-02,  1.1606e-01],\n        [-2.6638e-01,  1.4768e-01,  1.4406e-01, -1.8986e-01,  2.6221e-01, -5.8918e-02,  2.0941e-01, -1.5526e-01,  4.0685e-02, -1.7136e-01, -1.5636e-01,  1.3606e-01,  2.2197e-02,  2.5250e-01],\n        [ 6.3160e-02, -2.6879e-02, -1.4409e-01, -1.2793e-01, -1.6502e-02, -2.4940e-01, -7.3425e-04,  2.5011e-01, -1.7892e-02, -9.1516e-02, -1.5109e-01, -2.0283e-01,  1.6986e-01, -7.0012e-02],\n        [-2.5102e-01, -9.4462e-02, -7.8574e-02, -1.9343e-01,  2.2856e-01,  2.0453e-01, -2.3662e-01, -2.3344e-01,  3.8906e-02,  1.0073e-01, -1.1929e-01,  4.4055e-02, -4.8582e-03, -1.5281e-01],\n        [-1.8828e-01,  5.5187e-02,  2.0884e-01, -4.7271e-02, -1.2328e-01,  2.3343e-01, -1.3472e-03,  9.5353e-02, -2.6565e-01, -1.2259e-01, -1.8506e-01, -2.0421e-01,  1.0926e-01, -5.8437e-02],\n        [-7.3283e-02, -1.1755e-01, -5.8690e-02, -2.3509e-01, -2.2218e-01,  1.1010e-01,  2.5958e-01, -1.2695e-01, -7.8341e-02, -4.4926e-02,  2.5863e-01,  2.0155e-01, -3.8255e-02, -2.3557e-01],\n        [ 2.0229e-01, -4.6706e-02,  4.5241e-02,  2.3674e-01,  2.1215e-01, -1.8990e-01,  1.6077e-01, -1.4331e-01,  4.2754e-02, -6.5244e-02, -1.2658e-01, -2.3665e-01, -2.0676e-01, -3.6405e-02],\n        [-2.5777e-01,  1.8555e-01,  1.6814e-01,  2.2250e-01,  1.3778e-01,  3.3424e-02, -1.6050e-01, -6.2608e-03,  1.3958e-01,  1.8175e-01, -8.0454e-02, -2.2287e-01,  1.9179e-01, -5.6102e-02],\n        [ 1.6053e-02, -4.4412e-02, -2.3561e-01,  1.8631e-01, -2.5845e-01, -1.6741e-01,  3.1997e-02, -6.3935e-02,  1.5272e-01,  2.3086e-01,  2.9391e-03,  6.6783e-02,  1.7754e-01,  1.7257e-01],\n        [ 2.2760e-01,  1.1611e-01, -1.4264e-01,  2.6649e-01,  2.1102e-01,  9.9483e-02, -8.5161e-02, -2.4859e-01, -1.4466e-02,  2.1033e-01, -1.3370e-01,  1.5922e-01,  9.8630e-02, -1.3888e-01],\n        [ 1.8585e-01,  8.4122e-02,  1.2315e-01,  1.7109e-01,  7.9902e-02, -3.5035e-02, -4.4407e-02, -6.3367e-02,  8.8464e-03, -1.8796e-01,  6.5970e-02,  9.5882e-02, -8.9496e-02,  2.5440e-01],\n        [-2.1786e-01, -1.8532e-01, -5.9229e-03, -2.5747e-01, -1.5753e-01,  1.9291e-01,  7.8648e-02, -1.8566e-01,  1.7343e-01,  3.7847e-02, -1.9957e-01,  1.7606e-01, -2.6714e-01,  6.6346e-02],\n        [ 1.5897e-01, -1.5243e-01,  2.5920e-01, -1.9925e-01,  5.4111e-02,  1.9093e-01, -1.2087e-02,  9.6751e-02,  7.9388e-02, -2.4628e-01,  2.2967e-01,  4.0033e-02, -7.8816e-02,  7.1028e-02],\n        [ 1.8972e-01,  6.6851e-02, -1.8479e-01,  5.1944e-02,  1.7801e-01,  2.2074e-01, -9.4301e-02,  1.3866e-01, -2.5563e-01,  4.9658e-02,  3.0291e-03,  9.3280e-02,  9.3103e-02, -2.2014e-01],\n        [-1.2113e-01,  1.6689e-02, -9.8745e-02, -1.9194e-02, -1.4694e-01, -1.2676e-01,  2.3017e-01, -7.3561e-02, -2.6464e-01,  1.9820e-01, -1.4191e-01, -7.1159e-02, -2.5180e-01, -5.3073e-02],\n        [ 4.5370e-03, -1.2192e-01, -1.7515e-01, -2.0618e-01, -2.0122e-01, -2.1038e-01,  8.1290e-02,  1.7280e-01,  1.5379e-01,  2.4780e-02, -1.3754e-01, -4.8148e-02,  9.8011e-04, -7.6301e-02],\n        [-6.2884e-02,  1.0815e-01, -4.2734e-02,  2.0244e-01,  8.5328e-02,  2.0377e-02,  2.5227e-01,  3.5834e-02, -2.0844e-01,  1.7455e-01,  7.1985e-02, -2.3146e-01,  2.5353e-01,  6.8358e-02],\n        [-2.5033e-01, -1.0184e-01, -2.1436e-01, -1.3666e-01, -5.6017e-02, -1.8330e-02, -6.6974e-02,  1.2629e-03,  1.8446e-01, -9.8364e-02, -9.9517e-02,  3.5586e-02,  2.1269e-01,  2.0446e-01],\n        [-2.3431e-01, -1.0388e-01,  1.6687e-01,  4.3729e-02,  2.1220e-01,  1.8640e-01,  2.9424e-02, -1.4254e-01,  7.1076e-02, -1.5811e-01,  1.1769e-01, -1.1885e-01,  1.8860e-01,  6.4509e-03],\n        [-6.7716e-02, -2.4672e-01, -1.5904e-01, -2.3084e-01,  1.5887e-01,  1.4843e-01,  1.7455e-01,  1.5466e-01,  1.9048e-02, -1.4308e-01,  1.1113e-01, -1.1649e-01,  8.4612e-02, -1.1815e-02],\n        [-4.0258e-02, -4.5197e-02,  5.9040e-02,  2.1230e-01, -5.1367e-03, -9.6010e-02,  2.1730e-01, -1.3644e-01,  1.9521e-02,  2.1010e-01,  2.4047e-01,  2.2901e-01,  5.8593e-02,  2.5650e-01],\n        [-1.3300e-02,  1.8204e-01,  7.6405e-02, -1.7302e-01, -1.8506e-01,  2.1666e-02,  1.2657e-01, -1.5539e-01,  2.3764e-01,  1.3761e-01, -2.1877e-01, -4.7316e-02,  2.5639e-01,  8.4637e-02],\n        [-1.9039e-01, -6.2925e-02,  1.6893e-02, -2.1442e-01, -1.7962e-01, -2.0943e-01,  1.4622e-01,  2.4003e-01,  7.3459e-02, -4.8092e-02, -1.8428e-01,  1.3626e-01, -5.7273e-02, -2.0254e-01],\n        [ 1.1187e-01,  2.4695e-01, -2.2479e-01, -3.6963e-02, -2.1994e-01,  2.6606e-01, -1.6355e-01,  1.3928e-01,  3.1762e-02,  1.3170e-01,  2.5157e-01,  7.7332e-02, -7.4897e-02, -6.0256e-02],\n        [-1.8666e-01, -1.0839e-01,  5.5220e-02,  2.1787e-01, -2.2957e-01, -3.9731e-02,  2.2829e-01,  3.2424e-02,  1.8736e-01, -2.3457e-01,  1.0416e-01,  8.2516e-02,  6.6718e-02,  1.8444e-01],\n        [-1.9491e-01,  5.0806e-02,  1.1508e-02, -2.5776e-01,  4.4754e-02,  1.0748e-02, -1.7335e-01, -1.7913e-01, -8.5483e-02,  1.7328e-01,  1.1509e-01, -1.1509e-01,  5.4913e-03, -5.5981e-02],\n        [-7.3945e-02, -6.1938e-02, -2.1577e-02, -7.0558e-02, -8.3135e-02,  2.0199e-01,  1.1971e-01, -5.6338e-02, -1.0292e-02,  1.3283e-01, -6.2319e-02,  2.6583e-01, -3.9133e-02,  8.8497e-03],\n        [ 2.6023e-01,  1.7663e-01,  2.1848e-01, -1.0679e-01,  3.3589e-03, -2.6427e-02,  1.8295e-01,  8.7308e-02,  2.4415e-01, -1.5593e-01, -1.4288e-01, -1.9963e-01,  3.7020e-02, -8.2315e-02],\n        [ 2.0188e-01, -2.2353e-01, -2.3641e-01, -3.4818e-02,  5.1397e-02, -2.1885e-01,  1.3869e-01, -1.2945e-01,  2.6388e-01, -1.2482e-01,  4.4672e-02, -2.5517e-01, -2.5695e-01, -5.8412e-02],\n        [-8.2284e-02,  4.0452e-02, -2.3559e-01, -1.4609e-01,  2.1479e-01,  4.3606e-02,  2.2137e-01,  1.1527e-01, -3.9305e-02,  1.2737e-01, -1.5832e-01, -1.6002e-01,  2.2983e-01, -1.0430e-01],\n        [-2.2360e-01, -8.0961e-02,  1.8981e-01, -9.7308e-02, -1.1272e-01, -2.0424e-01, -1.8382e-01, -1.3099e-02, -1.5867e-01,  1.1612e-01, -1.0788e-01,  5.0077e-02, -7.0249e-03,  4.4437e-03],\n        [-1.5280e-01,  9.7910e-03,  7.8761e-02, -7.1967e-02, -6.1109e-02,  1.5028e-01,  3.2488e-02, -2.3181e-01, -2.2960e-01, -4.3074e-02,  9.7969e-02, -1.5287e-01,  2.1621e-01,  1.2700e-02],\n        [-7.5005e-02,  3.8133e-02,  6.4172e-02,  1.9064e-01,  1.6095e-01,  1.0120e-01,  1.8556e-01, -4.0965e-02,  9.9654e-02,  1.5852e-01, -1.0928e-01, -1.7518e-01,  1.6733e-01,  2.3091e-01],\n        [-1.5516e-01, -2.1759e-01,  2.2211e-01,  9.8535e-02, -1.7361e-01,  2.0919e-01,  3.3309e-02,  2.0937e-02, -1.9608e-01,  1.4962e-01,  4.3062e-02, -1.5934e-01,  1.5935e-01,  1.8499e-02],\n        [-2.3192e-01,  1.0990e-01,  2.5381e-02,  2.3073e-01, -1.7886e-01, -2.2414e-02, -1.2534e-01, -3.6925e-02, -5.7650e-02, -7.1470e-02,  1.5720e-01, -2.2000e-01, -2.0681e-01, -3.4369e-02],\n        [-3.1430e-02, -1.9505e-02, -6.6441e-02,  2.5560e-01,  5.6279e-02,  1.0299e-02,  6.9487e-02, -1.5081e-01, -5.8474e-03, -1.0272e-02,  1.2653e-01,  1.3200e-02, -9.1251e-03, -1.6072e-01],\n        [ 4.7151e-02,  1.8583e-01, -1.5815e-01, -2.2891e-01, -1.7960e-01,  1.4770e-01,  4.7740e-02,  1.5463e-01,  2.0680e-01, -1.6069e-01,  1.2288e-02,  1.9132e-01,  1.3877e-01,  8.7784e-02],\n        [ 1.7090e-01,  1.3946e-01, -8.8441e-02, -4.1378e-02,  1.2891e-01,  2.5330e-01, -2.7316e-03,  9.7564e-02, -8.8202e-02,  1.3334e-01,  7.6443e-02, -1.9426e-02,  8.3693e-02,  1.2477e-01],\n        [ 1.6687e-01,  2.6710e-01, -2.4784e-01,  1.9393e-01,  1.9618e-01,  2.1514e-01,  2.0381e-01, -5.8465e-02, -4.7141e-02,  1.6619e-01, -1.2294e-01, -1.3607e-01, -9.4586e-02, -1.8164e-01],\n        [-2.1577e-01,  8.8599e-02, -2.2317e-01, -2.5707e-01,  4.3863e-02, -1.4171e-01, -9.9219e-02,  2.5265e-01, -1.0604e-01, -4.4423e-02, -2.2129e-02, -5.4564e-02, -1.1278e-03, -3.4776e-02],\n        [-1.6263e-01,  1.9892e-01,  7.5697e-02,  1.4942e-01, -2.1313e-01,  1.1737e-01, -2.1348e-01,  2.1191e-01, -9.8924e-02, -1.6449e-01, -2.2114e-01, -2.5588e-01, -7.1368e-02,  2.0704e-01],\n        [ 1.9251e-01,  2.9537e-02, -1.1157e-01, -2.6259e-01,  1.6155e-01,  2.3818e-01,  1.5788e-01, -1.2531e-01,  1.2496e-01,  1.6802e-01,  4.7933e-02, -1.6363e-01,  6.0865e-02,  1.1087e-01],\n        [-2.1435e-01,  1.0335e-01, -2.3618e-01,  1.6231e-01,  1.7959e-01,  4.8873e-05,  2.5359e-01,  1.1111e-01,  1.4668e-01, -1.7957e-01,  3.7349e-02, -1.2047e-01,  4.2183e-02, -1.3368e-01],\n        [ 2.7254e-02,  7.0278e-02, -1.3840e-01,  2.0951e-02, -1.7258e-01,  4.6635e-02, -3.5666e-02, -2.1896e-01,  9.1865e-02, -2.5387e-01, -2.4731e-01,  1.5956e-01, -7.4785e-02,  1.1237e-01],\n        [ 7.0688e-02, -2.1493e-01, -1.2264e-01,  2.4097e-01,  2.1899e-01, -6.2829e-02,  2.6049e-02, -2.4585e-01, -2.3125e-01,  1.7654e-01,  2.5452e-01, -1.8893e-01,  7.8523e-02,  9.5917e-02],\n        [ 3.1261e-02,  9.9466e-02, -1.5584e-01, -2.0491e-01, -1.5617e-01,  2.2702e-01, -3.7525e-04,  1.6740e-01, -1.6725e-01, -7.6815e-02, -1.8521e-01,  5.9596e-02,  2.6822e-02, -8.0341e-03],\n        [ 1.3122e-02, -7.4493e-02,  1.8639e-01, -1.0174e-01,  8.7859e-02,  1.5182e-01, -1.1910e-01,  1.6090e-01,  1.7768e-01,  1.1898e-01, -7.7851e-02, -1.2086e-01, -2.3593e-01, -1.2835e-01],\n        [ 5.1462e-02, -1.2067e-01,  1.0861e-01, -1.2246e-01, -1.2721e-01, -2.4995e-01, -1.6675e-01, -2.4885e-01,  1.7384e-01,  1.2917e-01, -1.2148e-01, -1.9554e-01,  1.8937e-01, -1.1277e-01],\n        [ 2.1530e-01, -2.3248e-01, -4.9243e-02,  1.4223e-01,  2.0357e-01, -2.4415e-01,  1.2761e-01, -5.2167e-03,  1.0985e-01,  1.4083e-01, -2.5040e-01,  2.5974e-01,  3.2132e-02,  7.0974e-02],\n        [-1.3658e-01,  2.4942e-01,  3.7344e-02,  2.0776e-01,  1.1186e-01, -2.2076e-01,  2.6566e-01,  1.2690e-01,  1.5223e-01, -1.2611e-01,  7.6145e-02,  9.8138e-02, -2.0374e-01,  3.6895e-02],\n        [-1.3942e-01,  1.6615e-01, -7.1710e-02,  8.9187e-02, -2.0487e-01, -2.3009e-01, -2.4455e-01, -3.1620e-02,  8.4134e-02,  6.2943e-02,  4.2699e-03, -2.6197e-01, -1.9935e-01,  1.6845e-01],\n        [-1.4010e-01,  6.7252e-02, -1.5973e-02, -5.6966e-02, -9.6122e-02,  1.1572e-01, -2.2260e-01, -1.9268e-01,  1.5140e-01, -1.7956e-01, -3.0630e-02, -2.0223e-01,  7.4535e-02, -1.3558e-02],\n        [ 1.1590e-01, -2.2287e-01, -1.3942e-01, -5.8579e-02, -9.6985e-02, -2.3677e-02, -2.6967e-02, -1.5316e-01,  1.0674e-02, -2.3848e-01, -1.7572e-02, -2.4463e-01,  1.3009e-01,  2.3520e-01],\n        [ 2.5278e-01,  1.4658e-01, -2.3616e-01,  2.3583e-01, -2.2289e-01, -1.9034e-01, -6.3352e-02,  1.1255e-01,  2.4585e-01,  1.7541e-01,  1.8739e-01, -1.4602e-01, -1.8899e-01, -1.0942e-01],\n        [-8.7991e-02,  1.8629e-01, -2.3021e-01, -8.4871e-02,  1.5980e-02,  2.4185e-01, -7.3709e-02,  2.3594e-01, -1.4160e-01,  2.0758e-01, -2.4369e-01,  2.9913e-02,  2.3313e-01,  7.8981e-03],\n        [ 2.3504e-01,  2.2293e-02, -1.5177e-01, -1.2586e-01, -1.0183e-01, -2.3963e-01, -1.4937e-01, -8.5840e-02, -2.6712e-01,  2.9084e-02, -7.0020e-02,  9.1747e-02,  2.5112e-01,  7.8263e-02],\n        [-4.3037e-03, -4.4147e-03, -1.3626e-01, -2.6406e-02, -2.4234e-02,  1.9090e-02,  5.2964e-02, -1.8983e-01, -3.6342e-02, -2.0860e-01, -4.5039e-02, -1.9490e-01, -2.3186e-01,  9.4657e-02],\n        [-1.2713e-01,  1.5270e-01, -2.4873e-01,  3.4769e-02, -1.2295e-01,  2.5768e-01, -5.8691e-02,  2.5568e-01, -8.3915e-02,  9.1454e-02,  2.9158e-02, -1.8228e-01,  6.3635e-02, -1.6821e-01],\n        [ 1.7873e-01,  8.7124e-02,  2.4842e-03,  5.9197e-02,  1.4047e-01,  2.6273e-01,  2.4372e-01,  1.1826e-01,  9.5018e-02, -1.7892e-02,  1.8516e-02, -1.9350e-01, -1.0111e-01,  1.8969e-01],\n        [-1.7477e-01,  9.6107e-02, -1.1949e-01,  1.0428e-01,  2.9562e-02,  3.1427e-03,  1.2989e-01,  1.3003e-01, -1.9844e-01, -1.5393e-01,  9.6896e-02,  1.8737e-01,  1.5600e-01, -1.4601e-01],\n        [-2.0841e-02, -2.5763e-01,  6.0037e-02, -2.2567e-01, -2.4978e-01, -7.7683e-02, -9.9496e-02, -1.1802e-01,  2.4571e-01, -3.0517e-02,  1.9349e-01,  2.2206e-01, -2.0519e-02,  1.9966e-01],\n        [ 5.4341e-02, -2.1247e-01, -1.3025e-01, -1.2244e-01, -1.4156e-01, -1.6124e-01,  4.7145e-02,  1.3776e-01, -1.9168e-01, -2.3752e-01,  1.3710e-01, -1.5720e-01,  8.1108e-02,  2.2980e-01],\n        [ 2.5233e-01, -1.7118e-01, -8.2876e-02,  1.0847e-01, -6.8028e-02, -2.3654e-01,  2.1894e-01, -1.4646e-01,  2.0169e-01,  5.3931e-02, -1.7778e-01,  5.5934e-02,  1.4307e-01,  8.6023e-02],\n        [-2.0915e-01,  8.8895e-02, -2.5456e-01, -9.5613e-02,  1.8580e-01, -1.2339e-01,  4.9276e-02,  2.3763e-01,  3.3217e-02, -9.3342e-02, -2.0100e-01, -5.0296e-02,  4.9333e-03, -1.4973e-01],\n        [-8.7450e-02,  1.5835e-01,  2.3352e-01, -1.3844e-01, -1.9335e-01,  1.6205e-01, -1.4774e-01,  6.7894e-02, -2.6104e-01,  1.9470e-01, -2.3618e-01,  2.6299e-01, -1.2281e-01,  1.9562e-01],\n        [-5.4104e-02,  1.9516e-01, -5.0301e-02,  1.2092e-01, -1.9108e-01,  2.0577e-02,  1.9259e-01,  2.5320e-01,  9.3237e-02, -4.4707e-02, -2.2818e-01, -1.4459e-02, -2.3704e-01,  2.3542e-01],\n        [ 2.0573e-01, -1.7611e-01, -2.4869e-01,  1.4546e-01,  8.4269e-02, -2.1939e-01, -1.4305e-02, -1.0445e-01,  1.1869e-01, -1.8594e-01,  1.7054e-01,  1.2831e-01,  1.4631e-01, -2.0057e-01],\n        [ 1.0766e-01, -2.5493e-01, -6.3163e-02, -4.3755e-02,  1.1826e-02, -1.1477e-01, -1.5592e-01, -1.9386e-01,  2.1770e-01, -2.0330e-01,  1.9897e-01,  2.1689e-01, -1.0635e-01, -7.8326e-03],\n        [ 3.6001e-02, -1.9753e-01, -1.7730e-01, -4.9139e-02, -6.6730e-02,  1.2559e-01, -2.0118e-01,  1.2891e-01,  1.9340e-02,  7.4119e-02,  1.7217e-01,  9.7579e-02, -2.5208e-01,  1.1015e-01],\n        [-1.6794e-01,  6.7343e-02, -1.4587e-01, -1.8337e-01,  1.3382e-01, -7.4288e-02,  1.9539e-01,  6.2862e-02, -4.2649e-02,  2.5288e-01,  6.1199e-02, -1.3591e-01, -2.5899e-02, -1.7499e-02],\n        [-2.0147e-01,  4.7482e-02, -2.6445e-01, -4.8386e-02,  2.1610e-01,  4.6003e-02,  1.2947e-01, -1.8635e-01, -9.8503e-02, -2.3933e-01,  2.5227e-02, -2.3070e-01,  3.4365e-02,  9.2821e-02],\n        [ 3.1883e-02, -2.4115e-01, -2.4027e-01, -1.8986e-01,  2.3607e-01,  2.5016e-01,  1.2537e-01,  2.1715e-01, -1.3471e-01,  1.6805e-01,  7.2180e-02,  3.5617e-02,  8.7467e-02,  2.3695e-01],\n        [-1.8691e-01,  1.5514e-01, -1.8631e-01,  2.5530e-01,  3.8263e-02, -1.3318e-01,  2.5271e-01,  1.5702e-01,  1.0481e-01,  5.2334e-02,  2.1559e-02, -1.6015e-01, -1.5317e-01,  1.9163e-01],\n        [-1.5267e-01, -1.2581e-01,  7.4147e-02,  1.9845e-01, -1.9343e-01,  1.2493e-01, -6.5750e-02,  3.6233e-02,  1.2681e-01,  1.2730e-01, -7.7456e-02,  2.5055e-01, -2.0219e-01, -9.1875e-03],\n        [-2.0992e-01,  1.7710e-01, -2.1059e-01, -2.5648e-01, -2.6644e-01,  2.4440e-01,  1.3300e-01,  1.2254e-01, -2.1045e-01, -2.6711e-01, -2.5757e-01,  1.3216e-04, -2.2004e-01, -2.8023e-02],\n        [ 1.7196e-01,  1.9394e-01, -2.5316e-01, -1.1236e-01,  1.1028e-01, -9.2437e-02, -4.3186e-03,  2.1724e-01,  1.7914e-01,  1.8935e-01,  1.6326e-01,  7.3804e-02, -6.4403e-03, -5.3884e-02],\n        [ 1.5362e-01,  2.4060e-01,  1.7159e-01, -7.4614e-02,  2.6618e-01,  1.2896e-01, -2.8644e-02,  1.8726e-01, -1.3107e-01,  2.0073e-01, -1.4601e-01, -2.6315e-01,  2.4304e-01,  1.7176e-01],\n        [ 2.0240e-01, -1.7669e-01,  1.2179e-01,  2.2777e-01,  2.4668e-01,  2.3519e-01, -5.4698e-02, -3.4425e-03,  3.1034e-02, -1.1163e-01, -1.3142e-01, -8.9459e-02, -9.3739e-02, -1.8078e-01],\n        [ 1.3615e-01, -1.2971e-01, -1.0249e-01, -2.4029e-01,  2.2959e-01, -2.0363e-01,  7.7847e-02,  1.8142e-01,  1.5790e-01,  8.6009e-02,  1.3524e-01,  1.4828e-01, -1.9005e-02,  2.1573e-01],\n        [ 2.2370e-01,  1.0068e-02, -6.1208e-02,  1.5770e-01, -4.6741e-02, -2.3171e-01, -2.4298e-01,  5.0948e-02,  4.1577e-05,  2.5831e-01,  6.4494e-02,  8.9798e-02, -1.8137e-01, -2.2878e-01],\n        [ 1.1844e-01, -2.5567e-01,  2.1762e-01, -1.4962e-02,  1.1559e-01,  8.8614e-02, -2.9602e-02, -1.5439e-01,  2.2132e-01,  1.2634e-01, -1.4837e-02, -1.5565e-01,  2.4771e-01,  1.4244e-01],\n        [-1.9068e-01, -8.9610e-02,  2.4935e-01,  2.0849e-01, -1.1491e-01, -1.6389e-01,  9.5637e-02, -8.2503e-02,  2.2968e-01,  2.3137e-03,  4.6706e-02, -2.1325e-01, -2.3238e-01,  1.6388e-02],\n        [-2.4593e-02, -1.9578e-01, -2.0389e-03,  2.6585e-01, -1.8261e-01, -2.5102e-01,  1.7030e-01, -1.6073e-01, -1.3107e-01, -6.1745e-02, -2.1030e-01,  1.4119e-01,  2.3360e-01, -2.3507e-01],\n        [ 1.1553e-02,  6.6151e-02,  5.1774e-02, -5.1083e-02,  1.0141e-01,  1.7183e-01, -1.0399e-01, -1.2385e-01,  6.9521e-02, -2.5253e-01, -1.4228e-01,  1.7901e-01, -1.9612e-01, -1.7488e-01],\n        [-1.4196e-02,  1.7612e-01,  5.9233e-02,  1.9333e-01, -1.5777e-01, -1.2496e-02, -5.6904e-02,  1.0890e-01,  2.5913e-01, -4.9229e-02, -5.7722e-03,  1.8842e-01,  2.2771e-01,  7.5703e-03],\n        [ 2.1198e-01,  1.8886e-01,  4.3563e-02, -7.7879e-02, -1.2328e-01, -1.4900e-01, -1.9583e-01, -1.6235e-01, -1.5745e-01,  2.4452e-01, -2.0748e-01, -4.4023e-02, -3.7443e-02,  1.4908e-01],\n        [ 2.4348e-01,  2.0631e-01, -1.1194e-01,  1.5699e-01, -1.6086e-01, -2.1225e-01, -7.0801e-02,  2.5133e-01, -2.2237e-01, -2.4286e-01, -4.8562e-02, -1.7540e-01,  1.2117e-01,  3.0587e-02],\n        [ 5.8693e-02,  1.3712e-01, -1.1350e-01, -9.4385e-02,  8.0763e-02, -1.4682e-01, -2.6494e-01, -1.6620e-01, -1.0355e-01,  5.6766e-02, -1.0869e-01,  3.2515e-02,  2.0245e-01, -2.1382e-01],\n        [ 2.0734e-01,  9.1685e-02, -1.5338e-01, -1.4378e-01, -2.6672e-01, -9.6517e-02, -1.1135e-01, -7.7399e-02, -1.9702e-01, -2.1963e-01, -2.4671e-01, -1.7207e-01, -3.2709e-02, -1.3225e-01],\n        [-1.2207e-01, -2.2687e-01, -6.9728e-02,  1.8432e-01,  3.5592e-02, -8.6199e-03,  1.2391e-01, -6.5675e-02, -7.9336e-02, -1.7732e-02,  2.1729e-01, -2.4916e-01, -7.8140e-02,  2.6559e-01],\n        [ 7.2859e-02, -5.6248e-02, -2.3734e-01, -1.5988e-01,  1.3096e-01,  7.8374e-03,  2.2222e-01, -3.1599e-02, -3.4099e-02,  2.6330e-01,  1.0032e-01, -1.3390e-01,  5.0140e-02, -1.9185e-01],\n        [-1.2075e-01, -8.0109e-02,  2.2733e-01,  3.3677e-02, -1.3077e-01, -6.7382e-03,  1.9911e-01, -6.6327e-02, -1.8526e-01, -1.5431e-01, -2.2926e-01, -1.8666e-01, -2.6201e-01, -4.0007e-03],\n        [-2.0215e-01, -2.0944e-01, -9.1061e-02, -1.8865e-01, -1.4917e-01,  1.2435e-01, -2.2995e-01, -5.6175e-02,  1.7343e-01,  1.5358e-01, -1.4379e-01,  1.5046e-01,  1.7796e-01,  6.4559e-02],\n        [ 1.3097e-01, -6.5197e-02, -4.5996e-02,  9.5111e-02, -3.8530e-02,  9.5819e-02,  1.7443e-01,  8.4327e-02,  4.7987e-02, -1.2648e-01, -8.3435e-02,  2.8378e-02, -3.3895e-02,  1.6432e-01],\n        [ 1.4121e-01,  2.5395e-01,  9.2157e-02, -1.7188e-01,  2.3368e-01,  1.9741e-02, -2.4297e-01,  1.8485e-01, -1.7022e-01,  1.7807e-01,  4.0015e-02, -1.7729e-01,  1.7420e-01, -4.2184e-02],\n        [-1.9965e-01,  1.6550e-01, -1.6187e-01,  7.6323e-02,  1.4551e-01,  1.0793e-01,  7.3533e-02,  1.2643e-01, -2.0998e-01, -1.3026e-01,  1.3607e-01,  2.0027e-01,  4.9812e-02,  3.2566e-02],\n        [-2.1176e-01, -5.2008e-02, -1.3464e-01,  2.6073e-01,  2.6467e-01,  4.8879e-02,  2.1306e-01,  2.0816e-02, -7.6826e-02, -1.9319e-01,  2.2612e-01,  7.6374e-02, -4.3499e-02, -1.7189e-01],\n        [ 1.9218e-01,  1.1759e-01, -1.5245e-01,  2.5049e-01, -1.9777e-01, -3.7318e-02,  1.7420e-02,  2.2566e-01,  4.8537e-02,  1.9191e-01, -1.7462e-01,  1.3681e-01,  2.1894e-01, -1.5583e-01],\n        [-7.0123e-02,  8.6483e-02,  5.3424e-02,  5.0077e-02, -4.0794e-02, -6.8064e-02,  1.7331e-01, -6.1889e-02,  8.0509e-02, -1.5641e-01, -1.9517e-01, -2.1853e-01,  2.4122e-01, -1.1747e-01],\n        [-1.5346e-01,  4.5717e-02,  2.2869e-01,  3.6497e-02,  9.9083e-02,  7.4480e-02,  2.1646e-01,  1.0341e-01, -2.1214e-01,  1.9539e-03,  2.2724e-01, -2.4711e-01, -8.0716e-02, -2.5201e-01],\n        [-2.3728e-01,  1.0251e-01,  2.3264e-01,  2.4339e-01, -9.6290e-02,  2.0595e-01, -1.9818e-01, -2.3242e-01,  2.0550e-01,  1.1739e-01,  3.3916e-02, -2.1494e-01, -1.0806e-02,  2.2040e-01],\n        [-1.1510e-01,  1.7575e-01, -1.2734e-01, -2.3466e-01,  2.3913e-01, -2.4621e-01, -1.2362e-01, -3.6895e-02,  2.0872e-01,  4.4368e-04, -5.9274e-02,  1.5741e-01, -5.3605e-02,  1.8857e-01],\n        [ 2.4118e-01,  1.1109e-01,  2.8141e-02,  2.0188e-01, -2.5520e-01, -2.6555e-01,  6.8649e-02, -1.7602e-01, -2.2971e-01, -7.2935e-02, -1.7160e-01,  4.6947e-02, -1.5117e-01,  4.2766e-04],\n        [ 1.4078e-01, -1.3981e-01,  2.2069e-01, -1.0316e-01,  2.0933e-01, -1.5412e-01, -1.8050e-01,  1.1713e-01,  2.5372e-01, -1.4166e-01, -2.4162e-01, -1.3704e-01,  1.5487e-01, -2.1332e-01],\n        [-7.7991e-02, -2.3132e-01, -1.6099e-02,  1.8847e-01,  5.8021e-03, -9.9176e-03, -3.8363e-02,  2.4775e-01,  4.9926e-02,  1.8939e-02,  4.9397e-02,  2.5749e-02,  1.9991e-01,  2.0560e-01],\n        [ 7.1538e-02,  1.4067e-01, -2.1657e-01, -2.1950e-01,  3.2813e-03,  8.3940e-02,  5.1857e-02,  1.3496e-01,  2.4227e-01, -2.5126e-01, -2.6600e-01,  1.3753e-02, -1.7092e-01,  9.5836e-02],\n        [ 6.3447e-02,  1.4696e-02,  3.0072e-02,  1.9001e-02, -2.4747e-01,  1.2882e-01,  9.3536e-02,  2.5452e-01,  5.1452e-02,  2.0934e-02,  1.5071e-02, -4.1523e-02,  9.0678e-02,  3.9901e-02],\n        [ 2.5775e-01, -1.3234e-01, -1.5198e-01,  2.2878e-01,  1.4015e-01,  3.4075e-02,  2.2745e-01, -7.3008e-03,  8.7175e-02,  5.5752e-02,  6.8411e-03,  1.6497e-01, -1.4736e-01, -8.8507e-02],\n        [ 2.0072e-01,  1.5452e-01,  1.0055e-01,  1.2186e-01,  1.7134e-01, -6.3812e-03,  2.6493e-01,  6.7840e-02, -5.7908e-02,  1.2490e-01,  9.3258e-02, -2.0086e-01,  1.0948e-02,  6.2014e-02],\n        [ 1.9208e-01, -2.1862e-01, -1.9533e-01, -2.0342e-01,  1.4946e-01, -1.1502e-02, -2.4969e-01, -1.7546e-01,  2.2757e-01,  1.3244e-01,  1.9947e-01, -2.6471e-01, -1.0681e-01,  7.1615e-02],\n        [ 1.2269e-01,  2.2560e-01, -2.4977e-01,  1.4489e-01,  4.4638e-02,  2.6594e-01, -2.2948e-01,  1.4409e-02, -2.5645e-01, -7.4220e-02,  1.4070e-01,  1.7770e-01, -2.4579e-01,  2.3757e-02],\n        [ 2.2181e-01, -5.2692e-02, -4.5437e-02,  3.4710e-02, -1.7098e-02, -1.1788e-02,  2.4031e-01, -9.4210e-02, -2.4290e-01,  2.2023e-01, -9.9651e-02, -1.1788e-01, -9.2492e-02,  1.6405e-01],\n        [ 1.5012e-02,  1.4978e-01, -1.7417e-01, -1.0522e-01, -1.8447e-01,  1.9697e-01, -2.4369e-01, -2.5038e-01,  1.5711e-01, -1.5322e-01,  2.2587e-01, -2.1363e-01, -7.6911e-03,  1.1874e-01],\n        [ 1.6419e-01, -1.0404e-01, -6.6811e-02,  9.6725e-02, -1.3600e-01, -1.5416e-01,  1.4439e-01,  6.6305e-02, -2.3724e-01, -1.1412e-02,  1.0112e-01, -2.5759e-01, -1.2437e-01, -2.3886e-01],\n        [-2.6419e-01, -1.1727e-01,  2.3747e-01, -5.0603e-04, -1.6224e-02,  5.0873e-02,  5.2847e-02, -5.0335e-03,  1.1120e-01, -8.0188e-02,  2.6567e-01,  1.9930e-02, -9.6523e-02, -2.0746e-02],\n        [ 1.5994e-01, -1.7950e-01,  2.4495e-01, -2.9678e-02, -2.2006e-02, -1.1112e-01,  1.5598e-01,  8.9397e-02,  9.6980e-02, -9.5027e-02, -1.6412e-01, -9.7933e-02,  7.5178e-02, -1.9650e-01],\n        [ 6.7910e-02, -1.9593e-01, -6.0040e-02,  1.0527e-02, -1.3078e-01, -2.4504e-01,  1.4417e-01, -5.4454e-02,  4.6207e-02,  2.4287e-01, -3.8049e-02, -1.8376e-01, -1.0277e-01,  2.3498e-01],\n        [ 1.7313e-01, -2.1243e-01, -1.3857e-01, -1.1795e-01,  1.3439e-01,  2.2154e-01,  1.4371e-01,  7.2086e-02,  1.8502e-02, -1.3456e-01,  1.5647e-01, -2.5449e-01, -2.0664e-01,  2.1948e-01],\n        [-6.5608e-02, -1.2843e-01,  1.1731e-01,  9.6362e-02, -1.3430e-02, -3.0198e-02,  2.1454e-01,  1.6197e-01,  2.4379e-01,  8.0759e-02,  1.0149e-01,  1.5353e-01, -1.8696e-01,  6.2833e-02],\n        [ 3.0005e-02,  1.4286e-01,  9.1608e-02,  2.4283e-03,  2.3487e-01, -2.6966e-02,  2.5474e-01, -2.5581e-01,  2.5586e-01, -1.8201e-01,  6.1901e-02,  2.2076e-01,  1.5720e-01, -2.5524e-01],\n        [ 1.0353e-01, -1.7745e-01,  2.3643e-01,  2.5344e-01,  1.8247e-01,  2.0455e-01, -1.6681e-01,  6.5227e-04, -5.2075e-02,  1.5893e-01, -5.8179e-02,  1.9912e-01, -1.7473e-01,  7.8176e-02],\n        [-1.1625e-01,  1.5353e-01,  1.1146e-01, -1.0275e-01, -1.0212e-01,  2.1579e-01,  7.9630e-02, -8.7382e-02, -2.0324e-01,  1.4511e-01,  1.8352e-01,  1.2163e-01, -2.1234e-01,  2.2287e-01],\n        [ 1.2903e-01,  1.2231e-01,  1.9593e-01, -1.5152e-01, -1.8644e-01,  4.9873e-03, -1.5777e-01, -8.9940e-02, -6.9548e-02, -1.8605e-01, -4.9891e-02, -3.8118e-02, -1.0189e-01,  1.0090e-01],\n        [ 3.6402e-02,  2.5314e-01, -1.8995e-01, -3.6128e-02, -8.6957e-02, -1.2296e-01, -2.4978e-01,  7.6144e-02, -7.2442e-02, -2.5760e-01, -1.6559e-01, -2.3068e-01, -1.6712e-01, -4.1727e-02],\n        [-1.0277e-01, -2.7524e-02,  4.7518e-02, -2.0183e-01, -1.9650e-01, -1.3209e-01,  2.5008e-01, -1.6146e-01,  1.3657e-01,  2.5280e-01, -6.7229e-02,  1.5871e-02, -2.2610e-01,  2.3212e-01],\n        [ 2.6179e-01, -1.4019e-01, -4.9275e-02, -5.6337e-02,  5.3673e-02,  2.2265e-01,  1.7808e-01,  1.1827e-01,  2.1020e-01,  2.2206e-01,  2.3079e-01, -1.1313e-01,  4.3038e-02,  1.4072e-01],\n        [ 2.3744e-01, -1.4607e-01,  1.8026e-01, -7.4190e-02,  1.0158e-01, -2.0770e-01, -2.3383e-01,  2.0109e-01,  1.9942e-01,  2.6331e-01,  1.4514e-01, -1.7708e-01, -2.3804e-02,  2.1019e-01],\n        [-1.9578e-01, -9.9549e-02,  7.1803e-04,  2.1049e-01,  1.5908e-01, -1.4726e-02,  2.1086e-01,  1.1636e-01, -8.0249e-02, -2.5785e-01,  2.6168e-01,  2.4462e-01, -1.5984e-01, -2.0265e-01],\n        [ 8.3301e-02,  1.8569e-01,  6.1800e-03, -2.5147e-01, -2.9277e-03, -9.6514e-02,  7.1301e-02, -2.3569e-01, -7.4826e-03,  2.5870e-01, -2.1843e-01, -2.1016e-01, -2.6621e-01, -1.1280e-01],\n        [-8.7030e-02,  1.0167e-01,  2.2188e-01, -2.1557e-01, -1.1416e-01,  8.9664e-02, -1.4586e-01,  7.1886e-02, -2.4988e-01, -2.3405e-01, -1.8082e-01,  3.3396e-02,  5.4634e-02,  2.3785e-01],\n        [ 2.2843e-01,  1.6695e-01,  1.5041e-02,  5.8828e-02,  1.7501e-01, -1.2712e-01,  2.5604e-01,  1.5031e-01, -2.3910e-01, -2.3502e-01,  1.1330e-01, -1.5820e-02, -6.9212e-02,  1.7209e-01],\n        [ 8.8616e-03, -1.2920e-01,  5.6123e-02,  1.5684e-01,  1.9368e-01,  1.8201e-01, -1.7030e-01,  3.2533e-02, -1.3879e-01, -1.9775e-01, -2.3963e-01, -1.5950e-01,  1.1588e-01,  1.2898e-01],\n        [-1.9412e-02, -1.9626e-01,  2.3750e-01,  4.6068e-02,  5.7870e-02, -1.9468e-01, -3.9822e-02, -1.8999e-01,  1.6070e-01, -7.8617e-02,  2.2357e-01, -2.4913e-01, -3.0172e-02, -1.0758e-01],\n        [ 5.5296e-02,  1.9091e-01,  5.3724e-03,  1.1136e-01,  1.1461e-01,  5.5982e-02,  1.7596e-01,  2.2157e-01, -2.0096e-01,  1.1907e-01, -2.3938e-01, -1.2583e-01, -1.0400e-01,  1.3089e-01],\n        [ 2.0866e-02, -8.4830e-02, -8.1892e-02,  2.0997e-01,  1.4604e-01, -1.7602e-01, -2.4898e-01, -9.9123e-02, -2.5607e-01,  2.6548e-01,  1.2713e-01,  1.3182e-01,  1.0454e-02, -2.3889e-01],\n        [-1.6189e-01, -1.2997e-01,  2.4806e-01,  2.4853e-01, -1.3512e-01, -7.0368e-02,  2.0910e-01,  1.0310e-01,  2.3579e-01,  8.5793e-02, -1.9467e-01,  7.8075e-02,  5.0520e-03, -1.7769e-02],\n        [-1.0475e-01,  4.7057e-02, -2.4073e-01, -2.6671e-01, -9.1919e-02, -1.2091e-01,  4.6306e-02,  1.4777e-01,  6.2874e-02, -2.4429e-01,  5.0628e-02,  2.5169e-01,  6.7270e-02, -1.4909e-01],\n        [-1.3344e-02, -1.3055e-01, -5.7914e-02,  1.1367e-01, -2.0361e-01,  2.5183e-02,  1.8191e-01,  1.2597e-01, -2.0688e-01,  1.5539e-01, -5.1908e-02, -6.2390e-02, -2.2671e-01,  2.5937e-01],\n        [ 1.2097e-01,  1.2066e-01,  1.1920e-01, -2.1686e-01, -1.3371e-01, -2.2476e-01,  1.4214e-01, -2.1263e-01,  9.3101e-03,  1.0785e-01, -1.4113e-01,  2.3113e-01,  1.0989e-01,  2.1522e-01],\n        [-2.1516e-01, -1.9809e-01,  1.9806e-01, -2.0555e-01, -1.9787e-02, -5.4313e-02,  1.0483e-02, -8.1452e-02,  1.5620e-01,  2.8027e-03,  7.2225e-02, -2.4281e-01, -9.0920e-02, -1.3791e-01],\n        [ 2.2842e-02, -2.2994e-01,  2.5466e-02, -2.5640e-01, -1.0481e-01, -4.4731e-02, -1.8234e-03,  5.9022e-02, -1.5265e-01,  1.7380e-01, -2.5771e-01, -6.6285e-02, -1.3707e-01, -1.1763e-01],\n        [-2.1323e-01,  2.3212e-01, -2.3653e-01,  2.1255e-01, -1.4038e-01,  2.3232e-01,  5.8473e-02, -1.3361e-01, -2.5865e-01, -4.9549e-02,  2.1918e-01,  1.0718e-01, -9.6036e-02, -1.5949e-01],\n        [ 1.7558e-02, -7.9969e-02, -1.6389e-01, -2.0148e-01, -1.7555e-01, -1.7988e-01,  2.5915e-01,  3.1750e-02,  3.5924e-02,  1.7914e-01, -2.2585e-01,  4.4339e-02, -7.3667e-02,  1.1931e-01],\n        [ 8.8865e-02, -1.7775e-01, -2.3909e-01, -5.6126e-02, -2.0230e-01, -7.5233e-02,  1.3028e-01, -2.3804e-02, -2.4605e-02,  8.5201e-02, -1.6135e-01,  2.2650e-01, -1.9572e-01,  9.5473e-02],\n        [ 1.6237e-01, -5.2274e-03, -1.1124e-01,  1.1562e-01, -7.2469e-02, -2.4078e-01,  1.3501e-01,  1.8467e-01,  1.0410e-01, -2.0197e-01, -1.7324e-01,  8.8793e-02,  1.3687e-01,  5.3870e-02],\n        [ 2.0136e-01,  5.6878e-02, -2.1294e-01,  1.2389e-01,  2.2655e-01, -1.8093e-01, -2.4912e-01,  1.0742e-02,  1.8869e-01,  1.2660e-01, -1.7356e-01,  1.2341e-01, -7.6322e-02, -4.3373e-02],\n        [-1.8619e-02, -2.1076e-01, -1.1134e-01,  9.9213e-03,  1.6344e-01, -1.5885e-01,  1.1090e-01,  5.2089e-02, -2.3451e-02, -6.7977e-02, -2.2173e-01,  8.8178e-02, -1.4587e-01,  2.5372e-01],\n        [ 1.0058e-01,  9.8815e-02,  2.5677e-01, -2.3886e-02, -1.0533e-01,  7.8923e-03, -8.3080e-02,  8.8505e-02, -2.6619e-01,  4.3696e-02, -4.1140e-02, -6.7915e-02,  1.5301e-01, -2.0704e-01],\n        [-6.3702e-02,  2.0391e-01,  4.3639e-02, -1.7411e-01,  1.5860e-01, -2.2288e-01,  1.7374e-01, -1.0878e-01,  9.3259e-02,  2.0080e-01,  2.3754e-01,  8.3785e-02,  1.1816e-01, -6.5750e-02],\n        [-6.6217e-02, -2.4393e-01,  1.2547e-01,  5.5304e-02,  8.0718e-02, -1.7046e-01, -2.6468e-01,  1.4061e-01, -1.2753e-01, -8.7390e-02,  7.9706e-02,  1.0408e-01, -2.5636e-01, -6.7371e-02],\n        [ 2.3039e-01,  4.8490e-02, -1.5590e-01,  2.1219e-01, -2.4890e-01,  2.7279e-02,  6.0659e-02,  2.6757e-02,  2.4407e-01,  4.9959e-02, -1.7173e-01, -3.9670e-02, -1.1051e-01,  2.2212e-01],\n        [ 3.4971e-02, -2.4204e-01,  4.0140e-03,  2.9303e-03,  1.5895e-02,  6.3542e-02,  1.4373e-01, -2.1272e-01,  8.2219e-02, -1.3412e-01,  1.7478e-01, -1.4631e-01, -1.9079e-01, -1.5370e-01],\n        [ 1.5400e-01, -5.0593e-02,  2.3315e-01, -2.2269e-01,  8.6373e-02, -9.5146e-02,  3.8935e-02,  1.6982e-01,  1.3661e-01, -2.4939e-01, -3.8685e-02,  1.1157e-01, -2.6472e-01, -1.9631e-01],\n        [-2.6477e-01,  3.6555e-02,  5.7795e-02, -1.1124e-01,  1.0784e-01,  4.0143e-02,  1.1373e-01,  2.3417e-01, -1.7878e-01, -2.0751e-01,  2.5381e-01, -1.3902e-01,  1.8826e-01, -9.0328e-02],\n        [-5.7605e-02, -1.1919e-01,  8.1724e-02, -1.7206e-02,  2.1346e-01, -2.6173e-01, -6.2545e-03,  3.9420e-02,  8.6479e-02,  2.0751e-01,  9.2356e-02, -2.3131e-01,  3.6680e-02, -1.1330e-01],\n        [ 1.6414e-01, -1.1643e-01, -3.2226e-02,  6.8546e-02,  1.7535e-01, -1.2821e-01,  1.9905e-01, -1.8825e-01, -6.3608e-02, -1.6758e-01,  5.6647e-02, -2.1654e-01, -2.6969e-03, -1.8483e-01],\n        [ 1.7201e-01, -2.4222e-01, -1.1722e-01, -2.2685e-01, -1.6843e-01,  1.9244e-02,  1.2700e-01,  1.3360e-01, -2.1101e-01, -4.7450e-02, -5.5915e-02,  1.1265e-01, -2.7223e-02, -3.5143e-02],\n        [-1.6707e-01, -1.8593e-01, -1.2631e-02,  1.2587e-01, -1.3263e-01,  5.6937e-02,  1.3083e-01, -9.7037e-02,  2.4179e-01,  2.4292e-01,  1.6002e-01,  1.1404e-01, -2.1206e-01,  1.4253e-01],\n        [-3.1148e-02,  2.0575e-01, -2.1956e-01, -9.5584e-03,  2.4260e-01,  5.2728e-02,  8.7336e-02, -1.1458e-03, -1.2653e-01,  1.3207e-01,  2.1117e-01,  1.4649e-01, -1.7987e-01, -1.7981e-01],\n        [ 2.5856e-01, -3.3511e-02,  1.1137e-01,  7.5891e-02, -1.6880e-01,  2.2973e-01,  4.7486e-02, -1.2820e-01,  2.5234e-01,  8.8218e-02,  6.4927e-02, -1.8335e-01,  2.3426e-01, -1.3539e-01],\n        [-9.3661e-02, -2.5937e-01,  7.4079e-02, -1.5150e-01, -2.3709e-01,  1.8786e-01, -2.4381e-01, -5.5370e-02, -1.0074e-01,  1.9286e-02,  3.9194e-02,  2.3609e-01,  8.1145e-02, -8.6467e-02],\n        [-1.5875e-01, -7.0952e-02,  1.4765e-01, -2.5771e-01,  1.0170e-01, -1.7492e-02, -1.2522e-01,  1.7596e-01,  1.3533e-01,  9.2611e-03, -1.3194e-01, -1.6265e-01, -7.1936e-02, -1.6554e-01],\n        [-4.6568e-02,  8.9886e-02, -2.9222e-02,  3.4744e-02, -9.7517e-02, -7.3343e-02,  1.5584e-01, -3.2611e-02,  6.6787e-02, -1.8439e-01, -1.5519e-01,  2.0403e-01, -8.2210e-03,  2.5996e-01],\n        [-9.0229e-02,  1.4221e-01,  1.1577e-01, -2.2972e-01, -1.5119e-01,  2.2531e-01, -8.9303e-02, -2.4673e-01,  1.9806e-01,  2.5409e-01,  1.7355e-01,  1.7039e-01, -8.1095e-02,  2.2498e-01],\n        [-1.1823e-01,  1.0708e-01, -2.5676e-01,  2.3513e-01, -1.9760e-01, -2.4864e-01,  1.1042e-01,  2.3110e-01, -2.1698e-01,  2.5908e-01,  4.5719e-02,  1.3541e-01,  5.4369e-02, -2.6606e-01],\n        [ 2.3070e-01,  2.1483e-01,  1.4068e-01,  3.3391e-03,  1.0816e-01,  1.4462e-02,  8.4131e-02, -3.1999e-02,  2.5102e-01, -8.8512e-02, -4.2137e-02,  7.8787e-02,  2.3688e-01, -1.4935e-01],\n        [-1.1612e-01, -2.3538e-01,  2.0206e-01,  2.4982e-01,  5.3316e-02, -2.2210e-01, -2.0819e-01,  2.1067e-01,  1.8095e-02,  5.8900e-02, -1.5793e-01, -2.2113e-01, -1.3497e-01, -2.5650e-01],\n        [ 1.8632e-01,  9.6262e-02, -2.2078e-01,  2.6592e-01, -6.1487e-02,  1.3415e-01,  5.7255e-02,  9.4642e-02, -2.4370e-02, -3.0191e-02, -1.2341e-01,  8.2654e-02,  2.7580e-02,  1.9013e-01],\n        [-2.0115e-01,  1.0557e-01,  8.4866e-02,  7.7321e-03, -1.5134e-01, -1.3481e-01,  2.0102e-01,  1.8406e-01,  1.0765e-01, -2.5369e-01, -1.7766e-01,  8.8585e-02,  2.6001e-01, -2.5427e-01],\n        [ 6.5454e-02, -2.5245e-01, -2.5658e-01, -2.0199e-01, -1.3153e-01, -1.6229e-01, -1.5478e-01,  2.2553e-02, -5.7234e-02,  1.5757e-01,  6.0907e-02, -1.0638e-01, -1.8105e-01,  2.0289e-01],\n        [-3.6242e-03,  5.6221e-02,  1.2557e-01, -5.3314e-02, -1.3631e-01,  9.1907e-03, -1.5349e-01,  1.3233e-01,  9.8269e-02, -1.3150e-01, -8.5314e-02,  5.6020e-02,  3.7970e-02,  4.0024e-02],\n        [-1.3290e-01,  5.2857e-02, -1.1399e-01,  5.6009e-02,  1.1367e-02,  1.3360e-02, -1.0903e-01, -2.1522e-01, -1.8927e-01,  6.0795e-02,  1.1661e-01, -6.3006e-02,  1.8469e-01, -1.0292e-01],\n        [ 1.4258e-01,  5.3147e-02,  2.3783e-01,  1.9991e-01, -7.8730e-02,  7.3004e-03, -2.0218e-01,  1.5486e-01, -6.1233e-02, -2.1147e-01, -1.1007e-01, -2.3988e-01, -7.5082e-02, -8.9653e-02],\n        [-2.5969e-01,  7.3183e-02, -1.9549e-01,  5.6234e-02, -1.2807e-01,  2.5065e-02, -1.2548e-01, -4.8089e-02,  2.0602e-02,  1.1371e-01,  8.7963e-02,  2.0576e-01,  2.4508e-01, -2.5352e-01],\n        [-6.5436e-02,  1.1969e-01, -1.0391e-01, -7.8383e-02, -1.7374e-01,  5.0069e-02, -7.8337e-02, -3.9551e-02,  1.4011e-01,  1.8964e-01, -1.7995e-01, -2.2229e-01,  1.1000e-01, -1.8430e-01],\n        [-2.8151e-02,  4.3205e-02, -2.6504e-01,  2.1122e-01,  8.0014e-02,  2.5672e-02,  2.2119e-02, -2.2203e-01,  1.4149e-01, -9.6851e-02, -1.8384e-01, -1.1051e-02,  2.0964e-01, -1.9397e-01],\n        [-4.7784e-02,  1.0382e-01, -2.1612e-01,  2.2239e-01,  1.0691e-02, -2.5732e-03,  8.3450e-02, -1.8529e-02,  1.3489e-01,  1.8245e-01, -2.6383e-01,  2.2992e-01,  2.5777e-01, -5.4151e-02],\n        [ 6.5248e-02,  1.6541e-01,  2.5585e-01,  8.3342e-02,  1.4175e-01, -1.8719e-01, -2.5960e-01,  8.1566e-02,  6.5626e-02, -7.6951e-02, -1.7927e-01, -5.7697e-02, -1.5777e-01, -1.3357e-01],\n        [ 7.4838e-02, -1.7600e-02, -1.3352e-02, -5.2816e-02, -1.6092e-01,  2.5686e-01,  1.5912e-01, -8.3011e-02,  7.2189e-02,  2.1758e-01, -1.6510e-01, -1.2892e-01, -2.0450e-01, -1.3932e-01],\n        [-4.1060e-03,  1.3597e-01,  2.4084e-01,  2.2388e-01,  1.5387e-01,  1.7402e-01, -6.8014e-02,  2.0421e-01, -4.6064e-02, -4.0212e-02,  5.0955e-02,  5.7921e-02, -1.9928e-01, -9.1004e-03],\n        [-2.5434e-01, -2.0024e-01,  7.5725e-02,  2.2121e-01,  4.6158e-02, -1.8777e-01,  1.6039e-01, -2.0512e-01,  6.2905e-02,  4.7543e-02,  2.5123e-01,  1.5762e-02, -1.7096e-01,  2.1290e-01],\n        [ 5.2149e-02, -9.7737e-02, -2.6204e-01, -1.1588e-01,  1.9608e-01,  1.7385e-01, -1.1882e-02, -1.1787e-01, -3.7608e-02,  2.1433e-01,  1.7466e-01, -2.4784e-02,  9.8214e-02, -2.0982e-03],\n        [ 1.0373e-01,  8.8175e-02,  1.8342e-01, -2.1864e-01,  1.2808e-02, -2.4893e-02, -5.2543e-02, -1.7068e-01,  8.8417e-02,  1.5487e-01, -1.4598e-02, -1.2058e-01, -7.4453e-02, -3.7263e-02],\n        [ 3.6627e-02,  2.3327e-01, -4.7054e-03, -7.1521e-03, -1.6318e-01,  1.7658e-01,  5.1964e-02,  2.4236e-01, -1.0548e-01,  3.3813e-02,  8.3230e-02,  2.1154e-01,  1.0270e-01,  1.4633e-01],\n        [-4.6527e-03, -1.2795e-01,  6.3057e-02,  1.3901e-01, -1.8061e-01, -4.9488e-02, -1.9201e-01, -1.1741e-01, -1.1602e-01,  2.0043e-02,  1.3571e-01,  1.1605e-01,  1.9174e-01,  1.5861e-01],\n        [-1.5675e-02, -2.0349e-01,  2.3526e-02, -1.7129e-01,  1.9720e-01, -2.6219e-01, -8.6638e-02,  1.4095e-02, -1.6056e-01, -1.9034e-01, -7.7935e-02, -2.3072e-01,  6.0427e-02, -1.9163e-01],\n        [ 1.4316e-01, -1.8244e-01, -2.1485e-01,  7.9422e-02, -2.4197e-01,  2.6324e-01,  1.6738e-01, -1.4917e-01,  1.3172e-01,  2.6050e-01, -1.7841e-01,  1.7217e-01,  7.2010e-02, -2.3106e-01],\n        [-6.0251e-02, -3.2904e-02, -1.8336e-01, -6.3743e-02,  1.5465e-01,  2.0070e-01,  3.1793e-02, -9.1245e-02, -2.5491e-01, -1.8050e-01, -1.3298e-02,  2.9344e-03,  9.1577e-02, -2.3787e-02],\n        [-2.4250e-01, -1.6496e-01, -6.4048e-02,  1.8308e-01,  6.3696e-02, -1.2586e-02, -2.4191e-01, -2.1847e-01,  1.6031e-02, -1.8497e-01,  2.3608e-01, -2.6630e-01, -1.3866e-01, -2.5347e-01],\n        [ 2.1457e-01,  2.3106e-01,  1.3441e-01, -1.5919e-01, -2.2752e-01, -2.5183e-01,  1.3483e-01,  4.1521e-02,  1.4514e-01,  9.4458e-02,  2.2592e-01,  2.6539e-01,  1.5762e-01,  5.8614e-02],\n        [-1.7640e-01, -2.6015e-01,  2.3818e-01,  7.4607e-02,  2.4996e-01, -1.3061e-01, -3.6924e-02, -2.1162e-01,  1.0150e-01,  2.1012e-01,  2.5192e-01,  2.5168e-02, -8.9794e-02,  1.7988e-01],\n        [ 1.2480e-01,  1.5560e-01, -1.2263e-01,  3.0391e-02, -6.7771e-02, -5.3559e-02, -9.4714e-02,  6.1842e-02,  9.2415e-02,  4.4013e-02, -2.7708e-02, -2.1493e-01, -8.9931e-02,  2.2179e-01],\n        [-1.8266e-01, -2.5296e-01, -6.5304e-02,  8.9503e-02, -1.9823e-01,  8.3986e-02, -1.4543e-01, -1.1105e-01, -1.6024e-01, -1.2164e-03, -2.1368e-01, -5.5999e-02, -1.2380e-01,  2.2358e-01],\n        [-1.3292e-01,  6.0109e-02, -1.5895e-01,  1.3217e-01, -2.3664e-01, -1.1776e-01,  1.2898e-01,  2.1209e-01,  9.9645e-02, -1.6538e-01,  3.0776e-02, -6.8642e-03,  1.9955e-01, -7.3984e-02],\n        [-1.8052e-01,  1.6561e-01,  9.7096e-02, -1.2465e-01,  2.1782e-01,  1.4984e-01, -2.4599e-01,  1.7846e-01, -1.2470e-01, -6.4469e-02,  1.6732e-01,  2.1039e-02,  2.5382e-01, -2.3236e-01],\n        [ 1.1734e-01,  3.6787e-02, -7.0783e-02, -8.4900e-02,  1.0215e-01,  1.9324e-01, -2.5922e-01,  1.0947e-01, -1.8864e-01,  3.0055e-03, -1.5116e-01,  2.6270e-02, -2.4373e-01,  1.2169e-01],\n        [-1.8758e-01,  1.7010e-01, -2.0319e-01,  1.9983e-01,  1.6376e-01, -2.6628e-01, -2.2877e-01, -1.2176e-01, -1.8379e-01, -2.6533e-01, -2.1984e-03, -1.7158e-01,  9.7019e-02, -2.3714e-01],\n        [-1.5604e-01,  1.1240e-01,  9.6548e-02,  2.0462e-01, -2.2340e-01, -2.0607e-01, -1.1077e-01,  3.6081e-02, -9.2863e-02,  7.3855e-02,  2.0152e-01,  7.0670e-02,  2.0000e-01,  6.5539e-02],\n        [ 1.1127e-01, -2.1713e-01, -4.7627e-02,  2.1831e-01, -1.0393e-01, -9.5188e-02, -1.1621e-01,  9.5995e-02,  2.3882e-01,  1.0942e-01,  2.3884e-01,  1.8845e-01, -3.6593e-02, -9.1187e-02],\n        [-8.8697e-02,  1.1310e-01,  1.7821e-01, -2.5583e-01, -1.2711e-01, -2.5738e-01, -1.6045e-01,  2.1192e-01,  2.4126e-01, -1.7277e-01, -1.3364e-01,  8.7366e-02,  1.0022e-01,  2.7484e-02],\n        [-4.9237e-02, -1.3731e-01, -2.4178e-01, -7.4863e-02, -2.1666e-01,  2.3484e-01,  2.3129e-01, -1.9160e-01,  1.1547e-01,  1.9571e-01,  2.2245e-01,  2.1706e-01, -1.8935e-01,  2.0950e-01],\n        [ 2.5319e-01,  1.7205e-02,  1.2935e-01, -1.0667e-01,  2.6637e-01, -2.4126e-01, -1.6409e-01,  4.6369e-02, -1.1680e-01, -9.7850e-02,  9.6862e-02,  1.5312e-01,  1.7613e-01,  2.2910e-01],\n        [ 1.3446e-01, -1.1686e-01,  9.4649e-02, -1.6102e-01,  1.5706e-01,  1.1377e-01,  2.2849e-01,  1.5347e-01,  2.4349e-01, -5.6955e-02, -2.3006e-01,  2.1271e-01, -2.4131e-01,  3.6344e-02],\n        [ 4.1757e-02,  4.2961e-02,  1.8643e-01,  5.5423e-02, -9.4479e-02, -1.6761e-02, -2.6367e-01, -1.8053e-01, -1.7037e-02,  2.4447e-01, -6.4613e-02,  1.4248e-01, -1.7310e-01, -2.1144e-01],\n        [ 1.2456e-01,  1.5418e-01, -1.4690e-02,  2.0124e-01,  1.0328e-01,  1.3399e-01,  5.1933e-02,  2.6681e-01, -2.0729e-01, -5.2183e-02, -1.7454e-01, -1.6287e-01,  5.3793e-02, -2.9467e-02],\n        [-2.4573e-02,  1.0472e-01,  2.4316e-01, -2.4336e-01, -8.2792e-02, -2.5501e-02,  1.5230e-01, -1.1230e-01, -2.4870e-01, -1.0522e-01, -1.4545e-01, -2.5331e-01, -1.8927e-01, -1.0551e-01],\n        [ 9.3659e-02, -1.4757e-01, -8.2746e-02, -2.2030e-01, -1.5668e-01, -4.6348e-02, -1.2092e-01, -2.1836e-02,  7.7702e-02,  3.5641e-02, -1.5285e-01, -1.0956e-02,  2.2208e-02,  2.1249e-02],\n        [ 1.0844e-01, -7.4523e-02, -2.3383e-01, -8.7884e-02, -6.9604e-02, -2.5263e-01,  1.1929e-02, -9.0063e-02,  1.9180e-01,  1.8608e-01,  1.8227e-01, -1.2140e-01,  6.7500e-03,  2.4564e-01],\n        [-2.3879e-01,  1.0330e-01, -1.0814e-01, -1.0217e-01,  1.4737e-01, -1.4270e-02, -5.6765e-02,  1.5875e-01, -8.2704e-02,  2.1628e-01,  1.6881e-02,  1.0865e-01,  2.3482e-01, -1.0137e-01],\n        [ 1.3867e-01,  4.9477e-02, -2.2893e-01,  2.6686e-01,  1.7679e-01,  6.6368e-03,  1.0021e-01, -8.1930e-02, -3.2229e-02, -7.8866e-03,  7.4872e-02,  5.2454e-02,  2.5244e-01,  2.1693e-01],\n        [ 1.5015e-01, -1.1026e-01, -1.3467e-01, -2.1803e-01,  1.2140e-01,  2.0970e-01,  7.2656e-02,  2.1164e-01, -1.9702e-01, -2.5984e-01,  9.8912e-02, -1.3018e-01,  2.1413e-01, -8.5491e-03],\n        [ 3.6036e-02,  8.6464e-02, -2.1164e-01, -5.3215e-02,  1.7770e-01, -8.8035e-02,  6.2616e-02, -1.4077e-01,  4.5001e-02, -1.3309e-01, -6.1153e-02,  1.4514e-01, -6.8950e-03,  1.6278e-01],\n        [ 1.5479e-01,  2.1591e-01, -7.2583e-02,  5.4948e-02,  6.3311e-02, -1.5615e-01,  1.3827e-01, -1.6472e-01, -2.1976e-01,  2.5025e-01,  3.7155e-02,  1.7801e-01,  2.3042e-01, -2.2430e-01],\n        [-2.1552e-01,  5.1092e-02,  3.8932e-02, -1.6231e-02, -1.6077e-01,  6.3241e-02,  1.3898e-01, -1.8452e-01,  1.7834e-01, -1.5476e-02,  2.2505e-01, -2.9212e-03,  6.4655e-02, -1.3955e-01],\n        [-7.6599e-02,  4.5267e-02,  3.9077e-02,  1.8621e-01, -1.5978e-01,  6.0647e-02, -1.3882e-01, -5.1899e-02, -1.7346e-01,  2.0330e-01,  1.7589e-01, -2.2899e-01, -2.0343e-01,  8.6688e-02],\n        [-1.9972e-01, -1.1435e-01, -2.6253e-01, -1.8847e-01,  1.9770e-01, -1.5630e-01,  1.7266e-01,  7.3824e-02, -1.8172e-01, -6.2116e-02, -1.6459e-02,  1.7967e-01, -2.5025e-01,  2.0958e-01],\n        [-2.5313e-01, -1.9492e-01, -1.7570e-01,  9.1730e-02, -1.8630e-01, -2.1035e-01,  9.8684e-02, -1.9762e-01, -1.8772e-01,  2.3711e-01,  1.5218e-01, -1.9265e-01,  6.0024e-02,  4.5662e-02],\n        [ 1.4395e-01,  1.4298e-01,  2.6350e-01,  6.9106e-02,  4.5640e-02, -2.9242e-02,  6.1682e-02, -7.0750e-02,  2.2247e-01,  2.5155e-01, -3.2909e-02,  6.1484e-02, -2.0179e-01, -2.0543e-01],\n        [-1.8569e-01, -5.1203e-02,  1.5263e-01, -1.6652e-02, -2.1846e-01,  7.1279e-02,  1.5648e-01, -2.2498e-01, -2.4899e-01, -1.2545e-01,  6.6208e-02,  8.0158e-02, -1.4689e-01,  4.8780e-02],\n        [-4.3346e-02, -2.0473e-02,  2.4314e-01,  9.9672e-02, -1.2134e-02,  4.1545e-02, -1.3739e-01,  1.9234e-01, -1.5241e-02,  1.2131e-01, -9.2409e-02,  8.1061e-02,  1.0033e-01,  1.3418e-01],\n        [-4.0930e-02, -1.3011e-01,  7.9795e-02,  2.0498e-01, -8.4337e-02, -3.0182e-02,  2.5192e-02,  1.4291e-01,  7.6151e-02, -6.9763e-02,  8.0241e-02, -9.5385e-02, -8.9252e-02,  2.1933e-01],\n        [-2.1566e-01, -2.4047e-01, -1.3558e-02, -2.4457e-01, -2.2576e-02,  1.5412e-01, -3.5693e-02,  2.3091e-02,  2.2280e-01, -1.7221e-01,  1.8652e-01,  1.8729e-01,  8.1015e-02,  1.6766e-01],\n        [-1.9368e-01,  4.2529e-02, -4.1225e-02, -1.3132e-01,  6.4248e-02, -2.6180e-01,  2.2028e-01, -2.5500e-01,  2.5973e-01,  7.5705e-02,  1.9285e-01, -1.4888e-01, -1.4527e-01,  4.4543e-02],\n        [-1.8061e-01, -1.7697e-01, -2.6234e-01,  1.5353e-01, -1.0860e-02,  1.9380e-01,  2.2293e-01,  7.4793e-02,  9.6936e-02, -1.3412e-01,  7.6389e-02, -3.2366e-02, -2.5902e-01,  4.8368e-02],\n        [-2.6000e-01, -2.6084e-01, -1.8822e-01, -1.0780e-01, -2.3603e-01,  7.0279e-02, -2.3237e-01,  4.7553e-02, -1.8277e-01,  1.3396e-01,  1.6162e-01,  1.9048e-02, -2.6147e-01,  1.4718e-01],\n        [ 1.2324e-01, -2.4804e-01,  8.1598e-02, -2.2982e-01, -2.0669e-01, -4.7321e-02, -1.6063e-01, -1.0499e-01,  1.7623e-01, -7.9916e-02,  5.6074e-02, -2.2395e-01,  9.2604e-03,  9.7014e-02],\n        [-2.4222e-01, -1.1266e-01, -4.8169e-02, -2.4008e-01, -1.0643e-01,  2.2507e-01,  2.5054e-01,  2.6219e-01,  2.6190e-01,  2.3767e-01, -2.0660e-01,  1.1206e-01, -2.3557e-01,  1.5717e-01],\n        [-6.8258e-02,  2.2515e-01, -1.1779e-02, -6.0983e-02,  1.6040e-01,  2.3546e-01,  9.3087e-02,  4.0394e-02,  1.3940e-01, -1.8687e-01, -1.0500e-02, -8.6391e-03, -1.8925e-01,  7.7034e-02],\n        [ 3.5083e-02, -6.1016e-02,  2.0919e-02, -2.3753e-01,  2.0329e-01, -6.1692e-02,  1.7724e-01, -2.1787e-01,  5.2109e-02, -1.2801e-01, -2.4965e-01,  2.3568e-01,  7.2111e-02, -8.7759e-02],\n        [ 1.3687e-01, -2.1716e-01,  2.4169e-01, -1.8424e-01, -4.2400e-02,  8.6619e-02, -2.6132e-01,  1.5524e-01,  1.3157e-02, -9.0140e-02,  1.2143e-01, -1.3784e-01, -6.1919e-02, -1.6031e-01],\n        [-6.2363e-02, -1.1364e-01, -1.1081e-01,  9.8578e-02,  9.5425e-02, -2.2015e-01,  1.2377e-03,  1.8777e-01, -1.1942e-01,  8.7797e-02, -1.9022e-01,  2.3148e-01,  6.6200e-02, -1.1786e-01],\n        [ 1.9294e-01,  2.4411e-03,  8.2154e-02, -1.4938e-01,  5.7722e-03, -1.5303e-01,  1.8344e-01,  1.8052e-01, -1.2907e-01, -1.1806e-02,  5.8634e-02,  9.4222e-02, -1.4216e-01,  2.6227e-02],\n        [ 1.3075e-01, -2.5427e-01, -5.3241e-02,  8.6921e-02,  1.7904e-01, -5.9696e-02, -1.6617e-01,  4.9033e-02,  3.7645e-02,  2.5874e-01,  1.8619e-01, -2.9338e-02, -3.3830e-02,  5.1475e-02],\n        [-2.5768e-01,  1.9361e-01,  1.2979e-01,  1.2373e-01, -1.8027e-01,  1.1523e-02,  1.8413e-01,  1.4217e-01, -1.1233e-01, -7.1287e-02, -1.7125e-01,  2.1700e-02, -2.5628e-01,  1.8487e-01],\n        [-7.4125e-02,  1.4924e-01, -8.0448e-02,  2.6218e-01, -1.7979e-01,  1.6275e-01, -4.5626e-03,  6.3186e-02,  3.6513e-02, -1.8126e-01,  2.5583e-01,  1.6072e-01,  1.1739e-01,  2.2220e-01],\n        [-6.3015e-02,  9.5619e-02, -1.0367e-01,  8.7192e-02, -1.9868e-01,  1.8690e-01,  1.0436e-01, -2.7875e-02, -5.9037e-02,  1.8806e-01,  1.7733e-01, -2.5693e-01, -1.3775e-01,  4.5845e-02],\n        [-6.5788e-03,  2.1168e-01, -1.9749e-01, -1.9218e-01, -7.8562e-02,  1.5119e-01,  1.4316e-01, -1.3523e-01, -7.9833e-02,  2.1926e-01, -1.3293e-01,  2.5423e-01, -1.4548e-01, -3.0617e-02],\n        [-2.0219e-01, -2.0463e-01,  1.1982e-01,  1.4641e-01,  2.3610e-01,  2.4852e-01,  2.1344e-01, -9.9836e-02, -5.9925e-02,  2.0485e-02, -2.1423e-01, -2.5538e-01,  1.6149e-01,  2.0461e-01],\n        [-1.5533e-01, -2.3511e-01,  2.0236e-01, -2.6746e-02, -2.2075e-01,  6.4006e-02,  3.6650e-02,  2.3141e-02,  1.1814e-01,  1.8304e-01, -2.6721e-01,  2.5496e-01, -8.7172e-02, -1.6836e-01],\n        [ 5.2018e-02, -1.7191e-01, -8.8659e-02,  1.9508e-01, -2.2478e-02, -1.1916e-01, -1.3218e-01,  1.1544e-01, -7.2085e-02, -2.1023e-01,  2.6544e-01,  1.7288e-01, -2.3113e-01,  2.8653e-02],\n        [-2.0788e-02,  2.5026e-01, -4.9973e-03, -2.0262e-03,  5.6709e-02, -1.4425e-01,  5.5721e-03, -1.9503e-01, -1.8311e-01, -2.5574e-01, -1.6915e-01, -1.3015e-01,  3.6098e-02,  1.8080e-01],\n        [-5.4818e-02,  1.8839e-01,  2.0814e-01, -2.0454e-01, -1.2833e-01, -7.6278e-02,  1.9338e-01,  2.4494e-01, -1.9795e-01, -1.2995e-02,  2.1310e-01,  1.4092e-01, -5.1700e-02,  1.0026e-01],\n        [ 1.5096e-01, -1.7438e-01, -2.2954e-01, -6.6656e-02,  4.8260e-02, -2.0653e-01,  1.7794e-01,  1.5654e-01, -6.4178e-02, -2.3263e-01, -2.4222e-01,  1.7580e-01,  5.9736e-02, -2.4409e-01],\n        [ 2.2699e-02,  2.5912e-02,  1.0428e-01,  3.1581e-02, -2.6420e-01,  2.5429e-01,  1.8282e-02,  1.6892e-01,  1.0020e-01, -9.7769e-02, -8.9385e-02,  9.9715e-02, -1.7132e-01, -5.1807e-02],\n        [ 1.4787e-01,  3.1420e-02, -2.4328e-01,  1.0492e-01, -1.2045e-01, -2.0107e-01,  1.0916e-01, -1.3395e-01, -2.3847e-01,  1.9204e-01, -1.9924e-01,  2.0822e-01, -2.5211e-01,  9.4959e-02],\n        [-2.0154e-01,  2.0286e-01, -7.4993e-02,  1.4556e-01, -2.1319e-01,  2.4265e-01,  1.7887e-01,  1.5293e-01, -1.3134e-01,  6.2153e-02,  3.5407e-02,  2.4166e-01, -1.6264e-01,  1.8616e-01],\n        [-3.5196e-02,  9.1060e-02, -1.8449e-01, -2.1823e-01, -2.8470e-02,  8.7933e-02, -2.1943e-01,  8.0005e-02, -2.3635e-01,  1.4877e-01, -9.3021e-02, -2.3245e-01,  1.6904e-01,  1.0740e-01],\n        [ 3.5111e-02,  2.6392e-01, -1.8206e-01,  1.6780e-01, -1.0373e-01, -6.2095e-02,  1.1925e-01, -1.8090e-02,  1.7192e-01, -3.6085e-02, -1.0772e-01, -1.5897e-01, -1.5077e-01,  6.1655e-02],\n        [ 1.4502e-01,  2.5587e-02, -1.8699e-01,  1.4502e-02,  1.4533e-01,  1.9116e-01, -2.0138e-01,  1.1315e-01, -1.6969e-01,  2.4107e-01,  2.3638e-01,  1.4826e-01, -1.7272e-01, -4.2659e-02],\n        [-2.7887e-03,  2.6207e-02,  3.0382e-02,  1.7345e-01,  7.6715e-02,  2.4616e-01, -9.3022e-02,  3.2603e-02, -1.8477e-01,  1.7610e-01, -2.6170e-01,  4.2101e-02,  6.1963e-03, -1.5933e-01],\n        [ 8.2978e-02, -1.5048e-01, -1.8044e-01,  2.3250e-01,  6.2855e-02, -3.2599e-02, -2.1383e-01,  2.0833e-01,  3.7284e-02, -1.7000e-01, -2.6696e-01,  2.4339e-01, -2.8801e-02,  2.2985e-01],\n        [ 6.6284e-02,  1.1209e-01,  2.5176e-01,  3.7406e-02, -1.3822e-01,  9.3636e-02,  2.2346e-01,  1.7196e-01,  2.3885e-02, -2.1100e-02, -7.8588e-03, -1.8451e-01,  6.5322e-02,  7.8327e-03],\n        [-1.4619e-01,  1.5443e-01, -2.6120e-01, -2.2417e-01,  2.2644e-01, -4.0432e-02, -2.5274e-01, -8.0205e-02, -1.8810e-01,  1.7522e-01,  1.5497e-01,  2.0790e-01,  2.4498e-01,  9.1390e-02],\n        [ 5.9670e-02,  1.2345e-01, -1.3980e-01, -4.1487e-02,  8.9241e-03, -1.3297e-01, -3.0330e-02, -9.1531e-03, -1.6368e-03,  3.8368e-02, -1.2484e-01,  2.5802e-01,  9.4110e-02, -5.4747e-02],\n        [-1.7178e-01, -1.6303e-01,  1.2101e-01, -2.0668e-01, -8.5582e-02, -1.2456e-01, -4.3152e-02, -2.0774e-01, -8.4130e-02, -1.1294e-01,  2.1917e-02,  1.5504e-02, -1.1533e-01,  5.1350e-02],\n        [-1.5245e-01, -5.2902e-02, -1.1153e-01, -2.1386e-01, -2.4997e-02,  3.9598e-03,  1.9116e-01, -1.1002e-01, -6.3297e-02,  2.4480e-01, -5.3638e-02,  1.7063e-01, -5.0257e-03,  1.3750e-01],\n        [-2.3777e-01, -2.4685e-01,  2.4777e-01,  1.7970e-01,  9.9839e-02, -9.7701e-02,  2.6372e-01,  3.9091e-02, -1.2993e-02,  7.7248e-02,  1.6796e-01, -1.1164e-01,  1.6148e-01, -2.2478e-01],\n        [-2.4020e-01, -1.2391e-01, -2.4284e-01, -1.3183e-01,  6.2421e-03,  2.5529e-01, -1.9602e-01,  2.3046e-01,  5.8318e-02, -1.4981e-02,  2.7137e-02, -1.8901e-01, -2.2908e-01, -4.0479e-03],\n        [ 4.3063e-02, -1.4301e-01, -9.1377e-02,  2.3122e-01, -1.9418e-01,  2.1781e-01,  2.8208e-02, -2.3723e-01, -3.6815e-02, -1.3955e-01,  1.3176e-01,  2.0191e-01, -1.2756e-01, -1.5030e-01],\n        [-1.3594e-02, -5.3858e-02,  9.2992e-02,  1.9339e-01,  1.2228e-01,  1.6211e-01,  1.2450e-01, -1.4846e-01, -2.4272e-01, -2.4321e-01,  2.4003e-01, -6.8342e-03, -1.9954e-01,  2.0915e-01],\n        [ 1.0566e-01, -1.2703e-01, -5.3024e-02,  2.3794e-03,  2.3957e-01, -1.6641e-01,  1.3603e-01, -1.2909e-01,  1.4166e-01,  2.4671e-01,  5.1385e-02, -7.0610e-02, -2.6324e-01,  1.4399e-01],\n        [ 1.5729e-01, -2.4644e-02, -2.1139e-01, -4.9945e-02,  1.7207e-01, -2.3597e-01, -2.3471e-01, -1.6866e-01,  2.2580e-02,  2.0725e-01,  2.3092e-01,  2.1263e-01, -8.4625e-02,  5.7294e-02],\n        [ 1.5435e-01, -3.3935e-02,  2.0936e-01,  8.8008e-02, -1.9336e-01, -1.9865e-01,  1.5955e-02, -2.6561e-01, -1.3791e-01,  5.0716e-02,  1.3314e-01, -1.7881e-01, -1.8131e-01, -6.6680e-02]])\nlayer_1.linear.bias\nParameter containing:\ntensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\nlayer_2.linear.weight\n","output_type":"stream"},{"name":"stderr","text":"Exception ignored in: <bound method IPythonKernel._clean_thread_parent_frames of <ipykernel.ipkernel.IPythonKernel object at 0x7dbdf518f370>>\nTraceback (most recent call last):\n  File \"/opt/conda/lib/python3.10/site-packages/ipykernel/ipkernel.py\", line 775, in _clean_thread_parent_frames\n    def _clean_thread_parent_frames(\nKeyboardInterrupt: \nException ignored in: <bound method IPythonKernel._clean_thread_parent_frames of <ipykernel.ipkernel.IPythonKernel object at 0x7dbdf518f370>>\nTraceback (most recent call last):\n  File \"/opt/conda/lib/python3.10/site-packages/ipykernel/ipkernel.py\", line 775, in _clean_thread_parent_frames\n    def _clean_thread_parent_frames(\nKeyboardInterrupt: \nException ignored in: <bound method IPythonKernel._clean_thread_parent_frames of <ipykernel.ipkernel.IPythonKernel object at 0x7dbdf518f370>>\nTraceback (most recent call last):\n  File \"/opt/conda/lib/python3.10/site-packages/ipykernel/ipkernel.py\", line 775, in _clean_thread_parent_frames\n    def _clean_thread_parent_frames(\nKeyboardInterrupt: \nException ignored in: <bound method IPythonKernel._clean_thread_parent_frames of <ipykernel.ipkernel.IPythonKernel object at 0x7dbdf518f370>>\nTraceback (most recent call last):\n  File \"/opt/conda/lib/python3.10/site-packages/ipykernel/ipkernel.py\", line 775, in _clean_thread_parent_frames\n    def _clean_thread_parent_frames(\nKeyboardInterrupt: \nException ignored in: <bound method IPythonKernel._clean_thread_parent_frames of <ipykernel.ipkernel.IPythonKernel object at 0x7dbdf518f370>>\nTraceback (most recent call last):\n  File \"/opt/conda/lib/python3.10/site-packages/ipykernel/ipkernel.py\", line 775, in _clean_thread_parent_frames\n    def _clean_thread_parent_frames(\nKeyboardInterrupt: \nException ignored in: <bound method IPythonKernel._clean_thread_parent_frames of <ipykernel.ipkernel.IPythonKernel object at 0x7dbdf518f370>>\nTraceback (most recent call last):\n  File \"/opt/conda/lib/python3.10/site-packages/ipykernel/ipkernel.py\", line 775, in _clean_thread_parent_frames\n    def _clean_thread_parent_frames(\nKeyboardInterrupt: \nException ignored in: <bound method IPythonKernel._clean_thread_parent_frames of <ipykernel.ipkernel.IPythonKernel object at 0x7dbdf518f370>>\nTraceback (most recent call last):\n  File \"/opt/conda/lib/python3.10/site-packages/ipykernel/ipkernel.py\", line 775, in _clean_thread_parent_frames\n    def _clean_thread_parent_frames(\nKeyboardInterrupt: \nException ignored in: <bound method IPythonKernel._clean_thread_parent_frames of <ipykernel.ipkernel.IPythonKernel object at 0x7dbdf518f370>>\nTraceback (most recent call last):\n  File \"/opt/conda/lib/python3.10/site-packages/ipykernel/ipkernel.py\", line 775, in _clean_thread_parent_frames\n    def _clean_thread_parent_frames(\nKeyboardInterrupt: \nException ignored in: <bound method IPythonKernel._clean_thread_parent_frames of <ipykernel.ipkernel.IPythonKernel object at 0x7dbdf518f370>>\nTraceback (most recent call last):\n  File \"/opt/conda/lib/python3.10/site-packages/ipykernel/ipkernel.py\", line 775, in _clean_thread_parent_frames\n    def _clean_thread_parent_frames(\nKeyboardInterrupt: \nException ignored in: <bound method IPythonKernel._clean_thread_parent_frames of <ipykernel.ipkernel.IPythonKernel object at 0x7dbdf518f370>>\nTraceback (most recent call last):\n  File \"/opt/conda/lib/python3.10/site-packages/ipykernel/ipkernel.py\", line 775, in _clean_thread_parent_frames\n    def _clean_thread_parent_frames(\nKeyboardInterrupt: \n","output_type":"stream"}],"execution_count":null},{"cell_type":"code","source":"def custom_lr_lambda(step):\n    num_step_threshold = 100\n\n    if step < num_step_threshold:\n        return step / num_step_threshold\n    if step == num_step_threshold:\n        print(\"here\")\n    return 0.99995 ** (step - num_step_threshold)\n\nstart_time = time.time()\nmodel = TabularDenseNet(num_features, num_classes, num_control_points, 3, corr_comb_indices).to(device)\n\noptimizer = optim.Adam(model.parameters(), lr=0.001 * 1.0)\nscheduler = LambdaLR(optimizer, lr_lambda=custom_lr_lambda)\n\ncriterion = CustomLoss(nn.CrossEntropyLoss(), 0.0, 0.0, 0.0, 0.0, 0.0)\nevaluate_model(model, custom_train_loader, criterion, optimizer, 10000, scheduler, batch_size, num_og_features, early_stopping_patience=10000)\n\nmodels.append(model)\nprint(f\"Execution time: {(time.time() - start_time):.6f} seconds\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-10T19:03:15.111423Z","iopub.execute_input":"2024-12-10T19:03:15.112269Z","iopub.status.idle":"2024-12-10T19:05:48.226504Z","shell.execute_reply.started":"2024-12-10T19:03:15.112236Z","shell.execute_reply":"2024-12-10T19:05:48.225289Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def custom_lr_lambda(step):\n    num_step_threshold = 100\n\n    if step < num_step_threshold:\n        return step / num_step_threshold\n    if step == num_step_threshold:\n        print(\"here\")\n    return 0.99995 ** (step - num_step_threshold)\n\nstart_time = time.time()\nmodel = TabularDenseNet(num_features, num_classes, num_control_points, 3, corr_comb_indices).to(device)\n\noptimizer = optim.Adam(model.parameters(), lr=0.001 * 1.0)\nscheduler = LambdaLR(optimizer, lr_lambda=custom_lr_lambda)\n\ncriterion = CustomLoss(nn.CrossEntropyLoss(), 0.0, 0.0, 0.0, 0.0, 0.0)\nevaluate_model(model, custom_train_loader, criterion, optimizer, 10000, scheduler, batch_size, num_og_features, early_stopping_patience=10000)\n\nmodels.append(model)\nprint(f\"Execution time: {(time.time() - start_time):.6f} seconds\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-10T18:53:05.805977Z","iopub.execute_input":"2024-12-10T18:53:05.806335Z","iopub.status.idle":"2024-12-10T19:02:13.313177Z","shell.execute_reply.started":"2024-12-10T18:53:05.806303Z","shell.execute_reply":"2024-12-10T19:02:13.31171Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def custom_lr_lambda(step):\n    num_step_threshold = 100\n\n    if step < num_step_threshold:\n        return step / num_step_threshold\n    if step == num_step_threshold:\n        print(\"here\")\n    return 0.99995 ** (step - num_step_threshold)\n\nstart_time = time.time()\nmodel = TabularDenseNet(num_features, num_classes, num_control_points, 3, corr_comb_indices).to(device)\n\noptimizer = optim.Adam(model.parameters(), lr=0.001 * 1.0)\nscheduler = LambdaLR(optimizer, lr_lambda=custom_lr_lambda)\n\ncriterion = CustomLoss(nn.CrossEntropyLoss(), 0.0, 0.0, 0.0, 0.0, 0.0)\nevaluate_model(model, custom_train_loader, criterion, optimizer, 10000, scheduler, batch_size, num_og_features, early_stopping_patience=10000)\n\nmodels.append(model)\nprint(f\"Execution time: {(time.time() - start_time):.6f} seconds\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-10T18:15:15.209191Z","iopub.execute_input":"2024-12-10T18:15:15.209537Z","iopub.status.idle":"2024-12-10T18:42:25.512105Z","shell.execute_reply.started":"2024-12-10T18:15:15.209505Z","shell.execute_reply":"2024-12-10T18:42:25.511164Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"data = pd.read_csv('/kaggle/input/playground-series-s4e10/test.csv')\n\ndata = data.drop([\"id\"], axis=1)\ndata['source'] = 0\n\ngrade_mapping = {'A': 7, 'B': 6, 'C': 5, 'D': 4, 'E': 3, 'F': 2, 'G': 1}\ndata['loan_grade'] = data['loan_grade'].map(grade_mapping)\n\npurpose_mapping = {\n    'DEBTCONSOLIDATION': 1,\n    'HOMEIMPROVEMENT': 2,\n    'MEDICAL': 3,\n    'PERSONAL': 4,\n    'EDUCATION': 5,\n    'VENTURE': 6\n}\ndata['loan_intent'] = data['loan_intent'].map(purpose_mapping)\n\nhome_ownership_mapping = {\n    'OWN': 1,\n    'MORTGAGE': 2,\n    'OTHER': 3,\n    'RENT': 4\n}\ndata['person_home_ownership'] = data['person_home_ownership'].map(home_ownership_mapping)\n\nprint(data.columns)\nprint(data.isnull().sum())\n\nX = data.drop([], axis=1)\n\nX = pd.get_dummies(X, drop_first=True)\n\ncolumn_to_log = [\n    'person_age',\n    'person_income',\n]\n\ncolumn_to_sqrt = [\n    'person_emp_length',\n    'loan_percent_income',\n]\n\nfor col in column_to_log:\n    if (X[col] <= 0).any():\n        print(f\"Column '{col}' contains non-positive values. Adding 1 to avoid log of non-positive numbers.\")\n        X[col] = np.log(X[col] + 1)\n    else:\n        X[col] = np.log(X[col])\n\nfor col in column_to_sqrt:\n    if (X[col] < 0).any():\n        print(f\"Column '{col}' contains negative values. Setting negative values to NaN before applying sqrt.\")\n        X[col] = np.sqrt(X[col].clip(lower=0))\n    else:\n        X[col] = np.sqrt(X[col])\n\nprint(X.shape)\nprint(X.columns)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-12T05:47:47.512892Z","iopub.execute_input":"2024-11-12T05:47:47.513707Z","iopub.status.idle":"2024-11-12T05:47:47.636662Z","shell.execute_reply.started":"2024-11-12T05:47:47.513664Z","shell.execute_reply":"2024-11-12T05:47:47.635475Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(x_scaled)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-12T05:48:01.72192Z","iopub.execute_input":"2024-11-12T05:48:01.722289Z","iopub.status.idle":"2024-11-12T05:48:01.728672Z","shell.execute_reply.started":"2024-11-12T05:48:01.722256Z","shell.execute_reply":"2024-11-12T05:48:01.727476Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(X.shape)\nX_scaled_test = x_scaler.transform(X)\nprint(X_scaled_test.shape)\nprint(X_scaled_test)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-12T05:48:11.357669Z","iopub.execute_input":"2024-11-12T05:48:11.358046Z","iopub.status.idle":"2024-11-12T05:48:11.392927Z","shell.execute_reply.started":"2024-11-12T05:48:11.358011Z","shell.execute_reply":"2024-11-12T05:48:11.391959Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"X_scaled_test_tensor = torch.tensor(X_scaled_test).float().to(device)\noutputs = models[-1](X_scaled_test_tensor)\nprint(outputs)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-11T21:19:52.781054Z","iopub.execute_input":"2024-11-11T21:19:52.781762Z","iopub.status.idle":"2024-11-11T21:19:52.864006Z","shell.execute_reply.started":"2024-11-11T21:19:52.781722Z","shell.execute_reply":"2024-11-11T21:19:52.863029Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"probabilities = F.softmax(outputs, dim=1)\nprint(probabilities)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-11T21:19:53.638494Z","iopub.execute_input":"2024-11-11T21:19:53.639135Z","iopub.status.idle":"2024-11-11T21:19:53.647569Z","shell.execute_reply.started":"2024-11-11T21:19:53.639093Z","shell.execute_reply":"2024-11-11T21:19:53.646456Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"positive_class_probs = probabilities[:, 1]\nprint(positive_class_probs)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-11T21:20:12.694515Z","iopub.execute_input":"2024-11-11T21:20:12.695268Z","iopub.status.idle":"2024-11-11T21:20:12.70197Z","shell.execute_reply.started":"2024-11-11T21:20:12.695225Z","shell.execute_reply":"2024-11-11T21:20:12.700992Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import pandas as pd\n\ntest_df = pd.read_csv('/kaggle/input/playground-series-s4e10/test.csv')\nids = test_df['id']\n\npositive_class_probs = positive_class_probs.cpu().detach().numpy()\n\nsubmission_df = pd.DataFrame({\n    'id': ids,\n    'loan_status': positive_class_probs\n})\n\nsubmission_df.to_csv('submission.csv', index=False)\nprint(\"Submission file created successfully.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-11T21:20:17.211548Z","iopub.execute_input":"2024-11-11T21:20:17.21195Z","iopub.status.idle":"2024-11-11T21:20:17.359435Z","shell.execute_reply.started":"2024-11-11T21:20:17.211911Z","shell.execute_reply":"2024-11-11T21:20:17.358447Z"}},"outputs":[],"execution_count":null}]}