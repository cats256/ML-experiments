{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":408,"sourceType":"datasetVersion","datasetId":180},{"sourceId":668,"sourceType":"datasetVersion","datasetId":308}],"dockerImageVersionId":30787,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np\nimport math\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom scipy.special import logit\nfrom scipy.stats import norm\n\nimport tensorflow as tf\nfrom keras import layers, models, datasets\n\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport torch.nn.functional as F\nfrom torch.utils.data import TensorDataset, DataLoader\nfrom torchinfo import summary\nfrom torch.optim.lr_scheduler import StepLR\nimport torch.autograd.profiler as profiler\n\nfrom sklearn.model_selection import train_test_split, LeaveOneOut, StratifiedKFold, cross_val_predict\nfrom sklearn.preprocessing import StandardScaler, MinMaxScaler, LabelEncoder, PowerTransformer\nfrom sklearn.metrics import f1_score, log_loss, accuracy_score\nfrom sklearn.linear_model import LogisticRegression\n\nimport sys\nimport time\nfrom learnable_activation import LearnableActivation\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-10-26T19:16:31.765606Z","iopub.execute_input":"2024-10-26T19:16:31.766041Z","iopub.status.idle":"2024-10-26T19:16:31.836679Z","shell.execute_reply.started":"2024-10-26T19:16:31.766003Z","shell.execute_reply":"2024-10-26T19:16:31.835644Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"pip install learnable-activation==0.0.1","metadata":{"execution":{"iopub.status.busy":"2024-10-26T19:16:14.994478Z","iopub.execute_input":"2024-10-26T19:16:14.995270Z","iopub.status.idle":"2024-10-26T19:16:28.020201Z","shell.execute_reply.started":"2024-10-26T19:16:14.995230Z","shell.execute_reply":"2024-10-26T19:16:28.019008Z"},"trusted":true},"execution_count":4,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/pty.py:89: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  pid, fd = os.forkpty()\n","output_type":"stream"},{"name":"stdout","text":"Collecting learnable-activation==0.0.1\n  Downloading learnable_activation-0.0.1-py3-none-any.whl.metadata (5.2 kB)\nRequirement already satisfied: torch in /opt/conda/lib/python3.10/site-packages (from learnable-activation==0.0.1) (2.4.0)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch->learnable-activation==0.0.1) (3.15.1)\nRequirement already satisfied: typing-extensions>=4.8.0 in /opt/conda/lib/python3.10/site-packages (from torch->learnable-activation==0.0.1) (4.12.2)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch->learnable-activation==0.0.1) (1.13.3)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch->learnable-activation==0.0.1) (3.3)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch->learnable-activation==0.0.1) (3.1.4)\nRequirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from torch->learnable-activation==0.0.1) (2024.6.1)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch->learnable-activation==0.0.1) (2.1.5)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/conda/lib/python3.10/site-packages (from sympy->torch->learnable-activation==0.0.1) (1.3.0)\nDownloading learnable_activation-0.0.1-py3-none-any.whl (5.0 kB)\nInstalling collected packages: learnable-activation\nSuccessfully installed learnable-activation-0.0.1\nNote: you may need to restart the kernel to use updated packages.\n","output_type":"stream"}]},{"cell_type":"markdown","source":"An interpolation based learning technique, driven through explicit regularization","metadata":{}},{"cell_type":"code","source":"def calculate_metrics(model, data_tensor, labels_tensor, batch_size=1024, num_features=54):\n    model.eval()\n    all_preds = []\n    all_labels = []\n\n    with torch.no_grad():\n        for start_idx in range(0, len(data_tensor), batch_size):\n            end_idx = min(start_idx + batch_size, len(data_tensor))\n            inputs = data_tensor[start_idx:end_idx].view(-1, num_features)\n            labels = labels_tensor[start_idx:end_idx]\n\n            outputs = model(inputs)\n            _, preds = torch.max(outputs, 1)\n            all_preds.extend(preds.cpu().numpy())\n            all_labels.extend(labels.cpu().numpy())\n\n    accuracy = accuracy_score(all_labels, all_preds)\n    f1 = f1_score(all_labels, all_preds, average='weighted')\n    return accuracy, f1","metadata":{"execution":{"iopub.status.busy":"2024-10-26T19:16:34.045169Z","iopub.execute_input":"2024-10-26T19:16:34.045609Z","iopub.status.idle":"2024-10-26T19:16:34.054346Z","shell.execute_reply.started":"2024-10-26T19:16:34.045567Z","shell.execute_reply":"2024-10-26T19:16:34.053328Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"class CustomDataLoader:\n    def __init__(self, features, labels, validation_size=0.2, is_classification=True):\n        if is_classification:\n            train_data, val_data, train_labels, val_labels = train_test_split(\n                features, labels, test_size=validation_size, stratify=labels, random_state=42\n            )\n            self.train_labels_tensor = torch.tensor(train_labels).long().to(device)\n            self.val_labels_tensor = torch.tensor(val_labels).long().to(device)\n        else:\n            train_data, val_data, train_labels, val_labels = train_test_split(\n                features, labels, test_size=validation_size, random_state=42\n            )\n            self.train_labels_tensor = torch.tensor(train_labels).float().to(device)\n            self.val_labels_tensor = torch.tensor(val_labels).float().to(device)\n\n        self.train_data_tensor = torch.tensor(train_data).float().to(device)\n        self.val_data_tensor = torch.tensor(val_data).float().to(device)","metadata":{"execution":{"iopub.status.busy":"2024-10-26T19:16:34.443001Z","iopub.execute_input":"2024-10-26T19:16:34.443379Z","iopub.status.idle":"2024-10-26T19:16:34.452800Z","shell.execute_reply.started":"2024-10-26T19:16:34.443339Z","shell.execute_reply":"2024-10-26T19:16:34.451738Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"def evaluate_model(model, custom_train_loader, criterion, optimizer, num_epochs, scheduler, batch_size=1024, num_features=54, is_classification=True):\n    for epoch in range(num_epochs):\n        running_loss = 0.0\n        \n        model.train()\n        for start_idx in range(0, len(custom_train_loader.train_data_tensor), batch_size):\n            end_idx = min(start_idx + batch_size, len(custom_train_loader.train_data_tensor))\n            inputs = custom_train_loader.train_data_tensor[start_idx:end_idx].view(-1, num_features)\n            labels = custom_train_loader.train_labels_tensor[start_idx:end_idx]\n\n            optimizer.zero_grad()\n            outputs = model(inputs)\n            loss = criterion(outputs, labels, model)\n            loss.backward()\n            optimizer.step()\n            scheduler.step()\n            running_loss += loss.item() * len(labels)\n                      \n        model.eval()\n        val_loss = 0.0\n        with torch.no_grad():\n            for start_idx in range(0, len(custom_train_loader.val_data_tensor), batch_size):\n                end_idx = min(start_idx + batch_size, len(custom_train_loader.val_data_tensor))\n                val_inputs = custom_train_loader.val_data_tensor[start_idx:end_idx].view(-1, num_features)\n                val_labels = custom_train_loader.val_labels_tensor[start_idx:end_idx]\n\n                val_outputs = model(val_inputs)\n                val_loss += criterion.regular_loss(val_outputs, val_labels).item() * len(val_labels)\n\n        avg_train_loss = running_loss / len(custom_train_loader.train_data_tensor)\n        avg_val_loss = val_loss / len(custom_train_loader.val_data_tensor)\n\n        print(f'Epoch {epoch + 1}, Training Loss: {avg_train_loss}, Validation Loss: {avg_val_loss}')\n\n        if is_classification:\n            train_accuracy, train_f1 = calculate_metrics(model, custom_train_loader.train_data_tensor, custom_train_loader.train_labels_tensor, num_features)\n            val_accuracy, val_f1 = calculate_metrics(model, custom_train_loader.val_data_tensor, custom_train_loader.val_labels_tensor, num_features)\n\n            print(f'Training Accuracy: {train_accuracy}, Training F1 Score: {train_f1}')\n            print(f'Validation Accuracy: {val_accuracy}, Validation F1 Score: {val_f1}')\n            \n        print()\n","metadata":{"execution":{"iopub.status.busy":"2024-10-26T19:16:34.945697Z","iopub.execute_input":"2024-10-26T19:16:34.946588Z","iopub.status.idle":"2024-10-26T19:16:34.962841Z","shell.execute_reply.started":"2024-10-26T19:16:34.946533Z","shell.execute_reply":"2024-10-26T19:16:34.961822Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"class CustomLoss(nn.Module):\n    def __init__(self, criterion, l1_lambda=0.0, l2_lambda=0.0, f1_lambda=0.0, f2_lambda=0.0):\n        super(CustomLoss, self).__init__()\n        self.criterion = criterion\n        self.l1_lambda = l1_lambda\n        self.l2_lambda = l2_lambda\n        self.f1_lambda = f1_lambda\n        self.f2_lambda = f2_lambda\n\n    def forward(self, outputs, labels, model):    \n        l1_norm = sum(p.abs().sum() for name, module in model.named_modules()  if isinstance(module, nn.Linear) for p in module.parameters() if 'bias' not in name)\n        l1_loss = self.l1_lambda * l1_norm\n        \n        l2_norm = sum(p.pow(2.0).sum() for name, module in model.named_modules() if isinstance(module, nn.Linear) for p in module.parameters() if 'bias' not in name)\n        l2_loss = self.l2_lambda * l2_norm\n                \n        f1_loss = 0\n        f2_loss = 0\n        for name, module in model.named_modules():\n            if isinstance(module, LearnableActivation):\n                copy_tensor = module.copy_tensor\n                                \n                f1_diff = (copy_tensor[:, 1:] - copy_tensor[:, :-1])\n                f1_loss += self.f1_lambda * f1_diff.abs().sum()\n\n                f2_diff = f1_diff[:, 1:] - f1_diff[:, :-1]\n                f2_loss += self.f2_lambda * f2_diff.abs().sum()\n\n        return self.criterion(outputs, labels) + l1_loss + l2_loss + f1_loss + f2_loss\n        \n    def regular_loss(self, outputs, labels):\n        return self.criterion(outputs, labels)","metadata":{"execution":{"iopub.status.busy":"2024-10-26T19:16:35.784870Z","iopub.execute_input":"2024-10-26T19:16:35.785335Z","iopub.status.idle":"2024-10-26T19:16:35.800203Z","shell.execute_reply.started":"2024-10-26T19:16:35.785290Z","shell.execute_reply":"2024-10-26T19:16:35.799080Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"data = pd.read_csv('/kaggle/input/forest-cover-type-dataset/covtype.csv')\n# data = pd.read_csv('/kaggle/input/breast-cancer-wisconsin-data/data.csv')\n\n\"\"\"data = data.dropna()\"\"\"\n\n\"\"\"\nX = data[[\n    \"Elevation\",\n    \"Aspect\",\n    \"Slope\",\n    \"Horizontal_Distance_To_Hydrology\",\n    \"Vertical_Distance_To_Hydrology\",\n    \"Horizontal_Distance_To_Roadways\",\n    \"Hillshade_9am\",\n    \"Hillshade_Noon\",\n    \"Hillshade_3pm\",\n    \"Horizontal_Distance_To_Fire_Points\"\n]]\n\"\"\"\n\n# print(data.columns)\n# X = data[['radius_worst', 'concave points_worst']]\n# X = data.drop([\"id\", \"diagnosis\", \"Unnamed: 32\"], axis=1)\n# y = data[\"diagnosis\"]\nX = data.drop([\"Cover_Type\"], axis=1)\ny = data[\"Cover_Type\"]\n\nX = pd.get_dummies(X, drop_first=True)\nfor col in X.columns:\n    if (X[col] > 0).all():\n        X[col] = np.log(X[col])\n\nprint(X.shape, y.shape)\nprint(X.columns)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"x_scaler = StandardScaler()\nx_scaled = x_scaler.fit_transform(X)\n\nlabel_encoder = LabelEncoder()\ny_encoded = label_encoder.fit_transform(y)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"x_values = np.linspace(-3, 3, 10000)\ny_values = np.cos(x_values * 4)\n\nprint(y_values)","metadata":{"execution":{"iopub.status.busy":"2024-10-26T19:16:39.550301Z","iopub.execute_input":"2024-10-26T19:16:39.551257Z","iopub.status.idle":"2024-10-26T19:16:39.560607Z","shell.execute_reply.started":"2024-10-26T19:16:39.551205Z","shell.execute_reply":"2024-10-26T19:16:39.559295Z"},"trusted":true},"execution_count":10,"outputs":[{"name":"stdout","text":"[0.84385396 0.84256363 0.84126844 ... 0.84126844 0.84256363 0.84385396]\n","output_type":"stream"}]},{"cell_type":"code","source":"# class LearnableActivation(nn.Module):\n#     def __init__(self, num_features, width=20, density=10):\n#         super(LearnableActivation, self).__init__()\n#         self.num_features = num_features\n#         self.width = width\n#         self.density = density\n        \n#         num_intervals = width * density\n#         range_values = torch.linspace(-width / 2, width / 2, num_intervals + 1)\n#         self.copy_tensor = nn.Parameter(range_values.repeat(num_features, 1))\n#         self.feature_idx = torch.arange(self.num_features).view(1, -1)\n        \n#     def forward(self, x):\n#         scaled_x = (x * self.density) + (self.width * self.density / 2)\n        \n#         lower_idx = torch.floor(scaled_x).long()\n#         lower_idx = torch.clamp(lower_idx, min=0, max=self.copy_tensor.size(1) - 2)\n#         upper_idx = lower_idx + 1\n\n#         lower_value = self.copy_tensor[self.feature_idx, lower_idx]\n#         upper_value = self.copy_tensor[self.feature_idx, upper_idx]\n        \n#         interp_factor = (scaled_x - lower_idx.float())\n#         interpolated_value = torch.lerp(lower_value, upper_value, interp_factor)\n#         return interpolated_value","metadata":{"execution":{"iopub.status.busy":"2024-10-26T19:16:40.599971Z","iopub.execute_input":"2024-10-26T19:16:40.600819Z","iopub.status.idle":"2024-10-26T19:16:40.606657Z","shell.execute_reply.started":"2024-10-26T19:16:40.600765Z","shell.execute_reply":"2024-10-26T19:16:40.605481Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"class TabularDenseNet(nn.Module):\n    def __init__(self, input_size, output_size, num_layers=2, width=20, density=10):\n        super(TabularDenseNet, self).__init__()\n                \n        self.layers = nn.ModuleList()\n        self.activations = nn.ModuleList()\n        \n        for i in range(num_layers):\n            self.activations.append(LearnableActivation(input_size, width, density))\n            self.layers.append(nn.Linear(input_size, input_size, bias=False))\n            \n            with torch.no_grad():\n                self.layers[-1].weight.copy_(torch.eye(input_size))\n\n            input_size *= 2\n\n        self.activation_second_last_layer = LearnableActivation(input_size, width, density)\n        self.last_layer = nn.Linear(input_size, output_size, bias=False)\n        \n        with torch.no_grad():\n            self.last_layer.weight.copy_(torch.zeros(output_size, input_size))\n        \n        self.activation_last_layer = LearnableActivation(output_size, width, density)\n\n    def forward(self, x):\n        outputs = [x]\n    \n        for i in range(len(self.layers)):\n            concatenated_outputs = torch.cat(outputs, dim=1)\n            outputs.append(self.layers[i](self.activations[i](concatenated_outputs)))\n\n        outputs = torch.cat(outputs, dim=1)\n        outputs = self.activation_second_last_layer(outputs)\n        outputs = self.last_layer(outputs)\n        outputs = self.activation_last_layer(outputs)\n        return outputs.squeeze()","metadata":{"execution":{"iopub.status.busy":"2024-10-26T19:16:41.106391Z","iopub.execute_input":"2024-10-26T19:16:41.106827Z","iopub.status.idle":"2024-10-26T19:16:41.117662Z","shell.execute_reply.started":"2024-10-26T19:16:41.106789Z","shell.execute_reply":"2024-10-26T19:16:41.116600Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"num_epochs = 1\nnum_features = 1\nnum_classes = 1\n\nmodel = TabularDenseNet(num_features, num_classes, 8, width=20, density=10).to(device)\ncriterion = CustomLoss(nn.MSELoss(), l1_lambda=0.001 * 0.0, l2_lambda=0.0, f1_lambda=0.001 * 0.001, f2_lambda=0.001 * 0.001)\ncustom_train_loader = CustomDataLoader(x_values, y_values, validation_size=0.2, is_classification=False)\n\nfor name, param in model.named_parameters():\n    break\n    print(f\"Layer: {name}\")\n    print(f\"Shape: {param.shape}\")\n    print(param)\n    \ntotal_params = sum(p.numel() for p in model.parameters())\nprint(f'Total number of parameters: {total_params}')","metadata":{"execution":{"iopub.status.busy":"2024-10-26T19:16:41.741322Z","iopub.execute_input":"2024-10-26T19:16:41.741712Z","iopub.status.idle":"2024-10-26T19:16:41.996151Z","shell.execute_reply.started":"2024-10-26T19:16:41.741676Z","shell.execute_reply":"2024-10-26T19:16:41.995183Z"},"trusted":true},"execution_count":13,"outputs":[{"name":"stdout","text":"Total number of parameters: 125013\n","output_type":"stream"}]},{"cell_type":"code","source":"for name, param in model.named_parameters():\n    print(name)\n    print(param)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import torch.optim as optim\nimport torch.optim.lr_scheduler as lr_scheduler\n\noptimizer = optim.Adam(model.parameters(), lr=0.01)\nscheduler = lr_scheduler.StepLR(optimizer, step_size=1, gamma=0.9995)\n\ntorch.cuda.synchronize()\nstart_time = time.time()\n\nevaluate_model(model, custom_train_loader, criterion, optimizer, 1000, scheduler, batch_size=1024, num_features=num_features, is_classification=False)\n\nelapsed_time = time.time() - start_time\nprint(f\"Execution time: {elapsed_time:.6f} seconds\")","metadata":{"execution":{"iopub.status.busy":"2024-10-26T19:16:50.091240Z","iopub.execute_input":"2024-10-26T19:16:50.092088Z","iopub.status.idle":"2024-10-26T19:17:00.323356Z","shell.execute_reply.started":"2024-10-26T19:16:50.092045Z","shell.execute_reply":"2024-10-26T19:17:00.321911Z"},"trusted":true},"execution_count":15,"outputs":[{"name":"stdout","text":"Epoch 1, Training Loss: 6.33522889482975, Validation Loss: 0.8357346439361573\n\nEpoch 2, Training Loss: 0.9096806735992432, Validation Loss: 0.8120481057167053\n\nEpoch 3, Training Loss: 0.45139398431777955, Validation Loss: 0.3436638979911804\n\nEpoch 4, Training Loss: 0.22252323317527772, Validation Loss: 0.10373967450857162\n\nEpoch 5, Training Loss: 0.06764566293358802, Validation Loss: 0.022920332849025726\n\nEpoch 6, Training Loss: 0.024744223058223724, Validation Loss: 0.005965645160526037\n\nEpoch 7, Training Loss: 0.017106633335351944, Validation Loss: 0.0037310792822390793\n\nEpoch 8, Training Loss: 0.014864832885563374, Validation Loss: 0.002122356364503503\n\nEpoch 9, Training Loss: 0.013751370176672936, Validation Loss: 0.001525257327593863\n\nEpoch 10, Training Loss: 0.013214686013758182, Validation Loss: 0.0012122828466817736\n\nEpoch 11, Training Loss: 0.012908970244228839, Validation Loss: 0.0009618541202507914\n\nEpoch 12, Training Loss: 0.012697959661483765, Validation Loss: 0.0008028683699667454\n\nEpoch 13, Training Loss: 0.012526179671287536, Validation Loss: 0.0007115399688482285\n\nEpoch 14, Training Loss: 0.012397139102220536, Validation Loss: 0.0005976866651326418\n\nEpoch 15, Training Loss: 0.012294170223176479, Validation Loss: 0.000517573766876012\n\nEpoch 16, Training Loss: 0.012205300197005272, Validation Loss: 0.000444900993257761\n\nEpoch 17, Training Loss: 0.012143172569572926, Validation Loss: 0.0004216313294600695\n\nEpoch 18, Training Loss: 0.012079081274569034, Validation Loss: 0.00041213620477356015\n\nEpoch 19, Training Loss: 0.012025864236056805, Validation Loss: 0.0003475543826352805\n\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","Cell \u001b[0;32mIn[15], line 10\u001b[0m\n\u001b[1;32m      7\u001b[0m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39msynchronize()\n\u001b[1;32m      8\u001b[0m start_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[0;32m---> 10\u001b[0m \u001b[43mevaluate_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcustom_train_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1000\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscheduler\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1024\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_features\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_features\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_classification\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m     12\u001b[0m elapsed_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;241m-\u001b[39m start_time\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExecution time: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00melapsed_time\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.6f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m seconds\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n","Cell \u001b[0;32mIn[8], line 14\u001b[0m, in \u001b[0;36mevaluate_model\u001b[0;34m(model, custom_train_loader, criterion, optimizer, num_epochs, scheduler, batch_size, num_features, is_classification)\u001b[0m\n\u001b[1;32m     12\u001b[0m outputs \u001b[38;5;241m=\u001b[39m model(inputs)\n\u001b[1;32m     13\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(outputs, labels, model)\n\u001b[0;32m---> 14\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     15\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m     16\u001b[0m scheduler\u001b[38;5;241m.\u001b[39mstep()\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/_tensor.py:521\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    511\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    512\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    513\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    514\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    519\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    520\u001b[0m     )\n\u001b[0;32m--> 521\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    522\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    523\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/autograd/__init__.py:289\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    284\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    286\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    287\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    288\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 289\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    290\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    291\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    292\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    293\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    294\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    295\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    296\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    297\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/autograd/graph.py:768\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    766\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[1;32m    767\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 768\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    769\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    770\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[1;32m    771\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    772\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "],"ename":"KeyboardInterrupt","evalue":"","output_type":"error"}]},{"cell_type":"code","source":"import torch.optim as optim\nimport torch.optim.lr_scheduler as lr_scheduler\n\noptimizer = optim.Adam(model.parameters(), lr=0.01)\nscheduler = lr_scheduler.StepLR(optimizer, step_size=1, gamma=0.9995)\n\ntorch.cuda.synchronize()\nstart_time = time.time()\n\nevaluate_model(model, custom_train_loader, criterion, optimizer, 1000, scheduler, batch_size=1024, num_features=num_features, is_classification=False)\n\nelapsed_time = time.time() - start_time\nprint(f\"Execution time: {elapsed_time:.6f} seconds\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"evaluate_model(model, custom_train_loader, criterion, optimizer, 500, scheduler, 1024 * 16)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for name, param in model.named_parameters():\n    print(name)\n    print(param)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import torch\nimport matplotlib.pyplot as plt\n\n# Step 1: Generate the tensor from -10 to 10\ninput_tensor = torch.linspace(-3, 3, 10000).to(device)  # 100 points between -10 and 10\n\n# Assuming `model` is your pre-trained model\nmodel.eval()  # Set the model to evaluation mode\n\n# Step 2: Pass the tensor to the model and obtain the result\nwith torch.no_grad():  # Disable gradient calculation for evaluation\n    output = model(input_tensor.unsqueeze(1))  # Add a dimension if model expects 2D input\n\n# Step 3: Plot the outputs\nplt.figure(figsize=(8, 5))\nplt.plot(input_tensor.cpu().numpy(), output.cpu().numpy(), label='Model Output')\nplt.xlabel('Input')\nplt.ylabel('Output')\nplt.title('Model Output vs. Input')\nplt.legend()\nplt.grid(True)\nplt.show()\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(model(torch.tensor([[0.5]]).to(device)))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}