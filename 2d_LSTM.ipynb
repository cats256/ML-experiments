{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "df232528",
   "metadata": {},
   "source": [
    "TODO:\n",
    "- Work on parallelizable, optimized 2d RNN version with CUDA/Triton/PyTorch\n",
    "- Investigate applying attention between hidden state top, hidden state left, and x\n",
    "- Apply rotations/reflections (rotate 0/90/180/270, flip vertically/horizontally/diagonally/antidiagonally) and use the same LSTM with surrounding border hidden + cell state learnable\n",
    "- Investigate applying attention to final hidden state vectors\n",
    "- Investigate multilayer 2d LSTM\n",
    "- Investigate GRU vs LSTM vs other approaches\n",
    "- Investigate multi-layer LSTM vs LSTM then hidden states of that LSTM -> another LSTM\n",
    "- Investigate skip connections in LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a7471fdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2fb20f07",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MDLSTMCell(nn.Module):\n",
    "    \"\"\"\n",
    "    A 2D LSTM cell following the equations in the provided image.\n",
    "    For each pixel (i, j), it takes:\n",
    "      - x: input at (i, j)\n",
    "      - y_i-1,j (top hidden state), y_i,j-1 (left hidden state)\n",
    "      - c_i-1,j (top cell state), c_i,j-1 (left cell state)\n",
    "    Returns:\n",
    "      - y_ij: output\n",
    "      - c_ij: cell state\n",
    "    \"\"\"\n",
    "    def __init__(self, input_size, hidden_size):\n",
    "        super().__init__()\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "        # Input weight matrices W_* for a, f, g, k, o\n",
    "        self.Wa = nn.Linear(input_size, hidden_size)\n",
    "        self.Wf = nn.Linear(input_size, hidden_size)\n",
    "        self.Wg = nn.Linear(input_size, hidden_size)\n",
    "        self.Wk = nn.Linear(input_size, hidden_size)\n",
    "        self.Wo = nn.Linear(input_size, hidden_size)\n",
    "\n",
    "        # Recurrent weight matrices U_* for y_{i-1,j} (top)\n",
    "        self.Ua = nn.Linear(hidden_size, hidden_size, bias=False)\n",
    "        self.Uf = nn.Linear(hidden_size, hidden_size, bias=False)\n",
    "        self.Ug = nn.Linear(hidden_size, hidden_size, bias=False)\n",
    "        self.Uk = nn.Linear(hidden_size, hidden_size, bias=False)\n",
    "        self.Uo = nn.Linear(hidden_size, hidden_size, bias=False)\n",
    "\n",
    "        # Recurrent weight matrices V_* for y_{i,j-1} (left)\n",
    "        self.Va = nn.Linear(hidden_size, hidden_size, bias=False)\n",
    "        self.Vf = nn.Linear(hidden_size, hidden_size, bias=False)\n",
    "        self.Vg = nn.Linear(hidden_size, hidden_size, bias=False)\n",
    "        self.Vk = nn.Linear(hidden_size, hidden_size, bias=False)\n",
    "        self.Vo = nn.Linear(hidden_size, hidden_size, bias=False)\n",
    "\n",
    "    def forward(self, x, y_top, c_top, y_left, c_left):\n",
    "        \"\"\"\n",
    "        x:       (B, input_size)\n",
    "        y_top:   (B, hidden_size)\n",
    "        c_top:   (B, hidden_size)\n",
    "        y_left:  (B, hidden_size)\n",
    "        c_left:  (B, hidden_size)\n",
    "        returns: y, c (B, hidden_size)\n",
    "        \"\"\"\n",
    "\n",
    "        a = torch.tanh(self.Wa(x) + self.Ua(y_top) + self.Va(y_left))\n",
    "        f = torch.sigmoid(self.Wf(x) + self.Uf(y_top) + self.Vf(y_left))\n",
    "        g = torch.sigmoid(self.Wg(x) + self.Ug(y_top) + self.Vg(y_left))\n",
    "        k = torch.sigmoid(self.Wk(x) + self.Uk(y_top) + self.Vk(y_left))\n",
    "        o = torch.sigmoid(self.Wo(x) + self.Uo(y_top) + self.Vo(y_left))\n",
    "\n",
    "        c = f * c_top + g * c_left + a * k\n",
    "        y = o * torch.tanh(c)\n",
    "        return y, c\n",
    "\n",
    "\n",
    "class MDLSTM(nn.Module):\n",
    "    \"\"\"\n",
    "    2D LSTM module that applies MDLSTMCell over a 2D grid.\n",
    "    Input shape: (B, H, W, input_size)\n",
    "    Output:      (B, H, W, hidden_size) of hidden states\n",
    "    \"\"\"\n",
    "    def __init__(self, input_size, hidden_size):\n",
    "        super().__init__()\n",
    "        self.input_size  = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.cell = MDLSTMCell(input_size, hidden_size)\n",
    "\n",
    "        self.h_top = nn.Parameter(torch.zeros(1, hidden_size))\n",
    "        self.c_top = nn.Parameter(torch.zeros(1, hidden_size))\n",
    "\n",
    "        self.h_left = nn.Parameter(torch.zeros(1, hidden_size))\n",
    "        self.c_left = nn.Parameter(torch.zeros(1, hidden_size))\n",
    "        \n",
    "        # self.h_bottom = nn.Parameter(torch.zeros(1, hidden_size))\n",
    "        # self.c_bottom = nn.Parameter(torch.zeros(1, hidden_size))\n",
    "\n",
    "        # self.h_right = nn.Parameter(torch.zeros(1, hidden_size))\n",
    "        # self.c_right = nn.Parameter(torch.zeros(1, hidden_size))\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        x: (B, H, W, input_size)\n",
    "        returns: h_out of shape (B, H, W, hidden_size)\n",
    "        \"\"\"\n",
    "        B, H, W, _ = x.size()\n",
    "\n",
    "        h_rows = []\n",
    "        c_rows = []\n",
    "\n",
    "        for i in range(H):\n",
    "            h_row = []\n",
    "            c_row = []\n",
    "            for j in range(W):\n",
    "                x_ij = x[:, i, j, :]  # (B, input_size)\n",
    "\n",
    "                if i > 0:\n",
    "                    h1 = h_rows[i-1][j]\n",
    "                    c1 = c_rows[i-1][j]\n",
    "                else:\n",
    "                    h1 = self.h_top.expand(B, -1)\n",
    "                    c1 = self.c_top.expand(B, -1)\n",
    "\n",
    "                if j > 0:\n",
    "                    h2 = h_row[j-1]\n",
    "                    c2 = c_row[j-1]\n",
    "                else:\n",
    "                    h2 = self.h_left.expand(B, -1)\n",
    "                    c2 = self.c_left.expand(B, -1)\n",
    "\n",
    "                h_ij, c_ij = self.cell(x_ij, h1, c1, h2, c2)\n",
    "\n",
    "                h_row.append(h_ij)\n",
    "                c_row.append(c_ij)\n",
    "\n",
    "            h_rows.append(h_row)\n",
    "            c_rows.append(c_row)\n",
    "\n",
    "        h_out = torch.stack([torch.stack(row, dim=1) for row in h_rows], dim=1) # shape (B, H, W, hidden_size)\n",
    "        return h_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9efb1155",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MDLSTM(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size):\n",
    "        super().__init__()\n",
    "        self.input_size   = input_size\n",
    "        self.hidden_size  = hidden_size\n",
    "        self.cell         = MDLSTMCell(input_size, hidden_size)\n",
    "\n",
    "        # only need top‐ and left‐boundary now:\n",
    "        self.h_top    = nn.Parameter(torch.zeros(1, hidden_size))\n",
    "        self.c_top    = nn.Parameter(torch.zeros(1, hidden_size))\n",
    "        self.h_left   = nn.Parameter(torch.zeros(1, hidden_size))\n",
    "        self.c_left   = nn.Parameter(torch.zeros(1, hidden_size))\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, H, W, _ = x.shape\n",
    "        dev        = x.device\n",
    "\n",
    "        # flatten the input grid to (B, H*W, input_size)\n",
    "        x_flat = x.view(B, H*W, self.input_size)\n",
    "\n",
    "        # prepare padded h/c of shape (B, (H+1)*(W+1), hidden_size)\n",
    "        # with row 0 = top‐boundary, col 0 = left‐boundary\n",
    "        pad_h = torch.zeros(B, (H+1)*(W+1), self.hidden_size, device=dev)\n",
    "        pad_c = torch.zeros_like(pad_h)\n",
    "\n",
    "        # fill in the boundaries:\n",
    "        # top boundary at padded indices [0,  W+1 ... 2*(W+1)-1 …]\n",
    "        pad_h[:, 1:(W+1), :] = self.h_top\n",
    "        pad_c[:, 1:(W+1), :] = self.c_top\n",
    "        # left boundary at every (i*(W+1) + 0)\n",
    "        rows = torch.arange(1, H+1, device=dev) * (W+1)\n",
    "        pad_h[:, rows, :]    = self.h_left\n",
    "        pad_c[:, rows, :]    = self.c_left\n",
    "\n",
    "        # now process anti‑diagonals\n",
    "        out_h = torch.zeros(B, H*W, self.hidden_size, device=dev)\n",
    "        out_c = torch.zeros_like(out_h)\n",
    "\n",
    "        for k in range(2, H+W+2):\n",
    "            # valid padded‑indices i',j' with i'+j' = k  (i',j' in [1..H]×[1..W])\n",
    "            i_pos = torch.arange(max(1, k-W), min(H+1, k), device=dev)\n",
    "            j_pos = k - i_pos\n",
    "\n",
    "            # compute flat‑indices into the padded array:\n",
    "            # idx = i'*(W+1) + j'\n",
    "            idx_cur  = i_pos*(W+1) + j_pos\n",
    "            idx_top  = (i_pos-1)*(W+1) + j_pos\n",
    "            idx_left = i_pos*(W+1) + (j_pos-1)\n",
    "\n",
    "            # gather inputs and states for the whole diagonal:\n",
    "            x_k  = x_flat[:, idx_cur-1, :]         # flatten index to original grid\n",
    "            h1   = pad_h .index_select(1, idx_top)\n",
    "            c1   = pad_c .index_select(1, idx_top)\n",
    "            h2   = pad_h .index_select(1, idx_left)\n",
    "            c2   = pad_c .index_select(1, idx_left)\n",
    "\n",
    "            # flatten batch×diagonal into one big batch\n",
    "            N    = idx_cur.size(0)\n",
    "            x_f  = x_k .reshape(B*N, -1)\n",
    "            h1_f = h1  .reshape(B*N, -1)\n",
    "            c1_f = c1  .reshape(B*N, -1)\n",
    "            h2_f = h2  .reshape(B*N, -1)\n",
    "            c2_f = c2  .reshape(B*N, -1)\n",
    "\n",
    "            # single call for the entire diagonal\n",
    "            h_f, c_f = self.cell(x_f, h1_f, c1_f, h2_f, c2_f)\n",
    "\n",
    "            # un‑flatten back to (B, N, hidden)\n",
    "            h_k = h_f.view(B, N, self.hidden_size)\n",
    "            c_k = c_f.view(B, N, self.hidden_size)\n",
    "\n",
    "            # scatter into both the padded state (for future diagonals)\n",
    "            pad_h[:, idx_cur, :] = h_k\n",
    "            pad_c[:, idx_cur, :] = c_k\n",
    "\n",
    "            # and into your output grid\n",
    "            out_h[:, idx_cur-1, :] = h_k\n",
    "            out_c[:, idx_cur-1, :] = c_k\n",
    "\n",
    "        # reshape to (B, H, W, hidden)\n",
    "        return out_h.view(B, H, W, self.hidden_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e10c2fec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# class MDLSTM(nn.Module):\n",
    "#     \"\"\"\n",
    "#     Multi-Directional 2D LSTM module with 4 directional passes.\n",
    "#     Input shape: (B, H, W, input_size)\n",
    "#     Output:      (B, H, W, 4 * hidden_size)\n",
    "#     \"\"\"\n",
    "#     def __init__(self, input_size, hidden_size):\n",
    "#         super().__init__()\n",
    "#         self.input_size = input_size\n",
    "#         self.hidden_size = hidden_size\n",
    "#         self.cell = MDLSTMCell(input_size, hidden_size)\n",
    "\n",
    "#         # Parameters for all 4 directions\n",
    "#         self.h_top    = nn.Parameter(torch.zeros(1, hidden_size))\n",
    "#         self.c_top    = nn.Parameter(torch.zeros(1, hidden_size))\n",
    "#         self.h_left   = nn.Parameter(torch.zeros(1, hidden_size))\n",
    "#         self.c_left   = nn.Parameter(torch.zeros(1, hidden_size))\n",
    "\n",
    "#         self.h_right  = nn.Parameter(torch.zeros(1, hidden_size))\n",
    "#         self.c_right  = nn.Parameter(torch.zeros(1, hidden_size))\n",
    "#         self.h_bottom = nn.Parameter(torch.zeros(1, hidden_size))\n",
    "#         self.c_bottom = nn.Parameter(torch.zeros(1, hidden_size))\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         B, H, W, _ = x.size()\n",
    "\n",
    "#         # Direction 1: top-left to bottom-right\n",
    "#         h_grid = [[None for _ in range(W)] for _ in range(H)]\n",
    "#         c_grid = [[None for _ in range(W)] for _ in range(H)]\n",
    "#         for i in range(H):\n",
    "#             for j in range(W):\n",
    "#                 x_ij = x[:, i, j, :]\n",
    "#                 h1 = h_grid[i-1][j] if i > 0 else self.h_top.expand(B, -1)\n",
    "#                 c1 = c_grid[i-1][j] if i > 0 else self.c_top.expand(B, -1)\n",
    "#                 h2 = h_grid[i][j-1] if j > 0 else self.h_left.expand(B, -1)\n",
    "#                 c2 = c_grid[i][j-1] if j > 0 else self.c_left.expand(B, -1)\n",
    "#                 h, c = self.cell(x_ij, h1, c1, h2, c2)\n",
    "#                 h_grid[i][j] = h\n",
    "#                 c_grid[i][j] = c\n",
    "#         dir1 = torch.stack([torch.stack(row, dim=1) for row in h_grid], dim=1)\n",
    "\n",
    "#         # Direction 2: top-right to bottom-left\n",
    "#         h_grid = [[None for _ in range(W)] for _ in range(H)]\n",
    "#         c_grid = [[None for _ in range(W)] for _ in range(H)]\n",
    "#         for i in range(H):\n",
    "#             for j in reversed(range(W)):\n",
    "#                 x_ij = x[:, i, j, :]\n",
    "#                 h1 = h_grid[i-1][j] if i > 0 else self.h_top.expand(B, -1)\n",
    "#                 c1 = c_grid[i-1][j] if i > 0 else self.c_top.expand(B, -1)\n",
    "#                 h2 = h_grid[i][j+1] if j < W-1 else self.h_right.expand(B, -1)\n",
    "#                 c2 = c_grid[i][j+1] if j < W-1 else self.c_right.expand(B, -1)\n",
    "#                 h, c = self.cell(x_ij, h1, c1, h2, c2)\n",
    "#                 h_grid[i][j] = h\n",
    "#                 c_grid[i][j] = c\n",
    "#         dir2 = torch.stack([torch.stack(row, dim=1) for row in h_grid], dim=1)\n",
    "\n",
    "#         # Direction 3: bottom-left to top-right\n",
    "#         h_grid = [[None for _ in range(W)] for _ in range(H)]\n",
    "#         c_grid = [[None for _ in range(W)] for _ in range(H)]\n",
    "#         for i in reversed(range(H)):\n",
    "#             for j in range(W):\n",
    "#                 x_ij = x[:, i, j, :]\n",
    "#                 h1 = h_grid[i+1][j] if i < H-1 else self.h_bottom.expand(B, -1)\n",
    "#                 c1 = c_grid[i+1][j] if i < H-1 else self.c_bottom.expand(B, -1)\n",
    "#                 h2 = h_grid[i][j-1] if j > 0 else self.h_left.expand(B, -1)\n",
    "#                 c2 = c_grid[i][j-1] if j > 0 else self.c_left.expand(B, -1)\n",
    "#                 h, c = self.cell(x_ij, h1, c1, h2, c2)\n",
    "#                 h_grid[i][j] = h\n",
    "#                 c_grid[i][j] = c\n",
    "#         dir3 = torch.stack([torch.stack(row, dim=1) for row in h_grid], dim=1)\n",
    "\n",
    "#         # Direction 4: bottom-right to top-left\n",
    "#         h_grid = [[None for _ in range(W)] for _ in range(H)]\n",
    "#         c_grid = [[None for _ in range(W)] for _ in range(H)]\n",
    "#         for i in reversed(range(H)):\n",
    "#             for j in reversed(range(W)):\n",
    "#                 x_ij = x[:, i, j, :]\n",
    "#                 h1 = h_grid[i+1][j] if i < H-1 else self.h_bottom.expand(B, -1)\n",
    "#                 c1 = c_grid[i+1][j] if i < H-1 else self.c_bottom.expand(B, -1)\n",
    "#                 h2 = h_grid[i][j+1] if j < W-1 else self.h_right.expand(B, -1)\n",
    "#                 c2 = c_grid[i][j+1] if j < W-1 else self.c_right.expand(B, -1)\n",
    "#                 h, c = self.cell(x_ij, h1, c1, h2, c2)\n",
    "#                 h_grid[i][j] = h\n",
    "#                 c_grid[i][j] = c\n",
    "#         dir4 = torch.stack([torch.stack(row, dim=1) for row in h_grid], dim=1)\n",
    "\n",
    "#         # Stack outputs from all directions: shape (B, H, W, hidden_size, 4)\n",
    "#         h_out = torch.stack([dir1, dir2, dir3, dir4], dim=-1)\n",
    "#         return h_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b0e652c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class AntidiagMDLSTM(nn.Module):\n",
    "    \"\"\"\n",
    "    MD-LSTM whose update order follows the antidiagonals\n",
    "    (i + j is constant).  At every step we launch a single\n",
    "    fused kernel that updates all cells on that antidiagonal.\n",
    "    \n",
    "    Input :  (B, H, W, input_size)\n",
    "    Output:  (B, H, W, hidden_size)\n",
    "    \"\"\"\n",
    "    def __init__(self, input_size: int, hidden_size: int, H: int, W: int):\n",
    "        super().__init__()\n",
    "        self.input_size  = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.H, self.W   = H, W\n",
    "\n",
    "        self.cell = MDLSTMCell(input_size, hidden_size)   # unchanged\n",
    "\n",
    "        # learnable borders\n",
    "        self.register_parameter(\"h_top\",  nn.Parameter(torch.zeros(1, hidden_size)))\n",
    "        self.register_parameter(\"c_top\",  nn.Parameter(torch.zeros(1, hidden_size)))\n",
    "        self.register_parameter(\"h_left\", nn.Parameter(torch.zeros(1, hidden_size)))\n",
    "        self.register_parameter(\"c_left\", nn.Parameter(torch.zeros(1, hidden_size)))\n",
    "\n",
    "        # -------------------- pre‑compute antidiagonal index tensors --------------------\n",
    "        diags = []\n",
    "        for d in range(H + W - 1):\n",
    "            # i + j = d  ▸  j = d − i\n",
    "            i0 = max(0, d - (W - 1))\n",
    "            i1 = min(H - 1, d)\n",
    "            i  = torch.arange(i0, i1 + 1)\n",
    "            j  = d - i\n",
    "            diags.append((i, j))     # each (N_d,) tensor lives on CPU; moved to GPU in forward\n",
    "        self.register_buffer(\"_diag_i\", nn.utils.rnn.pad_sequence([t[0] for t in diags],\n",
    "                                                                  batch_first=True, padding_value=-1),\n",
    "                             persistent=False)\n",
    "        self.register_buffer(\"_diag_j\", nn.utils.rnn.pad_sequence([t[1] for t in diags],\n",
    "                                                                  batch_first=True, padding_value=-1),\n",
    "                             persistent=False)\n",
    "        self._max_diag_len = self._diag_i.size(1)\n",
    "\n",
    "    # ------------------------------------------------------------------------------------\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        x: (B, H, W, input_size)\n",
    "        returns: (B, H, W, hidden_size)\n",
    "        \"\"\"\n",
    "        B, H, W, _ = x.shape\n",
    "        device     = x.device\n",
    "\n",
    "        h = x.new_zeros(B, H, W, self.hidden_size)\n",
    "        c = x.new_zeros(B, H, W, self.hidden_size)\n",
    "\n",
    "        # copy pre‑computed indices to the correct device once\n",
    "        diag_i = self._diag_i.to(device)\n",
    "        diag_j = self._diag_j.to(device)\n",
    "\n",
    "        for d in range(H + W - 1):                         # ❶ only loop that remains\n",
    "            i = diag_i[d]                                  # (N_pad,) ↦ padding value −1 for fillers\n",
    "            j = diag_j[d]\n",
    "\n",
    "            valid_mask = (i >= 0)                          # shape (N_pad,)\n",
    "            i_valid    = i[valid_mask]\n",
    "            j_valid    = j[valid_mask]\n",
    "            n          = i_valid.numel()                   # N_d for this antidiagonal\n",
    "            if n == 0:\n",
    "                continue\n",
    "\n",
    "            # gather current inputs in one read\n",
    "            x_d = x[:, i_valid, j_valid, :]                # (B, n, input_size)\n",
    "\n",
    "            # get neighbours\n",
    "            # top neighbours\n",
    "            top_exists = i_valid > 0\n",
    "            h1 = torch.where(\n",
    "                top_exists.unsqueeze(0).unsqueeze(-1),\n",
    "                h[:, i_valid - 1, j_valid, :],\n",
    "                self.h_top.expand(B, n, -1)\n",
    "            )\n",
    "            c1 = torch.where(\n",
    "                top_exists.unsqueeze(0).unsqueeze(-1),\n",
    "                c[:, i_valid - 1, j_valid, :],\n",
    "                self.c_top.expand(B, n, -1)\n",
    "            )\n",
    "\n",
    "            # left neighbours\n",
    "            left_exists = j_valid > 0\n",
    "            h2 = torch.where(\n",
    "                left_exists.unsqueeze(0).unsqueeze(-1),\n",
    "                h[:, i_valid, j_valid - 1, :],\n",
    "                self.h_left.expand(B, n, -1)\n",
    "            )\n",
    "            c2 = torch.where(\n",
    "                left_exists.unsqueeze(0).unsqueeze(-1),\n",
    "                c[:, i_valid, j_valid - 1, :],\n",
    "                self.c_left.expand(B, n, -1)\n",
    "            )\n",
    "\n",
    "            # ❷     fully‑vectorised cell update for the whole antidiagonal\n",
    "            h_new, c_new = self.cell(\n",
    "                x_d.reshape(B * n, -1),\n",
    "                h1.reshape(B * n, -1),\n",
    "                c1.reshape(B * n, -1),\n",
    "                h2.reshape(B * n, -1),\n",
    "                c2.reshape(B * n, -1),\n",
    "            )\n",
    "            h_new = h_new.view(B, n, -1)\n",
    "            c_new = c_new.view(B, n, -1)\n",
    "\n",
    "            # scatter results back\n",
    "            h[:, i_valid, j_valid, :] = h_new\n",
    "            c[:, i_valid, j_valid, :] = c_new\n",
    "\n",
    "        return h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5e2afda0",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "train_dataset = datasets.MNIST(root='./data', train=True, download=True, transform=transform)\n",
    "test_dataset = datasets.MNIST(root='./data', train=False, download=True, transform=transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ad1b9527",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mps\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9e681c0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = train_dataset.data.to(device).float() / 255.0\n",
    "train_targets = train_dataset.targets.to(device)\n",
    "\n",
    "test_data = test_dataset.data.to(device).float() / 255.0\n",
    "test_targets = test_dataset.targets.to(device)\n",
    "\n",
    "train_data = train_data.unsqueeze(1)\n",
    "test_data = test_data.unsqueeze(1)\n",
    "\n",
    "def get_batches(data, targets, batch_size):\n",
    "    for i in range(0, len(data), batch_size):\n",
    "        yield data[i:i + batch_size], targets[i:i + batch_size]\n",
    "\n",
    "batch_size = 10000\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f32e921b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MNIST2DLSTMClassifier(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        self.mdlstm = MDLSTM(input_size=1, hidden_size=32)\n",
    "        # self.mdlstm2 = MDGRU(input_size=8, hidden_size=32)\n",
    "        self.classifier = nn.Linear(32, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: (B, 1, 28, 28)\n",
    "\n",
    "        x = x.permute(0, 2, 3, 1).contiguous() # (B, H, W, 1)\n",
    "\n",
    "        h = self.mdlstm(x) # (B, H, W, hidden_size)\n",
    "        final_state = h[:, -1, -1, :].clone() # (B, hidden_size)\n",
    "\n",
    "        logits = self.classifier(final_state)\n",
    "        return logits\n",
    "\n",
    "        # h = self.mdlstm(x) # (B, H, W, hidden_size)\n",
    "        # h2 = self.mdlstm2(h) # (B, H, W, hidden_size)\n",
    "\n",
    "        # final_state = h[:, -1, -1, :].clone() # (B, hidden_size)\n",
    "        # final_state2 = h2[:, -1, -1, :].clone() # (B, hidden_size)\n",
    "        \n",
    "        # logits = self.classifier(torch.concat([final_state, final_state2], dim=-1))\n",
    "        # return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7945bd6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# class MNIST2DLSTMClassifier(nn.Module):\n",
    "#     def __init__(self):\n",
    "#         super().__init__()\n",
    "#         self.hidden_size = 8\n",
    "#         self.mdlstm = MDLSTM(input_size=1, hidden_size=self.hidden_size)\n",
    "#         self.cls_token = nn.Parameter(torch.zeros(1, 1, self.hidden_size))\n",
    "#         self.attn = nn.MultiheadAttention(embed_dim=self.hidden_size, num_heads=1, batch_first=True)\n",
    "#         self.classifier = nn.Linear(self.hidden_size, 10)\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         x = x.permute(0, 2, 3, 1).contiguous()  # (B, H, W, 1)\n",
    "#         h = self.mdlstm(x)  # (B, H, W, hidden_size, 4)\n",
    "#         print(h.shape)\n",
    "\n",
    "#         h_top_left = h[:, 0, 0, :, :].mean(dim=-1)\n",
    "#         h_top_right = h[:, 0, -1, :, :].mean(dim=-1)\n",
    "#         h_bottom_left = h[:, -1, 0, :, :].mean(dim=-1)\n",
    "#         h_bottom_right = h[:, -1, -1, :, :].mean(dim=-1)\n",
    "\n",
    "#         corners = torch.stack([h_top_left, h_top_right, h_bottom_left, h_bottom_right], dim=1)  # (B, 4, hidden_size)\n",
    "\n",
    "#         cls_token = self.cls_token.expand(x.size(0), -1, -1)  # (B, 1, hidden_size)\n",
    "#         sequence = torch.cat([cls_token, corners], dim=1)  # (B, 5, hidden_size)\n",
    "\n",
    "#         attn_output, _ = self.attn(sequence, sequence, sequence)  # (B, 5, hidden_size)\n",
    "#         cls_output = attn_output[:, 0, :]  # (B, hidden_size)\n",
    "\n",
    "#         logits = self.classifier(cls_output)\n",
    "#         return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b58b878f",
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.001 * 1\n",
    "epochs = 1000\n",
    "\n",
    "device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "model = MNIST2DLSTMClassifier().to(device)\n",
    "# model = torch.compile(MNIST2DLSTMClassifier()).to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "66bd9be2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mdlstm.h_top: 32 params, requires_grad=True\n",
      "mdlstm.c_top: 32 params, requires_grad=True\n",
      "mdlstm.h_left: 32 params, requires_grad=True\n",
      "mdlstm.c_left: 32 params, requires_grad=True\n",
      "mdlstm.cell.Wa.weight: 32 params, requires_grad=True\n",
      "mdlstm.cell.Wa.bias: 32 params, requires_grad=True\n",
      "mdlstm.cell.Wf.weight: 32 params, requires_grad=True\n",
      "mdlstm.cell.Wf.bias: 32 params, requires_grad=True\n",
      "mdlstm.cell.Wg.weight: 32 params, requires_grad=True\n",
      "mdlstm.cell.Wg.bias: 32 params, requires_grad=True\n",
      "mdlstm.cell.Wk.weight: 32 params, requires_grad=True\n",
      "mdlstm.cell.Wk.bias: 32 params, requires_grad=True\n",
      "mdlstm.cell.Wo.weight: 32 params, requires_grad=True\n",
      "mdlstm.cell.Wo.bias: 32 params, requires_grad=True\n",
      "mdlstm.cell.Ua.weight: 1024 params, requires_grad=True\n",
      "mdlstm.cell.Uf.weight: 1024 params, requires_grad=True\n",
      "mdlstm.cell.Ug.weight: 1024 params, requires_grad=True\n",
      "mdlstm.cell.Uk.weight: 1024 params, requires_grad=True\n",
      "mdlstm.cell.Uo.weight: 1024 params, requires_grad=True\n",
      "mdlstm.cell.Va.weight: 1024 params, requires_grad=True\n",
      "mdlstm.cell.Vf.weight: 1024 params, requires_grad=True\n",
      "mdlstm.cell.Vg.weight: 1024 params, requires_grad=True\n",
      "mdlstm.cell.Vk.weight: 1024 params, requires_grad=True\n",
      "mdlstm.cell.Vo.weight: 1024 params, requires_grad=True\n",
      "classifier.weight: 320 params, requires_grad=True\n",
      "classifier.bias: 10 params, requires_grad=True\n",
      "\n",
      "11018\n"
     ]
    }
   ],
   "source": [
    "for name, param in model.named_parameters():\n",
    "    print(f\"{name}: {param.numel()} params, requires_grad={param.requires_grad}\")\n",
    "\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "print()\n",
    "print(total_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "24ee6e51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/1000], Training Loss: 2.2956\n",
      "Epoch [1/1000], Validation Loss: 2.2787, Validation Accuracy: 18.41%\n",
      "Output Summary: Max=0.3589, Min=-0.3319, Median=-0.0739, Mean=-0.0826\n",
      "\n",
      "Epoch [2/1000], Training Loss: 2.2739\n",
      "Epoch [2/1000], Validation Loss: 2.2611, Validation Accuracy: 18.34%\n",
      "Output Summary: Max=0.2645, Min=-0.3802, Median=-0.0657, Mean=-0.0659\n",
      "\n",
      "Epoch [3/1000], Training Loss: 2.2547\n",
      "Epoch [3/1000], Validation Loss: 2.2413, Validation Accuracy: 18.60%\n",
      "Output Summary: Max=0.3291, Min=-0.4226, Median=-0.0528, Mean=-0.0485\n",
      "\n",
      "Epoch [4/1000], Training Loss: 2.2344\n",
      "Epoch [4/1000], Validation Loss: 2.2188, Validation Accuracy: 18.43%\n",
      "Output Summary: Max=0.4217, Min=-0.4403, Median=-0.0220, Mean=-0.0433\n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[13]\u001b[39m\u001b[32m, line 16\u001b[39m\n\u001b[32m     14\u001b[39m output = model(data)\n\u001b[32m     15\u001b[39m loss = criterion(output, target)\n\u001b[32m---> \u001b[39m\u001b[32m16\u001b[39m \u001b[43mloss\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     18\u001b[39m optimizer.step()\n\u001b[32m     19\u001b[39m running_loss += loss.item()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Downloads/All_Mac/Code/ML_experiments/.venv/lib/python3.11/site-packages/torch/_tensor.py:626\u001b[39m, in \u001b[36mTensor.backward\u001b[39m\u001b[34m(self, gradient, retain_graph, create_graph, inputs)\u001b[39m\n\u001b[32m    616\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    617\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[32m    618\u001b[39m         Tensor.backward,\n\u001b[32m    619\u001b[39m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[32m   (...)\u001b[39m\u001b[32m    624\u001b[39m         inputs=inputs,\n\u001b[32m    625\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m626\u001b[39m \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mautograd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    627\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m=\u001b[49m\u001b[43minputs\u001b[49m\n\u001b[32m    628\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Downloads/All_Mac/Code/ML_experiments/.venv/lib/python3.11/site-packages/torch/autograd/__init__.py:347\u001b[39m, in \u001b[36mbackward\u001b[39m\u001b[34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[39m\n\u001b[32m    342\u001b[39m     retain_graph = create_graph\n\u001b[32m    344\u001b[39m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[32m    345\u001b[39m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[32m    346\u001b[39m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m347\u001b[39m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    348\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    349\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    350\u001b[39m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    351\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    352\u001b[39m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    353\u001b[39m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    354\u001b[39m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    355\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Downloads/All_Mac/Code/ML_experiments/.venv/lib/python3.11/site-packages/torch/autograd/graph.py:823\u001b[39m, in \u001b[36m_engine_run_backward\u001b[39m\u001b[34m(t_outputs, *args, **kwargs)\u001b[39m\n\u001b[32m    821\u001b[39m     unregister_hooks = _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[32m    822\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m823\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_execution_engine\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[32m    824\u001b[39m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m    825\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[32m    826\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    827\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "patience = 1000\n",
    "best_val_loss = float('inf')\n",
    "no_improvement_epochs = 0\n",
    "\n",
    "all_outputs = []\n",
    "\n",
    "for epoch in range(10000):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    num_batches = 0\n",
    "    \n",
    "    for data, target in get_batches(train_data, train_targets, batch_size):\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = criterion(output, target)\n",
    "        loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "        num_batches += 1\n",
    "\n",
    "    print(f\"Epoch [{epoch + 1}/{epochs}], Training Loss: {running_loss / num_batches:.4f}\")\n",
    "\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    num_batches = 0\n",
    "    epoch_outputs = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for data, target in get_batches(test_data, test_targets, batch_size):\n",
    "            outputs = model(data)\n",
    "            loss = criterion(outputs, target)\n",
    "            val_loss += loss.item()\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            total += target.size(0)\n",
    "            num_batches += 1\n",
    "            correct += (predicted == target).sum().item()\n",
    "\n",
    "            epoch_outputs.append(outputs)\n",
    "\n",
    "    all_outputs_tensor = torch.cat(epoch_outputs, dim=0)\n",
    "    all_outputs.append(all_outputs_tensor)\n",
    "\n",
    "    max_val = torch.max(all_outputs_tensor).item()\n",
    "    min_val = torch.min(all_outputs_tensor).item()\n",
    "    median_val = torch.median(all_outputs_tensor).item()\n",
    "    mean_val = torch.mean(all_outputs_tensor).item()\n",
    "\n",
    "    accuracy = 100 * correct / total\n",
    "    val_loss /= num_batches\n",
    "    print(f\"Epoch [{epoch + 1}/{epochs}], Validation Loss: {val_loss:.4f}, Validation Accuracy: {accuracy:.2f}%\")\n",
    "    print(f\"Output Summary: Max={max_val:.4f}, Min={min_val:.4f}, Median={median_val:.4f}, Mean={mean_val:.4f}\")\n",
    "    print()\n",
    "\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        no_improvement_epochs = 0\n",
    "    else:\n",
    "        no_improvement_epochs += 1\n",
    "\n",
    "    if no_improvement_epochs >= patience:\n",
    "        print(f\"Early stopping triggered after {epoch + 1} epochs.\")\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "784a729e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/1000], Training Loss: 2.3316\n",
      "Epoch [1/1000], Validation Loss: 2.3313, Validation Accuracy: 11.35%\n",
      "Output Summary: Max=0.5718, Min=-0.2931, Median=0.1348, Mean=0.1629\n",
      "\n",
      "Epoch [2/1000], Training Loss: 2.3282\n",
      "Epoch [2/1000], Validation Loss: 2.3281, Validation Accuracy: 11.35%\n",
      "Output Summary: Max=0.5527, Min=-0.2773, Median=0.1500, Mean=0.1587\n",
      "\n",
      "Epoch [3/1000], Training Loss: 2.3252\n",
      "Epoch [3/1000], Validation Loss: 2.3252, Validation Accuracy: 11.35%\n",
      "Output Summary: Max=0.5342, Min=-0.2615, Median=0.1628, Mean=0.1547\n",
      "\n",
      "Epoch [4/1000], Training Loss: 2.3225\n",
      "Epoch [4/1000], Validation Loss: 2.3226, Validation Accuracy: 11.35%\n",
      "Output Summary: Max=0.5164, Min=-0.2457, Median=0.1605, Mean=0.1509\n",
      "\n",
      "Epoch [5/1000], Training Loss: 2.3201\n",
      "Epoch [5/1000], Validation Loss: 2.3203, Validation Accuracy: 11.35%\n",
      "Output Summary: Max=0.4993, Min=-0.2299, Median=0.1533, Mean=0.1471\n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[25]\u001b[39m\u001b[32m, line 19\u001b[39m\n\u001b[32m     16\u001b[39m     loss.backward()\n\u001b[32m     18\u001b[39m     optimizer.step()\n\u001b[32m---> \u001b[39m\u001b[32m19\u001b[39m     running_loss += \u001b[43mloss\u001b[49m\u001b[43m.\u001b[49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     20\u001b[39m     num_batches += \u001b[32m1\u001b[39m\n\u001b[32m     22\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mEpoch [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;250m \u001b[39m+\u001b[38;5;250m \u001b[39m\u001b[32m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepochs\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m], Training Loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrunning_loss\u001b[38;5;250m \u001b[39m/\u001b[38;5;250m \u001b[39mnum_batches\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "patience = 1000\n",
    "best_val_loss = float('inf')\n",
    "no_improvement_epochs = 0\n",
    "\n",
    "all_outputs = []\n",
    "\n",
    "for epoch in range(10000):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    num_batches = 0\n",
    "    \n",
    "    for data, target in get_batches(train_data, train_targets, batch_size):\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = criterion(output, target)\n",
    "        loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "        num_batches += 1\n",
    "\n",
    "    print(f\"Epoch [{epoch + 1}/{epochs}], Training Loss: {running_loss / num_batches:.4f}\")\n",
    "\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    num_batches = 0\n",
    "    epoch_outputs = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for data, target in get_batches(test_data, test_targets, batch_size):\n",
    "            outputs = model(data)\n",
    "            loss = criterion(outputs, target)\n",
    "            val_loss += loss.item()\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            total += target.size(0)\n",
    "            num_batches += 1\n",
    "            correct += (predicted == target).sum().item()\n",
    "\n",
    "            epoch_outputs.append(outputs)\n",
    "\n",
    "    all_outputs_tensor = torch.cat(epoch_outputs, dim=0)\n",
    "    all_outputs.append(all_outputs_tensor)\n",
    "\n",
    "    max_val = torch.max(all_outputs_tensor).item()\n",
    "    min_val = torch.min(all_outputs_tensor).item()\n",
    "    median_val = torch.median(all_outputs_tensor).item()\n",
    "    mean_val = torch.mean(all_outputs_tensor).item()\n",
    "\n",
    "    accuracy = 100 * correct / total\n",
    "    val_loss /= num_batches\n",
    "    print(f\"Epoch [{epoch + 1}/{epochs}], Validation Loss: {val_loss:.4f}, Validation Accuracy: {accuracy:.2f}%\")\n",
    "    print(f\"Output Summary: Max={max_val:.4f}, Min={min_val:.4f}, Median={median_val:.4f}, Mean={mean_val:.4f}\")\n",
    "    print()\n",
    "\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        no_improvement_epochs = 0\n",
    "    else:\n",
    "        no_improvement_epochs += 1\n",
    "\n",
    "    if no_improvement_epochs >= patience:\n",
    "        print(f\"Early stopping triggered after {epoch + 1} epochs.\")\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8ab7d07c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/1000], Training Loss: 2.3082\n",
      "Epoch [1/1000], Validation Loss: 2.2904, Validation Accuracy: 14.54%\n",
      "Output Summary: Max=0.4717, Min=0.0500, Median=0.2391, Mean=0.2231\n",
      "\n",
      "Epoch [2/1000], Training Loss: 2.2846\n",
      "Epoch [2/1000], Validation Loss: 2.2709, Validation Accuracy: 18.62%\n",
      "Output Summary: Max=0.4334, Min=-0.1091, Median=0.2322, Mean=0.2158\n",
      "\n",
      "Epoch [3/1000], Training Loss: 2.2611\n",
      "Epoch [3/1000], Validation Loss: 2.2473, Validation Accuracy: 16.00%\n",
      "Output Summary: Max=0.5065, Min=-0.1740, Median=0.2446, Mean=0.2056\n",
      "\n",
      "Epoch [4/1000], Training Loss: 2.2305\n",
      "Epoch [4/1000], Validation Loss: 2.2166, Validation Accuracy: 20.91%\n",
      "Output Summary: Max=0.5957, Min=-0.2189, Median=0.2327, Mean=0.1921\n",
      "\n",
      "Epoch [5/1000], Training Loss: 2.2063\n",
      "Epoch [5/1000], Validation Loss: 2.1864, Validation Accuracy: 24.79%\n",
      "Output Summary: Max=0.6781, Min=-0.4170, Median=0.1958, Mean=0.1710\n",
      "\n",
      "Epoch [6/1000], Training Loss: 2.1782\n",
      "Epoch [6/1000], Validation Loss: 2.1614, Validation Accuracy: 26.55%\n",
      "Output Summary: Max=0.7426, Min=-0.4414, Median=0.1991, Mean=0.1880\n",
      "\n",
      "Epoch [7/1000], Training Loss: 2.1517\n",
      "Epoch [7/1000], Validation Loss: 2.1460, Validation Accuracy: 27.96%\n",
      "Output Summary: Max=0.7948, Min=-0.4350, Median=0.2334, Mean=0.2046\n",
      "\n",
      "Epoch [8/1000], Training Loss: 2.1277\n",
      "Epoch [8/1000], Validation Loss: 2.1113, Validation Accuracy: 29.87%\n",
      "Output Summary: Max=0.8581, Min=-0.4592, Median=0.2267, Mean=0.2113\n",
      "\n",
      "Epoch [9/1000], Training Loss: 2.1013\n",
      "Epoch [9/1000], Validation Loss: 2.0847, Validation Accuracy: 30.56%\n",
      "Output Summary: Max=0.9273, Min=-0.4929, Median=0.2104, Mean=0.2089\n",
      "\n",
      "Epoch [10/1000], Training Loss: 2.0688\n",
      "Epoch [10/1000], Validation Loss: 2.0473, Validation Accuracy: 35.98%\n",
      "Output Summary: Max=0.9983, Min=-0.5620, Median=0.2196, Mean=0.2048\n",
      "\n",
      "Epoch [11/1000], Training Loss: 2.0389\n",
      "Epoch [11/1000], Validation Loss: 2.0173, Validation Accuracy: 36.85%\n",
      "Output Summary: Max=1.0877, Min=-0.6682, Median=0.2319, Mean=0.2132\n",
      "\n",
      "Epoch [12/1000], Training Loss: 2.0049\n",
      "Epoch [12/1000], Validation Loss: 1.9941, Validation Accuracy: 34.61%\n",
      "Output Summary: Max=1.1675, Min=-0.7507, Median=0.2320, Mean=0.2293\n",
      "\n",
      "Epoch [13/1000], Training Loss: 1.9794\n",
      "Epoch [13/1000], Validation Loss: 1.9699, Validation Accuracy: 36.74%\n",
      "Output Summary: Max=1.2338, Min=-0.8660, Median=0.2354, Mean=0.2172\n",
      "\n",
      "Epoch [14/1000], Training Loss: 1.9511\n",
      "Epoch [14/1000], Validation Loss: 1.9356, Validation Accuracy: 36.94%\n",
      "Output Summary: Max=1.3022, Min=-0.9458, Median=0.2526, Mean=0.2445\n",
      "\n",
      "Epoch [15/1000], Training Loss: 1.9185\n",
      "Epoch [15/1000], Validation Loss: 1.9109, Validation Accuracy: 36.64%\n",
      "Output Summary: Max=1.3910, Min=-1.0425, Median=0.2666, Mean=0.2516\n",
      "\n",
      "Epoch [16/1000], Training Loss: 1.8900\n",
      "Epoch [16/1000], Validation Loss: 1.8655, Validation Accuracy: 39.86%\n",
      "Output Summary: Max=1.4934, Min=-1.1486, Median=0.2791, Mean=0.2448\n",
      "\n",
      "Epoch [17/1000], Training Loss: 1.8590\n",
      "Epoch [17/1000], Validation Loss: 1.8420, Validation Accuracy: 41.08%\n",
      "Output Summary: Max=1.5951, Min=-1.2758, Median=0.3098, Mean=0.2542\n",
      "\n",
      "Epoch [18/1000], Training Loss: 1.8345\n",
      "Epoch [18/1000], Validation Loss: 1.8416, Validation Accuracy: 41.66%\n",
      "Output Summary: Max=1.6938, Min=-1.3920, Median=0.3369, Mean=0.2694\n",
      "\n",
      "Epoch [19/1000], Training Loss: 1.8497\n",
      "Epoch [19/1000], Validation Loss: 1.8129, Validation Accuracy: 40.59%\n",
      "Output Summary: Max=1.7581, Min=-1.4272, Median=0.3330, Mean=0.2683\n",
      "\n",
      "Epoch [20/1000], Training Loss: 1.7999\n",
      "Epoch [20/1000], Validation Loss: 1.7774, Validation Accuracy: 43.40%\n",
      "Output Summary: Max=1.8193, Min=-1.5333, Median=0.3065, Mean=0.2534\n",
      "\n",
      "Epoch [21/1000], Training Loss: 1.7606\n",
      "Epoch [21/1000], Validation Loss: 1.7377, Validation Accuracy: 45.60%\n",
      "Output Summary: Max=1.8796, Min=-1.6085, Median=0.3393, Mean=0.2652\n",
      "\n",
      "Epoch [22/1000], Training Loss: 1.7398\n",
      "Epoch [22/1000], Validation Loss: 1.7063, Validation Accuracy: 48.13%\n",
      "Output Summary: Max=1.9599, Min=-1.7092, Median=0.3382, Mean=0.2604\n",
      "\n",
      "Epoch [23/1000], Training Loss: 1.7038\n",
      "Epoch [23/1000], Validation Loss: 1.6841, Validation Accuracy: 48.21%\n",
      "Output Summary: Max=2.0492, Min=-1.7893, Median=0.3511, Mean=0.2678\n",
      "\n",
      "Epoch [24/1000], Training Loss: 1.6750\n",
      "Epoch [24/1000], Validation Loss: 1.6585, Validation Accuracy: 49.02%\n",
      "Output Summary: Max=2.1283, Min=-1.8571, Median=0.3609, Mean=0.2665\n",
      "\n",
      "Epoch [25/1000], Training Loss: 1.6742\n",
      "Epoch [25/1000], Validation Loss: 1.8048, Validation Accuracy: 41.69%\n",
      "Output Summary: Max=2.1801, Min=-1.9109, Median=0.4151, Mean=0.3163\n",
      "\n",
      "Epoch [26/1000], Training Loss: 1.7257\n",
      "Epoch [26/1000], Validation Loss: 1.6501, Validation Accuracy: 48.34%\n",
      "Output Summary: Max=2.2481, Min=-2.0086, Median=0.3410, Mean=0.2623\n",
      "\n",
      "Epoch [27/1000], Training Loss: 1.6506\n",
      "Epoch [27/1000], Validation Loss: 1.6206, Validation Accuracy: 48.64%\n",
      "Output Summary: Max=2.2938, Min=-2.0496, Median=0.3757, Mean=0.2773\n",
      "\n",
      "Epoch [28/1000], Training Loss: 1.6209\n",
      "Epoch [28/1000], Validation Loss: 1.5888, Validation Accuracy: 51.11%\n",
      "Output Summary: Max=2.3633, Min=-2.1218, Median=0.3626, Mean=0.2695\n",
      "\n",
      "Epoch [29/1000], Training Loss: 1.5809\n",
      "Epoch [29/1000], Validation Loss: 1.5673, Validation Accuracy: 51.23%\n",
      "Output Summary: Max=2.4233, Min=-2.1756, Median=0.3832, Mean=0.2770\n",
      "\n",
      "Epoch [30/1000], Training Loss: 1.5499\n",
      "Epoch [30/1000], Validation Loss: 1.5434, Validation Accuracy: 52.78%\n",
      "Output Summary: Max=2.4951, Min=-2.2300, Median=0.3977, Mean=0.2881\n",
      "\n",
      "Epoch [31/1000], Training Loss: 1.5593\n",
      "Epoch [31/1000], Validation Loss: 1.5366, Validation Accuracy: 52.55%\n",
      "Output Summary: Max=2.5418, Min=-2.2896, Median=0.4037, Mean=0.2945\n",
      "\n",
      "Epoch [32/1000], Training Loss: 1.5199\n",
      "Epoch [32/1000], Validation Loss: 1.5088, Validation Accuracy: 52.91%\n",
      "Output Summary: Max=2.5954, Min=-2.4830, Median=0.3397, Mean=0.2562\n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[18]\u001b[39m\u001b[32m, line 14\u001b[39m\n\u001b[32m     12\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m data, target \u001b[38;5;129;01min\u001b[39;00m get_batches(train_data, train_targets, batch_size):\n\u001b[32m     13\u001b[39m     optimizer.zero_grad()\n\u001b[32m---> \u001b[39m\u001b[32m14\u001b[39m     output = \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     15\u001b[39m     loss = criterion(output, target)\n\u001b[32m     16\u001b[39m     loss.backward()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Downloads/All_Mac/Code/ML_experiments/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1739\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1737\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1738\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1739\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Downloads/All_Mac/Code/ML_experiments/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1750\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1745\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1746\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1747\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1748\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1749\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1750\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1752\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1753\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[11]\u001b[39m\u001b[32m, line 14\u001b[39m, in \u001b[36mMNIST2DLSTMClassifier.forward\u001b[39m\u001b[34m(self, x)\u001b[39m\n\u001b[32m      9\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[32m     10\u001b[39m     \u001b[38;5;66;03m# x: (B, 1, 28, 28)\u001b[39;00m\n\u001b[32m     12\u001b[39m     x = x.permute(\u001b[32m0\u001b[39m, \u001b[32m2\u001b[39m, \u001b[32m3\u001b[39m, \u001b[32m1\u001b[39m).contiguous() \u001b[38;5;66;03m# (B, H, W, 1)\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m14\u001b[39m     h = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmdlstm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m# (B, H, W, hidden_size)\u001b[39;00m\n\u001b[32m     15\u001b[39m     final_state = h[:, -\u001b[32m1\u001b[39m, -\u001b[32m1\u001b[39m, :].clone() \u001b[38;5;66;03m# (B, hidden_size)\u001b[39;00m\n\u001b[32m     17\u001b[39m     logits = \u001b[38;5;28mself\u001b[39m.classifier(final_state)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Downloads/All_Mac/Code/ML_experiments/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1739\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1737\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1738\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1739\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Downloads/All_Mac/Code/ML_experiments/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1750\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1745\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1746\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1747\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1748\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1749\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1750\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1752\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1753\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 113\u001b[39m, in \u001b[36mMDLSTM.forward\u001b[39m\u001b[34m(self, x)\u001b[39m\n\u001b[32m    110\u001b[39m     h2 = \u001b[38;5;28mself\u001b[39m.h_left.expand(B, -\u001b[32m1\u001b[39m)\n\u001b[32m    111\u001b[39m     c2 = \u001b[38;5;28mself\u001b[39m.c_left.expand(B, -\u001b[32m1\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m113\u001b[39m h_ij, c_ij = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcell\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_ij\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mh1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mc1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mh2\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mc2\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    115\u001b[39m h_row.append(h_ij)\n\u001b[32m    116\u001b[39m c_row.append(c_ij)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Downloads/All_Mac/Code/ML_experiments/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1739\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1737\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1738\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1739\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Downloads/All_Mac/Code/ML_experiments/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1750\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1745\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1746\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1747\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1748\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1749\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1750\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1752\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1753\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 55\u001b[39m, in \u001b[36mMDLSTMCell.forward\u001b[39m\u001b[34m(self, x, y_top, c_top, y_left, c_left)\u001b[39m\n\u001b[32m     52\u001b[39m o = torch.sigmoid(\u001b[38;5;28mself\u001b[39m.Wo(x) + \u001b[38;5;28mself\u001b[39m.Uo(y_top) + \u001b[38;5;28mself\u001b[39m.Vo(y_left))\n\u001b[32m     54\u001b[39m c = f * c_top + g * c_left + a * k\n\u001b[32m---> \u001b[39m\u001b[32m55\u001b[39m y = o * \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtanh\u001b[49m\u001b[43m(\u001b[49m\u001b[43mc\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     56\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m y, c\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "patience = 1000\n",
    "best_val_loss = float('inf')\n",
    "no_improvement_epochs = 0\n",
    "\n",
    "all_outputs = []\n",
    "\n",
    "for epoch in range(10000):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    num_batches = 0\n",
    "    \n",
    "    for data, target in get_batches(train_data, train_targets, batch_size):\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = criterion(output, target)\n",
    "        loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "        num_batches += 1\n",
    "\n",
    "    print(f\"Epoch [{epoch + 1}/{epochs}], Training Loss: {running_loss / num_batches:.4f}\")\n",
    "\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    num_batches = 0\n",
    "    epoch_outputs = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for data, target in get_batches(test_data, test_targets, batch_size):\n",
    "            outputs = model(data)\n",
    "            loss = criterion(outputs, target)\n",
    "            val_loss += loss.item()\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            total += target.size(0)\n",
    "            num_batches += 1\n",
    "            correct += (predicted == target).sum().item()\n",
    "\n",
    "            epoch_outputs.append(outputs)\n",
    "\n",
    "    all_outputs_tensor = torch.cat(epoch_outputs, dim=0)\n",
    "    all_outputs.append(all_outputs_tensor)\n",
    "\n",
    "    max_val = torch.max(all_outputs_tensor).item()\n",
    "    min_val = torch.min(all_outputs_tensor).item()\n",
    "    median_val = torch.median(all_outputs_tensor).item()\n",
    "    mean_val = torch.mean(all_outputs_tensor).item()\n",
    "\n",
    "    accuracy = 100 * correct / total\n",
    "    val_loss /= num_batches\n",
    "    print(f\"Epoch [{epoch + 1}/{epochs}], Validation Loss: {val_loss:.4f}, Validation Accuracy: {accuracy:.2f}%\")\n",
    "    print(f\"Output Summary: Max={max_val:.4f}, Min={min_val:.4f}, Median={median_val:.4f}, Mean={mean_val:.4f}\")\n",
    "    print()\n",
    "\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        no_improvement_epochs = 0\n",
    "    else:\n",
    "        no_improvement_epochs += 1\n",
    "\n",
    "    if no_improvement_epochs >= patience:\n",
    "        print(f\"Early stopping triggered after {epoch + 1} epochs.\")\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "6bd44bd4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/1000], Training Loss: 2.3199\n",
      "Epoch [1/1000], Validation Loss: 2.2994, Validation Accuracy: 10.05%\n",
      "Output Summary: Max=0.3197, Min=-0.4100, Median=0.0177, Mean=-0.0204\n",
      "\n",
      "Epoch [2/1000], Training Loss: 2.2885\n",
      "Epoch [2/1000], Validation Loss: 2.2744, Validation Accuracy: 11.07%\n",
      "Output Summary: Max=0.2439, Min=-0.3253, Median=-0.0285, Mean=-0.0319\n",
      "\n",
      "Epoch [3/1000], Training Loss: 2.2686\n",
      "Epoch [3/1000], Validation Loss: 2.2575, Validation Accuracy: 21.28%\n",
      "Output Summary: Max=0.2824, Min=-0.2883, Median=-0.0509, Mean=-0.0330\n",
      "\n",
      "Epoch [4/1000], Training Loss: 2.2546\n",
      "Epoch [4/1000], Validation Loss: 2.2601, Validation Accuracy: 18.81%\n",
      "Output Summary: Max=0.4426, Min=-0.3655, Median=-0.0352, Mean=-0.0317\n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[61]\u001b[39m\u001b[32m, line 16\u001b[39m\n\u001b[32m     14\u001b[39m output = model(data)\n\u001b[32m     15\u001b[39m loss = criterion(output, target)\n\u001b[32m---> \u001b[39m\u001b[32m16\u001b[39m \u001b[43mloss\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     18\u001b[39m optimizer.step()\n\u001b[32m     19\u001b[39m running_loss += loss.item()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Downloads/All_Mac/Code/ML_experiments/.venv/lib/python3.11/site-packages/torch/_tensor.py:626\u001b[39m, in \u001b[36mTensor.backward\u001b[39m\u001b[34m(self, gradient, retain_graph, create_graph, inputs)\u001b[39m\n\u001b[32m    616\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    617\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[32m    618\u001b[39m         Tensor.backward,\n\u001b[32m    619\u001b[39m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[32m   (...)\u001b[39m\u001b[32m    624\u001b[39m         inputs=inputs,\n\u001b[32m    625\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m626\u001b[39m \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mautograd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    627\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m=\u001b[49m\u001b[43minputs\u001b[49m\n\u001b[32m    628\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Downloads/All_Mac/Code/ML_experiments/.venv/lib/python3.11/site-packages/torch/autograd/__init__.py:347\u001b[39m, in \u001b[36mbackward\u001b[39m\u001b[34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[39m\n\u001b[32m    342\u001b[39m     retain_graph = create_graph\n\u001b[32m    344\u001b[39m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[32m    345\u001b[39m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[32m    346\u001b[39m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m347\u001b[39m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    348\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    349\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    350\u001b[39m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    351\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    352\u001b[39m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    353\u001b[39m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    354\u001b[39m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    355\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Downloads/All_Mac/Code/ML_experiments/.venv/lib/python3.11/site-packages/torch/autograd/graph.py:823\u001b[39m, in \u001b[36m_engine_run_backward\u001b[39m\u001b[34m(t_outputs, *args, **kwargs)\u001b[39m\n\u001b[32m    821\u001b[39m     unregister_hooks = _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[32m    822\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m823\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_execution_engine\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[32m    824\u001b[39m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m    825\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[32m    826\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    827\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "patience = 1000\n",
    "best_val_loss = float('inf')\n",
    "no_improvement_epochs = 0\n",
    "\n",
    "all_outputs = []\n",
    "\n",
    "for epoch in range(10000):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    num_batches = 0\n",
    "    \n",
    "    for data, target in get_batches(train_data, train_targets, batch_size):\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = criterion(output, target)\n",
    "        loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "        num_batches += 1\n",
    "\n",
    "    print(f\"Epoch [{epoch + 1}/{epochs}], Training Loss: {running_loss / num_batches:.4f}\")\n",
    "\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    num_batches = 0\n",
    "    epoch_outputs = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for data, target in get_batches(test_data, test_targets, batch_size):\n",
    "            outputs = model(data)\n",
    "            loss = criterion(outputs, target)\n",
    "            val_loss += loss.item()\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            total += target.size(0)\n",
    "            num_batches += 1\n",
    "            correct += (predicted == target).sum().item()\n",
    "\n",
    "            epoch_outputs.append(outputs)\n",
    "\n",
    "    all_outputs_tensor = torch.cat(epoch_outputs, dim=0)\n",
    "    all_outputs.append(all_outputs_tensor)\n",
    "\n",
    "    max_val = torch.max(all_outputs_tensor).item()\n",
    "    min_val = torch.min(all_outputs_tensor).item()\n",
    "    median_val = torch.median(all_outputs_tensor).item()\n",
    "    mean_val = torch.mean(all_outputs_tensor).item()\n",
    "\n",
    "    accuracy = 100 * correct / total\n",
    "    val_loss /= num_batches\n",
    "    print(f\"Epoch [{epoch + 1}/{epochs}], Validation Loss: {val_loss:.4f}, Validation Accuracy: {accuracy:.2f}%\")\n",
    "    print(f\"Output Summary: Max={max_val:.4f}, Min={min_val:.4f}, Median={median_val:.4f}, Mean={mean_val:.4f}\")\n",
    "    print()\n",
    "\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        no_improvement_epochs = 0\n",
    "    else:\n",
    "        no_improvement_epochs += 1\n",
    "\n",
    "    if no_improvement_epochs >= patience:\n",
    "        print(f\"Early stopping triggered after {epoch + 1} epochs.\")\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "4934b6bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/1000], Training Loss: 2.2841\n",
      "Epoch [1/1000], Validation Loss: 2.2534, Validation Accuracy: 20.79%\n",
      "Output Summary: Max=0.6553, Min=-0.3376, Median=0.1000, Mean=0.1031\n",
      "\n",
      "Epoch [2/1000], Training Loss: 2.2355\n",
      "Epoch [2/1000], Validation Loss: 2.2170, Validation Accuracy: 20.38%\n",
      "Output Summary: Max=0.6273, Min=-0.3303, Median=0.1150, Mean=0.1061\n",
      "\n",
      "Epoch [3/1000], Training Loss: 2.2054\n",
      "Epoch [3/1000], Validation Loss: 2.1807, Validation Accuracy: 19.85%\n",
      "Output Summary: Max=0.6261, Min=-0.3204, Median=0.0984, Mean=0.1073\n",
      "\n",
      "Epoch [4/1000], Training Loss: 2.1697\n",
      "Epoch [4/1000], Validation Loss: 2.1444, Validation Accuracy: 26.45%\n",
      "Output Summary: Max=0.6688, Min=-0.3628, Median=0.1046, Mean=0.1189\n",
      "\n",
      "Epoch [5/1000], Training Loss: 2.1296\n",
      "Epoch [5/1000], Validation Loss: 2.1278, Validation Accuracy: 17.13%\n",
      "Output Summary: Max=0.8167, Min=-0.4004, Median=0.1101, Mean=0.1277\n",
      "\n",
      "Epoch [6/1000], Training Loss: 2.1130\n",
      "Epoch [6/1000], Validation Loss: 2.1043, Validation Accuracy: 20.50%\n",
      "Output Summary: Max=0.9396, Min=-0.4564, Median=0.1134, Mean=0.1073\n",
      "\n",
      "Epoch [7/1000], Training Loss: 2.0896\n",
      "Epoch [7/1000], Validation Loss: 2.0630, Validation Accuracy: 20.80%\n",
      "Output Summary: Max=1.0152, Min=-0.5508, Median=0.1116, Mean=0.0935\n",
      "\n",
      "Epoch [8/1000], Training Loss: 2.0518\n",
      "Epoch [8/1000], Validation Loss: 2.0279, Validation Accuracy: 21.21%\n",
      "Output Summary: Max=1.0848, Min=-0.6997, Median=0.1280, Mean=0.0972\n",
      "\n",
      "Epoch [9/1000], Training Loss: 2.0391\n",
      "Epoch [9/1000], Validation Loss: 2.0193, Validation Accuracy: 20.50%\n",
      "Output Summary: Max=1.2055, Min=-0.8387, Median=0.1715, Mean=0.1106\n",
      "\n",
      "Epoch [10/1000], Training Loss: 1.9894\n",
      "Epoch [10/1000], Validation Loss: 1.9537, Validation Accuracy: 21.77%\n",
      "Output Summary: Max=1.2739, Min=-0.9740, Median=0.3297, Mean=0.1081\n",
      "\n",
      "Epoch [11/1000], Training Loss: 1.9498\n",
      "Epoch [11/1000], Validation Loss: 1.9222, Validation Accuracy: 24.14%\n",
      "Output Summary: Max=1.3464, Min=-1.1069, Median=0.3937, Mean=0.1172\n",
      "\n",
      "Epoch [12/1000], Training Loss: 1.9126\n",
      "Epoch [12/1000], Validation Loss: 1.8873, Validation Accuracy: 24.66%\n",
      "Output Summary: Max=1.4176, Min=-1.2271, Median=0.4211, Mean=0.1142\n",
      "\n",
      "Epoch [13/1000], Training Loss: 1.8728\n",
      "Epoch [13/1000], Validation Loss: 1.8490, Validation Accuracy: 27.05%\n",
      "Output Summary: Max=1.5067, Min=-1.3369, Median=0.4133, Mean=0.1168\n",
      "\n",
      "Epoch [14/1000], Training Loss: 1.8358\n",
      "Epoch [14/1000], Validation Loss: 1.8191, Validation Accuracy: 25.86%\n",
      "Output Summary: Max=1.5936, Min=-1.4471, Median=0.4865, Mean=0.1179\n",
      "\n",
      "Epoch [15/1000], Training Loss: 1.8230\n",
      "Epoch [15/1000], Validation Loss: 1.8413, Validation Accuracy: 19.56%\n",
      "Output Summary: Max=1.6952, Min=-1.5482, Median=0.3681, Mean=0.1187\n",
      "\n",
      "Epoch [16/1000], Training Loss: 1.8060\n",
      "Epoch [16/1000], Validation Loss: 1.7790, Validation Accuracy: 27.15%\n",
      "Output Summary: Max=1.7414, Min=-1.6619, Median=0.4215, Mean=0.1275\n",
      "\n",
      "Epoch [17/1000], Training Loss: 1.7617\n",
      "Epoch [17/1000], Validation Loss: 1.7519, Validation Accuracy: 27.39%\n",
      "Output Summary: Max=1.8172, Min=-1.7647, Median=0.4069, Mean=0.1201\n",
      "\n",
      "Epoch [18/1000], Training Loss: 1.7381\n",
      "Epoch [18/1000], Validation Loss: 1.7233, Validation Accuracy: 32.69%\n",
      "Output Summary: Max=1.8918, Min=-1.8711, Median=0.4476, Mean=0.1201\n",
      "\n",
      "Epoch [19/1000], Training Loss: 1.7104\n",
      "Epoch [19/1000], Validation Loss: 1.6984, Validation Accuracy: 29.28%\n",
      "Output Summary: Max=1.9817, Min=-1.9663, Median=0.4138, Mean=0.1218\n",
      "\n",
      "Epoch [20/1000], Training Loss: 1.6890\n",
      "Epoch [20/1000], Validation Loss: 1.6872, Validation Accuracy: 30.78%\n",
      "Output Summary: Max=2.0753, Min=-2.0556, Median=0.3886, Mean=0.1216\n",
      "\n",
      "Epoch [21/1000], Training Loss: 1.6761\n",
      "Epoch [21/1000], Validation Loss: 1.6567, Validation Accuracy: 32.09%\n",
      "Output Summary: Max=2.1536, Min=-2.1464, Median=0.4088, Mean=0.1210\n",
      "\n",
      "Epoch [22/1000], Training Loss: 1.6925\n",
      "Epoch [22/1000], Validation Loss: 1.7831, Validation Accuracy: 31.19%\n",
      "Output Summary: Max=2.2348, Min=-2.2177, Median=0.2987, Mean=0.0941\n",
      "\n",
      "Epoch [23/1000], Training Loss: 1.7091\n",
      "Epoch [23/1000], Validation Loss: 1.6971, Validation Accuracy: 28.69%\n",
      "Output Summary: Max=2.2938, Min=-2.2874, Median=0.5153, Mean=0.1034\n",
      "\n",
      "Epoch [24/1000], Training Loss: 1.6891\n",
      "Epoch [24/1000], Validation Loss: 1.7047, Validation Accuracy: 28.16%\n",
      "Output Summary: Max=2.3272, Min=-2.3573, Median=0.3382, Mean=0.1319\n",
      "\n",
      "Epoch [25/1000], Training Loss: 1.6687\n",
      "Epoch [25/1000], Validation Loss: 1.6740, Validation Accuracy: 36.39%\n",
      "Output Summary: Max=2.3663, Min=-2.4117, Median=0.4314, Mean=0.1045\n",
      "\n",
      "Epoch [26/1000], Training Loss: 1.6535\n",
      "Epoch [26/1000], Validation Loss: 1.6225, Validation Accuracy: 36.90%\n",
      "Output Summary: Max=2.3717, Min=-2.4889, Median=0.5167, Mean=0.1065\n",
      "\n",
      "Epoch [27/1000], Training Loss: 1.6111\n",
      "Epoch [27/1000], Validation Loss: 1.6031, Validation Accuracy: 39.11%\n",
      "Output Summary: Max=2.4022, Min=-2.5541, Median=0.4640, Mean=0.1058\n",
      "\n",
      "Epoch [28/1000], Training Loss: 1.5920\n",
      "Epoch [28/1000], Validation Loss: 1.5774, Validation Accuracy: 36.31%\n",
      "Output Summary: Max=2.4749, Min=-2.6155, Median=0.4736, Mean=0.1185\n",
      "\n",
      "Epoch [29/1000], Training Loss: 1.5633\n",
      "Epoch [29/1000], Validation Loss: 1.5581, Validation Accuracy: 42.68%\n",
      "Output Summary: Max=2.5664, Min=-2.6683, Median=0.4245, Mean=0.1062\n",
      "\n",
      "Epoch [30/1000], Training Loss: 1.5493\n",
      "Epoch [30/1000], Validation Loss: 1.5399, Validation Accuracy: 42.51%\n",
      "Output Summary: Max=2.6588, Min=-2.7162, Median=0.4279, Mean=0.1037\n",
      "\n",
      "Epoch [31/1000], Training Loss: 1.5292\n",
      "Epoch [31/1000], Validation Loss: 1.5527, Validation Accuracy: 44.86%\n",
      "Output Summary: Max=2.7362, Min=-2.7706, Median=0.4190, Mean=0.1081\n",
      "\n",
      "Epoch [32/1000], Training Loss: 1.5242\n",
      "Epoch [32/1000], Validation Loss: 1.5204, Validation Accuracy: 41.87%\n",
      "Output Summary: Max=2.7886, Min=-2.8267, Median=0.5361, Mean=0.1109\n",
      "\n",
      "Epoch [33/1000], Training Loss: 1.5051\n",
      "Epoch [33/1000], Validation Loss: 1.5073, Validation Accuracy: 45.51%\n",
      "Output Summary: Max=2.8483, Min=-2.8790, Median=0.4750, Mean=0.1100\n",
      "\n",
      "Epoch [34/1000], Training Loss: 1.4845\n",
      "Epoch [34/1000], Validation Loss: 1.4839, Validation Accuracy: 42.23%\n",
      "Output Summary: Max=2.8947, Min=-2.9315, Median=0.4959, Mean=0.1144\n",
      "\n",
      "Epoch [35/1000], Training Loss: 1.4681\n",
      "Epoch [35/1000], Validation Loss: 1.4617, Validation Accuracy: 49.77%\n",
      "Output Summary: Max=2.9469, Min=-2.9820, Median=0.4751, Mean=0.1117\n",
      "\n",
      "Epoch [36/1000], Training Loss: 1.4478\n",
      "Epoch [36/1000], Validation Loss: 1.4428, Validation Accuracy: 46.00%\n",
      "Output Summary: Max=3.0038, Min=-3.0314, Median=0.4662, Mean=0.1102\n",
      "\n",
      "Epoch [37/1000], Training Loss: 1.4239\n",
      "Epoch [37/1000], Validation Loss: 1.4142, Validation Accuracy: 45.62%\n",
      "Output Summary: Max=3.0593, Min=-3.0882, Median=0.4778, Mean=0.1112\n",
      "\n",
      "Epoch [38/1000], Training Loss: 1.3981\n",
      "Epoch [38/1000], Validation Loss: 1.3946, Validation Accuracy: 45.97%\n",
      "Output Summary: Max=3.1106, Min=-3.1479, Median=0.5087, Mean=0.1125\n",
      "\n",
      "Epoch [39/1000], Training Loss: 1.3735\n",
      "Epoch [39/1000], Validation Loss: 1.3768, Validation Accuracy: 47.11%\n",
      "Output Summary: Max=3.1967, Min=-3.2038, Median=0.4207, Mean=0.1094\n",
      "\n",
      "Epoch [40/1000], Training Loss: 1.3481\n",
      "Epoch [40/1000], Validation Loss: 1.3398, Validation Accuracy: 52.17%\n",
      "Output Summary: Max=3.2422, Min=-3.2656, Median=0.4458, Mean=0.1143\n",
      "\n",
      "Epoch [41/1000], Training Loss: 1.3253\n",
      "Epoch [41/1000], Validation Loss: 1.3161, Validation Accuracy: 52.15%\n",
      "Output Summary: Max=3.2948, Min=-3.3269, Median=0.4694, Mean=0.1205\n",
      "\n",
      "Epoch [42/1000], Training Loss: 1.2927\n",
      "Epoch [42/1000], Validation Loss: 1.2879, Validation Accuracy: 54.79%\n",
      "Output Summary: Max=3.3847, Min=-3.3757, Median=0.3290, Mean=0.1129\n",
      "\n",
      "Epoch [43/1000], Training Loss: 1.2632\n",
      "Epoch [43/1000], Validation Loss: 1.2526, Validation Accuracy: 54.65%\n",
      "Output Summary: Max=3.4395, Min=-3.4352, Median=0.3731, Mean=0.1227\n",
      "\n",
      "Epoch [44/1000], Training Loss: 1.2369\n",
      "Epoch [44/1000], Validation Loss: 1.2380, Validation Accuracy: 55.87%\n",
      "Output Summary: Max=3.4995, Min=-3.4959, Median=0.2698, Mean=0.1111\n",
      "\n",
      "Epoch [45/1000], Training Loss: 1.2270\n",
      "Epoch [45/1000], Validation Loss: 1.3119, Validation Accuracy: 50.17%\n",
      "Output Summary: Max=3.5703, Min=-3.5584, Median=0.2629, Mean=0.1115\n",
      "\n",
      "Epoch [46/1000], Training Loss: 1.2652\n",
      "Epoch [46/1000], Validation Loss: 1.2379, Validation Accuracy: 52.68%\n",
      "Output Summary: Max=3.5510, Min=-3.6050, Median=0.2242, Mean=0.1361\n",
      "\n",
      "Epoch [47/1000], Training Loss: 1.1951\n",
      "Epoch [47/1000], Validation Loss: 1.1884, Validation Accuracy: 56.09%\n",
      "Output Summary: Max=3.6133, Min=-3.6473, Median=0.1555, Mean=0.1290\n",
      "\n",
      "Epoch [48/1000], Training Loss: 1.1810\n",
      "Epoch [48/1000], Validation Loss: 1.1933, Validation Accuracy: 57.15%\n",
      "Output Summary: Max=3.6571, Min=-3.6765, Median=0.1955, Mean=0.1335\n",
      "\n",
      "Epoch [49/1000], Training Loss: 1.1674\n",
      "Epoch [49/1000], Validation Loss: 1.1513, Validation Accuracy: 57.58%\n",
      "Output Summary: Max=3.6652, Min=-3.7038, Median=0.1893, Mean=0.1431\n",
      "\n",
      "Epoch [50/1000], Training Loss: 1.1345\n",
      "Epoch [50/1000], Validation Loss: 1.1369, Validation Accuracy: 57.89%\n",
      "Output Summary: Max=3.7264, Min=-3.7488, Median=0.0869, Mean=0.1292\n",
      "\n",
      "Epoch [51/1000], Training Loss: 1.1157\n",
      "Epoch [51/1000], Validation Loss: 1.1351, Validation Accuracy: 58.78%\n",
      "Output Summary: Max=3.7521, Min=-3.6881, Median=0.0487, Mean=0.1249\n",
      "\n",
      "Epoch [52/1000], Training Loss: 1.1105\n",
      "Epoch [52/1000], Validation Loss: 1.1561, Validation Accuracy: 54.25%\n",
      "Output Summary: Max=3.7439, Min=-3.8448, Median=-0.0006, Mean=0.1202\n",
      "\n",
      "Epoch [53/1000], Training Loss: 1.1103\n",
      "Epoch [53/1000], Validation Loss: 1.1015, Validation Accuracy: 57.59%\n",
      "Output Summary: Max=3.8360, Min=-3.8785, Median=0.0104, Mean=0.1450\n",
      "\n",
      "Epoch [54/1000], Training Loss: 1.0779\n",
      "Epoch [54/1000], Validation Loss: 1.0813, Validation Accuracy: 58.62%\n",
      "Output Summary: Max=3.9054, Min=-3.8623, Median=0.0188, Mean=0.1207\n",
      "\n",
      "Epoch [55/1000], Training Loss: 1.0621\n",
      "Epoch [55/1000], Validation Loss: 1.0667, Validation Accuracy: 57.98%\n",
      "Output Summary: Max=3.9596, Min=-3.8094, Median=0.0184, Mean=0.1326\n",
      "\n",
      "Epoch [56/1000], Training Loss: 1.0507\n",
      "Epoch [56/1000], Validation Loss: 1.0648, Validation Accuracy: 60.05%\n",
      "Output Summary: Max=4.0213, Min=-3.8313, Median=0.0101, Mean=0.1138\n",
      "\n",
      "Epoch [57/1000], Training Loss: 1.0430\n",
      "Epoch [57/1000], Validation Loss: 1.0586, Validation Accuracy: 58.75%\n",
      "Output Summary: Max=4.1016, Min=-3.9029, Median=0.0201, Mean=0.1167\n",
      "\n",
      "Epoch [58/1000], Training Loss: 1.0345\n",
      "Epoch [58/1000], Validation Loss: 1.0413, Validation Accuracy: 60.05%\n",
      "Output Summary: Max=4.1874, Min=-3.9296, Median=0.0273, Mean=0.1283\n",
      "\n",
      "Epoch [59/1000], Training Loss: 1.0250\n",
      "Epoch [59/1000], Validation Loss: 1.0790, Validation Accuracy: 59.82%\n",
      "Output Summary: Max=4.2536, Min=-3.9000, Median=-0.0336, Mean=0.1033\n",
      "\n",
      "Epoch [60/1000], Training Loss: 1.0634\n",
      "Epoch [60/1000], Validation Loss: 1.0507, Validation Accuracy: 57.73%\n",
      "Output Summary: Max=4.3384, Min=-3.9738, Median=-0.0158, Mean=0.1431\n",
      "\n",
      "Epoch [61/1000], Training Loss: 1.0299\n",
      "Epoch [61/1000], Validation Loss: 1.0380, Validation Accuracy: 58.34%\n",
      "Output Summary: Max=4.4034, Min=-3.9589, Median=-0.0182, Mean=0.1319\n",
      "\n",
      "Epoch [62/1000], Training Loss: 1.0121\n",
      "Epoch [62/1000], Validation Loss: 1.0177, Validation Accuracy: 60.16%\n",
      "Output Summary: Max=4.4500, Min=-3.8969, Median=-0.0325, Mean=0.1233\n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[43]\u001b[39m\u001b[32m, line 16\u001b[39m\n\u001b[32m     14\u001b[39m output = model(data)\n\u001b[32m     15\u001b[39m loss = criterion(output, target)\n\u001b[32m---> \u001b[39m\u001b[32m16\u001b[39m \u001b[43mloss\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     18\u001b[39m optimizer.step()\n\u001b[32m     19\u001b[39m running_loss += loss.item()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Downloads/All_Mac/Code/ML_experiments/.venv/lib/python3.11/site-packages/torch/_tensor.py:626\u001b[39m, in \u001b[36mTensor.backward\u001b[39m\u001b[34m(self, gradient, retain_graph, create_graph, inputs)\u001b[39m\n\u001b[32m    616\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    617\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[32m    618\u001b[39m         Tensor.backward,\n\u001b[32m    619\u001b[39m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[32m   (...)\u001b[39m\u001b[32m    624\u001b[39m         inputs=inputs,\n\u001b[32m    625\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m626\u001b[39m \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mautograd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    627\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m=\u001b[49m\u001b[43minputs\u001b[49m\n\u001b[32m    628\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Downloads/All_Mac/Code/ML_experiments/.venv/lib/python3.11/site-packages/torch/autograd/__init__.py:347\u001b[39m, in \u001b[36mbackward\u001b[39m\u001b[34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[39m\n\u001b[32m    342\u001b[39m     retain_graph = create_graph\n\u001b[32m    344\u001b[39m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[32m    345\u001b[39m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[32m    346\u001b[39m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m347\u001b[39m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    348\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    349\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    350\u001b[39m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    351\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    352\u001b[39m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    353\u001b[39m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    354\u001b[39m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    355\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Downloads/All_Mac/Code/ML_experiments/.venv/lib/python3.11/site-packages/torch/autograd/graph.py:823\u001b[39m, in \u001b[36m_engine_run_backward\u001b[39m\u001b[34m(t_outputs, *args, **kwargs)\u001b[39m\n\u001b[32m    821\u001b[39m     unregister_hooks = _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[32m    822\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m823\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_execution_engine\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[32m    824\u001b[39m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m    825\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[32m    826\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    827\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "patience = 1000\n",
    "best_val_loss = float('inf')\n",
    "no_improvement_epochs = 0\n",
    "\n",
    "all_outputs = []\n",
    "\n",
    "for epoch in range(10000):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    num_batches = 0\n",
    "    \n",
    "    for data, target in get_batches(train_data, train_targets, batch_size):\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = criterion(output, target)\n",
    "        loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "        num_batches += 1\n",
    "\n",
    "    print(f\"Epoch [{epoch + 1}/{epochs}], Training Loss: {running_loss / num_batches:.4f}\")\n",
    "\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    num_batches = 0\n",
    "    epoch_outputs = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for data, target in get_batches(test_data, test_targets, batch_size):\n",
    "            outputs = model(data)\n",
    "            loss = criterion(outputs, target)\n",
    "            val_loss += loss.item()\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            total += target.size(0)\n",
    "            num_batches += 1\n",
    "            correct += (predicted == target).sum().item()\n",
    "\n",
    "            epoch_outputs.append(outputs)\n",
    "\n",
    "    all_outputs_tensor = torch.cat(epoch_outputs, dim=0)\n",
    "    all_outputs.append(all_outputs_tensor)\n",
    "\n",
    "    max_val = torch.max(all_outputs_tensor).item()\n",
    "    min_val = torch.min(all_outputs_tensor).item()\n",
    "    median_val = torch.median(all_outputs_tensor).item()\n",
    "    mean_val = torch.mean(all_outputs_tensor).item()\n",
    "\n",
    "    accuracy = 100 * correct / total\n",
    "    val_loss /= num_batches\n",
    "    print(f\"Epoch [{epoch + 1}/{epochs}], Validation Loss: {val_loss:.4f}, Validation Accuracy: {accuracy:.2f}%\")\n",
    "    print(f\"Output Summary: Max={max_val:.4f}, Min={min_val:.4f}, Median={median_val:.4f}, Mean={mean_val:.4f}\")\n",
    "    print()\n",
    "\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        no_improvement_epochs = 0\n",
    "    else:\n",
    "        no_improvement_epochs += 1\n",
    "\n",
    "    if no_improvement_epochs >= patience:\n",
    "        print(f\"Early stopping triggered after {epoch + 1} epochs.\")\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "bb722e18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mdlstm.h_top\n",
      "Parameter containing:\n",
      "tensor([[ 0.0116, -0.0051, -0.0116,  0.0060,  0.0063, -0.0119,  0.0113,  0.0055,\n",
      "          0.0075,  0.0069,  0.0084, -0.0022,  0.0008, -0.0121,  0.0124, -0.0099,\n",
      "         -0.0023,  0.0312,  0.0016, -0.0096,  0.0381, -0.0050,  0.0045,  0.0029,\n",
      "         -0.0007,  0.0322, -0.0168,  0.0047, -0.0082,  0.0145, -0.0064,  0.0059]],\n",
      "       device='mps:0', requires_grad=True)\n",
      "mdlstm.c_top\n",
      "Parameter containing:\n",
      "tensor([[-0.0530, -0.0043, -0.0117,  0.0019, -0.0147, -0.0101, -0.0035, -0.0124,\n",
      "          0.0069, -0.0061,  0.0036,  0.0016, -0.0005,  0.0021,  0.0026, -0.0028,\n",
      "          0.0034,  0.0028,  0.0014, -0.0138,  0.0114, -0.0232,  0.0087, -0.0034,\n",
      "         -0.0057,  0.0190,  0.0038,  0.0015, -0.0064,  0.0016,  0.0047, -0.0061]],\n",
      "       device='mps:0', requires_grad=True)\n",
      "mdlstm.h_left\n",
      "Parameter containing:\n",
      "tensor([[-0.0113,  0.0151,  0.0132, -0.0168, -0.0094, -0.0049, -0.0044, -0.0004,\n",
      "          0.0180, -0.0222, -0.0023, -0.0076, -0.0136,  0.0012,  0.0054, -0.0068,\n",
      "          0.0092, -0.0066,  0.0098, -0.0321,  0.0122,  0.0129,  0.0006, -0.0116,\n",
      "         -0.0022,  0.0152,  0.0073,  0.0014, -0.0028,  0.0028,  0.0058, -0.0130]],\n",
      "       device='mps:0', requires_grad=True)\n",
      "mdlstm.c_left\n",
      "Parameter containing:\n",
      "tensor([[ 6.8983e-03, -9.1128e-03, -1.7540e-02,  2.9696e-03,  7.5088e-04,\n",
      "         -7.6452e-03,  1.3140e-03, -6.2088e-03,  1.7485e-02, -3.4606e-03,\n",
      "         -1.2783e-05, -9.9056e-03, -1.1258e-02,  1.1589e-04,  1.0488e-02,\n",
      "         -1.2086e-02,  4.9555e-03,  1.0388e-03,  9.9466e-03, -1.7022e-02,\n",
      "          1.3725e-02,  1.1244e-02, -1.5906e-03, -1.1101e-02, -4.5178e-03,\n",
      "          1.4926e-02,  7.3328e-03,  1.3624e-02, -3.8381e-03,  6.3978e-03,\n",
      "          2.7833e-03, -4.1235e-03]], device='mps:0', requires_grad=True)\n",
      "mdlstm.h_bottom\n",
      "Parameter containing:\n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0.]], device='mps:0', requires_grad=True)\n",
      "mdlstm.c_bottom\n",
      "Parameter containing:\n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0.]], device='mps:0', requires_grad=True)\n",
      "mdlstm.h_right\n",
      "Parameter containing:\n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0.]], device='mps:0', requires_grad=True)\n",
      "mdlstm.c_right\n",
      "Parameter containing:\n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0.]], device='mps:0', requires_grad=True)\n",
      "mdlstm.cell.Wa.weight\n",
      "Parameter containing:\n",
      "tensor([[-0.4436],\n",
      "        [-0.2608],\n",
      "        [ 0.7815],\n",
      "        [-0.3725],\n",
      "        [ 0.1601],\n",
      "        [ 0.1613],\n",
      "        [ 0.9425],\n",
      "        [-1.0548],\n",
      "        [-0.4143],\n",
      "        [ 0.7531],\n",
      "        [-0.4471],\n",
      "        [ 0.9650],\n",
      "        [ 0.9573],\n",
      "        [ 1.1423],\n",
      "        [ 0.1015],\n",
      "        [ 0.5169],\n",
      "        [ 1.0107],\n",
      "        [-0.2633],\n",
      "        [-0.2922],\n",
      "        [ 1.1198],\n",
      "        [ 0.3691],\n",
      "        [-1.0483],\n",
      "        [ 0.1038],\n",
      "        [ 0.1526],\n",
      "        [-0.7304],\n",
      "        [-0.2201],\n",
      "        [ 0.1957],\n",
      "        [ 0.2738],\n",
      "        [ 0.5177],\n",
      "        [ 0.8568],\n",
      "        [-1.0452],\n",
      "        [ 0.5190]], device='mps:0', requires_grad=True)\n",
      "mdlstm.cell.Wa.bias\n",
      "Parameter containing:\n",
      "tensor([ 0.1707, -0.2816,  0.5719,  0.1534, -0.0353,  0.9343, -0.3326,  0.3106,\n",
      "        -0.6818,  0.8631, -0.2494,  0.5281,  0.5589,  0.0118, -0.2555,  0.0898,\n",
      "        -0.6179,  0.7405,  0.7342,  0.7595, -0.8441,  0.6641, -0.7285, -0.0292,\n",
      "         0.7588,  0.4351, -0.3786,  0.4246, -0.7253,  0.0491,  0.4053,  0.5181],\n",
      "       device='mps:0', requires_grad=True)\n",
      "mdlstm.cell.Wf.weight\n",
      "Parameter containing:\n",
      "tensor([[ 0.8152],\n",
      "        [ 0.0034],\n",
      "        [-0.2059],\n",
      "        [ 0.6921],\n",
      "        [ 0.4925],\n",
      "        [-0.7870],\n",
      "        [ 0.1580],\n",
      "        [ 0.9047],\n",
      "        [ 0.1663],\n",
      "        [ 0.7975],\n",
      "        [ 0.3375],\n",
      "        [ 0.8474],\n",
      "        [-0.0399],\n",
      "        [ 0.6820],\n",
      "        [ 0.3884],\n",
      "        [-0.5366],\n",
      "        [-0.6716],\n",
      "        [-0.8406],\n",
      "        [ 0.0107],\n",
      "        [-0.7076],\n",
      "        [-0.2775],\n",
      "        [ 0.3070],\n",
      "        [-0.2567],\n",
      "        [-1.0780],\n",
      "        [ 0.4104],\n",
      "        [ 1.0079],\n",
      "        [ 0.2861],\n",
      "        [-0.8635],\n",
      "        [ 0.9795],\n",
      "        [ 0.5066],\n",
      "        [-0.6202],\n",
      "        [-0.3610]], device='mps:0', requires_grad=True)\n",
      "mdlstm.cell.Wf.bias\n",
      "Parameter containing:\n",
      "tensor([-0.6329,  0.6625, -0.5363,  0.3243, -0.2755,  0.0609, -0.2053,  0.7540,\n",
      "        -0.1979, -0.2352,  0.3279,  0.5495, -0.2587,  0.7322, -0.5109, -0.6245,\n",
      "        -0.5931,  0.6510,  0.8884,  0.8504,  0.3445,  0.9395,  0.0191,  0.2970,\n",
      "         0.7711,  0.3538,  0.9598,  0.9458,  0.6606,  0.5309, -0.1167, -0.1809],\n",
      "       device='mps:0', requires_grad=True)\n",
      "mdlstm.cell.Wg.weight\n",
      "Parameter containing:\n",
      "tensor([[-0.4299],\n",
      "        [ 0.0497],\n",
      "        [-0.5345],\n",
      "        [ 0.0847],\n",
      "        [ 0.1793],\n",
      "        [ 0.7031],\n",
      "        [-0.2037],\n",
      "        [-0.5313],\n",
      "        [-0.4749],\n",
      "        [ 0.2394],\n",
      "        [-0.4986],\n",
      "        [-0.0782],\n",
      "        [ 0.7082],\n",
      "        [-0.5548],\n",
      "        [-0.6367],\n",
      "        [-0.0328],\n",
      "        [-0.2621],\n",
      "        [-0.2212],\n",
      "        [-0.8444],\n",
      "        [ 0.7176],\n",
      "        [-0.7976],\n",
      "        [-0.4081],\n",
      "        [ 0.6972],\n",
      "        [-0.2192],\n",
      "        [ 0.1324],\n",
      "        [-0.5909],\n",
      "        [-0.9697],\n",
      "        [ 0.6000],\n",
      "        [-0.6219],\n",
      "        [-0.4127],\n",
      "        [-0.8631],\n",
      "        [-0.9230]], device='mps:0', requires_grad=True)\n",
      "mdlstm.cell.Wg.bias\n",
      "Parameter containing:\n",
      "tensor([ 0.5217,  0.7336, -0.6977,  0.5245,  0.6452, -0.5558, -0.0361,  0.8386,\n",
      "         0.0613, -0.8887,  0.9296,  0.4541, -0.1776, -0.1406, -0.1516, -0.1617,\n",
      "        -1.0068,  0.3607,  0.5445, -0.8470,  0.5847, -0.3751, -0.8165, -0.4214,\n",
      "        -0.5427,  0.6069, -0.5034, -0.4218,  0.3301, -0.2739, -0.8836, -0.6835],\n",
      "       device='mps:0', requires_grad=True)\n",
      "mdlstm.cell.Wk.weight\n",
      "Parameter containing:\n",
      "tensor([[-0.3392],\n",
      "        [ 0.1727],\n",
      "        [-0.6070],\n",
      "        [-0.1933],\n",
      "        [-0.2406],\n",
      "        [-0.9921],\n",
      "        [ 0.5728],\n",
      "        [-1.0084],\n",
      "        [ 0.0897],\n",
      "        [ 0.5576],\n",
      "        [ 0.4483],\n",
      "        [ 0.2589],\n",
      "        [-0.9515],\n",
      "        [ 0.6414],\n",
      "        [-0.3542],\n",
      "        [ 0.2199],\n",
      "        [-0.3539],\n",
      "        [ 0.4276],\n",
      "        [ 0.4076],\n",
      "        [ 0.0765],\n",
      "        [ 0.3707],\n",
      "        [-0.4613],\n",
      "        [-0.8272],\n",
      "        [ 0.7513],\n",
      "        [ 0.8524],\n",
      "        [ 0.5137],\n",
      "        [ 0.4392],\n",
      "        [-0.4732],\n",
      "        [ 0.0334],\n",
      "        [ 0.7602],\n",
      "        [-0.4709],\n",
      "        [-0.4095]], device='mps:0', requires_grad=True)\n",
      "mdlstm.cell.Wk.bias\n",
      "Parameter containing:\n",
      "tensor([-0.8796,  0.1914, -0.6758,  0.6498, -0.4233, -0.4958,  0.3099, -0.5417,\n",
      "         0.3066,  0.3588,  0.9326,  0.5086,  0.4691,  0.3896,  0.1658, -0.8325,\n",
      "        -0.8359, -0.3349, -0.2035,  0.6400,  0.7586,  0.5020, -0.9889,  0.9337,\n",
      "        -0.8053, -0.4407,  0.8762, -0.6946,  0.6296,  0.1254,  0.4170, -0.8619],\n",
      "       device='mps:0', requires_grad=True)\n",
      "mdlstm.cell.Wo.weight\n",
      "Parameter containing:\n",
      "tensor([[ 0.1667],\n",
      "        [-0.3788],\n",
      "        [ 0.1490],\n",
      "        [ 0.2318],\n",
      "        [ 0.3921],\n",
      "        [ 0.5857],\n",
      "        [ 0.3983],\n",
      "        [ 0.2006],\n",
      "        [ 0.4180],\n",
      "        [ 0.0482],\n",
      "        [ 0.7014],\n",
      "        [-0.2704],\n",
      "        [-0.0871],\n",
      "        [-0.9704],\n",
      "        [ 0.0681],\n",
      "        [ 0.6282],\n",
      "        [-0.3661],\n",
      "        [ 0.6703],\n",
      "        [ 0.2497],\n",
      "        [ 1.0414],\n",
      "        [ 0.1567],\n",
      "        [-0.0330],\n",
      "        [ 0.0894],\n",
      "        [-0.4281],\n",
      "        [ 0.4755],\n",
      "        [ 0.5435],\n",
      "        [-0.2946],\n",
      "        [ 0.7835],\n",
      "        [-0.5510],\n",
      "        [-0.7274],\n",
      "        [-0.7666],\n",
      "        [ 0.1107]], device='mps:0', requires_grad=True)\n",
      "mdlstm.cell.Wo.bias\n",
      "Parameter containing:\n",
      "tensor([-0.7195, -0.1930, -0.7505,  0.0972,  0.9310, -0.8084, -0.5883,  0.7823,\n",
      "        -0.8137, -0.0411,  0.3120, -0.2956, -0.5882,  0.8209,  0.2173,  0.0234,\n",
      "        -0.4765,  0.3054,  0.6733, -0.6705, -0.7662,  0.7688, -0.1411, -0.6494,\n",
      "         0.6683, -0.2331,  0.4457,  0.8515, -0.9438,  0.0148,  0.5073, -0.1160],\n",
      "       device='mps:0', requires_grad=True)\n",
      "mdlstm.cell.Ua.weight\n",
      "Parameter containing:\n",
      "tensor([[ 0.1325, -0.1483, -0.0795,  ..., -0.0830,  0.1651, -0.0694],\n",
      "        [-0.1151,  0.1019, -0.0758,  ..., -0.1374,  0.1601,  0.1418],\n",
      "        [-0.1671,  0.1779,  0.1415,  ...,  0.1624,  0.2559,  0.0345],\n",
      "        ...,\n",
      "        [-0.0096, -0.1740, -0.1009,  ..., -0.0527, -0.1789, -0.1511],\n",
      "        [ 0.1125,  0.1136, -0.1590,  ..., -0.0842,  0.0017, -0.0182],\n",
      "        [ 0.1033,  0.0069, -0.1139,  ..., -0.2238,  0.1453,  0.0764]],\n",
      "       device='mps:0', requires_grad=True)\n",
      "mdlstm.cell.Uf.weight\n",
      "Parameter containing:\n",
      "tensor([[-0.0618, -0.1007,  0.0354,  ..., -0.1206, -0.1343,  0.1215],\n",
      "        [ 0.1577,  0.0348,  0.0807,  ...,  0.1485,  0.0597, -0.1155],\n",
      "        [ 0.1002,  0.1293, -0.0936,  ...,  0.0639,  0.0604, -0.1614],\n",
      "        ...,\n",
      "        [-0.0563, -0.1026, -0.0857,  ..., -0.1639,  0.1730, -0.1563],\n",
      "        [ 0.1364, -0.1245, -0.0407,  ...,  0.0776, -0.0987,  0.0713],\n",
      "        [ 0.2178, -0.1780, -0.1121,  ...,  0.0899,  0.0536, -0.0745]],\n",
      "       device='mps:0', requires_grad=True)\n",
      "mdlstm.cell.Ug.weight\n",
      "Parameter containing:\n",
      "tensor([[ 0.0468,  0.0587,  0.0285,  ...,  0.1396,  0.1137,  0.0085],\n",
      "        [-0.1056, -0.1126,  0.1748,  ..., -0.1319,  0.1397,  0.0409],\n",
      "        [ 0.0194, -0.1603,  0.0481,  ..., -0.0228,  0.2566,  0.1195],\n",
      "        ...,\n",
      "        [ 0.0078, -0.0677, -0.1514,  ..., -0.0891, -0.1248, -0.1104],\n",
      "        [ 0.0152,  0.0395,  0.2057,  ...,  0.1088, -0.0708,  0.1124],\n",
      "        [-0.0514,  0.1093, -0.1661,  ...,  0.0018,  0.0346, -0.1102]],\n",
      "       device='mps:0', requires_grad=True)\n",
      "mdlstm.cell.Uk.weight\n",
      "Parameter containing:\n",
      "tensor([[-0.1687, -0.0571,  0.1085,  ..., -0.1042,  0.0564,  0.1187],\n",
      "        [-0.1603,  0.1399,  0.1470,  ..., -0.1001,  0.0053,  0.0449],\n",
      "        [ 0.0580, -0.0859,  0.1615,  ...,  0.1689,  0.0566,  0.0907],\n",
      "        ...,\n",
      "        [-0.0472,  0.1225,  0.0681,  ..., -0.1433, -0.1620, -0.1351],\n",
      "        [ 0.2126,  0.1257,  0.1766,  ..., -0.0387,  0.1388,  0.1672],\n",
      "        [-0.0283, -0.0427, -0.1257,  ...,  0.1301, -0.1411,  0.1238]],\n",
      "       device='mps:0', requires_grad=True)\n",
      "mdlstm.cell.Uo.weight\n",
      "Parameter containing:\n",
      "tensor([[ 0.0817, -0.1845,  0.0598,  ...,  0.0438, -0.2203,  0.2449],\n",
      "        [ 0.1065, -0.0939, -0.0553,  ..., -0.1893,  0.3539, -0.1335],\n",
      "        [ 0.0259, -0.1151,  0.2228,  ..., -0.1489,  0.0738,  0.1728],\n",
      "        ...,\n",
      "        [-0.0055,  0.0493,  0.0539,  ..., -0.0278, -0.0318, -0.0046],\n",
      "        [ 0.2140, -0.2339,  0.0391,  ..., -0.1243, -0.0046,  0.1214],\n",
      "        [ 0.3106, -0.0567,  0.0245,  ...,  0.0113, -0.2884,  0.1535]],\n",
      "       device='mps:0', requires_grad=True)\n",
      "mdlstm.cell.Va.weight\n",
      "Parameter containing:\n",
      "tensor([[-0.0347,  0.0253,  0.1001,  ...,  0.0102, -0.1024,  0.0693],\n",
      "        [-0.1800,  0.1007, -0.1586,  ...,  0.1618,  0.0170, -0.1221],\n",
      "        [-0.0512, -0.0604,  0.0657,  ...,  0.2143, -0.0909, -0.1618],\n",
      "        ...,\n",
      "        [-0.0187,  0.0521,  0.0497,  ...,  0.0568,  0.1073,  0.1566],\n",
      "        [ 0.1214, -0.1185,  0.0163,  ..., -0.1521, -0.0867,  0.1392],\n",
      "        [-0.0688,  0.1546,  0.0146,  ..., -0.0630, -0.0610,  0.0142]],\n",
      "       device='mps:0', requires_grad=True)\n",
      "mdlstm.cell.Vf.weight\n",
      "Parameter containing:\n",
      "tensor([[ 0.0498, -0.1341,  0.0910,  ..., -0.0038,  0.0404, -0.0996],\n",
      "        [-0.0075, -0.0769,  0.1257,  ..., -0.0471,  0.0333,  0.1584],\n",
      "        [-0.0673, -0.1726, -0.1222,  ...,  0.1840, -0.0818,  0.0536],\n",
      "        ...,\n",
      "        [-0.0909,  0.0048,  0.0952,  ...,  0.1810, -0.1233,  0.1118],\n",
      "        [-0.0310, -0.1331,  0.1282,  ...,  0.0620,  0.0372, -0.0439],\n",
      "        [-0.1063, -0.1023,  0.1948,  ...,  0.1871,  0.0517, -0.0070]],\n",
      "       device='mps:0', requires_grad=True)\n",
      "mdlstm.cell.Vg.weight\n",
      "Parameter containing:\n",
      "tensor([[-0.0937, -0.0348,  0.0701,  ...,  0.0229,  0.0375,  0.1240],\n",
      "        [-0.0508,  0.1273, -0.1184,  ..., -0.0152, -0.0867,  0.0709],\n",
      "        [ 0.0791,  0.1465,  0.0534,  ...,  0.2207,  0.2484,  0.1145],\n",
      "        ...,\n",
      "        [ 0.0475,  0.1324,  0.1325,  ..., -0.0918,  0.0259, -0.0786],\n",
      "        [-0.0358, -0.1692, -0.0882,  ...,  0.0104,  0.1382,  0.1798],\n",
      "        [-0.1463,  0.0746, -0.0274,  ..., -0.1547,  0.0434,  0.0797]],\n",
      "       device='mps:0', requires_grad=True)\n",
      "mdlstm.cell.Vk.weight\n",
      "Parameter containing:\n",
      "tensor([[-0.0009,  0.1333, -0.0160,  ...,  0.1849, -0.0498,  0.1474],\n",
      "        [ 0.1424, -0.0641, -0.0393,  ...,  0.0774, -0.0311, -0.0487],\n",
      "        [ 0.0332, -0.0913, -0.0805,  ..., -0.0722,  0.0414, -0.0269],\n",
      "        ...,\n",
      "        [-0.1458,  0.1406,  0.0550,  ..., -0.0638, -0.1043,  0.0569],\n",
      "        [-0.0005, -0.0295, -0.1107,  ..., -0.0926,  0.0946,  0.2352],\n",
      "        [ 0.1849,  0.1314, -0.1233,  ...,  0.1730, -0.1385,  0.0332]],\n",
      "       device='mps:0', requires_grad=True)\n",
      "mdlstm.cell.Vo.weight\n",
      "Parameter containing:\n",
      "tensor([[-0.0704,  0.1426, -0.0519,  ..., -0.0808, -0.0423,  0.1961],\n",
      "        [-0.1097, -0.0899,  0.0701,  ..., -0.0708,  0.3292, -0.0525],\n",
      "        [-0.0140, -0.2443, -0.0669,  ..., -0.1542,  0.2915,  0.1642],\n",
      "        ...,\n",
      "        [ 0.1877, -0.1492, -0.0150,  ..., -0.1596,  0.0409, -0.0113],\n",
      "        [ 0.2123, -0.0104,  0.0683,  ...,  0.0912, -0.0896,  0.3008],\n",
      "        [ 0.3295, -0.0153,  0.0607,  ..., -0.0341, -0.2390,  0.1009]],\n",
      "       device='mps:0', requires_grad=True)\n",
      "classifier.weight\n",
      "Parameter containing:\n",
      "tensor([[-2.7392e-01,  8.8189e-02, -2.2605e-02,  4.7015e-02, -3.2886e-01,\n",
      "         -1.9803e-01, -3.4200e-01, -1.5581e-01, -2.4326e-01,  3.5074e-01,\n",
      "          4.4042e-01,  3.6057e-01,  1.3561e-01,  6.3860e-02,  6.2322e-01,\n",
      "         -1.7418e-01,  7.9775e-02,  1.5778e-01, -2.4806e-01, -2.7388e-01,\n",
      "         -5.7058e-01,  5.3027e-01, -1.1054e-01,  3.7996e-02, -4.4169e-01,\n",
      "          1.6605e-01, -2.7672e-01,  2.9903e-01, -4.6756e-01,  6.9421e-02,\n",
      "         -1.1788e-01, -3.6661e-01],\n",
      "        [-5.0363e-01,  2.4005e-01, -4.7823e-02, -1.1565e-01,  3.6293e-02,\n",
      "          7.3851e-01,  2.5030e-01,  6.3156e-02, -1.9695e-01,  4.5030e-01,\n",
      "         -2.8639e-02, -2.0172e-02,  8.6988e-02, -1.5790e-01, -1.9512e-01,\n",
      "         -4.5329e-01, -1.2673e-01, -1.1410e-01, -2.6591e-01, -2.1755e-01,\n",
      "         -2.9141e-01,  3.5315e-01, -2.9054e-01,  2.8215e-01, -1.8177e-01,\n",
      "          3.6805e-01, -9.6999e-02,  2.3956e-02, -3.8009e-01,  2.4553e-01,\n",
      "         -1.4779e-01, -4.3771e-01],\n",
      "        [ 1.6699e-01,  3.2503e-01, -2.3971e-01, -5.7507e-02, -9.3187e-02,\n",
      "         -3.2026e-01, -1.9707e-01, -3.3206e-01, -1.3615e-01,  1.7388e-01,\n",
      "          5.6466e-03, -2.9488e-02,  2.5924e-01, -6.7289e-02,  4.3743e-01,\n",
      "         -9.3272e-02,  3.0043e-01, -7.1071e-02, -9.3042e-02, -2.6732e-01,\n",
      "          2.4580e-01, -1.2547e-01, -4.6475e-01,  2.1306e-01, -2.1559e-01,\n",
      "          1.0059e-01, -1.5458e-01,  1.9680e-01,  1.9102e-02,  1.0622e-01,\n",
      "         -2.6362e-01,  1.4029e-01],\n",
      "        [-2.9762e-01,  1.5860e-01, -1.3484e-01, -6.5549e-02, -1.1604e-01,\n",
      "          2.4020e-01, -6.4869e-02,  8.4347e-02, -3.0331e-01,  3.1493e-01,\n",
      "          2.5925e-02, -2.2965e-02,  2.7402e-01,  1.3043e-01, -2.3599e-01,\n",
      "         -4.9956e-02, -1.8993e-01,  7.6749e-02, -1.0320e-02, -2.3170e-01,\n",
      "         -4.1000e-01,  4.8087e-01,  5.5470e-02,  2.3566e-01, -2.1214e-01,\n",
      "          3.5750e-01, -1.1666e-01,  1.3679e-02, -2.6185e-01,  3.6683e-02,\n",
      "         -2.6415e-01, -2.6086e-01],\n",
      "        [-9.9069e-02,  2.0924e-01,  3.8243e-03,  7.4454e-03,  4.8274e-02,\n",
      "         -3.2861e-01, -3.6935e-01, -4.4907e-02, -1.1214e-02,  3.3440e-01,\n",
      "          1.2709e-01,  2.7773e-01,  1.1391e-01,  2.2706e-01,  3.5556e-01,\n",
      "         -2.0203e-01,  2.9911e-01, -7.1627e-02,  1.5087e-01, -4.4432e-02,\n",
      "         -1.3052e-01, -2.6934e-01, -3.1476e-01,  2.7952e-01,  4.8031e-02,\n",
      "          2.0707e-01, -2.9685e-01,  2.3494e-02,  1.5239e-01,  1.2403e-01,\n",
      "         -1.8283e-02,  2.5200e-01],\n",
      "        [-3.8437e-02, -1.2890e-01,  3.0982e-01,  9.3674e-02,  1.8095e-01,\n",
      "         -1.8997e-01,  3.3830e-01,  2.8762e-01,  3.5592e-01, -2.1672e-01,\n",
      "         -2.2688e-01, -3.3614e-01, -1.1823e-01, -3.3682e-01, -1.4906e-01,\n",
      "          3.6501e-01, -5.6531e-01, -1.3472e-01,  1.6585e-01,  3.7616e-01,\n",
      "          2.6967e-01,  3.6676e-02,  2.9914e-01, -2.7102e-01,  9.9861e-02,\n",
      "         -8.1711e-02,  3.0568e-01, -2.4459e-01,  2.8700e-01, -2.9626e-01,\n",
      "          3.3752e-01,  3.4139e-01],\n",
      "        [-1.6768e-01,  5.0140e-02, -5.3856e-02, -5.2165e-03, -6.7908e-02,\n",
      "         -4.2451e-01, -3.1234e-01, -2.8145e-01, -1.5697e-01,  1.0669e-01,\n",
      "         -8.7543e-02,  2.1964e-01,  3.1219e-01,  4.2816e-03,  2.7581e-01,\n",
      "         -2.7957e-01, -4.3513e-03,  1.9126e-02, -3.1269e-02, -3.7174e-02,\n",
      "          4.0156e-02, -3.1619e-02, -3.6429e-01,  2.1865e-01, -2.1962e-01,\n",
      "          3.0277e-02, -6.7049e-02,  1.7440e-01, -1.0851e-02,  1.6087e-01,\n",
      "         -3.8217e-01, -6.9186e-02],\n",
      "        [ 1.2643e-04, -4.2430e-01,  5.0031e-01, -4.6926e-02,  3.6585e-01,\n",
      "         -2.8549e-01,  2.9655e-01,  1.5038e-01,  3.6410e-01, -8.5083e-02,\n",
      "         -2.0108e-01, -1.7435e-01, -2.7576e-01, -2.8891e-01, -1.0251e-01,\n",
      "          2.9158e-01, -5.0218e-01, -5.4091e-02,  1.7737e-01,  1.9026e-01,\n",
      "         -3.7904e-02,  1.0979e-01,  3.2724e-01, -3.7723e-01,  1.1530e-01,\n",
      "         -2.3505e-01,  5.4783e-02, -2.9555e-01,  4.2534e-01, -2.5455e-01,\n",
      "          1.3974e-01,  2.2626e-01],\n",
      "        [ 2.0032e-01,  2.9627e-01, -3.3993e-01, -9.1953e-02, -1.8267e-02,\n",
      "          3.3134e-01, -4.0337e-01,  2.5358e-01,  7.2080e-03, -3.8649e-01,\n",
      "         -1.6006e-01,  3.0917e-01,  2.6833e-01,  1.6435e-01, -2.8308e-01,\n",
      "          4.1251e-02,  9.0987e-02,  8.4965e-04, -1.0536e-02,  3.1127e-02,\n",
      "          4.5647e-01,  1.1149e-01,  4.6827e-01, -3.0614e-01,  2.3365e-01,\n",
      "         -1.5896e-01,  3.5815e-02, -3.5746e-01,  6.7771e-02,  2.3443e-01,\n",
      "         -4.4324e-01,  3.5221e-01],\n",
      "        [ 4.0813e-01, -1.7827e-02, -3.1862e-01,  2.6148e-01,  1.0650e-01,\n",
      "          2.9321e-02,  4.8024e-01,  2.8001e-01,  2.5466e-01, -2.6236e-01,\n",
      "          1.9230e-01, -1.4201e-01, -3.9809e-01, -3.1152e-02, -2.9959e-01,\n",
      "          3.4911e-01,  4.1437e-01,  5.0929e-02,  2.6524e-01,  1.4944e-01,\n",
      "          8.7302e-02, -3.0513e-01,  2.4070e-01, -2.4427e-01,  4.0533e-02,\n",
      "         -1.8499e-01,  2.8309e-02, -2.6685e-02,  1.6343e-01, -2.6800e-01,\n",
      "          2.1532e-01,  2.5898e-01]], device='mps:0', requires_grad=True)\n",
      "classifier.bias\n",
      "Parameter containing:\n",
      "tensor([ 0.1661, -0.1825,  0.1195,  0.1782, -0.0045,  0.0838, -0.0187,  0.0078,\n",
      "         0.2215,  0.0564], device='mps:0', requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "for name, param in model.named_parameters():\n",
    "    print(name)\n",
    "    print(param)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "273364af",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
